{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4Wbw_GGqUN18",
        "outputId": "53320e66-8760-4128-b884-1d0a06601c59"
      },
      "outputs": [],
      "source": [
        "# Installation des dépendances si nécessaire\n",
        "!pip install transformers peft accelerate datasets torch evaluate scikit-learn codecarbon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**FINETUNING AVEC LoRa**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1krnXtXVURRc",
        "outputId": "3fb2ee58-6eb6-47e2-ff03-e37d622d6ac7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon ERROR @ 14:03:31] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
            "[codecarbon WARNING @ 14:03:31] Another instance of codecarbon is already running. Exiting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utilisation de l'appareil : cuda\n",
            "Chargement du modèle et du tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuning : base_model.model.model.layers.0.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.0.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.0.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.0.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.0.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.0.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.0.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.0.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.1.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.1.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.1.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.1.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.1.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.1.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.1.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.1.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.2.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.2.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.2.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.2.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.2.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.2.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.2.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.2.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.3.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.3.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.3.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.3.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.3.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.3.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.3.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.3.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.4.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.4.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.4.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.4.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.4.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.4.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.4.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.4.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.5.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.5.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.5.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.5.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.5.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.5.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.5.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.5.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.6.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.6.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.6.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.6.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.6.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.6.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.6.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.6.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.7.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.7.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.7.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.7.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.7.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.7.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.7.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.7.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.8.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.8.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.8.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.8.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.8.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.8.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.8.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.8.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.9.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.9.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.9.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.9.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.9.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.9.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.9.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.9.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.10.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.10.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.10.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.10.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.10.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.10.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.10.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.10.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.11.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.11.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.11.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.11.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.11.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.11.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.11.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.11.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.12.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.12.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.12.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.12.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.12.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.12.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.12.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.12.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.13.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.13.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.13.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.13.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.13.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.13.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.13.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.13.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.14.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.14.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.14.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.14.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.14.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.14.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.14.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.14.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.15.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.15.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.15.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.15.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.15.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.15.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.15.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.15.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.16.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.16.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.16.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.16.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.16.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.16.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.16.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.16.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.17.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.17.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.17.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.17.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.17.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.17.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.17.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.17.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.18.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.18.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.18.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.18.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.18.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.18.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.18.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.18.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.19.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.19.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.19.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.19.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.19.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.19.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.19.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.19.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.20.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.20.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.20.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.20.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.20.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.20.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.20.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.20.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.21.attn.Wqkv.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.21.attn.Wqkv.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.21.attn.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.21.attn.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.21.mlp.Wi.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.21.mlp.Wi.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.21.mlp.Wo.lora_A.default.weight\n",
            "Fine-tuning : base_model.model.model.layers.21.mlp.Wo.lora_B.default.weight\n",
            "Fine-tuning : base_model.model.classifier.original_module.weight\n",
            "Fine-tuning : base_model.model.classifier.original_module.bias\n",
            "Fine-tuning : base_model.model.classifier.modules_to_save.default.weight\n",
            "Fine-tuning : base_model.model.classifier.modules_to_save.default.bias\n",
            "\n",
            "Paramètres entraînables :\n",
            "base_model.model.model.layers.0.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.0.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.0.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.0.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.0.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.0.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.0.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.0.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.1.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.1.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.1.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.1.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.1.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.1.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.1.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.1.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.2.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.2.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.2.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.2.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.2.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.2.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.2.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.2.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.3.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.3.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.3.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.3.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.3.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.3.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.3.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.3.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.4.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.4.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.4.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.4.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.4.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.4.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.4.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.4.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.5.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.5.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.5.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.5.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.5.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.5.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.5.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.5.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.6.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.6.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.6.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.6.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.6.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.6.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.6.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.6.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.7.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.7.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.7.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.7.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.7.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.7.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.7.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.7.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.8.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.8.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.8.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.8.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.8.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.8.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.8.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.8.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.9.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.9.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.9.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.9.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.9.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.9.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.9.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.9.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.10.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.10.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.10.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.10.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.10.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.10.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.10.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.10.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.11.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.11.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.11.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.11.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.11.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.11.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.11.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.11.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.12.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.12.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.12.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.12.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.12.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.12.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.12.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.12.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.13.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.13.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.13.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.13.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.13.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.13.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.13.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.13.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.14.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.14.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.14.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.14.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.14.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.14.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.14.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.14.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.15.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.15.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.15.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.15.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.15.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.15.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.15.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.15.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.16.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.16.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.16.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.16.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.16.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.16.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.16.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.16.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.17.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.17.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.17.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.17.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.17.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.17.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.17.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.17.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.18.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.18.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.18.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.18.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.18.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.18.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.18.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.18.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.19.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.19.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.19.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.19.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.19.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.19.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.19.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.19.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.20.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.20.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.20.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.20.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.20.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.20.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.20.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.20.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.21.attn.Wqkv.lora_A.default.weight\n",
            "base_model.model.model.layers.21.attn.Wqkv.lora_B.default.weight\n",
            "base_model.model.model.layers.21.attn.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.21.attn.Wo.lora_B.default.weight\n",
            "base_model.model.model.layers.21.mlp.Wi.lora_A.default.weight\n",
            "base_model.model.model.layers.21.mlp.Wi.lora_B.default.weight\n",
            "base_model.model.model.layers.21.mlp.Wo.lora_A.default.weight\n",
            "base_model.model.model.layers.21.mlp.Wo.lora_B.default.weight\n",
            "base_model.model.classifier.original_module.weight\n",
            "base_model.model.classifier.original_module.bias\n",
            "base_model.model.classifier.modules_to_save.default.weight\n",
            "base_model.model.classifier.modules_to_save.default.bias\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "[codecarbon ERROR @ 14:03:38] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Début du fine-tuning avec LoRA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon WARNING @ 14:03:39] Another instance of codecarbon is already running. Exiting.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14065' max='14065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14065/14065 50:42, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision Global</th>\n",
              "      <th>Recall Global</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>Precision Neg</th>\n",
              "      <th>Recall Neg</th>\n",
              "      <th>F1 Neg</th>\n",
              "      <th>Precision Pos</th>\n",
              "      <th>Recall Pos</th>\n",
              "      <th>F1 Pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.462200</td>\n",
              "      <td>0.441329</td>\n",
              "      <td>0.795600</td>\n",
              "      <td>0.808083</td>\n",
              "      <td>0.796160</td>\n",
              "      <td>0.793715</td>\n",
              "      <td>0.744652</td>\n",
              "      <td>0.896219</td>\n",
              "      <td>0.813436</td>\n",
              "      <td>0.871514</td>\n",
              "      <td>0.696102</td>\n",
              "      <td>0.773994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.339600</td>\n",
              "      <td>0.403245</td>\n",
              "      <td>0.827200</td>\n",
              "      <td>0.828484</td>\n",
              "      <td>0.827374</td>\n",
              "      <td>0.827079</td>\n",
              "      <td>0.806500</td>\n",
              "      <td>0.858407</td>\n",
              "      <td>0.831645</td>\n",
              "      <td>0.850467</td>\n",
              "      <td>0.796340</td>\n",
              "      <td>0.822514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.457900</td>\n",
              "      <td>0.394334</td>\n",
              "      <td>0.830400</td>\n",
              "      <td>0.830409</td>\n",
              "      <td>0.830417</td>\n",
              "      <td>0.830400</td>\n",
              "      <td>0.826816</td>\n",
              "      <td>0.833467</td>\n",
              "      <td>0.830128</td>\n",
              "      <td>0.834002</td>\n",
              "      <td>0.827367</td>\n",
              "      <td>0.830671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.358300</td>\n",
              "      <td>0.390067</td>\n",
              "      <td>0.834800</td>\n",
              "      <td>0.835947</td>\n",
              "      <td>0.834963</td>\n",
              "      <td>0.834702</td>\n",
              "      <td>0.814871</td>\n",
              "      <td>0.864039</td>\n",
              "      <td>0.838735</td>\n",
              "      <td>0.857022</td>\n",
              "      <td>0.805887</td>\n",
              "      <td>0.830668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.313500</td>\n",
              "      <td>0.389587</td>\n",
              "      <td>0.834400</td>\n",
              "      <td>0.836145</td>\n",
              "      <td>0.834601</td>\n",
              "      <td>0.834239</td>\n",
              "      <td>0.810487</td>\n",
              "      <td>0.870475</td>\n",
              "      <td>0.839410</td>\n",
              "      <td>0.861803</td>\n",
              "      <td>0.798727</td>\n",
              "      <td>0.829067</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "[codecarbon WARNING @ 14:54:22] Another instance of codecarbon is already running. Exiting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Évaluation du modèle après l'entraînement...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [313/313 00:56]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon WARNING @ 14:55:19] Another instance of codecarbon is already running. Exiting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Résultats de l'évaluation : {'eval_loss': 0.3895866572856903, 'eval_accuracy': 0.8344, 'eval_precision_global': 0.8361447332465319, 'eval_recall_global': 0.8346008930840072, 'eval_f1_macro': 0.834238641262951, 'eval_precision_neg': 0.8104868913857678, 'eval_recall_neg': 0.8704746580852776, 'eval_f1_neg': 0.839410395655547, 'eval_precision_pos': 0.8618025751072962, 'eval_recall_pos': 0.7987271280827367, 'eval_f1_pos': 0.8290668868703551, 'eval_runtime': 56.8691, 'eval_samples_per_second': 43.961, 'eval_steps_per_second': 5.504, 'epoch': 5.0}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "# Création du dossier pour les logs CodeCarbon\n",
        "os.makedirs(\"./codecarbon_logs\", exist_ok=True)\n",
        "\n",
        "# Suivi de l'empreinte carbone\n",
        "tracker = EmissionsTracker(project_name=\"lora-fine-tuning-modernbert\", output_dir=\"./codecarbon_logs\")\n",
        "tracker.start()\n",
        "\n",
        "# Vérification de l'appareil (GPU/CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Utilisation de l'appareil : {device}\")\n",
        "\n",
        "# Chargement du dataset IMDB\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Modèle de base\n",
        "model_name = \"answerdotai/ModernBERT-base\"\n",
        "\n",
        "# Tokenizer & modèle\n",
        "print(\"Chargement du modèle et du tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "id2label = {0: \"neg\", 1: \"pos\"}\n",
        "label2id = {\"neg\": 0, \"pos\": 1}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "# Configuration LoRA\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"attn.Wqkv\", \"attn.Wo\", \"mlp.Wi\", \"mlp.Wo\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Activation du gradient checkpointing (optionnel)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Geler les couches sauf LoRA et classifier\n",
        "for name, param in model.named_parameters():\n",
        "    if \"lora\" in name or \"classifier\" in name:\n",
        "        param.requires_grad = True\n",
        "        print(f\"Fine-tuning : {name}\")\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Affichage des paramètres entraînables\n",
        "print(\"\\nParamètres entraînables :\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)\n",
        "\n",
        "# Tokenisation\n",
        "def tokenize_function(examples):\n",
        "    tokens = tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=256)\n",
        "    tokens[\"label\"] = examples[\"label\"]\n",
        "    return tokens\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Split train/test\n",
        "split = tokenized_datasets[\"train\"].train_test_split(test_size=0.1)\n",
        "train_dataset = split[\"train\"]\n",
        "eval_dataset = split[\"test\"]\n",
        "\n",
        "# Chargement des métriques\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "class_names = [\"neg\", \"pos\"]\n",
        "\n",
        "# Fonction de calcul des métriques\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
        "\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    prec, rec, f1_scores, _ = precision_recall_fscore_support(labels, predictions, average=None, zero_division=1)\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision_global\": prec.mean(),\n",
        "        \"recall_global\": rec.mean(),\n",
        "        \"f1_macro\": f1_scores.mean()\n",
        "    }\n",
        "\n",
        "    for i, (p, r, f) in enumerate(zip(prec, rec, f1_scores)):\n",
        "        metrics[f\"precision_{class_names[i]}\"] = p\n",
        "        metrics[f\"recall_{class_names[i]}\"] = r\n",
        "        metrics[f\"f1_{class_names[i]}\"] = f\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Configuration de l'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    run_name=\"lora-modernbert-wandb\",\n",
        "    overwrite_output_dir=True,\n",
        "    disable_tqdm=False\n",
        ")\n",
        "\n",
        "# Initialisation du Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Fine-tuning\n",
        "print(\"Début du fine-tuning avec LoRA...\")\n",
        "trainer.train()\n",
        "\n",
        "# Évaluation finale\n",
        "print(\"Évaluation du modèle après l'entraînement...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Résultats de l'évaluation : {eval_results}\")\n",
        "\n",
        "# Arrêt du suivi de CodeCarbon\n",
        "tracker.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**FINETUNING SANS LoRa**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d9a538f6f99246108ca83d2a3e5ad716",
            "5ab1aafc0a0b4f5e8f0427f31426e1df",
            "c2faf320c29d4c7dafc03a3da916ffe9",
            "4b13f832674f4a529cd282e8b51dc8f2",
            "6333282853fc487db4fb6ef36215ae13",
            "3fb3081eed6c4c4883005e5196cbe07c",
            "f644a4b17cd54ef8a96b48d653ac04f0",
            "61698aecfb964dc5bb31e10a515f68f5",
            "aa49621aeb8b4453ad5b642f89c66736",
            "f3d060f577534db3822c66ebe954e85b",
            "929b6fd4c26c4bc7b7a7096164e11e89"
          ]
        },
        "id": "6NWpdUdcVvXV",
        "outputId": "eaeda3a6-feb5-462f-d38c-2796eed4c558"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon ERROR @ 21:03:18] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
            "[codecarbon WARNING @ 21:03:18] Another instance of codecarbon is already running. Exiting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utilisation de l'appareil : cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement du modèle et du tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Paramètres entraînables :\n",
            "model.embeddings.tok_embeddings.weight\n",
            "model.embeddings.norm.weight\n",
            "model.layers.0.attn.Wqkv.weight\n",
            "model.layers.0.attn.Wo.weight\n",
            "model.layers.0.mlp_norm.weight\n",
            "model.layers.0.mlp.Wi.weight\n",
            "model.layers.0.mlp.Wo.weight\n",
            "model.layers.1.attn_norm.weight\n",
            "model.layers.1.attn.Wqkv.weight\n",
            "model.layers.1.attn.Wo.weight\n",
            "model.layers.1.mlp_norm.weight\n",
            "model.layers.1.mlp.Wi.weight\n",
            "model.layers.1.mlp.Wo.weight\n",
            "model.layers.2.attn_norm.weight\n",
            "model.layers.2.attn.Wqkv.weight\n",
            "model.layers.2.attn.Wo.weight\n",
            "model.layers.2.mlp_norm.weight\n",
            "model.layers.2.mlp.Wi.weight\n",
            "model.layers.2.mlp.Wo.weight\n",
            "model.layers.3.attn_norm.weight\n",
            "model.layers.3.attn.Wqkv.weight\n",
            "model.layers.3.attn.Wo.weight\n",
            "model.layers.3.mlp_norm.weight\n",
            "model.layers.3.mlp.Wi.weight\n",
            "model.layers.3.mlp.Wo.weight\n",
            "model.layers.4.attn_norm.weight\n",
            "model.layers.4.attn.Wqkv.weight\n",
            "model.layers.4.attn.Wo.weight\n",
            "model.layers.4.mlp_norm.weight\n",
            "model.layers.4.mlp.Wi.weight\n",
            "model.layers.4.mlp.Wo.weight\n",
            "model.layers.5.attn_norm.weight\n",
            "model.layers.5.attn.Wqkv.weight\n",
            "model.layers.5.attn.Wo.weight\n",
            "model.layers.5.mlp_norm.weight\n",
            "model.layers.5.mlp.Wi.weight\n",
            "model.layers.5.mlp.Wo.weight\n",
            "model.layers.6.attn_norm.weight\n",
            "model.layers.6.attn.Wqkv.weight\n",
            "model.layers.6.attn.Wo.weight\n",
            "model.layers.6.mlp_norm.weight\n",
            "model.layers.6.mlp.Wi.weight\n",
            "model.layers.6.mlp.Wo.weight\n",
            "model.layers.7.attn_norm.weight\n",
            "model.layers.7.attn.Wqkv.weight\n",
            "model.layers.7.attn.Wo.weight\n",
            "model.layers.7.mlp_norm.weight\n",
            "model.layers.7.mlp.Wi.weight\n",
            "model.layers.7.mlp.Wo.weight\n",
            "model.layers.8.attn_norm.weight\n",
            "model.layers.8.attn.Wqkv.weight\n",
            "model.layers.8.attn.Wo.weight\n",
            "model.layers.8.mlp_norm.weight\n",
            "model.layers.8.mlp.Wi.weight\n",
            "model.layers.8.mlp.Wo.weight\n",
            "model.layers.9.attn_norm.weight\n",
            "model.layers.9.attn.Wqkv.weight\n",
            "model.layers.9.attn.Wo.weight\n",
            "model.layers.9.mlp_norm.weight\n",
            "model.layers.9.mlp.Wi.weight\n",
            "model.layers.9.mlp.Wo.weight\n",
            "model.layers.10.attn_norm.weight\n",
            "model.layers.10.attn.Wqkv.weight\n",
            "model.layers.10.attn.Wo.weight\n",
            "model.layers.10.mlp_norm.weight\n",
            "model.layers.10.mlp.Wi.weight\n",
            "model.layers.10.mlp.Wo.weight\n",
            "model.layers.11.attn_norm.weight\n",
            "model.layers.11.attn.Wqkv.weight\n",
            "model.layers.11.attn.Wo.weight\n",
            "model.layers.11.mlp_norm.weight\n",
            "model.layers.11.mlp.Wi.weight\n",
            "model.layers.11.mlp.Wo.weight\n",
            "model.layers.12.attn_norm.weight\n",
            "model.layers.12.attn.Wqkv.weight\n",
            "model.layers.12.attn.Wo.weight\n",
            "model.layers.12.mlp_norm.weight\n",
            "model.layers.12.mlp.Wi.weight\n",
            "model.layers.12.mlp.Wo.weight\n",
            "model.layers.13.attn_norm.weight\n",
            "model.layers.13.attn.Wqkv.weight\n",
            "model.layers.13.attn.Wo.weight\n",
            "model.layers.13.mlp_norm.weight\n",
            "model.layers.13.mlp.Wi.weight\n",
            "model.layers.13.mlp.Wo.weight\n",
            "model.layers.14.attn_norm.weight\n",
            "model.layers.14.attn.Wqkv.weight\n",
            "model.layers.14.attn.Wo.weight\n",
            "model.layers.14.mlp_norm.weight\n",
            "model.layers.14.mlp.Wi.weight\n",
            "model.layers.14.mlp.Wo.weight\n",
            "model.layers.15.attn_norm.weight\n",
            "model.layers.15.attn.Wqkv.weight\n",
            "model.layers.15.attn.Wo.weight\n",
            "model.layers.15.mlp_norm.weight\n",
            "model.layers.15.mlp.Wi.weight\n",
            "model.layers.15.mlp.Wo.weight\n",
            "model.layers.16.attn_norm.weight\n",
            "model.layers.16.attn.Wqkv.weight\n",
            "model.layers.16.attn.Wo.weight\n",
            "model.layers.16.mlp_norm.weight\n",
            "model.layers.16.mlp.Wi.weight\n",
            "model.layers.16.mlp.Wo.weight\n",
            "model.layers.17.attn_norm.weight\n",
            "model.layers.17.attn.Wqkv.weight\n",
            "model.layers.17.attn.Wo.weight\n",
            "model.layers.17.mlp_norm.weight\n",
            "model.layers.17.mlp.Wi.weight\n",
            "model.layers.17.mlp.Wo.weight\n",
            "model.layers.18.attn_norm.weight\n",
            "model.layers.18.attn.Wqkv.weight\n",
            "model.layers.18.attn.Wo.weight\n",
            "model.layers.18.mlp_norm.weight\n",
            "model.layers.18.mlp.Wi.weight\n",
            "model.layers.18.mlp.Wo.weight\n",
            "model.layers.19.attn_norm.weight\n",
            "model.layers.19.attn.Wqkv.weight\n",
            "model.layers.19.attn.Wo.weight\n",
            "model.layers.19.mlp_norm.weight\n",
            "model.layers.19.mlp.Wi.weight\n",
            "model.layers.19.mlp.Wo.weight\n",
            "model.layers.20.attn_norm.weight\n",
            "model.layers.20.attn.Wqkv.weight\n",
            "model.layers.20.attn.Wo.weight\n",
            "model.layers.20.mlp_norm.weight\n",
            "model.layers.20.mlp.Wi.weight\n",
            "model.layers.20.mlp.Wo.weight\n",
            "model.layers.21.attn_norm.weight\n",
            "model.layers.21.attn.Wqkv.weight\n",
            "model.layers.21.attn.Wo.weight\n",
            "model.layers.21.mlp_norm.weight\n",
            "model.layers.21.mlp.Wi.weight\n",
            "model.layers.21.mlp.Wo.weight\n",
            "model.final_norm.weight\n",
            "head.dense.weight\n",
            "head.norm.weight\n",
            "classifier.weight\n",
            "classifier.bias\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9a538f6f99246108ca83d2a3e5ad716",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "[codecarbon ERROR @ 21:04:06] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Début du fine-tuning sans LoRA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malex-lochain\u001b[0m (\u001b[33malex-lochain-le-mans-universit-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250409_210407-sut7y41n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alex-lochain-le-mans-universit-/huggingface/runs/sut7y41n' target=\"_blank\">finetuning-modernbert</a></strong> to <a href='https://wandb.ai/alex-lochain-le-mans-universit-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/alex-lochain-le-mans-universit-/huggingface' target=\"_blank\">https://wandb.ai/alex-lochain-le-mans-universit-/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/alex-lochain-le-mans-universit-/huggingface/runs/sut7y41n' target=\"_blank\">https://wandb.ai/alex-lochain-le-mans-universit-/huggingface/runs/sut7y41n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon WARNING @ 21:04:08] Another instance of codecarbon is already running. Exiting.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8439' max='8439' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8439/8439 1:23:49, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision Global</th>\n",
              "      <th>Recall Global</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>Precision Neg</th>\n",
              "      <th>Recall Neg</th>\n",
              "      <th>F1 Neg</th>\n",
              "      <th>Precision Pos</th>\n",
              "      <th>Recall Pos</th>\n",
              "      <th>F1 Pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.338900</td>\n",
              "      <td>0.335505</td>\n",
              "      <td>0.872000</td>\n",
              "      <td>0.872028</td>\n",
              "      <td>0.872200</td>\n",
              "      <td>0.871988</td>\n",
              "      <td>0.883721</td>\n",
              "      <td>0.862960</td>\n",
              "      <td>0.873217</td>\n",
              "      <td>0.860335</td>\n",
              "      <td>0.881439</td>\n",
              "      <td>0.870759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.172300</td>\n",
              "      <td>0.412705</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.869984</td>\n",
              "      <td>0.870155</td>\n",
              "      <td>0.869983</td>\n",
              "      <td>0.880192</td>\n",
              "      <td>0.862960</td>\n",
              "      <td>0.871491</td>\n",
              "      <td>0.859776</td>\n",
              "      <td>0.877351</td>\n",
              "      <td>0.868474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.007200</td>\n",
              "      <td>0.482037</td>\n",
              "      <td>0.868400</td>\n",
              "      <td>0.868438</td>\n",
              "      <td>0.868607</td>\n",
              "      <td>0.868389</td>\n",
              "      <td>0.880417</td>\n",
              "      <td>0.859045</td>\n",
              "      <td>0.869600</td>\n",
              "      <td>0.856459</td>\n",
              "      <td>0.878168</td>\n",
              "      <td>0.867178</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0409 21:31:02.617000 2413 torch/_inductor/utils.py:1137] [1/1] Not enough SMs to use max_autotune_gemm mode\n",
            "[codecarbon WARNING @ 22:28:02] Another instance of codecarbon is already running. Exiting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Évaluation du modèle après l'entraînement...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [313/313 00:57]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon WARNING @ 22:29:00] Another instance of codecarbon is already running. Exiting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Résultats de l'évaluation : {'eval_loss': 0.48203736543655396, 'eval_accuracy': 0.8684, 'eval_precision_global': 0.8684383328085279, 'eval_recall_global': 0.8686065370659335, 'eval_f1_macro': 0.8683888604331471, 'eval_precision_neg': 0.8804173354735152, 'eval_recall_neg': 0.8590446358653093, 'eval_f1_neg': 0.869599682917162, 'eval_precision_pos': 0.8564593301435407, 'eval_recall_pos': 0.8781684382665577, 'eval_f1_pos': 0.8671780379491321, 'eval_runtime': 57.632, 'eval_samples_per_second': 43.379, 'eval_steps_per_second': 5.431, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "#SANS LORA\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "# Création du dossier pour les logs CodeCarbon\n",
        "os.makedirs(\"./codecarbon_logs\", exist_ok=True)\n",
        "\n",
        "# Suivi de l'empreinte carbone\n",
        "tracker = EmissionsTracker(project_name=\"finetuning-modernbert\", output_dir=\"./codecarbon_logs\")\n",
        "tracker.start()\n",
        "\n",
        "# Vérification de l'appareil (GPU/CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Utilisation de l'appareil : {device}\")\n",
        "\n",
        "# Chargement du dataset IMDB\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Modèle de base\n",
        "model_name = \"answerdotai/ModernBERT-base\"\n",
        "\n",
        "# Tokenizer & modèle\n",
        "print(\"Chargement du modèle et du tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "id2label = {0: \"neg\", 1: \"pos\"}\n",
        "label2id = {\"neg\": 0, \"pos\": 1}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "# Affichage des paramètres entraînables\n",
        "print(\"\\nParamètres entraînables :\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)\n",
        "\n",
        "# Tokenisation\n",
        "def tokenize_function(examples):\n",
        "    tokens = tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=256)\n",
        "    tokens[\"label\"] = examples[\"label\"]\n",
        "    return tokens\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Split train/test\n",
        "split = tokenized_datasets[\"train\"].train_test_split(test_size=0.1)\n",
        "train_dataset = split[\"train\"]\n",
        "eval_dataset = split[\"test\"]\n",
        "\n",
        "# Chargement des métriques\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "class_names = [\"neg\", \"pos\"]\n",
        "\n",
        "# Fonction de calcul des métriques\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
        "\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    prec, rec, f1_scores, _ = precision_recall_fscore_support(labels, predictions, average=None, zero_division=1)\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision_global\": prec.mean(),\n",
        "        \"recall_global\": rec.mean(),\n",
        "        \"f1_macro\": f1_scores.mean()\n",
        "    }\n",
        "\n",
        "    for i, (p, r, f) in enumerate(zip(prec, rec, f1_scores)):\n",
        "        metrics[f\"precision_{class_names[i]}\"] = p\n",
        "        metrics[f\"recall_{class_names[i]}\"] = r\n",
        "        metrics[f\"f1_{class_names[i]}\"] = f\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Configuration de l'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    run_name=\"finetuning-modernbert\",\n",
        "    overwrite_output_dir=True,\n",
        "    disable_tqdm=False\n",
        ")\n",
        "\n",
        "# Initialisation du Trainer\n",
        "trainer = Trainer(\n",
        "    model=model.to(device),\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Fine-tuning\n",
        "print(\"Début du fine-tuning sans LoRA...\")\n",
        "trainer.train()\n",
        "\n",
        "# Évaluation finale\n",
        "print(\"Évaluation du modèle après l'entraînement...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Résultats de l'évaluation : {eval_results}\")\n",
        "\n",
        "# Arrêt du suivi de CodeCarbon\n",
        "tracker.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrJldHIoqkEu",
        "outputId": "fc324918-4f6d-470b-8f8f-8de26ac1a922"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "model\n",
            "model.embeddings\n",
            "model.embeddings.tok_embeddings\n",
            "model.embeddings.norm\n",
            "model.embeddings.drop\n",
            "model.layers\n",
            "model.layers.0\n",
            "model.layers.0.attn_norm\n",
            "model.layers.0.attn\n",
            "model.layers.0.attn.Wqkv\n",
            "model.layers.0.attn.rotary_emb\n",
            "model.layers.0.attn.Wo\n",
            "model.layers.0.attn.out_drop\n",
            "model.layers.0.mlp_norm\n",
            "model.layers.0.mlp\n",
            "model.layers.0.mlp.Wi\n",
            "model.layers.0.mlp.act\n",
            "model.layers.0.mlp.drop\n",
            "model.layers.0.mlp.Wo\n",
            "model.layers.1\n",
            "model.layers.1.attn_norm\n",
            "model.layers.1.attn\n",
            "model.layers.1.attn.Wqkv\n",
            "model.layers.1.attn.rotary_emb\n",
            "model.layers.1.attn.Wo\n",
            "model.layers.1.attn.out_drop\n",
            "model.layers.1.mlp_norm\n",
            "model.layers.1.mlp\n",
            "model.layers.1.mlp.Wi\n",
            "model.layers.1.mlp.act\n",
            "model.layers.1.mlp.drop\n",
            "model.layers.1.mlp.Wo\n",
            "model.layers.2\n",
            "model.layers.2.attn_norm\n",
            "model.layers.2.attn\n",
            "model.layers.2.attn.Wqkv\n",
            "model.layers.2.attn.rotary_emb\n",
            "model.layers.2.attn.Wo\n",
            "model.layers.2.attn.out_drop\n",
            "model.layers.2.mlp_norm\n",
            "model.layers.2.mlp\n",
            "model.layers.2.mlp.Wi\n",
            "model.layers.2.mlp.act\n",
            "model.layers.2.mlp.drop\n",
            "model.layers.2.mlp.Wo\n",
            "model.layers.3\n",
            "model.layers.3.attn_norm\n",
            "model.layers.3.attn\n",
            "model.layers.3.attn.Wqkv\n",
            "model.layers.3.attn.rotary_emb\n",
            "model.layers.3.attn.Wo\n",
            "model.layers.3.attn.out_drop\n",
            "model.layers.3.mlp_norm\n",
            "model.layers.3.mlp\n",
            "model.layers.3.mlp.Wi\n",
            "model.layers.3.mlp.act\n",
            "model.layers.3.mlp.drop\n",
            "model.layers.3.mlp.Wo\n",
            "model.layers.4\n",
            "model.layers.4.attn_norm\n",
            "model.layers.4.attn\n",
            "model.layers.4.attn.Wqkv\n",
            "model.layers.4.attn.rotary_emb\n",
            "model.layers.4.attn.Wo\n",
            "model.layers.4.attn.out_drop\n",
            "model.layers.4.mlp_norm\n",
            "model.layers.4.mlp\n",
            "model.layers.4.mlp.Wi\n",
            "model.layers.4.mlp.act\n",
            "model.layers.4.mlp.drop\n",
            "model.layers.4.mlp.Wo\n",
            "model.layers.5\n",
            "model.layers.5.attn_norm\n",
            "model.layers.5.attn\n",
            "model.layers.5.attn.Wqkv\n",
            "model.layers.5.attn.rotary_emb\n",
            "model.layers.5.attn.Wo\n",
            "model.layers.5.attn.out_drop\n",
            "model.layers.5.mlp_norm\n",
            "model.layers.5.mlp\n",
            "model.layers.5.mlp.Wi\n",
            "model.layers.5.mlp.act\n",
            "model.layers.5.mlp.drop\n",
            "model.layers.5.mlp.Wo\n",
            "model.layers.6\n",
            "model.layers.6.attn_norm\n",
            "model.layers.6.attn\n",
            "model.layers.6.attn.Wqkv\n",
            "model.layers.6.attn.rotary_emb\n",
            "model.layers.6.attn.Wo\n",
            "model.layers.6.attn.out_drop\n",
            "model.layers.6.mlp_norm\n",
            "model.layers.6.mlp\n",
            "model.layers.6.mlp.Wi\n",
            "model.layers.6.mlp.act\n",
            "model.layers.6.mlp.drop\n",
            "model.layers.6.mlp.Wo\n",
            "model.layers.7\n",
            "model.layers.7.attn_norm\n",
            "model.layers.7.attn\n",
            "model.layers.7.attn.Wqkv\n",
            "model.layers.7.attn.rotary_emb\n",
            "model.layers.7.attn.Wo\n",
            "model.layers.7.attn.out_drop\n",
            "model.layers.7.mlp_norm\n",
            "model.layers.7.mlp\n",
            "model.layers.7.mlp.Wi\n",
            "model.layers.7.mlp.act\n",
            "model.layers.7.mlp.drop\n",
            "model.layers.7.mlp.Wo\n",
            "model.layers.8\n",
            "model.layers.8.attn_norm\n",
            "model.layers.8.attn\n",
            "model.layers.8.attn.Wqkv\n",
            "model.layers.8.attn.rotary_emb\n",
            "model.layers.8.attn.Wo\n",
            "model.layers.8.attn.out_drop\n",
            "model.layers.8.mlp_norm\n",
            "model.layers.8.mlp\n",
            "model.layers.8.mlp.Wi\n",
            "model.layers.8.mlp.act\n",
            "model.layers.8.mlp.drop\n",
            "model.layers.8.mlp.Wo\n",
            "model.layers.9\n",
            "model.layers.9.attn_norm\n",
            "model.layers.9.attn\n",
            "model.layers.9.attn.Wqkv\n",
            "model.layers.9.attn.rotary_emb\n",
            "model.layers.9.attn.Wo\n",
            "model.layers.9.attn.out_drop\n",
            "model.layers.9.mlp_norm\n",
            "model.layers.9.mlp\n",
            "model.layers.9.mlp.Wi\n",
            "model.layers.9.mlp.act\n",
            "model.layers.9.mlp.drop\n",
            "model.layers.9.mlp.Wo\n",
            "model.layers.10\n",
            "model.layers.10.attn_norm\n",
            "model.layers.10.attn\n",
            "model.layers.10.attn.Wqkv\n",
            "model.layers.10.attn.rotary_emb\n",
            "model.layers.10.attn.Wo\n",
            "model.layers.10.attn.out_drop\n",
            "model.layers.10.mlp_norm\n",
            "model.layers.10.mlp\n",
            "model.layers.10.mlp.Wi\n",
            "model.layers.10.mlp.act\n",
            "model.layers.10.mlp.drop\n",
            "model.layers.10.mlp.Wo\n",
            "model.layers.11\n",
            "model.layers.11.attn_norm\n",
            "model.layers.11.attn\n",
            "model.layers.11.attn.Wqkv\n",
            "model.layers.11.attn.rotary_emb\n",
            "model.layers.11.attn.Wo\n",
            "model.layers.11.attn.out_drop\n",
            "model.layers.11.mlp_norm\n",
            "model.layers.11.mlp\n",
            "model.layers.11.mlp.Wi\n",
            "model.layers.11.mlp.act\n",
            "model.layers.11.mlp.drop\n",
            "model.layers.11.mlp.Wo\n",
            "model.layers.12\n",
            "model.layers.12.attn_norm\n",
            "model.layers.12.attn\n",
            "model.layers.12.attn.Wqkv\n",
            "model.layers.12.attn.rotary_emb\n",
            "model.layers.12.attn.Wo\n",
            "model.layers.12.attn.out_drop\n",
            "model.layers.12.mlp_norm\n",
            "model.layers.12.mlp\n",
            "model.layers.12.mlp.Wi\n",
            "model.layers.12.mlp.act\n",
            "model.layers.12.mlp.drop\n",
            "model.layers.12.mlp.Wo\n",
            "model.layers.13\n",
            "model.layers.13.attn_norm\n",
            "model.layers.13.attn\n",
            "model.layers.13.attn.Wqkv\n",
            "model.layers.13.attn.rotary_emb\n",
            "model.layers.13.attn.Wo\n",
            "model.layers.13.attn.out_drop\n",
            "model.layers.13.mlp_norm\n",
            "model.layers.13.mlp\n",
            "model.layers.13.mlp.Wi\n",
            "model.layers.13.mlp.act\n",
            "model.layers.13.mlp.drop\n",
            "model.layers.13.mlp.Wo\n",
            "model.layers.14\n",
            "model.layers.14.attn_norm\n",
            "model.layers.14.attn\n",
            "model.layers.14.attn.Wqkv\n",
            "model.layers.14.attn.rotary_emb\n",
            "model.layers.14.attn.Wo\n",
            "model.layers.14.attn.out_drop\n",
            "model.layers.14.mlp_norm\n",
            "model.layers.14.mlp\n",
            "model.layers.14.mlp.Wi\n",
            "model.layers.14.mlp.act\n",
            "model.layers.14.mlp.drop\n",
            "model.layers.14.mlp.Wo\n",
            "model.layers.15\n",
            "model.layers.15.attn_norm\n",
            "model.layers.15.attn\n",
            "model.layers.15.attn.Wqkv\n",
            "model.layers.15.attn.rotary_emb\n",
            "model.layers.15.attn.Wo\n",
            "model.layers.15.attn.out_drop\n",
            "model.layers.15.mlp_norm\n",
            "model.layers.15.mlp\n",
            "model.layers.15.mlp.Wi\n",
            "model.layers.15.mlp.act\n",
            "model.layers.15.mlp.drop\n",
            "model.layers.15.mlp.Wo\n",
            "model.layers.16\n",
            "model.layers.16.attn_norm\n",
            "model.layers.16.attn\n",
            "model.layers.16.attn.Wqkv\n",
            "model.layers.16.attn.rotary_emb\n",
            "model.layers.16.attn.Wo\n",
            "model.layers.16.attn.out_drop\n",
            "model.layers.16.mlp_norm\n",
            "model.layers.16.mlp\n",
            "model.layers.16.mlp.Wi\n",
            "model.layers.16.mlp.act\n",
            "model.layers.16.mlp.drop\n",
            "model.layers.16.mlp.Wo\n",
            "model.layers.17\n",
            "model.layers.17.attn_norm\n",
            "model.layers.17.attn\n",
            "model.layers.17.attn.Wqkv\n",
            "model.layers.17.attn.rotary_emb\n",
            "model.layers.17.attn.Wo\n",
            "model.layers.17.attn.out_drop\n",
            "model.layers.17.mlp_norm\n",
            "model.layers.17.mlp\n",
            "model.layers.17.mlp.Wi\n",
            "model.layers.17.mlp.act\n",
            "model.layers.17.mlp.drop\n",
            "model.layers.17.mlp.Wo\n",
            "model.layers.18\n",
            "model.layers.18.attn_norm\n",
            "model.layers.18.attn\n",
            "model.layers.18.attn.Wqkv\n",
            "model.layers.18.attn.rotary_emb\n",
            "model.layers.18.attn.Wo\n",
            "model.layers.18.attn.out_drop\n",
            "model.layers.18.mlp_norm\n",
            "model.layers.18.mlp\n",
            "model.layers.18.mlp.Wi\n",
            "model.layers.18.mlp.act\n",
            "model.layers.18.mlp.drop\n",
            "model.layers.18.mlp.Wo\n",
            "model.layers.19\n",
            "model.layers.19.attn_norm\n",
            "model.layers.19.attn\n",
            "model.layers.19.attn.Wqkv\n",
            "model.layers.19.attn.rotary_emb\n",
            "model.layers.19.attn.Wo\n",
            "model.layers.19.attn.out_drop\n",
            "model.layers.19.mlp_norm\n",
            "model.layers.19.mlp\n",
            "model.layers.19.mlp.Wi\n",
            "model.layers.19.mlp.act\n",
            "model.layers.19.mlp.drop\n",
            "model.layers.19.mlp.Wo\n",
            "model.layers.20\n",
            "model.layers.20.attn_norm\n",
            "model.layers.20.attn\n",
            "model.layers.20.attn.Wqkv\n",
            "model.layers.20.attn.rotary_emb\n",
            "model.layers.20.attn.Wo\n",
            "model.layers.20.attn.out_drop\n",
            "model.layers.20.mlp_norm\n",
            "model.layers.20.mlp\n",
            "model.layers.20.mlp.Wi\n",
            "model.layers.20.mlp.act\n",
            "model.layers.20.mlp.drop\n",
            "model.layers.20.mlp.Wo\n",
            "model.layers.21\n",
            "model.layers.21.attn_norm\n",
            "model.layers.21.attn\n",
            "model.layers.21.attn.Wqkv\n",
            "model.layers.21.attn.rotary_emb\n",
            "model.layers.21.attn.Wo\n",
            "model.layers.21.attn.out_drop\n",
            "model.layers.21.mlp_norm\n",
            "model.layers.21.mlp\n",
            "model.layers.21.mlp.Wi\n",
            "model.layers.21.mlp.act\n",
            "model.layers.21.mlp.drop\n",
            "model.layers.21.mlp.Wo\n",
            "model.final_norm\n",
            "head\n",
            "head.dense\n",
            "head.act\n",
            "head.norm\n",
            "drop\n",
            "classifier\n"
          ]
        }
      ],
      "source": [
        "# Inspecter les noms des modules du modèle pour trouver ceux à cibler avec LoRA\n",
        "for name, module in model.named_modules():\n",
        "    print(name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvc9vtApvMrC",
        "outputId": "519fc4f0-4025-48bf-87d6-fb334e525aec"
      },
      "outputs": [],
      "source": [
        "!zip -r modernbert_imdb_sans_lora.zip ./"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3fb3081eed6c4c4883005e5196cbe07c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b13f832674f4a529cd282e8b51dc8f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3d060f577534db3822c66ebe954e85b",
            "placeholder": "​",
            "style": "IPY_MODEL_929b6fd4c26c4bc7b7a7096164e11e89",
            "value": " 25000/25000 [00:34&lt;00:00, 1141.34 examples/s]"
          }
        },
        "5ab1aafc0a0b4f5e8f0427f31426e1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fb3081eed6c4c4883005e5196cbe07c",
            "placeholder": "​",
            "style": "IPY_MODEL_f644a4b17cd54ef8a96b48d653ac04f0",
            "value": "Map: 100%"
          }
        },
        "61698aecfb964dc5bb31e10a515f68f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6333282853fc487db4fb6ef36215ae13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "929b6fd4c26c4bc7b7a7096164e11e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa49621aeb8b4453ad5b642f89c66736": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2faf320c29d4c7dafc03a3da916ffe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61698aecfb964dc5bb31e10a515f68f5",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa49621aeb8b4453ad5b642f89c66736",
            "value": 25000
          }
        },
        "d9a538f6f99246108ca83d2a3e5ad716": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ab1aafc0a0b4f5e8f0427f31426e1df",
              "IPY_MODEL_c2faf320c29d4c7dafc03a3da916ffe9",
              "IPY_MODEL_4b13f832674f4a529cd282e8b51dc8f2"
            ],
            "layout": "IPY_MODEL_6333282853fc487db4fb6ef36215ae13"
          }
        },
        "f3d060f577534db3822c66ebe954e85b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f644a4b17cd54ef8a96b48d653ac04f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
