{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 14065,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0035549235691432635,
      "grad_norm": 10.288626670837402,
      "learning_rate": 0.00019985780305723427,
      "loss": 0.8218,
      "step": 10
    },
    {
      "epoch": 0.007109847138286527,
      "grad_norm": 10.307035446166992,
      "learning_rate": 0.00019971560611446856,
      "loss": 0.7343,
      "step": 20
    },
    {
      "epoch": 0.01066477070742979,
      "grad_norm": 3.762219190597534,
      "learning_rate": 0.00019957340917170282,
      "loss": 0.6655,
      "step": 30
    },
    {
      "epoch": 0.014219694276573054,
      "grad_norm": 3.039576292037964,
      "learning_rate": 0.00019943121222893708,
      "loss": 0.714,
      "step": 40
    },
    {
      "epoch": 0.017774617845716316,
      "grad_norm": 3.397226095199585,
      "learning_rate": 0.00019928901528617137,
      "loss": 0.7092,
      "step": 50
    },
    {
      "epoch": 0.02132954141485958,
      "grad_norm": 2.0413825511932373,
      "learning_rate": 0.00019914681834340563,
      "loss": 0.6592,
      "step": 60
    },
    {
      "epoch": 0.024884464984002843,
      "grad_norm": 2.6915230751037598,
      "learning_rate": 0.0001990046214006399,
      "loss": 0.6655,
      "step": 70
    },
    {
      "epoch": 0.028439388553146108,
      "grad_norm": 2.8192532062530518,
      "learning_rate": 0.00019886242445787418,
      "loss": 0.6634,
      "step": 80
    },
    {
      "epoch": 0.03199431212228937,
      "grad_norm": 2.468008041381836,
      "learning_rate": 0.00019872022751510845,
      "loss": 0.6768,
      "step": 90
    },
    {
      "epoch": 0.03554923569143263,
      "grad_norm": 3.960765838623047,
      "learning_rate": 0.0001985780305723427,
      "loss": 0.7064,
      "step": 100
    },
    {
      "epoch": 0.0391041592605759,
      "grad_norm": 1.5498672723770142,
      "learning_rate": 0.000198435833629577,
      "loss": 0.6727,
      "step": 110
    },
    {
      "epoch": 0.04265908282971916,
      "grad_norm": 5.6992340087890625,
      "learning_rate": 0.00019829363668681126,
      "loss": 0.6616,
      "step": 120
    },
    {
      "epoch": 0.046214006398862424,
      "grad_norm": 2.7982137203216553,
      "learning_rate": 0.00019815143974404552,
      "loss": 0.6332,
      "step": 130
    },
    {
      "epoch": 0.049768929968005686,
      "grad_norm": 5.144481658935547,
      "learning_rate": 0.00019800924280127978,
      "loss": 0.6444,
      "step": 140
    },
    {
      "epoch": 0.053323853537148955,
      "grad_norm": 1.3417972326278687,
      "learning_rate": 0.00019786704585851404,
      "loss": 0.6627,
      "step": 150
    },
    {
      "epoch": 0.056878777106292217,
      "grad_norm": 1.4808021783828735,
      "learning_rate": 0.0001977248489157483,
      "loss": 0.6273,
      "step": 160
    },
    {
      "epoch": 0.06043370067543548,
      "grad_norm": 6.022276401519775,
      "learning_rate": 0.00019758265197298257,
      "loss": 0.6393,
      "step": 170
    },
    {
      "epoch": 0.06398862424457874,
      "grad_norm": 5.25471305847168,
      "learning_rate": 0.00019744045503021686,
      "loss": 0.6294,
      "step": 180
    },
    {
      "epoch": 0.067543547813722,
      "grad_norm": 6.556371688842773,
      "learning_rate": 0.00019729825808745112,
      "loss": 0.6488,
      "step": 190
    },
    {
      "epoch": 0.07109847138286526,
      "grad_norm": 5.5543012619018555,
      "learning_rate": 0.00019715606114468538,
      "loss": 0.6511,
      "step": 200
    },
    {
      "epoch": 0.07465339495200853,
      "grad_norm": 2.057485342025757,
      "learning_rate": 0.00019701386420191967,
      "loss": 0.6018,
      "step": 210
    },
    {
      "epoch": 0.0782083185211518,
      "grad_norm": 3.5703792572021484,
      "learning_rate": 0.00019687166725915393,
      "loss": 0.6284,
      "step": 220
    },
    {
      "epoch": 0.08176324209029506,
      "grad_norm": 4.932772636413574,
      "learning_rate": 0.0001967294703163882,
      "loss": 0.6162,
      "step": 230
    },
    {
      "epoch": 0.08531816565943832,
      "grad_norm": 4.624451637268066,
      "learning_rate": 0.00019658727337362248,
      "loss": 0.5755,
      "step": 240
    },
    {
      "epoch": 0.08887308922858159,
      "grad_norm": 3.21665096282959,
      "learning_rate": 0.00019644507643085674,
      "loss": 0.6019,
      "step": 250
    },
    {
      "epoch": 0.09242801279772485,
      "grad_norm": 1.3254451751708984,
      "learning_rate": 0.000196302879488091,
      "loss": 0.6397,
      "step": 260
    },
    {
      "epoch": 0.09598293636686811,
      "grad_norm": 2.0367138385772705,
      "learning_rate": 0.0001961606825453253,
      "loss": 0.5707,
      "step": 270
    },
    {
      "epoch": 0.09953785993601137,
      "grad_norm": 4.684772491455078,
      "learning_rate": 0.00019601848560255955,
      "loss": 0.6422,
      "step": 280
    },
    {
      "epoch": 0.10309278350515463,
      "grad_norm": 1.5324161052703857,
      "learning_rate": 0.00019587628865979381,
      "loss": 0.5892,
      "step": 290
    },
    {
      "epoch": 0.10664770707429791,
      "grad_norm": 8.9147367477417,
      "learning_rate": 0.0001957340917170281,
      "loss": 0.5891,
      "step": 300
    },
    {
      "epoch": 0.11020263064344117,
      "grad_norm": 8.854146003723145,
      "learning_rate": 0.00019559189477426237,
      "loss": 0.6342,
      "step": 310
    },
    {
      "epoch": 0.11375755421258443,
      "grad_norm": 4.2772345542907715,
      "learning_rate": 0.00019544969783149663,
      "loss": 0.5901,
      "step": 320
    },
    {
      "epoch": 0.1173124777817277,
      "grad_norm": 1.734931230545044,
      "learning_rate": 0.00019530750088873092,
      "loss": 0.5822,
      "step": 330
    },
    {
      "epoch": 0.12086740135087096,
      "grad_norm": 3.3918209075927734,
      "learning_rate": 0.00019516530394596518,
      "loss": 0.5581,
      "step": 340
    },
    {
      "epoch": 0.12442232492001422,
      "grad_norm": 10.37774658203125,
      "learning_rate": 0.00019502310700319944,
      "loss": 0.595,
      "step": 350
    },
    {
      "epoch": 0.12797724848915748,
      "grad_norm": 2.9510698318481445,
      "learning_rate": 0.00019488091006043373,
      "loss": 0.5906,
      "step": 360
    },
    {
      "epoch": 0.13153217205830076,
      "grad_norm": 1.4823359251022339,
      "learning_rate": 0.000194738713117668,
      "loss": 0.5442,
      "step": 370
    },
    {
      "epoch": 0.135087095627444,
      "grad_norm": 2.3618850708007812,
      "learning_rate": 0.00019459651617490225,
      "loss": 0.6078,
      "step": 380
    },
    {
      "epoch": 0.13864201919658728,
      "grad_norm": 1.4531853199005127,
      "learning_rate": 0.00019445431923213654,
      "loss": 0.5513,
      "step": 390
    },
    {
      "epoch": 0.14219694276573053,
      "grad_norm": 1.7649575471878052,
      "learning_rate": 0.0001943121222893708,
      "loss": 0.5987,
      "step": 400
    },
    {
      "epoch": 0.1457518663348738,
      "grad_norm": 1.7568925619125366,
      "learning_rate": 0.00019416992534660506,
      "loss": 0.5726,
      "step": 410
    },
    {
      "epoch": 0.14930678990401705,
      "grad_norm": 2.060812473297119,
      "learning_rate": 0.00019402772840383932,
      "loss": 0.5735,
      "step": 420
    },
    {
      "epoch": 0.15286171347316033,
      "grad_norm": 3.0051848888397217,
      "learning_rate": 0.00019388553146107359,
      "loss": 0.4978,
      "step": 430
    },
    {
      "epoch": 0.1564166370423036,
      "grad_norm": 3.166701316833496,
      "learning_rate": 0.00019374333451830788,
      "loss": 0.5906,
      "step": 440
    },
    {
      "epoch": 0.15997156061144685,
      "grad_norm": 3.9351561069488525,
      "learning_rate": 0.00019360113757554214,
      "loss": 0.5671,
      "step": 450
    },
    {
      "epoch": 0.16352648418059013,
      "grad_norm": 1.5831347703933716,
      "learning_rate": 0.0001934589406327764,
      "loss": 0.551,
      "step": 460
    },
    {
      "epoch": 0.16708140774973337,
      "grad_norm": 1.0339293479919434,
      "learning_rate": 0.00019331674369001066,
      "loss": 0.5137,
      "step": 470
    },
    {
      "epoch": 0.17063633131887665,
      "grad_norm": 2.162466526031494,
      "learning_rate": 0.00019317454674724492,
      "loss": 0.5486,
      "step": 480
    },
    {
      "epoch": 0.1741912548880199,
      "grad_norm": 2.9500417709350586,
      "learning_rate": 0.0001930323498044792,
      "loss": 0.5602,
      "step": 490
    },
    {
      "epoch": 0.17774617845716317,
      "grad_norm": 1.4111783504486084,
      "learning_rate": 0.00019289015286171347,
      "loss": 0.5797,
      "step": 500
    },
    {
      "epoch": 0.18130110202630642,
      "grad_norm": 5.17648458480835,
      "learning_rate": 0.00019274795591894773,
      "loss": 0.4992,
      "step": 510
    },
    {
      "epoch": 0.1848560255954497,
      "grad_norm": 1.6854419708251953,
      "learning_rate": 0.00019260575897618202,
      "loss": 0.4901,
      "step": 520
    },
    {
      "epoch": 0.18841094916459297,
      "grad_norm": 3.5792582035064697,
      "learning_rate": 0.00019246356203341628,
      "loss": 0.5513,
      "step": 530
    },
    {
      "epoch": 0.19196587273373622,
      "grad_norm": 5.6421332359313965,
      "learning_rate": 0.00019232136509065055,
      "loss": 0.5694,
      "step": 540
    },
    {
      "epoch": 0.1955207963028795,
      "grad_norm": 1.5187588930130005,
      "learning_rate": 0.00019217916814788483,
      "loss": 0.5306,
      "step": 550
    },
    {
      "epoch": 0.19907571987202274,
      "grad_norm": 2.546149730682373,
      "learning_rate": 0.0001920369712051191,
      "loss": 0.5454,
      "step": 560
    },
    {
      "epoch": 0.20263064344116602,
      "grad_norm": 5.7680768966674805,
      "learning_rate": 0.00019189477426235336,
      "loss": 0.5531,
      "step": 570
    },
    {
      "epoch": 0.20618556701030927,
      "grad_norm": 1.2834416627883911,
      "learning_rate": 0.00019175257731958765,
      "loss": 0.5042,
      "step": 580
    },
    {
      "epoch": 0.20974049057945254,
      "grad_norm": 2.169295310974121,
      "learning_rate": 0.0001916103803768219,
      "loss": 0.5465,
      "step": 590
    },
    {
      "epoch": 0.21329541414859582,
      "grad_norm": 1.7213661670684814,
      "learning_rate": 0.00019146818343405617,
      "loss": 0.5013,
      "step": 600
    },
    {
      "epoch": 0.21685033771773907,
      "grad_norm": 7.752023220062256,
      "learning_rate": 0.00019132598649129046,
      "loss": 0.5611,
      "step": 610
    },
    {
      "epoch": 0.22040526128688234,
      "grad_norm": 4.439359188079834,
      "learning_rate": 0.00019118378954852472,
      "loss": 0.5324,
      "step": 620
    },
    {
      "epoch": 0.2239601848560256,
      "grad_norm": 3.3996517658233643,
      "learning_rate": 0.00019104159260575898,
      "loss": 0.5083,
      "step": 630
    },
    {
      "epoch": 0.22751510842516887,
      "grad_norm": 1.36747145652771,
      "learning_rate": 0.00019089939566299327,
      "loss": 0.5107,
      "step": 640
    },
    {
      "epoch": 0.23107003199431211,
      "grad_norm": 3.2105371952056885,
      "learning_rate": 0.00019075719872022753,
      "loss": 0.5336,
      "step": 650
    },
    {
      "epoch": 0.2346249555634554,
      "grad_norm": 3.966279983520508,
      "learning_rate": 0.0001906150017774618,
      "loss": 0.484,
      "step": 660
    },
    {
      "epoch": 0.23817987913259864,
      "grad_norm": 5.734463214874268,
      "learning_rate": 0.00019047280483469608,
      "loss": 0.5466,
      "step": 670
    },
    {
      "epoch": 0.2417348027017419,
      "grad_norm": 1.0564100742340088,
      "learning_rate": 0.00019033060789193034,
      "loss": 0.4905,
      "step": 680
    },
    {
      "epoch": 0.2452897262708852,
      "grad_norm": 1.0232311487197876,
      "learning_rate": 0.0001901884109491646,
      "loss": 0.4933,
      "step": 690
    },
    {
      "epoch": 0.24884464984002844,
      "grad_norm": 4.731696605682373,
      "learning_rate": 0.0001900462140063989,
      "loss": 0.509,
      "step": 700
    },
    {
      "epoch": 0.2523995734091717,
      "grad_norm": 1.1033765077590942,
      "learning_rate": 0.00018990401706363316,
      "loss": 0.4944,
      "step": 710
    },
    {
      "epoch": 0.25595449697831496,
      "grad_norm": 2.5340564250946045,
      "learning_rate": 0.00018976182012086742,
      "loss": 0.5476,
      "step": 720
    },
    {
      "epoch": 0.25950942054745824,
      "grad_norm": 4.478963851928711,
      "learning_rate": 0.00018961962317810168,
      "loss": 0.4855,
      "step": 730
    },
    {
      "epoch": 0.2630643441166015,
      "grad_norm": 4.581516265869141,
      "learning_rate": 0.00018947742623533594,
      "loss": 0.585,
      "step": 740
    },
    {
      "epoch": 0.26661926768574473,
      "grad_norm": 2.1484949588775635,
      "learning_rate": 0.0001893352292925702,
      "loss": 0.4701,
      "step": 750
    },
    {
      "epoch": 0.270174191254888,
      "grad_norm": 3.8942930698394775,
      "learning_rate": 0.00018919303234980447,
      "loss": 0.5152,
      "step": 760
    },
    {
      "epoch": 0.2737291148240313,
      "grad_norm": 1.2755234241485596,
      "learning_rate": 0.00018905083540703875,
      "loss": 0.5901,
      "step": 770
    },
    {
      "epoch": 0.27728403839317456,
      "grad_norm": 1.3917487859725952,
      "learning_rate": 0.00018890863846427302,
      "loss": 0.505,
      "step": 780
    },
    {
      "epoch": 0.28083896196231783,
      "grad_norm": 3.687986135482788,
      "learning_rate": 0.00018876644152150728,
      "loss": 0.4971,
      "step": 790
    },
    {
      "epoch": 0.28439388553146105,
      "grad_norm": 4.316529750823975,
      "learning_rate": 0.00018862424457874157,
      "loss": 0.5095,
      "step": 800
    },
    {
      "epoch": 0.28794880910060433,
      "grad_norm": 2.9778871536254883,
      "learning_rate": 0.00018848204763597583,
      "loss": 0.5656,
      "step": 810
    },
    {
      "epoch": 0.2915037326697476,
      "grad_norm": 4.716859340667725,
      "learning_rate": 0.0001883398506932101,
      "loss": 0.497,
      "step": 820
    },
    {
      "epoch": 0.2950586562388909,
      "grad_norm": 2.411679983139038,
      "learning_rate": 0.00018819765375044438,
      "loss": 0.5451,
      "step": 830
    },
    {
      "epoch": 0.2986135798080341,
      "grad_norm": 3.7006356716156006,
      "learning_rate": 0.00018805545680767864,
      "loss": 0.5453,
      "step": 840
    },
    {
      "epoch": 0.3021685033771774,
      "grad_norm": 2.003748893737793,
      "learning_rate": 0.0001879132598649129,
      "loss": 0.5076,
      "step": 850
    },
    {
      "epoch": 0.30572342694632065,
      "grad_norm": 3.4141533374786377,
      "learning_rate": 0.0001877710629221472,
      "loss": 0.5221,
      "step": 860
    },
    {
      "epoch": 0.30927835051546393,
      "grad_norm": 3.6184346675872803,
      "learning_rate": 0.00018762886597938145,
      "loss": 0.5016,
      "step": 870
    },
    {
      "epoch": 0.3128332740846072,
      "grad_norm": 1.9160243272781372,
      "learning_rate": 0.0001874866690366157,
      "loss": 0.5688,
      "step": 880
    },
    {
      "epoch": 0.3163881976537504,
      "grad_norm": 5.124145030975342,
      "learning_rate": 0.00018734447209385,
      "loss": 0.5108,
      "step": 890
    },
    {
      "epoch": 0.3199431212228937,
      "grad_norm": 2.9103732109069824,
      "learning_rate": 0.00018720227515108426,
      "loss": 0.4946,
      "step": 900
    },
    {
      "epoch": 0.323498044792037,
      "grad_norm": 2.9437427520751953,
      "learning_rate": 0.00018706007820831853,
      "loss": 0.4457,
      "step": 910
    },
    {
      "epoch": 0.32705296836118025,
      "grad_norm": 2.092282772064209,
      "learning_rate": 0.00018691788126555281,
      "loss": 0.437,
      "step": 920
    },
    {
      "epoch": 0.33060789193032347,
      "grad_norm": 1.8397374153137207,
      "learning_rate": 0.00018677568432278708,
      "loss": 0.5184,
      "step": 930
    },
    {
      "epoch": 0.33416281549946675,
      "grad_norm": 1.8431442975997925,
      "learning_rate": 0.00018663348738002134,
      "loss": 0.4917,
      "step": 940
    },
    {
      "epoch": 0.33771773906861,
      "grad_norm": 1.3864508867263794,
      "learning_rate": 0.00018649129043725563,
      "loss": 0.5169,
      "step": 950
    },
    {
      "epoch": 0.3412726626377533,
      "grad_norm": 3.6694486141204834,
      "learning_rate": 0.0001863490934944899,
      "loss": 0.479,
      "step": 960
    },
    {
      "epoch": 0.3448275862068966,
      "grad_norm": 3.0991063117980957,
      "learning_rate": 0.00018620689655172415,
      "loss": 0.5316,
      "step": 970
    },
    {
      "epoch": 0.3483825097760398,
      "grad_norm": 2.221529245376587,
      "learning_rate": 0.00018606469960895844,
      "loss": 0.5687,
      "step": 980
    },
    {
      "epoch": 0.35193743334518307,
      "grad_norm": 2.9730560779571533,
      "learning_rate": 0.0001859225026661927,
      "loss": 0.4919,
      "step": 990
    },
    {
      "epoch": 0.35549235691432635,
      "grad_norm": 1.20293390750885,
      "learning_rate": 0.00018578030572342696,
      "loss": 0.4602,
      "step": 1000
    },
    {
      "epoch": 0.3590472804834696,
      "grad_norm": 2.417027235031128,
      "learning_rate": 0.00018563810878066122,
      "loss": 0.523,
      "step": 1010
    },
    {
      "epoch": 0.36260220405261284,
      "grad_norm": 1.4152472019195557,
      "learning_rate": 0.00018549591183789549,
      "loss": 0.5371,
      "step": 1020
    },
    {
      "epoch": 0.3661571276217561,
      "grad_norm": 2.151843309402466,
      "learning_rate": 0.00018535371489512975,
      "loss": 0.4933,
      "step": 1030
    },
    {
      "epoch": 0.3697120511908994,
      "grad_norm": 2.603557586669922,
      "learning_rate": 0.00018521151795236404,
      "loss": 0.4867,
      "step": 1040
    },
    {
      "epoch": 0.37326697476004267,
      "grad_norm": 1.2984999418258667,
      "learning_rate": 0.0001850693210095983,
      "loss": 0.4951,
      "step": 1050
    },
    {
      "epoch": 0.37682189832918594,
      "grad_norm": 1.5645866394042969,
      "learning_rate": 0.00018492712406683256,
      "loss": 0.5157,
      "step": 1060
    },
    {
      "epoch": 0.38037682189832916,
      "grad_norm": 1.8709992170333862,
      "learning_rate": 0.00018478492712406682,
      "loss": 0.4228,
      "step": 1070
    },
    {
      "epoch": 0.38393174546747244,
      "grad_norm": 3.035959005355835,
      "learning_rate": 0.0001846427301813011,
      "loss": 0.4752,
      "step": 1080
    },
    {
      "epoch": 0.3874866690366157,
      "grad_norm": 2.165433883666992,
      "learning_rate": 0.00018450053323853537,
      "loss": 0.4497,
      "step": 1090
    },
    {
      "epoch": 0.391041592605759,
      "grad_norm": 1.1345160007476807,
      "learning_rate": 0.00018435833629576963,
      "loss": 0.4074,
      "step": 1100
    },
    {
      "epoch": 0.3945965161749022,
      "grad_norm": 4.592012882232666,
      "learning_rate": 0.00018421613935300392,
      "loss": 0.444,
      "step": 1110
    },
    {
      "epoch": 0.3981514397440455,
      "grad_norm": 1.4613499641418457,
      "learning_rate": 0.00018407394241023818,
      "loss": 0.4631,
      "step": 1120
    },
    {
      "epoch": 0.40170636331318876,
      "grad_norm": 1.5896302461624146,
      "learning_rate": 0.00018393174546747244,
      "loss": 0.4901,
      "step": 1130
    },
    {
      "epoch": 0.40526128688233204,
      "grad_norm": 3.705432653427124,
      "learning_rate": 0.00018378954852470673,
      "loss": 0.5115,
      "step": 1140
    },
    {
      "epoch": 0.4088162104514753,
      "grad_norm": 1.3428820371627808,
      "learning_rate": 0.000183647351581941,
      "loss": 0.4789,
      "step": 1150
    },
    {
      "epoch": 0.41237113402061853,
      "grad_norm": 5.069384574890137,
      "learning_rate": 0.00018350515463917526,
      "loss": 0.4749,
      "step": 1160
    },
    {
      "epoch": 0.4159260575897618,
      "grad_norm": 1.3626999855041504,
      "learning_rate": 0.00018336295769640955,
      "loss": 0.4443,
      "step": 1170
    },
    {
      "epoch": 0.4194809811589051,
      "grad_norm": 2.7186837196350098,
      "learning_rate": 0.0001832207607536438,
      "loss": 0.4619,
      "step": 1180
    },
    {
      "epoch": 0.42303590472804836,
      "grad_norm": 1.5095049142837524,
      "learning_rate": 0.00018307856381087807,
      "loss": 0.4909,
      "step": 1190
    },
    {
      "epoch": 0.42659082829719164,
      "grad_norm": 1.6939311027526855,
      "learning_rate": 0.00018293636686811236,
      "loss": 0.4851,
      "step": 1200
    },
    {
      "epoch": 0.43014575186633486,
      "grad_norm": 4.269196510314941,
      "learning_rate": 0.00018279416992534662,
      "loss": 0.4083,
      "step": 1210
    },
    {
      "epoch": 0.43370067543547813,
      "grad_norm": 2.798036575317383,
      "learning_rate": 0.00018265197298258088,
      "loss": 0.5138,
      "step": 1220
    },
    {
      "epoch": 0.4372555990046214,
      "grad_norm": 0.8668493628501892,
      "learning_rate": 0.00018250977603981517,
      "loss": 0.3984,
      "step": 1230
    },
    {
      "epoch": 0.4408105225737647,
      "grad_norm": 1.2978408336639404,
      "learning_rate": 0.00018236757909704943,
      "loss": 0.4782,
      "step": 1240
    },
    {
      "epoch": 0.4443654461429079,
      "grad_norm": 5.130323886871338,
      "learning_rate": 0.0001822253821542837,
      "loss": 0.461,
      "step": 1250
    },
    {
      "epoch": 0.4479203697120512,
      "grad_norm": 3.484015941619873,
      "learning_rate": 0.00018208318521151798,
      "loss": 0.428,
      "step": 1260
    },
    {
      "epoch": 0.45147529328119446,
      "grad_norm": 1.4412846565246582,
      "learning_rate": 0.00018194098826875224,
      "loss": 0.4421,
      "step": 1270
    },
    {
      "epoch": 0.45503021685033773,
      "grad_norm": 1.9427812099456787,
      "learning_rate": 0.0001817987913259865,
      "loss": 0.4107,
      "step": 1280
    },
    {
      "epoch": 0.458585140419481,
      "grad_norm": 3.4258432388305664,
      "learning_rate": 0.00018165659438322077,
      "loss": 0.4384,
      "step": 1290
    },
    {
      "epoch": 0.46214006398862423,
      "grad_norm": 3.7377963066101074,
      "learning_rate": 0.00018151439744045503,
      "loss": 0.3867,
      "step": 1300
    },
    {
      "epoch": 0.4656949875577675,
      "grad_norm": 3.3480594158172607,
      "learning_rate": 0.00018137220049768932,
      "loss": 0.4219,
      "step": 1310
    },
    {
      "epoch": 0.4692499111269108,
      "grad_norm": 1.3651622533798218,
      "learning_rate": 0.00018123000355492358,
      "loss": 0.5323,
      "step": 1320
    },
    {
      "epoch": 0.47280483469605405,
      "grad_norm": 8.270944595336914,
      "learning_rate": 0.00018108780661215784,
      "loss": 0.4755,
      "step": 1330
    },
    {
      "epoch": 0.4763597582651973,
      "grad_norm": 4.716622829437256,
      "learning_rate": 0.0001809456096693921,
      "loss": 0.4898,
      "step": 1340
    },
    {
      "epoch": 0.47991468183434055,
      "grad_norm": 0.7428897619247437,
      "learning_rate": 0.00018080341272662636,
      "loss": 0.4504,
      "step": 1350
    },
    {
      "epoch": 0.4834696054034838,
      "grad_norm": 1.3217942714691162,
      "learning_rate": 0.00018066121578386065,
      "loss": 0.4675,
      "step": 1360
    },
    {
      "epoch": 0.4870245289726271,
      "grad_norm": 1.5948665142059326,
      "learning_rate": 0.00018051901884109491,
      "loss": 0.4908,
      "step": 1370
    },
    {
      "epoch": 0.4905794525417704,
      "grad_norm": 2.7838218212127686,
      "learning_rate": 0.00018037682189832918,
      "loss": 0.4599,
      "step": 1380
    },
    {
      "epoch": 0.4941343761109136,
      "grad_norm": 1.3821260929107666,
      "learning_rate": 0.00018023462495556346,
      "loss": 0.4227,
      "step": 1390
    },
    {
      "epoch": 0.4976892996800569,
      "grad_norm": 1.1327378749847412,
      "learning_rate": 0.00018009242801279773,
      "loss": 0.4714,
      "step": 1400
    },
    {
      "epoch": 0.5012442232492001,
      "grad_norm": 0.9013640880584717,
      "learning_rate": 0.000179950231070032,
      "loss": 0.4294,
      "step": 1410
    },
    {
      "epoch": 0.5047991468183434,
      "grad_norm": 3.2272825241088867,
      "learning_rate": 0.00017980803412726628,
      "loss": 0.4701,
      "step": 1420
    },
    {
      "epoch": 0.5083540703874867,
      "grad_norm": 4.344588756561279,
      "learning_rate": 0.00017966583718450054,
      "loss": 0.4649,
      "step": 1430
    },
    {
      "epoch": 0.5119089939566299,
      "grad_norm": 2.807389497756958,
      "learning_rate": 0.0001795236402417348,
      "loss": 0.452,
      "step": 1440
    },
    {
      "epoch": 0.5154639175257731,
      "grad_norm": 2.9535086154937744,
      "learning_rate": 0.0001793814432989691,
      "loss": 0.4517,
      "step": 1450
    },
    {
      "epoch": 0.5190188410949165,
      "grad_norm": 1.6022378206253052,
      "learning_rate": 0.00017923924635620335,
      "loss": 0.4927,
      "step": 1460
    },
    {
      "epoch": 0.5225737646640597,
      "grad_norm": 2.233816146850586,
      "learning_rate": 0.0001790970494134376,
      "loss": 0.4742,
      "step": 1470
    },
    {
      "epoch": 0.526128688233203,
      "grad_norm": 2.327324628829956,
      "learning_rate": 0.0001789548524706719,
      "loss": 0.4275,
      "step": 1480
    },
    {
      "epoch": 0.5296836118023462,
      "grad_norm": 4.145824909210205,
      "learning_rate": 0.00017881265552790616,
      "loss": 0.5273,
      "step": 1490
    },
    {
      "epoch": 0.5332385353714895,
      "grad_norm": 2.9498109817504883,
      "learning_rate": 0.00017867045858514042,
      "loss": 0.4316,
      "step": 1500
    },
    {
      "epoch": 0.5367934589406328,
      "grad_norm": 1.977393388748169,
      "learning_rate": 0.0001785282616423747,
      "loss": 0.3991,
      "step": 1510
    },
    {
      "epoch": 0.540348382509776,
      "grad_norm": 4.877723217010498,
      "learning_rate": 0.00017838606469960897,
      "loss": 0.4499,
      "step": 1520
    },
    {
      "epoch": 0.5439033060789193,
      "grad_norm": 1.634731650352478,
      "learning_rate": 0.00017824386775684324,
      "loss": 0.5031,
      "step": 1530
    },
    {
      "epoch": 0.5474582296480626,
      "grad_norm": 4.0140700340271,
      "learning_rate": 0.00017810167081407753,
      "loss": 0.5345,
      "step": 1540
    },
    {
      "epoch": 0.5510131532172058,
      "grad_norm": 6.485020160675049,
      "learning_rate": 0.0001779594738713118,
      "loss": 0.4379,
      "step": 1550
    },
    {
      "epoch": 0.5545680767863491,
      "grad_norm": 5.986883163452148,
      "learning_rate": 0.00017781727692854605,
      "loss": 0.4779,
      "step": 1560
    },
    {
      "epoch": 0.5581230003554923,
      "grad_norm": 5.812918186187744,
      "learning_rate": 0.00017767507998578034,
      "loss": 0.4935,
      "step": 1570
    },
    {
      "epoch": 0.5616779239246357,
      "grad_norm": 4.487712383270264,
      "learning_rate": 0.0001775328830430146,
      "loss": 0.4871,
      "step": 1580
    },
    {
      "epoch": 0.5652328474937789,
      "grad_norm": 5.596385955810547,
      "learning_rate": 0.00017739068610024886,
      "loss": 0.5097,
      "step": 1590
    },
    {
      "epoch": 0.5687877710629221,
      "grad_norm": 5.2666826248168945,
      "learning_rate": 0.00017724848915748312,
      "loss": 0.5088,
      "step": 1600
    },
    {
      "epoch": 0.5723426946320654,
      "grad_norm": 2.0763099193573,
      "learning_rate": 0.00017710629221471738,
      "loss": 0.4439,
      "step": 1610
    },
    {
      "epoch": 0.5758976182012087,
      "grad_norm": 4.282370567321777,
      "learning_rate": 0.00017696409527195165,
      "loss": 0.4426,
      "step": 1620
    },
    {
      "epoch": 0.579452541770352,
      "grad_norm": 3.224487066268921,
      "learning_rate": 0.0001768218983291859,
      "loss": 0.5022,
      "step": 1630
    },
    {
      "epoch": 0.5830074653394952,
      "grad_norm": 2.6189794540405273,
      "learning_rate": 0.0001766797013864202,
      "loss": 0.4086,
      "step": 1640
    },
    {
      "epoch": 0.5865623889086384,
      "grad_norm": 1.325169563293457,
      "learning_rate": 0.00017653750444365446,
      "loss": 0.4708,
      "step": 1650
    },
    {
      "epoch": 0.5901173124777818,
      "grad_norm": 4.688090801239014,
      "learning_rate": 0.00017639530750088872,
      "loss": 0.4996,
      "step": 1660
    },
    {
      "epoch": 0.593672236046925,
      "grad_norm": 1.1148343086242676,
      "learning_rate": 0.000176253110558123,
      "loss": 0.3673,
      "step": 1670
    },
    {
      "epoch": 0.5972271596160682,
      "grad_norm": 4.038937091827393,
      "learning_rate": 0.00017611091361535727,
      "loss": 0.5064,
      "step": 1680
    },
    {
      "epoch": 0.6007820831852115,
      "grad_norm": 6.364986419677734,
      "learning_rate": 0.00017596871667259153,
      "loss": 0.436,
      "step": 1690
    },
    {
      "epoch": 0.6043370067543548,
      "grad_norm": 3.929471015930176,
      "learning_rate": 0.00017582651972982582,
      "loss": 0.4783,
      "step": 1700
    },
    {
      "epoch": 0.6078919303234981,
      "grad_norm": 2.282290458679199,
      "learning_rate": 0.00017568432278706008,
      "loss": 0.4252,
      "step": 1710
    },
    {
      "epoch": 0.6114468538926413,
      "grad_norm": 1.6397638320922852,
      "learning_rate": 0.00017554212584429434,
      "loss": 0.4894,
      "step": 1720
    },
    {
      "epoch": 0.6150017774617845,
      "grad_norm": 1.6880836486816406,
      "learning_rate": 0.00017539992890152863,
      "loss": 0.4979,
      "step": 1730
    },
    {
      "epoch": 0.6185567010309279,
      "grad_norm": 1.446676254272461,
      "learning_rate": 0.0001752577319587629,
      "loss": 0.4926,
      "step": 1740
    },
    {
      "epoch": 0.6221116246000711,
      "grad_norm": 1.8497309684753418,
      "learning_rate": 0.00017511553501599716,
      "loss": 0.4455,
      "step": 1750
    },
    {
      "epoch": 0.6256665481692144,
      "grad_norm": 4.676792144775391,
      "learning_rate": 0.00017497333807323144,
      "loss": 0.4675,
      "step": 1760
    },
    {
      "epoch": 0.6292214717383576,
      "grad_norm": 1.8788617849349976,
      "learning_rate": 0.0001748311411304657,
      "loss": 0.5122,
      "step": 1770
    },
    {
      "epoch": 0.6327763953075008,
      "grad_norm": 3.419785976409912,
      "learning_rate": 0.00017468894418769997,
      "loss": 0.3726,
      "step": 1780
    },
    {
      "epoch": 0.6363313188766442,
      "grad_norm": 3.8723978996276855,
      "learning_rate": 0.00017454674724493426,
      "loss": 0.441,
      "step": 1790
    },
    {
      "epoch": 0.6398862424457874,
      "grad_norm": 2.0782461166381836,
      "learning_rate": 0.00017440455030216852,
      "loss": 0.4379,
      "step": 1800
    },
    {
      "epoch": 0.6434411660149307,
      "grad_norm": 1.477799892425537,
      "learning_rate": 0.00017426235335940278,
      "loss": 0.4934,
      "step": 1810
    },
    {
      "epoch": 0.646996089584074,
      "grad_norm": 2.8407135009765625,
      "learning_rate": 0.00017412015641663707,
      "loss": 0.4363,
      "step": 1820
    },
    {
      "epoch": 0.6505510131532172,
      "grad_norm": 2.2356157302856445,
      "learning_rate": 0.00017397795947387133,
      "loss": 0.3781,
      "step": 1830
    },
    {
      "epoch": 0.6541059367223605,
      "grad_norm": 5.332653522491455,
      "learning_rate": 0.0001738357625311056,
      "loss": 0.5385,
      "step": 1840
    },
    {
      "epoch": 0.6576608602915037,
      "grad_norm": 0.6333057880401611,
      "learning_rate": 0.00017369356558833988,
      "loss": 0.4618,
      "step": 1850
    },
    {
      "epoch": 0.6612157838606469,
      "grad_norm": 1.8563694953918457,
      "learning_rate": 0.00017355136864557414,
      "loss": 0.4899,
      "step": 1860
    },
    {
      "epoch": 0.6647707074297903,
      "grad_norm": 1.5850247144699097,
      "learning_rate": 0.0001734091717028084,
      "loss": 0.3934,
      "step": 1870
    },
    {
      "epoch": 0.6683256309989335,
      "grad_norm": 3.3412604331970215,
      "learning_rate": 0.00017326697476004267,
      "loss": 0.4776,
      "step": 1880
    },
    {
      "epoch": 0.6718805545680768,
      "grad_norm": 1.1890685558319092,
      "learning_rate": 0.00017312477781727693,
      "loss": 0.4768,
      "step": 1890
    },
    {
      "epoch": 0.67543547813722,
      "grad_norm": 1.8717646598815918,
      "learning_rate": 0.0001729825808745112,
      "loss": 0.4445,
      "step": 1900
    },
    {
      "epoch": 0.6789904017063633,
      "grad_norm": 1.797988772392273,
      "learning_rate": 0.00017284038393174548,
      "loss": 0.5334,
      "step": 1910
    },
    {
      "epoch": 0.6825453252755066,
      "grad_norm": 2.3883466720581055,
      "learning_rate": 0.00017269818698897974,
      "loss": 0.3765,
      "step": 1920
    },
    {
      "epoch": 0.6861002488446498,
      "grad_norm": 2.9416892528533936,
      "learning_rate": 0.000172555990046214,
      "loss": 0.4193,
      "step": 1930
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 1.334517478942871,
      "learning_rate": 0.00017241379310344826,
      "loss": 0.4976,
      "step": 1940
    },
    {
      "epoch": 0.6932100959829364,
      "grad_norm": 1.732743501663208,
      "learning_rate": 0.00017227159616068255,
      "loss": 0.4567,
      "step": 1950
    },
    {
      "epoch": 0.6967650195520796,
      "grad_norm": 4.121254920959473,
      "learning_rate": 0.0001721293992179168,
      "loss": 0.4214,
      "step": 1960
    },
    {
      "epoch": 0.7003199431212229,
      "grad_norm": 7.363585948944092,
      "learning_rate": 0.00017198720227515107,
      "loss": 0.4907,
      "step": 1970
    },
    {
      "epoch": 0.7038748666903661,
      "grad_norm": 1.3179618120193481,
      "learning_rate": 0.00017184500533238536,
      "loss": 0.4407,
      "step": 1980
    },
    {
      "epoch": 0.7074297902595095,
      "grad_norm": 0.7833192944526672,
      "learning_rate": 0.00017170280838961963,
      "loss": 0.4417,
      "step": 1990
    },
    {
      "epoch": 0.7109847138286527,
      "grad_norm": 1.5184956789016724,
      "learning_rate": 0.0001715606114468539,
      "loss": 0.387,
      "step": 2000
    },
    {
      "epoch": 0.7145396373977959,
      "grad_norm": 1.632971167564392,
      "learning_rate": 0.00017141841450408818,
      "loss": 0.3651,
      "step": 2010
    },
    {
      "epoch": 0.7180945609669392,
      "grad_norm": 1.7293777465820312,
      "learning_rate": 0.00017127621756132244,
      "loss": 0.4846,
      "step": 2020
    },
    {
      "epoch": 0.7216494845360825,
      "grad_norm": 1.5703171491622925,
      "learning_rate": 0.0001711340206185567,
      "loss": 0.4546,
      "step": 2030
    },
    {
      "epoch": 0.7252044081052257,
      "grad_norm": 2.648024559020996,
      "learning_rate": 0.000170991823675791,
      "loss": 0.4058,
      "step": 2040
    },
    {
      "epoch": 0.728759331674369,
      "grad_norm": 2.864840030670166,
      "learning_rate": 0.00017084962673302525,
      "loss": 0.4202,
      "step": 2050
    },
    {
      "epoch": 0.7323142552435122,
      "grad_norm": 2.319556951522827,
      "learning_rate": 0.0001707074297902595,
      "loss": 0.4486,
      "step": 2060
    },
    {
      "epoch": 0.7358691788126556,
      "grad_norm": 5.174098014831543,
      "learning_rate": 0.0001705652328474938,
      "loss": 0.4675,
      "step": 2070
    },
    {
      "epoch": 0.7394241023817988,
      "grad_norm": 7.764122486114502,
      "learning_rate": 0.00017042303590472806,
      "loss": 0.4892,
      "step": 2080
    },
    {
      "epoch": 0.742979025950942,
      "grad_norm": 1.6187516450881958,
      "learning_rate": 0.00017028083896196232,
      "loss": 0.4157,
      "step": 2090
    },
    {
      "epoch": 0.7465339495200853,
      "grad_norm": 1.8785767555236816,
      "learning_rate": 0.0001701386420191966,
      "loss": 0.498,
      "step": 2100
    },
    {
      "epoch": 0.7500888730892286,
      "grad_norm": 1.6714591979980469,
      "learning_rate": 0.00016999644507643087,
      "loss": 0.457,
      "step": 2110
    },
    {
      "epoch": 0.7536437966583719,
      "grad_norm": 0.7793387174606323,
      "learning_rate": 0.00016985424813366514,
      "loss": 0.424,
      "step": 2120
    },
    {
      "epoch": 0.7571987202275151,
      "grad_norm": 0.7299866080284119,
      "learning_rate": 0.00016971205119089942,
      "loss": 0.353,
      "step": 2130
    },
    {
      "epoch": 0.7607536437966583,
      "grad_norm": 1.8940660953521729,
      "learning_rate": 0.00016956985424813369,
      "loss": 0.4576,
      "step": 2140
    },
    {
      "epoch": 0.7643085673658017,
      "grad_norm": 2.547611951828003,
      "learning_rate": 0.00016942765730536795,
      "loss": 0.4472,
      "step": 2150
    },
    {
      "epoch": 0.7678634909349449,
      "grad_norm": 1.3383015394210815,
      "learning_rate": 0.0001692854603626022,
      "loss": 0.3756,
      "step": 2160
    },
    {
      "epoch": 0.7714184145040882,
      "grad_norm": 1.3104238510131836,
      "learning_rate": 0.00016914326341983647,
      "loss": 0.421,
      "step": 2170
    },
    {
      "epoch": 0.7749733380732314,
      "grad_norm": 4.499561786651611,
      "learning_rate": 0.00016900106647707076,
      "loss": 0.4609,
      "step": 2180
    },
    {
      "epoch": 0.7785282616423747,
      "grad_norm": 2.9858877658843994,
      "learning_rate": 0.00016885886953430502,
      "loss": 0.4325,
      "step": 2190
    },
    {
      "epoch": 0.782083185211518,
      "grad_norm": 1.4427738189697266,
      "learning_rate": 0.00016871667259153928,
      "loss": 0.5006,
      "step": 2200
    },
    {
      "epoch": 0.7856381087806612,
      "grad_norm": 8.617220878601074,
      "learning_rate": 0.00016857447564877354,
      "loss": 0.5056,
      "step": 2210
    },
    {
      "epoch": 0.7891930323498044,
      "grad_norm": 3.3759684562683105,
      "learning_rate": 0.00016843227870600783,
      "loss": 0.3955,
      "step": 2220
    },
    {
      "epoch": 0.7927479559189478,
      "grad_norm": 5.892530918121338,
      "learning_rate": 0.0001682900817632421,
      "loss": 0.4914,
      "step": 2230
    },
    {
      "epoch": 0.796302879488091,
      "grad_norm": 2.384136199951172,
      "learning_rate": 0.00016814788482047636,
      "loss": 0.3884,
      "step": 2240
    },
    {
      "epoch": 0.7998578030572343,
      "grad_norm": 1.2960783243179321,
      "learning_rate": 0.00016800568787771062,
      "loss": 0.4268,
      "step": 2250
    },
    {
      "epoch": 0.8034127266263775,
      "grad_norm": 1.3442342281341553,
      "learning_rate": 0.0001678634909349449,
      "loss": 0.5048,
      "step": 2260
    },
    {
      "epoch": 0.8069676501955207,
      "grad_norm": 2.142413854598999,
      "learning_rate": 0.00016772129399217917,
      "loss": 0.4138,
      "step": 2270
    },
    {
      "epoch": 0.8105225737646641,
      "grad_norm": 1.2963452339172363,
      "learning_rate": 0.00016757909704941343,
      "loss": 0.4065,
      "step": 2280
    },
    {
      "epoch": 0.8140774973338073,
      "grad_norm": 4.990139007568359,
      "learning_rate": 0.00016743690010664772,
      "loss": 0.4203,
      "step": 2290
    },
    {
      "epoch": 0.8176324209029506,
      "grad_norm": 4.210527420043945,
      "learning_rate": 0.00016729470316388198,
      "loss": 0.4165,
      "step": 2300
    },
    {
      "epoch": 0.8211873444720938,
      "grad_norm": 1.2070574760437012,
      "learning_rate": 0.00016715250622111624,
      "loss": 0.4621,
      "step": 2310
    },
    {
      "epoch": 0.8247422680412371,
      "grad_norm": 0.9502354264259338,
      "learning_rate": 0.00016701030927835053,
      "loss": 0.4805,
      "step": 2320
    },
    {
      "epoch": 0.8282971916103804,
      "grad_norm": 1.2186535596847534,
      "learning_rate": 0.0001668681123355848,
      "loss": 0.487,
      "step": 2330
    },
    {
      "epoch": 0.8318521151795236,
      "grad_norm": 1.6464999914169312,
      "learning_rate": 0.00016672591539281905,
      "loss": 0.4945,
      "step": 2340
    },
    {
      "epoch": 0.835407038748667,
      "grad_norm": 2.3770720958709717,
      "learning_rate": 0.00016658371845005334,
      "loss": 0.4241,
      "step": 2350
    },
    {
      "epoch": 0.8389619623178102,
      "grad_norm": 0.76971435546875,
      "learning_rate": 0.0001664415215072876,
      "loss": 0.3961,
      "step": 2360
    },
    {
      "epoch": 0.8425168858869534,
      "grad_norm": 4.290452480316162,
      "learning_rate": 0.00016629932456452187,
      "loss": 0.4606,
      "step": 2370
    },
    {
      "epoch": 0.8460718094560967,
      "grad_norm": 1.2298712730407715,
      "learning_rate": 0.00016615712762175616,
      "loss": 0.3582,
      "step": 2380
    },
    {
      "epoch": 0.8496267330252399,
      "grad_norm": 1.092344880104065,
      "learning_rate": 0.00016601493067899042,
      "loss": 0.4304,
      "step": 2390
    },
    {
      "epoch": 0.8531816565943833,
      "grad_norm": 6.698636054992676,
      "learning_rate": 0.00016587273373622468,
      "loss": 0.3881,
      "step": 2400
    },
    {
      "epoch": 0.8567365801635265,
      "grad_norm": 2.2687177658081055,
      "learning_rate": 0.00016573053679345897,
      "loss": 0.4014,
      "step": 2410
    },
    {
      "epoch": 0.8602915037326697,
      "grad_norm": 1.2264786958694458,
      "learning_rate": 0.00016558833985069323,
      "loss": 0.4346,
      "step": 2420
    },
    {
      "epoch": 0.863846427301813,
      "grad_norm": 4.40191650390625,
      "learning_rate": 0.0001654461429079275,
      "loss": 0.4733,
      "step": 2430
    },
    {
      "epoch": 0.8674013508709563,
      "grad_norm": 2.540693998336792,
      "learning_rate": 0.00016530394596516178,
      "loss": 0.3937,
      "step": 2440
    },
    {
      "epoch": 0.8709562744400995,
      "grad_norm": 2.256662607192993,
      "learning_rate": 0.00016516174902239604,
      "loss": 0.4475,
      "step": 2450
    },
    {
      "epoch": 0.8745111980092428,
      "grad_norm": 6.33316707611084,
      "learning_rate": 0.0001650195520796303,
      "loss": 0.3636,
      "step": 2460
    },
    {
      "epoch": 0.878066121578386,
      "grad_norm": 3.263216972351074,
      "learning_rate": 0.00016487735513686456,
      "loss": 0.3978,
      "step": 2470
    },
    {
      "epoch": 0.8816210451475294,
      "grad_norm": 2.033317804336548,
      "learning_rate": 0.00016473515819409883,
      "loss": 0.4923,
      "step": 2480
    },
    {
      "epoch": 0.8851759687166726,
      "grad_norm": 2.068673610687256,
      "learning_rate": 0.0001645929612513331,
      "loss": 0.3788,
      "step": 2490
    },
    {
      "epoch": 0.8887308922858158,
      "grad_norm": 5.018685340881348,
      "learning_rate": 0.00016445076430856738,
      "loss": 0.5135,
      "step": 2500
    },
    {
      "epoch": 0.8922858158549591,
      "grad_norm": 0.9505700469017029,
      "learning_rate": 0.00016430856736580164,
      "loss": 0.3968,
      "step": 2510
    },
    {
      "epoch": 0.8958407394241024,
      "grad_norm": 2.0885188579559326,
      "learning_rate": 0.0001641663704230359,
      "loss": 0.4218,
      "step": 2520
    },
    {
      "epoch": 0.8993956629932457,
      "grad_norm": 5.069573402404785,
      "learning_rate": 0.00016402417348027016,
      "loss": 0.4482,
      "step": 2530
    },
    {
      "epoch": 0.9029505865623889,
      "grad_norm": 2.2143099308013916,
      "learning_rate": 0.00016388197653750445,
      "loss": 0.5404,
      "step": 2540
    },
    {
      "epoch": 0.9065055101315321,
      "grad_norm": 3.792084217071533,
      "learning_rate": 0.0001637397795947387,
      "loss": 0.421,
      "step": 2550
    },
    {
      "epoch": 0.9100604337006755,
      "grad_norm": 0.7150763273239136,
      "learning_rate": 0.00016359758265197297,
      "loss": 0.406,
      "step": 2560
    },
    {
      "epoch": 0.9136153572698187,
      "grad_norm": 2.6890060901641846,
      "learning_rate": 0.00016345538570920726,
      "loss": 0.4897,
      "step": 2570
    },
    {
      "epoch": 0.917170280838962,
      "grad_norm": 4.951836585998535,
      "learning_rate": 0.00016331318876644152,
      "loss": 0.5371,
      "step": 2580
    },
    {
      "epoch": 0.9207252044081052,
      "grad_norm": 3.7341766357421875,
      "learning_rate": 0.00016317099182367579,
      "loss": 0.4028,
      "step": 2590
    },
    {
      "epoch": 0.9242801279772485,
      "grad_norm": 1.8137602806091309,
      "learning_rate": 0.00016302879488091007,
      "loss": 0.5412,
      "step": 2600
    },
    {
      "epoch": 0.9278350515463918,
      "grad_norm": 0.7807763814926147,
      "learning_rate": 0.00016288659793814434,
      "loss": 0.3621,
      "step": 2610
    },
    {
      "epoch": 0.931389975115535,
      "grad_norm": 2.35147762298584,
      "learning_rate": 0.0001627444009953786,
      "loss": 0.4523,
      "step": 2620
    },
    {
      "epoch": 0.9349448986846782,
      "grad_norm": 2.4737460613250732,
      "learning_rate": 0.0001626022040526129,
      "loss": 0.4217,
      "step": 2630
    },
    {
      "epoch": 0.9384998222538216,
      "grad_norm": 3.209743022918701,
      "learning_rate": 0.00016246000710984715,
      "loss": 0.4459,
      "step": 2640
    },
    {
      "epoch": 0.9420547458229648,
      "grad_norm": 3.5394442081451416,
      "learning_rate": 0.0001623178101670814,
      "loss": 0.4568,
      "step": 2650
    },
    {
      "epoch": 0.9456096693921081,
      "grad_norm": 1.4756144285202026,
      "learning_rate": 0.0001621756132243157,
      "loss": 0.3798,
      "step": 2660
    },
    {
      "epoch": 0.9491645929612513,
      "grad_norm": 4.934248447418213,
      "learning_rate": 0.00016203341628154996,
      "loss": 0.4524,
      "step": 2670
    },
    {
      "epoch": 0.9527195165303946,
      "grad_norm": 5.275265693664551,
      "learning_rate": 0.00016189121933878422,
      "loss": 0.4521,
      "step": 2680
    },
    {
      "epoch": 0.9562744400995379,
      "grad_norm": 5.15658712387085,
      "learning_rate": 0.0001617490223960185,
      "loss": 0.4188,
      "step": 2690
    },
    {
      "epoch": 0.9598293636686811,
      "grad_norm": 2.4909143447875977,
      "learning_rate": 0.00016160682545325277,
      "loss": 0.4227,
      "step": 2700
    },
    {
      "epoch": 0.9633842872378244,
      "grad_norm": 1.5760912895202637,
      "learning_rate": 0.00016146462851048703,
      "loss": 0.596,
      "step": 2710
    },
    {
      "epoch": 0.9669392108069677,
      "grad_norm": 4.7868852615356445,
      "learning_rate": 0.00016132243156772132,
      "loss": 0.4487,
      "step": 2720
    },
    {
      "epoch": 0.9704941343761109,
      "grad_norm": 1.4535108804702759,
      "learning_rate": 0.00016118023462495558,
      "loss": 0.487,
      "step": 2730
    },
    {
      "epoch": 0.9740490579452542,
      "grad_norm": 2.7770447731018066,
      "learning_rate": 0.00016103803768218985,
      "loss": 0.4397,
      "step": 2740
    },
    {
      "epoch": 0.9776039815143974,
      "grad_norm": 4.420011520385742,
      "learning_rate": 0.0001608958407394241,
      "loss": 0.5041,
      "step": 2750
    },
    {
      "epoch": 0.9811589050835408,
      "grad_norm": 2.4708170890808105,
      "learning_rate": 0.00016075364379665837,
      "loss": 0.3898,
      "step": 2760
    },
    {
      "epoch": 0.984713828652684,
      "grad_norm": 5.599372386932373,
      "learning_rate": 0.00016061144685389263,
      "loss": 0.4827,
      "step": 2770
    },
    {
      "epoch": 0.9882687522218272,
      "grad_norm": 1.4011149406433105,
      "learning_rate": 0.00016046924991112692,
      "loss": 0.3605,
      "step": 2780
    },
    {
      "epoch": 0.9918236757909705,
      "grad_norm": 3.7904725074768066,
      "learning_rate": 0.00016032705296836118,
      "loss": 0.4493,
      "step": 2790
    },
    {
      "epoch": 0.9953785993601137,
      "grad_norm": 2.6256654262542725,
      "learning_rate": 0.00016018485602559544,
      "loss": 0.4868,
      "step": 2800
    },
    {
      "epoch": 0.998933522929257,
      "grad_norm": 1.2848936319351196,
      "learning_rate": 0.00016004265908282973,
      "loss": 0.4622,
      "step": 2810
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7956,
      "eval_f1_macro": 0.7937146842365239,
      "eval_f1_neg": 0.8134355604235123,
      "eval_f1_pos": 0.7739938080495357,
      "eval_loss": 0.44132909178733826,
      "eval_precision_global": 0.8080831753201099,
      "eval_precision_neg": 0.7446524064171123,
      "eval_precision_pos": 0.8715139442231076,
      "eval_recall_global": 0.7961603275878731,
      "eval_recall_neg": 0.8962188254223652,
      "eval_recall_pos": 0.6961018297533811,
      "eval_runtime": 56.8611,
      "eval_samples_per_second": 43.967,
      "eval_steps_per_second": 5.505,
      "step": 2813
    },
    {
      "epoch": 1.0024884464984003,
      "grad_norm": 1.6655397415161133,
      "learning_rate": 0.000159900462140064,
      "loss": 0.4928,
      "step": 2820
    },
    {
      "epoch": 1.0060433700675435,
      "grad_norm": 1.3571481704711914,
      "learning_rate": 0.00015975826519729826,
      "loss": 0.3819,
      "step": 2830
    },
    {
      "epoch": 1.0095982936366867,
      "grad_norm": 3.210447072982788,
      "learning_rate": 0.00015961606825453252,
      "loss": 0.4883,
      "step": 2840
    },
    {
      "epoch": 1.0131532172058302,
      "grad_norm": 2.514181613922119,
      "learning_rate": 0.0001594738713117668,
      "loss": 0.4236,
      "step": 2850
    },
    {
      "epoch": 1.0167081407749734,
      "grad_norm": 1.8976815938949585,
      "learning_rate": 0.00015933167436900107,
      "loss": 0.4325,
      "step": 2860
    },
    {
      "epoch": 1.0202630643441166,
      "grad_norm": 5.236108779907227,
      "learning_rate": 0.00015918947742623533,
      "loss": 0.392,
      "step": 2870
    },
    {
      "epoch": 1.0238179879132598,
      "grad_norm": 1.018488883972168,
      "learning_rate": 0.00015904728048346962,
      "loss": 0.4685,
      "step": 2880
    },
    {
      "epoch": 1.027372911482403,
      "grad_norm": 3.9125313758850098,
      "learning_rate": 0.00015890508354070388,
      "loss": 0.4814,
      "step": 2890
    },
    {
      "epoch": 1.0309278350515463,
      "grad_norm": 0.8325517177581787,
      "learning_rate": 0.00015876288659793814,
      "loss": 0.4266,
      "step": 2900
    },
    {
      "epoch": 1.0344827586206897,
      "grad_norm": 2.9488699436187744,
      "learning_rate": 0.00015862068965517243,
      "loss": 0.5,
      "step": 2910
    },
    {
      "epoch": 1.038037682189833,
      "grad_norm": 5.423763275146484,
      "learning_rate": 0.0001584784927124067,
      "loss": 0.5108,
      "step": 2920
    },
    {
      "epoch": 1.0415926057589762,
      "grad_norm": 1.9915192127227783,
      "learning_rate": 0.00015833629576964095,
      "loss": 0.4158,
      "step": 2930
    },
    {
      "epoch": 1.0451475293281194,
      "grad_norm": 3.424898624420166,
      "learning_rate": 0.00015819409882687524,
      "loss": 0.4252,
      "step": 2940
    },
    {
      "epoch": 1.0487024528972626,
      "grad_norm": 0.8381733298301697,
      "learning_rate": 0.0001580519018841095,
      "loss": 0.3684,
      "step": 2950
    },
    {
      "epoch": 1.052257376466406,
      "grad_norm": 1.64619779586792,
      "learning_rate": 0.00015790970494134377,
      "loss": 0.3884,
      "step": 2960
    },
    {
      "epoch": 1.0558123000355493,
      "grad_norm": 4.274718761444092,
      "learning_rate": 0.00015776750799857805,
      "loss": 0.3592,
      "step": 2970
    },
    {
      "epoch": 1.0593672236046925,
      "grad_norm": 5.5071024894714355,
      "learning_rate": 0.00015762531105581232,
      "loss": 0.4728,
      "step": 2980
    },
    {
      "epoch": 1.0629221471738357,
      "grad_norm": 1.122297763824463,
      "learning_rate": 0.00015748311411304658,
      "loss": 0.3907,
      "step": 2990
    },
    {
      "epoch": 1.066477070742979,
      "grad_norm": 1.876133680343628,
      "learning_rate": 0.00015734091717028087,
      "loss": 0.4465,
      "step": 3000
    },
    {
      "epoch": 1.0700319943121224,
      "grad_norm": 3.9092414379119873,
      "learning_rate": 0.00015719872022751513,
      "loss": 0.453,
      "step": 3010
    },
    {
      "epoch": 1.0735869178812656,
      "grad_norm": 2.9677586555480957,
      "learning_rate": 0.0001570565232847494,
      "loss": 0.4627,
      "step": 3020
    },
    {
      "epoch": 1.0771418414504088,
      "grad_norm": 5.761443614959717,
      "learning_rate": 0.00015691432634198365,
      "loss": 0.3969,
      "step": 3030
    },
    {
      "epoch": 1.080696765019552,
      "grad_norm": 3.4493534564971924,
      "learning_rate": 0.00015677212939921794,
      "loss": 0.4554,
      "step": 3040
    },
    {
      "epoch": 1.0842516885886953,
      "grad_norm": 0.7241718173027039,
      "learning_rate": 0.0001566299324564522,
      "loss": 0.4324,
      "step": 3050
    },
    {
      "epoch": 1.0878066121578387,
      "grad_norm": 1.297896146774292,
      "learning_rate": 0.00015648773551368646,
      "loss": 0.3924,
      "step": 3060
    },
    {
      "epoch": 1.091361535726982,
      "grad_norm": 1.8678874969482422,
      "learning_rate": 0.00015634553857092072,
      "loss": 0.4317,
      "step": 3070
    },
    {
      "epoch": 1.0949164592961251,
      "grad_norm": 2.6760475635528564,
      "learning_rate": 0.000156203341628155,
      "loss": 0.412,
      "step": 3080
    },
    {
      "epoch": 1.0984713828652684,
      "grad_norm": 1.261675477027893,
      "learning_rate": 0.00015606114468538928,
      "loss": 0.4092,
      "step": 3090
    },
    {
      "epoch": 1.1020263064344116,
      "grad_norm": 3.6757938861846924,
      "learning_rate": 0.00015591894774262354,
      "loss": 0.3405,
      "step": 3100
    },
    {
      "epoch": 1.105581230003555,
      "grad_norm": 4.326717376708984,
      "learning_rate": 0.0001557767507998578,
      "loss": 0.394,
      "step": 3110
    },
    {
      "epoch": 1.1091361535726982,
      "grad_norm": 1.3301929235458374,
      "learning_rate": 0.00015563455385709206,
      "loss": 0.3698,
      "step": 3120
    },
    {
      "epoch": 1.1126910771418415,
      "grad_norm": 3.692289352416992,
      "learning_rate": 0.00015549235691432635,
      "loss": 0.3951,
      "step": 3130
    },
    {
      "epoch": 1.1162460007109847,
      "grad_norm": 0.8935545086860657,
      "learning_rate": 0.0001553501599715606,
      "loss": 0.3774,
      "step": 3140
    },
    {
      "epoch": 1.119800924280128,
      "grad_norm": 1.4531371593475342,
      "learning_rate": 0.00015520796302879487,
      "loss": 0.3375,
      "step": 3150
    },
    {
      "epoch": 1.1233558478492713,
      "grad_norm": 0.6989932656288147,
      "learning_rate": 0.00015506576608602916,
      "loss": 0.3515,
      "step": 3160
    },
    {
      "epoch": 1.1269107714184146,
      "grad_norm": 3.375300407409668,
      "learning_rate": 0.00015492356914326342,
      "loss": 0.5049,
      "step": 3170
    },
    {
      "epoch": 1.1304656949875578,
      "grad_norm": 0.987343430519104,
      "learning_rate": 0.00015478137220049768,
      "loss": 0.3817,
      "step": 3180
    },
    {
      "epoch": 1.134020618556701,
      "grad_norm": 3.3528008460998535,
      "learning_rate": 0.00015463917525773197,
      "loss": 0.3275,
      "step": 3190
    },
    {
      "epoch": 1.1375755421258442,
      "grad_norm": 3.885150194168091,
      "learning_rate": 0.00015449697831496623,
      "loss": 0.3838,
      "step": 3200
    },
    {
      "epoch": 1.1411304656949874,
      "grad_norm": 4.577058792114258,
      "learning_rate": 0.0001543547813722005,
      "loss": 0.4085,
      "step": 3210
    },
    {
      "epoch": 1.1446853892641309,
      "grad_norm": 1.9993233680725098,
      "learning_rate": 0.00015421258442943479,
      "loss": 0.3352,
      "step": 3220
    },
    {
      "epoch": 1.148240312833274,
      "grad_norm": 2.838120698928833,
      "learning_rate": 0.00015407038748666905,
      "loss": 0.3811,
      "step": 3230
    },
    {
      "epoch": 1.1517952364024173,
      "grad_norm": 1.0693382024765015,
      "learning_rate": 0.0001539281905439033,
      "loss": 0.3734,
      "step": 3240
    },
    {
      "epoch": 1.1553501599715605,
      "grad_norm": 1.604543685913086,
      "learning_rate": 0.0001537859936011376,
      "loss": 0.4098,
      "step": 3250
    },
    {
      "epoch": 1.158905083540704,
      "grad_norm": 3.789198398590088,
      "learning_rate": 0.00015364379665837186,
      "loss": 0.4742,
      "step": 3260
    },
    {
      "epoch": 1.1624600071098472,
      "grad_norm": 2.221893310546875,
      "learning_rate": 0.00015350159971560612,
      "loss": 0.4068,
      "step": 3270
    },
    {
      "epoch": 1.1660149306789904,
      "grad_norm": 2.192840337753296,
      "learning_rate": 0.0001533594027728404,
      "loss": 0.4063,
      "step": 3280
    },
    {
      "epoch": 1.1695698542481336,
      "grad_norm": 1.8905593156814575,
      "learning_rate": 0.00015321720583007467,
      "loss": 0.4065,
      "step": 3290
    },
    {
      "epoch": 1.1731247778172769,
      "grad_norm": 1.3351342678070068,
      "learning_rate": 0.00015307500888730893,
      "loss": 0.4003,
      "step": 3300
    },
    {
      "epoch": 1.17667970138642,
      "grad_norm": 1.0650038719177246,
      "learning_rate": 0.00015293281194454322,
      "loss": 0.3474,
      "step": 3310
    },
    {
      "epoch": 1.1802346249555635,
      "grad_norm": 1.397690773010254,
      "learning_rate": 0.00015279061500177748,
      "loss": 0.4834,
      "step": 3320
    },
    {
      "epoch": 1.1837895485247067,
      "grad_norm": 3.4186017513275146,
      "learning_rate": 0.00015264841805901174,
      "loss": 0.4061,
      "step": 3330
    },
    {
      "epoch": 1.18734447209385,
      "grad_norm": 5.4354658126831055,
      "learning_rate": 0.000152506221116246,
      "loss": 0.4824,
      "step": 3340
    },
    {
      "epoch": 1.1908993956629932,
      "grad_norm": 4.036208629608154,
      "learning_rate": 0.00015236402417348027,
      "loss": 0.4178,
      "step": 3350
    },
    {
      "epoch": 1.1944543192321366,
      "grad_norm": 2.1612448692321777,
      "learning_rate": 0.00015222182723071453,
      "loss": 0.4158,
      "step": 3360
    },
    {
      "epoch": 1.1980092428012798,
      "grad_norm": 1.862813949584961,
      "learning_rate": 0.00015207963028794882,
      "loss": 0.3908,
      "step": 3370
    },
    {
      "epoch": 1.201564166370423,
      "grad_norm": 3.2875101566314697,
      "learning_rate": 0.00015193743334518308,
      "loss": 0.5189,
      "step": 3380
    },
    {
      "epoch": 1.2051190899395663,
      "grad_norm": 1.6391031742095947,
      "learning_rate": 0.00015179523640241734,
      "loss": 0.4524,
      "step": 3390
    },
    {
      "epoch": 1.2086740135087095,
      "grad_norm": 2.3005270957946777,
      "learning_rate": 0.00015165303945965163,
      "loss": 0.425,
      "step": 3400
    },
    {
      "epoch": 1.2122289370778527,
      "grad_norm": 3.9649150371551514,
      "learning_rate": 0.0001515108425168859,
      "loss": 0.3871,
      "step": 3410
    },
    {
      "epoch": 1.2157838606469962,
      "grad_norm": 0.9428673386573792,
      "learning_rate": 0.00015136864557412015,
      "loss": 0.4261,
      "step": 3420
    },
    {
      "epoch": 1.2193387842161394,
      "grad_norm": 6.605735778808594,
      "learning_rate": 0.00015122644863135442,
      "loss": 0.4202,
      "step": 3430
    },
    {
      "epoch": 1.2228937077852826,
      "grad_norm": 3.174891948699951,
      "learning_rate": 0.0001510842516885887,
      "loss": 0.4696,
      "step": 3440
    },
    {
      "epoch": 1.2264486313544258,
      "grad_norm": 3.8428070545196533,
      "learning_rate": 0.00015094205474582297,
      "loss": 0.394,
      "step": 3450
    },
    {
      "epoch": 1.230003554923569,
      "grad_norm": 1.4233968257904053,
      "learning_rate": 0.00015079985780305723,
      "loss": 0.4717,
      "step": 3460
    },
    {
      "epoch": 1.2335584784927125,
      "grad_norm": 1.5241405963897705,
      "learning_rate": 0.00015065766086029152,
      "loss": 0.3468,
      "step": 3470
    },
    {
      "epoch": 1.2371134020618557,
      "grad_norm": 1.7210347652435303,
      "learning_rate": 0.00015051546391752578,
      "loss": 0.3973,
      "step": 3480
    },
    {
      "epoch": 1.240668325630999,
      "grad_norm": 0.7447659969329834,
      "learning_rate": 0.00015037326697476004,
      "loss": 0.4502,
      "step": 3490
    },
    {
      "epoch": 1.2442232492001422,
      "grad_norm": 3.238734483718872,
      "learning_rate": 0.00015023107003199433,
      "loss": 0.4337,
      "step": 3500
    },
    {
      "epoch": 1.2477781727692854,
      "grad_norm": 2.184797763824463,
      "learning_rate": 0.0001500888730892286,
      "loss": 0.4353,
      "step": 3510
    },
    {
      "epoch": 1.2513330963384286,
      "grad_norm": 4.534921646118164,
      "learning_rate": 0.00014994667614646285,
      "loss": 0.431,
      "step": 3520
    },
    {
      "epoch": 1.254888019907572,
      "grad_norm": 2.3937859535217285,
      "learning_rate": 0.00014980447920369714,
      "loss": 0.4657,
      "step": 3530
    },
    {
      "epoch": 1.2584429434767153,
      "grad_norm": 3.074774742126465,
      "learning_rate": 0.0001496622822609314,
      "loss": 0.444,
      "step": 3540
    },
    {
      "epoch": 1.2619978670458585,
      "grad_norm": 1.7782500982284546,
      "learning_rate": 0.00014952008531816566,
      "loss": 0.3613,
      "step": 3550
    },
    {
      "epoch": 1.2655527906150017,
      "grad_norm": 1.3258048295974731,
      "learning_rate": 0.00014937788837539995,
      "loss": 0.4586,
      "step": 3560
    },
    {
      "epoch": 1.2691077141841451,
      "grad_norm": 1.4150588512420654,
      "learning_rate": 0.00014923569143263421,
      "loss": 0.4203,
      "step": 3570
    },
    {
      "epoch": 1.2726626377532884,
      "grad_norm": 4.002852439880371,
      "learning_rate": 0.00014909349448986848,
      "loss": 0.4548,
      "step": 3580
    },
    {
      "epoch": 1.2762175613224316,
      "grad_norm": 2.3821191787719727,
      "learning_rate": 0.00014895129754710276,
      "loss": 0.5174,
      "step": 3590
    },
    {
      "epoch": 1.2797724848915748,
      "grad_norm": 3.5102150440216064,
      "learning_rate": 0.00014880910060433703,
      "loss": 0.4956,
      "step": 3600
    },
    {
      "epoch": 1.283327408460718,
      "grad_norm": 6.993862628936768,
      "learning_rate": 0.0001486669036615713,
      "loss": 0.5513,
      "step": 3610
    },
    {
      "epoch": 1.2868823320298612,
      "grad_norm": 1.1498262882232666,
      "learning_rate": 0.00014852470671880555,
      "loss": 0.4815,
      "step": 3620
    },
    {
      "epoch": 1.2904372555990047,
      "grad_norm": 1.1596399545669556,
      "learning_rate": 0.0001483825097760398,
      "loss": 0.3956,
      "step": 3630
    },
    {
      "epoch": 1.293992179168148,
      "grad_norm": 2.781158447265625,
      "learning_rate": 0.00014824031283327407,
      "loss": 0.3686,
      "step": 3640
    },
    {
      "epoch": 1.2975471027372911,
      "grad_norm": 1.199354887008667,
      "learning_rate": 0.00014809811589050836,
      "loss": 0.5274,
      "step": 3650
    },
    {
      "epoch": 1.3011020263064343,
      "grad_norm": 1.4576565027236938,
      "learning_rate": 0.00014795591894774262,
      "loss": 0.3999,
      "step": 3660
    },
    {
      "epoch": 1.3046569498755778,
      "grad_norm": 1.624248743057251,
      "learning_rate": 0.00014781372200497689,
      "loss": 0.3461,
      "step": 3670
    },
    {
      "epoch": 1.308211873444721,
      "grad_norm": 2.5503339767456055,
      "learning_rate": 0.00014767152506221117,
      "loss": 0.4264,
      "step": 3680
    },
    {
      "epoch": 1.3117667970138642,
      "grad_norm": 1.343367099761963,
      "learning_rate": 0.00014752932811944544,
      "loss": 0.4559,
      "step": 3690
    },
    {
      "epoch": 1.3153217205830074,
      "grad_norm": 1.6846154928207397,
      "learning_rate": 0.0001473871311766797,
      "loss": 0.3917,
      "step": 3700
    },
    {
      "epoch": 1.3188766441521507,
      "grad_norm": 1.1491341590881348,
      "learning_rate": 0.00014724493423391396,
      "loss": 0.437,
      "step": 3710
    },
    {
      "epoch": 1.3224315677212939,
      "grad_norm": 1.970238208770752,
      "learning_rate": 0.00014710273729114825,
      "loss": 0.3884,
      "step": 3720
    },
    {
      "epoch": 1.3259864912904373,
      "grad_norm": 1.484695315361023,
      "learning_rate": 0.0001469605403483825,
      "loss": 0.4786,
      "step": 3730
    },
    {
      "epoch": 1.3295414148595806,
      "grad_norm": 1.7722115516662598,
      "learning_rate": 0.00014681834340561677,
      "loss": 0.3801,
      "step": 3740
    },
    {
      "epoch": 1.3330963384287238,
      "grad_norm": 3.583369016647339,
      "learning_rate": 0.00014667614646285106,
      "loss": 0.352,
      "step": 3750
    },
    {
      "epoch": 1.336651261997867,
      "grad_norm": 5.107283592224121,
      "learning_rate": 0.00014653394952008532,
      "loss": 0.4206,
      "step": 3760
    },
    {
      "epoch": 1.3402061855670104,
      "grad_norm": 2.752567768096924,
      "learning_rate": 0.00014639175257731958,
      "loss": 0.4101,
      "step": 3770
    },
    {
      "epoch": 1.3437611091361537,
      "grad_norm": 1.5095306634902954,
      "learning_rate": 0.00014624955563455387,
      "loss": 0.4112,
      "step": 3780
    },
    {
      "epoch": 1.3473160327052969,
      "grad_norm": 2.574457883834839,
      "learning_rate": 0.00014610735869178813,
      "loss": 0.4328,
      "step": 3790
    },
    {
      "epoch": 1.35087095627444,
      "grad_norm": 6.872222900390625,
      "learning_rate": 0.0001459651617490224,
      "loss": 0.4359,
      "step": 3800
    },
    {
      "epoch": 1.3544258798435833,
      "grad_norm": 3.8163864612579346,
      "learning_rate": 0.00014582296480625668,
      "loss": 0.4598,
      "step": 3810
    },
    {
      "epoch": 1.3579808034127265,
      "grad_norm": 4.868524074554443,
      "learning_rate": 0.00014568076786349095,
      "loss": 0.4121,
      "step": 3820
    },
    {
      "epoch": 1.3615357269818698,
      "grad_norm": 2.6572391986846924,
      "learning_rate": 0.0001455385709207252,
      "loss": 0.3807,
      "step": 3830
    },
    {
      "epoch": 1.3650906505510132,
      "grad_norm": 1.867331862449646,
      "learning_rate": 0.0001453963739779595,
      "loss": 0.4293,
      "step": 3840
    },
    {
      "epoch": 1.3686455741201564,
      "grad_norm": 3.437626600265503,
      "learning_rate": 0.00014525417703519376,
      "loss": 0.414,
      "step": 3850
    },
    {
      "epoch": 1.3722004976892996,
      "grad_norm": 2.0431957244873047,
      "learning_rate": 0.00014511198009242802,
      "loss": 0.362,
      "step": 3860
    },
    {
      "epoch": 1.375755421258443,
      "grad_norm": 0.8132467865943909,
      "learning_rate": 0.0001449697831496623,
      "loss": 0.3553,
      "step": 3870
    },
    {
      "epoch": 1.3793103448275863,
      "grad_norm": 4.657334327697754,
      "learning_rate": 0.00014482758620689657,
      "loss": 0.4256,
      "step": 3880
    },
    {
      "epoch": 1.3828652683967295,
      "grad_norm": 2.146864175796509,
      "learning_rate": 0.00014468538926413083,
      "loss": 0.4751,
      "step": 3890
    },
    {
      "epoch": 1.3864201919658727,
      "grad_norm": 5.0537919998168945,
      "learning_rate": 0.0001445431923213651,
      "loss": 0.4547,
      "step": 3900
    },
    {
      "epoch": 1.389975115535016,
      "grad_norm": 2.8403542041778564,
      "learning_rate": 0.00014440099537859938,
      "loss": 0.4119,
      "step": 3910
    },
    {
      "epoch": 1.3935300391041592,
      "grad_norm": 1.8484618663787842,
      "learning_rate": 0.00014425879843583364,
      "loss": 0.3988,
      "step": 3920
    },
    {
      "epoch": 1.3970849626733024,
      "grad_norm": 3.503229856491089,
      "learning_rate": 0.0001441166014930679,
      "loss": 0.4058,
      "step": 3930
    },
    {
      "epoch": 1.4006398862424458,
      "grad_norm": 2.3565878868103027,
      "learning_rate": 0.00014397440455030217,
      "loss": 0.4481,
      "step": 3940
    },
    {
      "epoch": 1.404194809811589,
      "grad_norm": 1.8389188051223755,
      "learning_rate": 0.00014383220760753643,
      "loss": 0.3313,
      "step": 3950
    },
    {
      "epoch": 1.4077497333807323,
      "grad_norm": 2.1354458332061768,
      "learning_rate": 0.00014369001066477072,
      "loss": 0.4061,
      "step": 3960
    },
    {
      "epoch": 1.4113046569498755,
      "grad_norm": 2.4495222568511963,
      "learning_rate": 0.00014354781372200498,
      "loss": 0.3863,
      "step": 3970
    },
    {
      "epoch": 1.414859580519019,
      "grad_norm": 2.7629404067993164,
      "learning_rate": 0.00014340561677923924,
      "loss": 0.2947,
      "step": 3980
    },
    {
      "epoch": 1.4184145040881622,
      "grad_norm": 1.0141273736953735,
      "learning_rate": 0.00014326341983647353,
      "loss": 0.4172,
      "step": 3990
    },
    {
      "epoch": 1.4219694276573054,
      "grad_norm": 1.3274555206298828,
      "learning_rate": 0.0001431212228937078,
      "loss": 0.3686,
      "step": 4000
    },
    {
      "epoch": 1.4255243512264486,
      "grad_norm": 7.257177829742432,
      "learning_rate": 0.00014297902595094205,
      "loss": 0.4367,
      "step": 4010
    },
    {
      "epoch": 1.4290792747955918,
      "grad_norm": 6.22064733505249,
      "learning_rate": 0.00014283682900817631,
      "loss": 0.422,
      "step": 4020
    },
    {
      "epoch": 1.432634198364735,
      "grad_norm": 2.058225154876709,
      "learning_rate": 0.0001426946320654106,
      "loss": 0.3847,
      "step": 4030
    },
    {
      "epoch": 1.4361891219338785,
      "grad_norm": 1.1365188360214233,
      "learning_rate": 0.00014255243512264486,
      "loss": 0.389,
      "step": 4040
    },
    {
      "epoch": 1.4397440455030217,
      "grad_norm": 1.400333285331726,
      "learning_rate": 0.00014241023817987913,
      "loss": 0.3728,
      "step": 4050
    },
    {
      "epoch": 1.443298969072165,
      "grad_norm": 2.3251841068267822,
      "learning_rate": 0.00014226804123711342,
      "loss": 0.5157,
      "step": 4060
    },
    {
      "epoch": 1.4468538926413081,
      "grad_norm": 3.3075320720672607,
      "learning_rate": 0.00014212584429434768,
      "loss": 0.4361,
      "step": 4070
    },
    {
      "epoch": 1.4504088162104516,
      "grad_norm": 5.449594020843506,
      "learning_rate": 0.00014198364735158194,
      "loss": 0.3613,
      "step": 4080
    },
    {
      "epoch": 1.4539637397795948,
      "grad_norm": 3.5881683826446533,
      "learning_rate": 0.00014184145040881623,
      "loss": 0.4292,
      "step": 4090
    },
    {
      "epoch": 1.457518663348738,
      "grad_norm": 3.005009889602661,
      "learning_rate": 0.0001416992534660505,
      "loss": 0.4545,
      "step": 4100
    },
    {
      "epoch": 1.4610735869178813,
      "grad_norm": 4.0536675453186035,
      "learning_rate": 0.00014155705652328475,
      "loss": 0.5109,
      "step": 4110
    },
    {
      "epoch": 1.4646285104870245,
      "grad_norm": 1.7486093044281006,
      "learning_rate": 0.00014141485958051904,
      "loss": 0.4813,
      "step": 4120
    },
    {
      "epoch": 1.4681834340561677,
      "grad_norm": 1.3576971292495728,
      "learning_rate": 0.0001412726626377533,
      "loss": 0.4123,
      "step": 4130
    },
    {
      "epoch": 1.4717383576253111,
      "grad_norm": 5.746690273284912,
      "learning_rate": 0.00014113046569498756,
      "loss": 0.5215,
      "step": 4140
    },
    {
      "epoch": 1.4752932811944544,
      "grad_norm": 0.7688632607460022,
      "learning_rate": 0.00014098826875222185,
      "loss": 0.3823,
      "step": 4150
    },
    {
      "epoch": 1.4788482047635976,
      "grad_norm": 1.2415984869003296,
      "learning_rate": 0.0001408460718094561,
      "loss": 0.4165,
      "step": 4160
    },
    {
      "epoch": 1.4824031283327408,
      "grad_norm": 5.407537937164307,
      "learning_rate": 0.00014070387486669037,
      "loss": 0.352,
      "step": 4170
    },
    {
      "epoch": 1.4859580519018842,
      "grad_norm": 1.771783709526062,
      "learning_rate": 0.00014056167792392466,
      "loss": 0.4511,
      "step": 4180
    },
    {
      "epoch": 1.4895129754710275,
      "grad_norm": 2.840488910675049,
      "learning_rate": 0.00014041948098115893,
      "loss": 0.4413,
      "step": 4190
    },
    {
      "epoch": 1.4930678990401707,
      "grad_norm": 3.157501697540283,
      "learning_rate": 0.0001402772840383932,
      "loss": 0.3502,
      "step": 4200
    },
    {
      "epoch": 1.496622822609314,
      "grad_norm": 2.764508008956909,
      "learning_rate": 0.00014013508709562745,
      "loss": 0.425,
      "step": 4210
    },
    {
      "epoch": 1.5001777461784571,
      "grad_norm": 1.0828279256820679,
      "learning_rate": 0.0001399928901528617,
      "loss": 0.3483,
      "step": 4220
    },
    {
      "epoch": 1.5037326697476003,
      "grad_norm": 1.5668201446533203,
      "learning_rate": 0.00013985069321009597,
      "loss": 0.4201,
      "step": 4230
    },
    {
      "epoch": 1.5072875933167436,
      "grad_norm": 4.587153434753418,
      "learning_rate": 0.00013970849626733026,
      "loss": 0.4102,
      "step": 4240
    },
    {
      "epoch": 1.510842516885887,
      "grad_norm": 2.0265116691589355,
      "learning_rate": 0.00013956629932456452,
      "loss": 0.3345,
      "step": 4250
    },
    {
      "epoch": 1.5143974404550302,
      "grad_norm": 1.881068468093872,
      "learning_rate": 0.00013942410238179878,
      "loss": 0.4383,
      "step": 4260
    },
    {
      "epoch": 1.5179523640241734,
      "grad_norm": 1.1979033946990967,
      "learning_rate": 0.00013928190543903307,
      "loss": 0.4171,
      "step": 4270
    },
    {
      "epoch": 1.5215072875933169,
      "grad_norm": 0.9554183483123779,
      "learning_rate": 0.00013913970849626733,
      "loss": 0.3718,
      "step": 4280
    },
    {
      "epoch": 1.52506221116246,
      "grad_norm": 2.816958427429199,
      "learning_rate": 0.0001389975115535016,
      "loss": 0.4127,
      "step": 4290
    },
    {
      "epoch": 1.5286171347316033,
      "grad_norm": 2.441953420639038,
      "learning_rate": 0.00013885531461073586,
      "loss": 0.3647,
      "step": 4300
    },
    {
      "epoch": 1.5321720583007465,
      "grad_norm": 1.533288836479187,
      "learning_rate": 0.00013871311766797015,
      "loss": 0.3675,
      "step": 4310
    },
    {
      "epoch": 1.5357269818698898,
      "grad_norm": 1.766777515411377,
      "learning_rate": 0.0001385709207252044,
      "loss": 0.4365,
      "step": 4320
    },
    {
      "epoch": 1.539281905439033,
      "grad_norm": 2.6789190769195557,
      "learning_rate": 0.00013842872378243867,
      "loss": 0.4343,
      "step": 4330
    },
    {
      "epoch": 1.5428368290081762,
      "grad_norm": 1.7619160413742065,
      "learning_rate": 0.00013828652683967296,
      "loss": 0.4348,
      "step": 4340
    },
    {
      "epoch": 1.5463917525773194,
      "grad_norm": 0.6310242414474487,
      "learning_rate": 0.00013814432989690722,
      "loss": 0.4874,
      "step": 4350
    },
    {
      "epoch": 1.5499466761464629,
      "grad_norm": 1.8300822973251343,
      "learning_rate": 0.00013800213295414148,
      "loss": 0.3882,
      "step": 4360
    },
    {
      "epoch": 1.553501599715606,
      "grad_norm": 2.8148183822631836,
      "learning_rate": 0.00013785993601137577,
      "loss": 0.4027,
      "step": 4370
    },
    {
      "epoch": 1.5570565232847495,
      "grad_norm": 0.955513596534729,
      "learning_rate": 0.00013771773906861003,
      "loss": 0.4083,
      "step": 4380
    },
    {
      "epoch": 1.5606114468538927,
      "grad_norm": 3.094334840774536,
      "learning_rate": 0.0001375755421258443,
      "loss": 0.4642,
      "step": 4390
    },
    {
      "epoch": 1.564166370423036,
      "grad_norm": 3.617480754852295,
      "learning_rate": 0.00013743334518307858,
      "loss": 0.2932,
      "step": 4400
    },
    {
      "epoch": 1.5677212939921792,
      "grad_norm": 1.2545394897460938,
      "learning_rate": 0.00013729114824031284,
      "loss": 0.4252,
      "step": 4410
    },
    {
      "epoch": 1.5712762175613224,
      "grad_norm": 2.9673588275909424,
      "learning_rate": 0.0001371489512975471,
      "loss": 0.4181,
      "step": 4420
    },
    {
      "epoch": 1.5748311411304656,
      "grad_norm": 1.7703744173049927,
      "learning_rate": 0.0001370067543547814,
      "loss": 0.4885,
      "step": 4430
    },
    {
      "epoch": 1.5783860646996088,
      "grad_norm": 4.024093151092529,
      "learning_rate": 0.00013686455741201566,
      "loss": 0.4776,
      "step": 4440
    },
    {
      "epoch": 1.581940988268752,
      "grad_norm": 2.671248197555542,
      "learning_rate": 0.00013672236046924992,
      "loss": 0.4262,
      "step": 4450
    },
    {
      "epoch": 1.5854959118378955,
      "grad_norm": 3.49684476852417,
      "learning_rate": 0.0001365801635264842,
      "loss": 0.4685,
      "step": 4460
    },
    {
      "epoch": 1.5890508354070387,
      "grad_norm": 2.1319937705993652,
      "learning_rate": 0.00013643796658371847,
      "loss": 0.3809,
      "step": 4470
    },
    {
      "epoch": 1.5926057589761822,
      "grad_norm": 1.4413634538650513,
      "learning_rate": 0.00013629576964095273,
      "loss": 0.4105,
      "step": 4480
    },
    {
      "epoch": 1.5961606825453254,
      "grad_norm": 0.9257795810699463,
      "learning_rate": 0.000136153572698187,
      "loss": 0.4425,
      "step": 4490
    },
    {
      "epoch": 1.5997156061144686,
      "grad_norm": 0.7958492040634155,
      "learning_rate": 0.00013601137575542125,
      "loss": 0.4699,
      "step": 4500
    },
    {
      "epoch": 1.6032705296836118,
      "grad_norm": 2.8868463039398193,
      "learning_rate": 0.00013586917881265552,
      "loss": 0.4229,
      "step": 4510
    },
    {
      "epoch": 1.606825453252755,
      "grad_norm": 0.39374157786369324,
      "learning_rate": 0.0001357269818698898,
      "loss": 0.4069,
      "step": 4520
    },
    {
      "epoch": 1.6103803768218983,
      "grad_norm": 2.7795684337615967,
      "learning_rate": 0.00013558478492712407,
      "loss": 0.3176,
      "step": 4530
    },
    {
      "epoch": 1.6139353003910415,
      "grad_norm": 1.9576536417007446,
      "learning_rate": 0.00013544258798435833,
      "loss": 0.4506,
      "step": 4540
    },
    {
      "epoch": 1.6174902239601847,
      "grad_norm": 3.136056423187256,
      "learning_rate": 0.00013530039104159262,
      "loss": 0.3744,
      "step": 4550
    },
    {
      "epoch": 1.6210451475293282,
      "grad_norm": 5.012406826019287,
      "learning_rate": 0.00013515819409882688,
      "loss": 0.3857,
      "step": 4560
    },
    {
      "epoch": 1.6246000710984714,
      "grad_norm": 1.9263415336608887,
      "learning_rate": 0.00013501599715606114,
      "loss": 0.3661,
      "step": 4570
    },
    {
      "epoch": 1.6281549946676146,
      "grad_norm": 1.5761315822601318,
      "learning_rate": 0.00013487380021329543,
      "loss": 0.3123,
      "step": 4580
    },
    {
      "epoch": 1.631709918236758,
      "grad_norm": 2.5586154460906982,
      "learning_rate": 0.0001347316032705297,
      "loss": 0.3866,
      "step": 4590
    },
    {
      "epoch": 1.6352648418059013,
      "grad_norm": 1.6515222787857056,
      "learning_rate": 0.00013458940632776395,
      "loss": 0.3923,
      "step": 4600
    },
    {
      "epoch": 1.6388197653750445,
      "grad_norm": 1.5176160335540771,
      "learning_rate": 0.0001344472093849982,
      "loss": 0.3307,
      "step": 4610
    },
    {
      "epoch": 1.6423746889441877,
      "grad_norm": 4.010136127471924,
      "learning_rate": 0.0001343050124422325,
      "loss": 0.4231,
      "step": 4620
    },
    {
      "epoch": 1.645929612513331,
      "grad_norm": 0.757968008518219,
      "learning_rate": 0.00013416281549946676,
      "loss": 0.3165,
      "step": 4630
    },
    {
      "epoch": 1.6494845360824741,
      "grad_norm": 1.3470019102096558,
      "learning_rate": 0.00013402061855670103,
      "loss": 0.3748,
      "step": 4640
    },
    {
      "epoch": 1.6530394596516174,
      "grad_norm": 2.19628643989563,
      "learning_rate": 0.00013387842161393531,
      "loss": 0.4023,
      "step": 4650
    },
    {
      "epoch": 1.6565943832207608,
      "grad_norm": 3.8666133880615234,
      "learning_rate": 0.00013373622467116958,
      "loss": 0.4002,
      "step": 4660
    },
    {
      "epoch": 1.660149306789904,
      "grad_norm": 0.7556409239768982,
      "learning_rate": 0.00013359402772840384,
      "loss": 0.3337,
      "step": 4670
    },
    {
      "epoch": 1.6637042303590472,
      "grad_norm": 2.383657217025757,
      "learning_rate": 0.00013345183078563813,
      "loss": 0.3744,
      "step": 4680
    },
    {
      "epoch": 1.6672591539281907,
      "grad_norm": 3.9192898273468018,
      "learning_rate": 0.0001333096338428724,
      "loss": 0.401,
      "step": 4690
    },
    {
      "epoch": 1.670814077497334,
      "grad_norm": 1.7168611288070679,
      "learning_rate": 0.00013316743690010665,
      "loss": 0.5033,
      "step": 4700
    },
    {
      "epoch": 1.6743690010664771,
      "grad_norm": 2.4807515144348145,
      "learning_rate": 0.00013302523995734094,
      "loss": 0.4615,
      "step": 4710
    },
    {
      "epoch": 1.6779239246356203,
      "grad_norm": 4.929152488708496,
      "learning_rate": 0.0001328830430145752,
      "loss": 0.3659,
      "step": 4720
    },
    {
      "epoch": 1.6814788482047636,
      "grad_norm": 1.225122094154358,
      "learning_rate": 0.00013274084607180946,
      "loss": 0.4431,
      "step": 4730
    },
    {
      "epoch": 1.6850337717739068,
      "grad_norm": 7.903830051422119,
      "learning_rate": 0.00013259864912904375,
      "loss": 0.4083,
      "step": 4740
    },
    {
      "epoch": 1.68858869534305,
      "grad_norm": 0.7504833340644836,
      "learning_rate": 0.000132456452186278,
      "loss": 0.4295,
      "step": 4750
    },
    {
      "epoch": 1.6921436189121932,
      "grad_norm": 1.1011013984680176,
      "learning_rate": 0.00013231425524351227,
      "loss": 0.3597,
      "step": 4760
    },
    {
      "epoch": 1.6956985424813367,
      "grad_norm": 6.777919769287109,
      "learning_rate": 0.00013217205830074654,
      "loss": 0.4081,
      "step": 4770
    },
    {
      "epoch": 1.6992534660504799,
      "grad_norm": 2.0813851356506348,
      "learning_rate": 0.00013202986135798082,
      "loss": 0.4922,
      "step": 4780
    },
    {
      "epoch": 1.7028083896196233,
      "grad_norm": 2.5261502265930176,
      "learning_rate": 0.00013188766441521509,
      "loss": 0.4503,
      "step": 4790
    },
    {
      "epoch": 1.7063633131887666,
      "grad_norm": 3.2478222846984863,
      "learning_rate": 0.00013174546747244935,
      "loss": 0.4551,
      "step": 4800
    },
    {
      "epoch": 1.7099182367579098,
      "grad_norm": 2.21451735496521,
      "learning_rate": 0.0001316032705296836,
      "loss": 0.3697,
      "step": 4810
    },
    {
      "epoch": 1.713473160327053,
      "grad_norm": 1.7233251333236694,
      "learning_rate": 0.00013146107358691787,
      "loss": 0.4219,
      "step": 4820
    },
    {
      "epoch": 1.7170280838961962,
      "grad_norm": 5.547037124633789,
      "learning_rate": 0.00013131887664415216,
      "loss": 0.3947,
      "step": 4830
    },
    {
      "epoch": 1.7205830074653394,
      "grad_norm": 3.2643609046936035,
      "learning_rate": 0.00013117667970138642,
      "loss": 0.4801,
      "step": 4840
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 2.4825706481933594,
      "learning_rate": 0.00013103448275862068,
      "loss": 0.3084,
      "step": 4850
    },
    {
      "epoch": 1.7276928546036259,
      "grad_norm": 3.8660800457000732,
      "learning_rate": 0.00013089228581585497,
      "loss": 0.5095,
      "step": 4860
    },
    {
      "epoch": 1.7312477781727693,
      "grad_norm": 7.365355491638184,
      "learning_rate": 0.00013075008887308923,
      "loss": 0.4434,
      "step": 4870
    },
    {
      "epoch": 1.7348027017419125,
      "grad_norm": 4.169778823852539,
      "learning_rate": 0.0001306078919303235,
      "loss": 0.3753,
      "step": 4880
    },
    {
      "epoch": 1.738357625311056,
      "grad_norm": 0.6571418642997742,
      "learning_rate": 0.00013046569498755776,
      "loss": 0.3603,
      "step": 4890
    },
    {
      "epoch": 1.7419125488801992,
      "grad_norm": 1.8990724086761475,
      "learning_rate": 0.00013032349804479205,
      "loss": 0.3835,
      "step": 4900
    },
    {
      "epoch": 1.7454674724493424,
      "grad_norm": 1.3628591299057007,
      "learning_rate": 0.0001301813011020263,
      "loss": 0.4379,
      "step": 4910
    },
    {
      "epoch": 1.7490223960184856,
      "grad_norm": 5.618069648742676,
      "learning_rate": 0.00013003910415926057,
      "loss": 0.469,
      "step": 4920
    },
    {
      "epoch": 1.7525773195876289,
      "grad_norm": 5.513634204864502,
      "learning_rate": 0.00012989690721649486,
      "loss": 0.3607,
      "step": 4930
    },
    {
      "epoch": 1.756132243156772,
      "grad_norm": 2.921001672744751,
      "learning_rate": 0.00012975471027372912,
      "loss": 0.4961,
      "step": 4940
    },
    {
      "epoch": 1.7596871667259153,
      "grad_norm": 2.262559175491333,
      "learning_rate": 0.00012961251333096338,
      "loss": 0.4574,
      "step": 4950
    },
    {
      "epoch": 1.7632420902950585,
      "grad_norm": 5.2904815673828125,
      "learning_rate": 0.00012947031638819767,
      "loss": 0.5871,
      "step": 4960
    },
    {
      "epoch": 1.766797013864202,
      "grad_norm": 2.082887887954712,
      "learning_rate": 0.00012932811944543193,
      "loss": 0.4605,
      "step": 4970
    },
    {
      "epoch": 1.7703519374333452,
      "grad_norm": 0.7156855463981628,
      "learning_rate": 0.0001291859225026662,
      "loss": 0.4618,
      "step": 4980
    },
    {
      "epoch": 1.7739068610024884,
      "grad_norm": 0.7048903107643127,
      "learning_rate": 0.00012904372555990048,
      "loss": 0.4043,
      "step": 4990
    },
    {
      "epoch": 1.7774617845716318,
      "grad_norm": 2.590019464492798,
      "learning_rate": 0.00012890152861713474,
      "loss": 0.3701,
      "step": 5000
    },
    {
      "epoch": 1.781016708140775,
      "grad_norm": 4.797647953033447,
      "learning_rate": 0.000128759331674369,
      "loss": 0.391,
      "step": 5010
    },
    {
      "epoch": 1.7845716317099183,
      "grad_norm": 1.5014867782592773,
      "learning_rate": 0.0001286171347316033,
      "loss": 0.4207,
      "step": 5020
    },
    {
      "epoch": 1.7881265552790615,
      "grad_norm": 1.8433735370635986,
      "learning_rate": 0.00012847493778883756,
      "loss": 0.4783,
      "step": 5030
    },
    {
      "epoch": 1.7916814788482047,
      "grad_norm": 2.8532259464263916,
      "learning_rate": 0.00012833274084607182,
      "loss": 0.4147,
      "step": 5040
    },
    {
      "epoch": 1.795236402417348,
      "grad_norm": 3.7453958988189697,
      "learning_rate": 0.0001281905439033061,
      "loss": 0.4848,
      "step": 5050
    },
    {
      "epoch": 1.7987913259864912,
      "grad_norm": 1.7062923908233643,
      "learning_rate": 0.00012804834696054037,
      "loss": 0.3795,
      "step": 5060
    },
    {
      "epoch": 1.8023462495556346,
      "grad_norm": 3.1361899375915527,
      "learning_rate": 0.00012790615001777463,
      "loss": 0.3884,
      "step": 5070
    },
    {
      "epoch": 1.8059011731247778,
      "grad_norm": 3.911566734313965,
      "learning_rate": 0.0001277639530750089,
      "loss": 0.4049,
      "step": 5080
    },
    {
      "epoch": 1.809456096693921,
      "grad_norm": 5.078324794769287,
      "learning_rate": 0.00012762175613224315,
      "loss": 0.3846,
      "step": 5090
    },
    {
      "epoch": 1.8130110202630645,
      "grad_norm": 0.593169629573822,
      "learning_rate": 0.00012747955918947741,
      "loss": 0.3939,
      "step": 5100
    },
    {
      "epoch": 1.8165659438322077,
      "grad_norm": 3.445998191833496,
      "learning_rate": 0.0001273373622467117,
      "loss": 0.3764,
      "step": 5110
    },
    {
      "epoch": 1.820120867401351,
      "grad_norm": 3.674372434616089,
      "learning_rate": 0.00012719516530394596,
      "loss": 0.4242,
      "step": 5120
    },
    {
      "epoch": 1.8236757909704941,
      "grad_norm": 0.8294265866279602,
      "learning_rate": 0.00012705296836118023,
      "loss": 0.376,
      "step": 5130
    },
    {
      "epoch": 1.8272307145396374,
      "grad_norm": 2.583573579788208,
      "learning_rate": 0.00012691077141841451,
      "loss": 0.4369,
      "step": 5140
    },
    {
      "epoch": 1.8307856381087806,
      "grad_norm": 1.172696590423584,
      "learning_rate": 0.00012676857447564878,
      "loss": 0.3444,
      "step": 5150
    },
    {
      "epoch": 1.8343405616779238,
      "grad_norm": 3.9097700119018555,
      "learning_rate": 0.00012662637753288304,
      "loss": 0.3209,
      "step": 5160
    },
    {
      "epoch": 1.837895485247067,
      "grad_norm": 1.0202739238739014,
      "learning_rate": 0.00012648418059011733,
      "loss": 0.4065,
      "step": 5170
    },
    {
      "epoch": 1.8414504088162105,
      "grad_norm": 5.030560493469238,
      "learning_rate": 0.0001263419836473516,
      "loss": 0.5017,
      "step": 5180
    },
    {
      "epoch": 1.8450053323853537,
      "grad_norm": 2.5364341735839844,
      "learning_rate": 0.00012619978670458585,
      "loss": 0.4853,
      "step": 5190
    },
    {
      "epoch": 1.8485602559544971,
      "grad_norm": 1.1636422872543335,
      "learning_rate": 0.0001260575897618201,
      "loss": 0.4306,
      "step": 5200
    },
    {
      "epoch": 1.8521151795236404,
      "grad_norm": 5.8969926834106445,
      "learning_rate": 0.0001259153928190544,
      "loss": 0.5876,
      "step": 5210
    },
    {
      "epoch": 1.8556701030927836,
      "grad_norm": 3.504162311553955,
      "learning_rate": 0.00012577319587628866,
      "loss": 0.3913,
      "step": 5220
    },
    {
      "epoch": 1.8592250266619268,
      "grad_norm": 0.9876344203948975,
      "learning_rate": 0.00012563099893352292,
      "loss": 0.3691,
      "step": 5230
    },
    {
      "epoch": 1.86277995023107,
      "grad_norm": 5.475845813751221,
      "learning_rate": 0.0001254888019907572,
      "loss": 0.4206,
      "step": 5240
    },
    {
      "epoch": 1.8663348738002132,
      "grad_norm": 4.254812717437744,
      "learning_rate": 0.00012534660504799147,
      "loss": 0.4826,
      "step": 5250
    },
    {
      "epoch": 1.8698897973693565,
      "grad_norm": 3.907266139984131,
      "learning_rate": 0.00012520440810522574,
      "loss": 0.4599,
      "step": 5260
    },
    {
      "epoch": 1.8734447209384997,
      "grad_norm": 1.1490696668624878,
      "learning_rate": 0.00012506221116246002,
      "loss": 0.4001,
      "step": 5270
    },
    {
      "epoch": 1.8769996445076431,
      "grad_norm": 2.4285666942596436,
      "learning_rate": 0.0001249200142196943,
      "loss": 0.5026,
      "step": 5280
    },
    {
      "epoch": 1.8805545680767863,
      "grad_norm": 2.3468172550201416,
      "learning_rate": 0.00012477781727692855,
      "loss": 0.3629,
      "step": 5290
    },
    {
      "epoch": 1.8841094916459296,
      "grad_norm": 1.0061686038970947,
      "learning_rate": 0.00012463562033416284,
      "loss": 0.4531,
      "step": 5300
    },
    {
      "epoch": 1.887664415215073,
      "grad_norm": 2.146210193634033,
      "learning_rate": 0.0001244934233913971,
      "loss": 0.3826,
      "step": 5310
    },
    {
      "epoch": 1.8912193387842162,
      "grad_norm": 1.7335233688354492,
      "learning_rate": 0.00012435122644863136,
      "loss": 0.4416,
      "step": 5320
    },
    {
      "epoch": 1.8947742623533594,
      "grad_norm": 2.21732497215271,
      "learning_rate": 0.00012420902950586565,
      "loss": 0.3875,
      "step": 5330
    },
    {
      "epoch": 1.8983291859225027,
      "grad_norm": 1.0620509386062622,
      "learning_rate": 0.0001240668325630999,
      "loss": 0.3836,
      "step": 5340
    },
    {
      "epoch": 1.9018841094916459,
      "grad_norm": 2.3964009284973145,
      "learning_rate": 0.00012392463562033417,
      "loss": 0.3743,
      "step": 5350
    },
    {
      "epoch": 1.905439033060789,
      "grad_norm": 1.1146577596664429,
      "learning_rate": 0.00012378243867756843,
      "loss": 0.435,
      "step": 5360
    },
    {
      "epoch": 1.9089939566299323,
      "grad_norm": 2.1808719635009766,
      "learning_rate": 0.0001236402417348027,
      "loss": 0.3865,
      "step": 5370
    },
    {
      "epoch": 1.9125488801990758,
      "grad_norm": 2.912276268005371,
      "learning_rate": 0.00012349804479203696,
      "loss": 0.4216,
      "step": 5380
    },
    {
      "epoch": 1.916103803768219,
      "grad_norm": 0.5452566742897034,
      "learning_rate": 0.00012335584784927125,
      "loss": 0.4456,
      "step": 5390
    },
    {
      "epoch": 1.9196587273373622,
      "grad_norm": 2.7402327060699463,
      "learning_rate": 0.0001232136509065055,
      "loss": 0.4315,
      "step": 5400
    },
    {
      "epoch": 1.9232136509065056,
      "grad_norm": 0.7993608713150024,
      "learning_rate": 0.00012307145396373977,
      "loss": 0.3493,
      "step": 5410
    },
    {
      "epoch": 1.9267685744756489,
      "grad_norm": 2.465360641479492,
      "learning_rate": 0.00012292925702097406,
      "loss": 0.4869,
      "step": 5420
    },
    {
      "epoch": 1.930323498044792,
      "grad_norm": 0.9386070966720581,
      "learning_rate": 0.00012278706007820832,
      "loss": 0.2722,
      "step": 5430
    },
    {
      "epoch": 1.9338784216139353,
      "grad_norm": 3.9434049129486084,
      "learning_rate": 0.00012264486313544258,
      "loss": 0.3445,
      "step": 5440
    },
    {
      "epoch": 1.9374333451830785,
      "grad_norm": 1.862243413925171,
      "learning_rate": 0.00012250266619267687,
      "loss": 0.522,
      "step": 5450
    },
    {
      "epoch": 1.9409882687522217,
      "grad_norm": 2.90763258934021,
      "learning_rate": 0.00012236046924991113,
      "loss": 0.6126,
      "step": 5460
    },
    {
      "epoch": 1.944543192321365,
      "grad_norm": 1.9836353063583374,
      "learning_rate": 0.0001222182723071454,
      "loss": 0.4322,
      "step": 5470
    },
    {
      "epoch": 1.9480981158905084,
      "grad_norm": 2.162787675857544,
      "learning_rate": 0.00012207607536437966,
      "loss": 0.3563,
      "step": 5480
    },
    {
      "epoch": 1.9516530394596516,
      "grad_norm": 0.8419181704521179,
      "learning_rate": 0.00012193387842161394,
      "loss": 0.3646,
      "step": 5490
    },
    {
      "epoch": 1.9552079630287948,
      "grad_norm": 2.0152885913848877,
      "learning_rate": 0.0001217916814788482,
      "loss": 0.3932,
      "step": 5500
    },
    {
      "epoch": 1.9587628865979383,
      "grad_norm": 3.8103277683258057,
      "learning_rate": 0.00012164948453608247,
      "loss": 0.5221,
      "step": 5510
    },
    {
      "epoch": 1.9623178101670815,
      "grad_norm": 0.6785652041435242,
      "learning_rate": 0.00012150728759331676,
      "loss": 0.4207,
      "step": 5520
    },
    {
      "epoch": 1.9658727337362247,
      "grad_norm": 1.973351001739502,
      "learning_rate": 0.00012136509065055102,
      "loss": 0.3566,
      "step": 5530
    },
    {
      "epoch": 1.969427657305368,
      "grad_norm": 0.4623633921146393,
      "learning_rate": 0.00012122289370778528,
      "loss": 0.3447,
      "step": 5540
    },
    {
      "epoch": 1.9729825808745112,
      "grad_norm": 1.9000844955444336,
      "learning_rate": 0.00012108069676501957,
      "loss": 0.4793,
      "step": 5550
    },
    {
      "epoch": 1.9765375044436544,
      "grad_norm": 1.3363327980041504,
      "learning_rate": 0.00012093849982225383,
      "loss": 0.302,
      "step": 5560
    },
    {
      "epoch": 1.9800924280127976,
      "grad_norm": 0.7662012577056885,
      "learning_rate": 0.00012079630287948809,
      "loss": 0.3871,
      "step": 5570
    },
    {
      "epoch": 1.9836473515819408,
      "grad_norm": 7.1869306564331055,
      "learning_rate": 0.00012065410593672237,
      "loss": 0.4252,
      "step": 5580
    },
    {
      "epoch": 1.9872022751510843,
      "grad_norm": 1.221182107925415,
      "learning_rate": 0.00012051190899395663,
      "loss": 0.377,
      "step": 5590
    },
    {
      "epoch": 1.9907571987202275,
      "grad_norm": 1.6830369234085083,
      "learning_rate": 0.0001203697120511909,
      "loss": 0.4061,
      "step": 5600
    },
    {
      "epoch": 1.994312122289371,
      "grad_norm": 4.524092197418213,
      "learning_rate": 0.00012022751510842518,
      "loss": 0.3362,
      "step": 5610
    },
    {
      "epoch": 1.9978670458585142,
      "grad_norm": 1.6412944793701172,
      "learning_rate": 0.00012008531816565944,
      "loss": 0.3396,
      "step": 5620
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8272,
      "eval_f1_macro": 0.8270794813152975,
      "eval_f1_neg": 0.8316445830085737,
      "eval_f1_pos": 0.8225143796220213,
      "eval_loss": 0.40324506163597107,
      "eval_precision_global": 0.8284838338242877,
      "eval_precision_neg": 0.8065003779289494,
      "eval_precision_pos": 0.8504672897196262,
      "eval_recall_global": 0.8273737864419428,
      "eval_recall_neg": 0.8584070796460177,
      "eval_recall_pos": 0.7963404932378679,
      "eval_runtime": 56.8277,
      "eval_samples_per_second": 43.993,
      "eval_steps_per_second": 5.508,
      "step": 5626
    },
    {
      "epoch": 2.0014219694276574,
      "grad_norm": 1.6576815843582153,
      "learning_rate": 0.0001199431212228937,
      "loss": 0.3363,
      "step": 5630
    },
    {
      "epoch": 2.0049768929968006,
      "grad_norm": 4.678613662719727,
      "learning_rate": 0.00011980092428012799,
      "loss": 0.3982,
      "step": 5640
    },
    {
      "epoch": 2.008531816565944,
      "grad_norm": 0.8918868899345398,
      "learning_rate": 0.00011965872733736225,
      "loss": 0.3541,
      "step": 5650
    },
    {
      "epoch": 2.012086740135087,
      "grad_norm": 1.9400179386138916,
      "learning_rate": 0.00011951653039459651,
      "loss": 0.4173,
      "step": 5660
    },
    {
      "epoch": 2.0156416637042303,
      "grad_norm": 1.15445876121521,
      "learning_rate": 0.0001193743334518308,
      "loss": 0.3578,
      "step": 5670
    },
    {
      "epoch": 2.0191965872733735,
      "grad_norm": 2.0515012741088867,
      "learning_rate": 0.00011923213650906506,
      "loss": 0.5345,
      "step": 5680
    },
    {
      "epoch": 2.0227515108425167,
      "grad_norm": 1.958544135093689,
      "learning_rate": 0.00011908993956629933,
      "loss": 0.4511,
      "step": 5690
    },
    {
      "epoch": 2.0263064344116604,
      "grad_norm": 1.5236200094223022,
      "learning_rate": 0.00011894774262353362,
      "loss": 0.3283,
      "step": 5700
    },
    {
      "epoch": 2.0298613579808036,
      "grad_norm": 1.7933573722839355,
      "learning_rate": 0.00011880554568076788,
      "loss": 0.3381,
      "step": 5710
    },
    {
      "epoch": 2.033416281549947,
      "grad_norm": 0.6019594669342041,
      "learning_rate": 0.00011866334873800214,
      "loss": 0.4105,
      "step": 5720
    },
    {
      "epoch": 2.03697120511909,
      "grad_norm": 5.429192066192627,
      "learning_rate": 0.00011852115179523641,
      "loss": 0.4454,
      "step": 5730
    },
    {
      "epoch": 2.0405261286882332,
      "grad_norm": 0.8611922264099121,
      "learning_rate": 0.00011837895485247068,
      "loss": 0.3226,
      "step": 5740
    },
    {
      "epoch": 2.0440810522573765,
      "grad_norm": 2.8605151176452637,
      "learning_rate": 0.00011823675790970494,
      "loss": 0.4136,
      "step": 5750
    },
    {
      "epoch": 2.0476359758265197,
      "grad_norm": 2.391462802886963,
      "learning_rate": 0.00011809456096693923,
      "loss": 0.472,
      "step": 5760
    },
    {
      "epoch": 2.051190899395663,
      "grad_norm": 2.774988889694214,
      "learning_rate": 0.00011795236402417349,
      "loss": 0.382,
      "step": 5770
    },
    {
      "epoch": 2.054745822964806,
      "grad_norm": 1.3447870016098022,
      "learning_rate": 0.00011781016708140775,
      "loss": 0.4016,
      "step": 5780
    },
    {
      "epoch": 2.0583007465339493,
      "grad_norm": 1.7344259023666382,
      "learning_rate": 0.00011766797013864201,
      "loss": 0.3711,
      "step": 5790
    },
    {
      "epoch": 2.0618556701030926,
      "grad_norm": 5.895674705505371,
      "learning_rate": 0.0001175257731958763,
      "loss": 0.4962,
      "step": 5800
    },
    {
      "epoch": 2.0654105936722362,
      "grad_norm": 3.6969127655029297,
      "learning_rate": 0.00011738357625311056,
      "loss": 0.3479,
      "step": 5810
    },
    {
      "epoch": 2.0689655172413794,
      "grad_norm": 1.1580616235733032,
      "learning_rate": 0.00011724137931034482,
      "loss": 0.401,
      "step": 5820
    },
    {
      "epoch": 2.0725204408105227,
      "grad_norm": 2.488861322402954,
      "learning_rate": 0.00011709918236757911,
      "loss": 0.4353,
      "step": 5830
    },
    {
      "epoch": 2.076075364379666,
      "grad_norm": 3.505368232727051,
      "learning_rate": 0.00011695698542481337,
      "loss": 0.445,
      "step": 5840
    },
    {
      "epoch": 2.079630287948809,
      "grad_norm": 1.6646314859390259,
      "learning_rate": 0.00011681478848204764,
      "loss": 0.3469,
      "step": 5850
    },
    {
      "epoch": 2.0831852115179523,
      "grad_norm": 0.8983281254768372,
      "learning_rate": 0.00011667259153928191,
      "loss": 0.3089,
      "step": 5860
    },
    {
      "epoch": 2.0867401350870955,
      "grad_norm": 2.720797061920166,
      "learning_rate": 0.00011653039459651619,
      "loss": 0.406,
      "step": 5870
    },
    {
      "epoch": 2.0902950586562388,
      "grad_norm": 3.7890121936798096,
      "learning_rate": 0.00011638819765375045,
      "loss": 0.417,
      "step": 5880
    },
    {
      "epoch": 2.093849982225382,
      "grad_norm": 3.1719810962677,
      "learning_rate": 0.00011624600071098472,
      "loss": 0.3573,
      "step": 5890
    },
    {
      "epoch": 2.097404905794525,
      "grad_norm": 1.983666181564331,
      "learning_rate": 0.00011610380376821898,
      "loss": 0.4284,
      "step": 5900
    },
    {
      "epoch": 2.100959829363669,
      "grad_norm": 2.423609495162964,
      "learning_rate": 0.00011596160682545325,
      "loss": 0.4255,
      "step": 5910
    },
    {
      "epoch": 2.104514752932812,
      "grad_norm": 1.7912209033966064,
      "learning_rate": 0.00011581940988268753,
      "loss": 0.3473,
      "step": 5920
    },
    {
      "epoch": 2.1080696765019553,
      "grad_norm": 1.3065968751907349,
      "learning_rate": 0.0001156772129399218,
      "loss": 0.3442,
      "step": 5930
    },
    {
      "epoch": 2.1116246000710985,
      "grad_norm": 5.233953475952148,
      "learning_rate": 0.00011553501599715606,
      "loss": 0.412,
      "step": 5940
    },
    {
      "epoch": 2.1151795236402418,
      "grad_norm": 2.04306697845459,
      "learning_rate": 0.00011539281905439035,
      "loss": 0.4353,
      "step": 5950
    },
    {
      "epoch": 2.118734447209385,
      "grad_norm": 1.4722715616226196,
      "learning_rate": 0.00011525062211162461,
      "loss": 0.3663,
      "step": 5960
    },
    {
      "epoch": 2.122289370778528,
      "grad_norm": 1.8264634609222412,
      "learning_rate": 0.00011510842516885887,
      "loss": 0.3879,
      "step": 5970
    },
    {
      "epoch": 2.1258442943476714,
      "grad_norm": 3.0302517414093018,
      "learning_rate": 0.00011496622822609316,
      "loss": 0.3554,
      "step": 5980
    },
    {
      "epoch": 2.1293992179168146,
      "grad_norm": 2.3395421504974365,
      "learning_rate": 0.00011482403128332742,
      "loss": 0.3165,
      "step": 5990
    },
    {
      "epoch": 2.132954141485958,
      "grad_norm": 1.4679234027862549,
      "learning_rate": 0.00011468183434056168,
      "loss": 0.3696,
      "step": 6000
    },
    {
      "epoch": 2.1365090650551015,
      "grad_norm": 0.9620301127433777,
      "learning_rate": 0.00011453963739779596,
      "loss": 0.3981,
      "step": 6010
    },
    {
      "epoch": 2.1400639886242447,
      "grad_norm": 1.2210932970046997,
      "learning_rate": 0.00011439744045503022,
      "loss": 0.344,
      "step": 6020
    },
    {
      "epoch": 2.143618912193388,
      "grad_norm": 6.022966384887695,
      "learning_rate": 0.00011425524351226448,
      "loss": 0.3387,
      "step": 6030
    },
    {
      "epoch": 2.147173835762531,
      "grad_norm": 4.9372239112854,
      "learning_rate": 0.00011411304656949877,
      "loss": 0.456,
      "step": 6040
    },
    {
      "epoch": 2.1507287593316744,
      "grad_norm": 4.16904878616333,
      "learning_rate": 0.00011397084962673303,
      "loss": 0.3997,
      "step": 6050
    },
    {
      "epoch": 2.1542836829008176,
      "grad_norm": 0.829242467880249,
      "learning_rate": 0.00011382865268396729,
      "loss": 0.3696,
      "step": 6060
    },
    {
      "epoch": 2.157838606469961,
      "grad_norm": 3.5486466884613037,
      "learning_rate": 0.00011368645574120158,
      "loss": 0.3427,
      "step": 6070
    },
    {
      "epoch": 2.161393530039104,
      "grad_norm": 8.998382568359375,
      "learning_rate": 0.00011354425879843584,
      "loss": 0.5283,
      "step": 6080
    },
    {
      "epoch": 2.1649484536082473,
      "grad_norm": 2.1660964488983154,
      "learning_rate": 0.0001134020618556701,
      "loss": 0.448,
      "step": 6090
    },
    {
      "epoch": 2.1685033771773905,
      "grad_norm": 2.5020084381103516,
      "learning_rate": 0.00011325986491290437,
      "loss": 0.3446,
      "step": 6100
    },
    {
      "epoch": 2.172058300746534,
      "grad_norm": 2.7196812629699707,
      "learning_rate": 0.00011311766797013866,
      "loss": 0.4126,
      "step": 6110
    },
    {
      "epoch": 2.1756132243156774,
      "grad_norm": 6.627326488494873,
      "learning_rate": 0.00011297547102737292,
      "loss": 0.5111,
      "step": 6120
    },
    {
      "epoch": 2.1791681478848206,
      "grad_norm": 2.770364284515381,
      "learning_rate": 0.00011283327408460718,
      "loss": 0.3887,
      "step": 6130
    },
    {
      "epoch": 2.182723071453964,
      "grad_norm": 0.8539982438087463,
      "learning_rate": 0.00011269107714184147,
      "loss": 0.3586,
      "step": 6140
    },
    {
      "epoch": 2.186277995023107,
      "grad_norm": 1.4692631959915161,
      "learning_rate": 0.00011254888019907573,
      "loss": 0.3097,
      "step": 6150
    },
    {
      "epoch": 2.1898329185922503,
      "grad_norm": 2.8978259563446045,
      "learning_rate": 0.00011240668325630999,
      "loss": 0.3507,
      "step": 6160
    },
    {
      "epoch": 2.1933878421613935,
      "grad_norm": 0.9572954773902893,
      "learning_rate": 0.00011226448631354427,
      "loss": 0.4664,
      "step": 6170
    },
    {
      "epoch": 2.1969427657305367,
      "grad_norm": 3.348449230194092,
      "learning_rate": 0.00011212228937077853,
      "loss": 0.4489,
      "step": 6180
    },
    {
      "epoch": 2.20049768929968,
      "grad_norm": 3.119269609451294,
      "learning_rate": 0.00011198009242801279,
      "loss": 0.3453,
      "step": 6190
    },
    {
      "epoch": 2.204052612868823,
      "grad_norm": 5.0473175048828125,
      "learning_rate": 0.00011183789548524708,
      "loss": 0.4504,
      "step": 6200
    },
    {
      "epoch": 2.2076075364379664,
      "grad_norm": 3.434882640838623,
      "learning_rate": 0.00011169569854248134,
      "loss": 0.489,
      "step": 6210
    },
    {
      "epoch": 2.21116246000711,
      "grad_norm": 0.8134509921073914,
      "learning_rate": 0.0001115535015997156,
      "loss": 0.4151,
      "step": 6220
    },
    {
      "epoch": 2.2147173835762533,
      "grad_norm": 1.6700198650360107,
      "learning_rate": 0.00011141130465694989,
      "loss": 0.3944,
      "step": 6230
    },
    {
      "epoch": 2.2182723071453965,
      "grad_norm": 3.4960877895355225,
      "learning_rate": 0.00011126910771418415,
      "loss": 0.4128,
      "step": 6240
    },
    {
      "epoch": 2.2218272307145397,
      "grad_norm": 1.5795387029647827,
      "learning_rate": 0.00011112691077141841,
      "loss": 0.4398,
      "step": 6250
    },
    {
      "epoch": 2.225382154283683,
      "grad_norm": 0.8247532844543457,
      "learning_rate": 0.0001109847138286527,
      "loss": 0.3236,
      "step": 6260
    },
    {
      "epoch": 2.228937077852826,
      "grad_norm": 3.3254635334014893,
      "learning_rate": 0.00011084251688588696,
      "loss": 0.396,
      "step": 6270
    },
    {
      "epoch": 2.2324920014219694,
      "grad_norm": 2.8499841690063477,
      "learning_rate": 0.00011070031994312123,
      "loss": 0.5159,
      "step": 6280
    },
    {
      "epoch": 2.2360469249911126,
      "grad_norm": 1.527963399887085,
      "learning_rate": 0.0001105581230003555,
      "loss": 0.4285,
      "step": 6290
    },
    {
      "epoch": 2.239601848560256,
      "grad_norm": 2.3481991291046143,
      "learning_rate": 0.00011041592605758978,
      "loss": 0.4293,
      "step": 6300
    },
    {
      "epoch": 2.243156772129399,
      "grad_norm": 3.1543710231781006,
      "learning_rate": 0.00011027372911482404,
      "loss": 0.3748,
      "step": 6310
    },
    {
      "epoch": 2.2467116956985427,
      "grad_norm": 3.4794299602508545,
      "learning_rate": 0.00011013153217205831,
      "loss": 0.5019,
      "step": 6320
    },
    {
      "epoch": 2.250266619267686,
      "grad_norm": 2.1037559509277344,
      "learning_rate": 0.00010998933522929257,
      "loss": 0.3448,
      "step": 6330
    },
    {
      "epoch": 2.253821542836829,
      "grad_norm": 1.4098246097564697,
      "learning_rate": 0.00010984713828652684,
      "loss": 0.3697,
      "step": 6340
    },
    {
      "epoch": 2.2573764664059723,
      "grad_norm": 5.143521785736084,
      "learning_rate": 0.00010970494134376112,
      "loss": 0.3689,
      "step": 6350
    },
    {
      "epoch": 2.2609313899751156,
      "grad_norm": 1.409430742263794,
      "learning_rate": 0.00010956274440099539,
      "loss": 0.3845,
      "step": 6360
    },
    {
      "epoch": 2.2644863135442588,
      "grad_norm": 1.5941791534423828,
      "learning_rate": 0.00010942054745822965,
      "loss": 0.4374,
      "step": 6370
    },
    {
      "epoch": 2.268041237113402,
      "grad_norm": 6.269241809844971,
      "learning_rate": 0.00010927835051546391,
      "loss": 0.4312,
      "step": 6380
    },
    {
      "epoch": 2.271596160682545,
      "grad_norm": 1.5118499994277954,
      "learning_rate": 0.0001091361535726982,
      "loss": 0.3435,
      "step": 6390
    },
    {
      "epoch": 2.2751510842516884,
      "grad_norm": 3.8421363830566406,
      "learning_rate": 0.00010899395662993246,
      "loss": 0.4031,
      "step": 6400
    },
    {
      "epoch": 2.2787060078208317,
      "grad_norm": 1.466983675956726,
      "learning_rate": 0.00010885175968716672,
      "loss": 0.4136,
      "step": 6410
    },
    {
      "epoch": 2.282260931389975,
      "grad_norm": 2.940114736557007,
      "learning_rate": 0.00010870956274440101,
      "loss": 0.4088,
      "step": 6420
    },
    {
      "epoch": 2.2858158549591185,
      "grad_norm": 1.6677480936050415,
      "learning_rate": 0.00010856736580163527,
      "loss": 0.4232,
      "step": 6430
    },
    {
      "epoch": 2.2893707785282618,
      "grad_norm": 2.8697690963745117,
      "learning_rate": 0.00010842516885886953,
      "loss": 0.3057,
      "step": 6440
    },
    {
      "epoch": 2.292925702097405,
      "grad_norm": 3.423152208328247,
      "learning_rate": 0.00010828297191610381,
      "loss": 0.3865,
      "step": 6450
    },
    {
      "epoch": 2.296480625666548,
      "grad_norm": 1.2586559057235718,
      "learning_rate": 0.00010814077497333807,
      "loss": 0.4685,
      "step": 6460
    },
    {
      "epoch": 2.3000355492356914,
      "grad_norm": 2.9692654609680176,
      "learning_rate": 0.00010799857803057235,
      "loss": 0.3782,
      "step": 6470
    },
    {
      "epoch": 2.3035904728048346,
      "grad_norm": 2.0680906772613525,
      "learning_rate": 0.00010785638108780662,
      "loss": 0.3484,
      "step": 6480
    },
    {
      "epoch": 2.307145396373978,
      "grad_norm": 1.1971752643585205,
      "learning_rate": 0.00010771418414504088,
      "loss": 0.4449,
      "step": 6490
    },
    {
      "epoch": 2.310700319943121,
      "grad_norm": 1.2264997959136963,
      "learning_rate": 0.00010757198720227514,
      "loss": 0.4723,
      "step": 6500
    },
    {
      "epoch": 2.3142552435122643,
      "grad_norm": 1.0124270915985107,
      "learning_rate": 0.00010742979025950943,
      "loss": 0.3912,
      "step": 6510
    },
    {
      "epoch": 2.317810167081408,
      "grad_norm": 1.8556504249572754,
      "learning_rate": 0.0001072875933167437,
      "loss": 0.3318,
      "step": 6520
    },
    {
      "epoch": 2.321365090650551,
      "grad_norm": 0.8709293603897095,
      "learning_rate": 0.00010714539637397796,
      "loss": 0.3859,
      "step": 6530
    },
    {
      "epoch": 2.3249200142196944,
      "grad_norm": 2.4313533306121826,
      "learning_rate": 0.00010700319943121225,
      "loss": 0.4998,
      "step": 6540
    },
    {
      "epoch": 2.3284749377888376,
      "grad_norm": 3.946960687637329,
      "learning_rate": 0.00010686100248844651,
      "loss": 0.3417,
      "step": 6550
    },
    {
      "epoch": 2.332029861357981,
      "grad_norm": 3.410098075866699,
      "learning_rate": 0.00010671880554568077,
      "loss": 0.3068,
      "step": 6560
    },
    {
      "epoch": 2.335584784927124,
      "grad_norm": 2.210906505584717,
      "learning_rate": 0.00010657660860291506,
      "loss": 0.4062,
      "step": 6570
    },
    {
      "epoch": 2.3391397084962673,
      "grad_norm": 0.8396600484848022,
      "learning_rate": 0.00010643441166014932,
      "loss": 0.3615,
      "step": 6580
    },
    {
      "epoch": 2.3426946320654105,
      "grad_norm": 2.4494547843933105,
      "learning_rate": 0.00010629221471738358,
      "loss": 0.3738,
      "step": 6590
    },
    {
      "epoch": 2.3462495556345537,
      "grad_norm": 0.8014348149299622,
      "learning_rate": 0.00010615001777461786,
      "loss": 0.2788,
      "step": 6600
    },
    {
      "epoch": 2.349804479203697,
      "grad_norm": 3.0722267627716064,
      "learning_rate": 0.00010600782083185212,
      "loss": 0.3294,
      "step": 6610
    },
    {
      "epoch": 2.35335940277284,
      "grad_norm": 2.390000581741333,
      "learning_rate": 0.00010586562388908638,
      "loss": 0.4048,
      "step": 6620
    },
    {
      "epoch": 2.356914326341984,
      "grad_norm": 0.4083511531352997,
      "learning_rate": 0.00010572342694632067,
      "loss": 0.5261,
      "step": 6630
    },
    {
      "epoch": 2.360469249911127,
      "grad_norm": 1.3534939289093018,
      "learning_rate": 0.00010558123000355493,
      "loss": 0.5263,
      "step": 6640
    },
    {
      "epoch": 2.3640241734802703,
      "grad_norm": 4.971299648284912,
      "learning_rate": 0.00010543903306078919,
      "loss": 0.4673,
      "step": 6650
    },
    {
      "epoch": 2.3675790970494135,
      "grad_norm": 3.024746894836426,
      "learning_rate": 0.00010529683611802348,
      "loss": 0.3523,
      "step": 6660
    },
    {
      "epoch": 2.3711340206185567,
      "grad_norm": 0.5723050236701965,
      "learning_rate": 0.00010515463917525774,
      "loss": 0.3531,
      "step": 6670
    },
    {
      "epoch": 2.3746889441877,
      "grad_norm": 0.9071127772331238,
      "learning_rate": 0.000105012442232492,
      "loss": 0.4413,
      "step": 6680
    },
    {
      "epoch": 2.378243867756843,
      "grad_norm": 1.1845554113388062,
      "learning_rate": 0.00010487024528972627,
      "loss": 0.4434,
      "step": 6690
    },
    {
      "epoch": 2.3817987913259864,
      "grad_norm": 5.940527439117432,
      "learning_rate": 0.00010472804834696055,
      "loss": 0.4734,
      "step": 6700
    },
    {
      "epoch": 2.3853537148951296,
      "grad_norm": 1.573986530303955,
      "learning_rate": 0.00010458585140419482,
      "loss": 0.3993,
      "step": 6710
    },
    {
      "epoch": 2.3889086384642733,
      "grad_norm": 0.8952229619026184,
      "learning_rate": 0.00010444365446142908,
      "loss": 0.3469,
      "step": 6720
    },
    {
      "epoch": 2.3924635620334165,
      "grad_norm": 1.3832588195800781,
      "learning_rate": 0.00010430145751866335,
      "loss": 0.3899,
      "step": 6730
    },
    {
      "epoch": 2.3960184856025597,
      "grad_norm": 3.624831199645996,
      "learning_rate": 0.00010415926057589763,
      "loss": 0.6043,
      "step": 6740
    },
    {
      "epoch": 2.399573409171703,
      "grad_norm": 0.8075658679008484,
      "learning_rate": 0.00010401706363313189,
      "loss": 0.4751,
      "step": 6750
    },
    {
      "epoch": 2.403128332740846,
      "grad_norm": 1.8127492666244507,
      "learning_rate": 0.00010387486669036616,
      "loss": 0.4035,
      "step": 6760
    },
    {
      "epoch": 2.4066832563099894,
      "grad_norm": 3.3025295734405518,
      "learning_rate": 0.00010373266974760043,
      "loss": 0.285,
      "step": 6770
    },
    {
      "epoch": 2.4102381798791326,
      "grad_norm": 1.5748724937438965,
      "learning_rate": 0.00010359047280483469,
      "loss": 0.4674,
      "step": 6780
    },
    {
      "epoch": 2.413793103448276,
      "grad_norm": 3.118234157562256,
      "learning_rate": 0.00010344827586206898,
      "loss": 0.4153,
      "step": 6790
    },
    {
      "epoch": 2.417348027017419,
      "grad_norm": 1.105180263519287,
      "learning_rate": 0.00010330607891930324,
      "loss": 0.4024,
      "step": 6800
    },
    {
      "epoch": 2.4209029505865622,
      "grad_norm": 1.8427550792694092,
      "learning_rate": 0.0001031638819765375,
      "loss": 0.3526,
      "step": 6810
    },
    {
      "epoch": 2.4244578741557055,
      "grad_norm": 2.5731427669525146,
      "learning_rate": 0.00010302168503377179,
      "loss": 0.3811,
      "step": 6820
    },
    {
      "epoch": 2.4280127977248487,
      "grad_norm": 5.605142593383789,
      "learning_rate": 0.00010287948809100605,
      "loss": 0.4508,
      "step": 6830
    },
    {
      "epoch": 2.4315677212939923,
      "grad_norm": 1.0336002111434937,
      "learning_rate": 0.00010273729114824031,
      "loss": 0.3324,
      "step": 6840
    },
    {
      "epoch": 2.4351226448631356,
      "grad_norm": 1.9007627964019775,
      "learning_rate": 0.0001025950942054746,
      "loss": 0.4042,
      "step": 6850
    },
    {
      "epoch": 2.438677568432279,
      "grad_norm": 4.218600749969482,
      "learning_rate": 0.00010245289726270886,
      "loss": 0.4311,
      "step": 6860
    },
    {
      "epoch": 2.442232492001422,
      "grad_norm": 0.9768900871276855,
      "learning_rate": 0.00010231070031994312,
      "loss": 0.4868,
      "step": 6870
    },
    {
      "epoch": 2.4457874155705652,
      "grad_norm": 1.3822051286697388,
      "learning_rate": 0.0001021685033771774,
      "loss": 0.4777,
      "step": 6880
    },
    {
      "epoch": 2.4493423391397084,
      "grad_norm": 1.5388710498809814,
      "learning_rate": 0.00010202630643441166,
      "loss": 0.4319,
      "step": 6890
    },
    {
      "epoch": 2.4528972627088517,
      "grad_norm": 1.559431791305542,
      "learning_rate": 0.00010188410949164592,
      "loss": 0.2475,
      "step": 6900
    },
    {
      "epoch": 2.456452186277995,
      "grad_norm": 1.311346411705017,
      "learning_rate": 0.00010174191254888021,
      "loss": 0.3751,
      "step": 6910
    },
    {
      "epoch": 2.460007109847138,
      "grad_norm": 1.0313892364501953,
      "learning_rate": 0.00010159971560611447,
      "loss": 0.3399,
      "step": 6920
    },
    {
      "epoch": 2.4635620334162818,
      "grad_norm": 1.7251368761062622,
      "learning_rate": 0.00010145751866334873,
      "loss": 0.4234,
      "step": 6930
    },
    {
      "epoch": 2.467116956985425,
      "grad_norm": 3.023902654647827,
      "learning_rate": 0.00010131532172058302,
      "loss": 0.4694,
      "step": 6940
    },
    {
      "epoch": 2.470671880554568,
      "grad_norm": 1.3218497037887573,
      "learning_rate": 0.00010117312477781729,
      "loss": 0.4039,
      "step": 6950
    },
    {
      "epoch": 2.4742268041237114,
      "grad_norm": 2.919591188430786,
      "learning_rate": 0.00010103092783505155,
      "loss": 0.3693,
      "step": 6960
    },
    {
      "epoch": 2.4777817276928547,
      "grad_norm": 2.0987374782562256,
      "learning_rate": 0.00010088873089228581,
      "loss": 0.3508,
      "step": 6970
    },
    {
      "epoch": 2.481336651261998,
      "grad_norm": 1.4790312051773071,
      "learning_rate": 0.0001007465339495201,
      "loss": 0.456,
      "step": 6980
    },
    {
      "epoch": 2.484891574831141,
      "grad_norm": 1.6752407550811768,
      "learning_rate": 0.00010060433700675436,
      "loss": 0.3493,
      "step": 6990
    },
    {
      "epoch": 2.4884464984002843,
      "grad_norm": 1.7854092121124268,
      "learning_rate": 0.00010046214006398862,
      "loss": 0.374,
      "step": 7000
    },
    {
      "epoch": 2.4920014219694275,
      "grad_norm": 5.1014885902404785,
      "learning_rate": 0.00010031994312122291,
      "loss": 0.4194,
      "step": 7010
    },
    {
      "epoch": 2.4955563455385708,
      "grad_norm": 2.9292101860046387,
      "learning_rate": 0.00010017774617845717,
      "loss": 0.3728,
      "step": 7020
    },
    {
      "epoch": 2.499111269107714,
      "grad_norm": 1.4314125776290894,
      "learning_rate": 0.00010003554923569143,
      "loss": 0.5569,
      "step": 7030
    },
    {
      "epoch": 2.502666192676857,
      "grad_norm": 1.5981254577636719,
      "learning_rate": 9.989335229292571e-05,
      "loss": 0.3548,
      "step": 7040
    },
    {
      "epoch": 2.506221116246001,
      "grad_norm": 3.345536231994629,
      "learning_rate": 9.975115535015997e-05,
      "loss": 0.3282,
      "step": 7050
    },
    {
      "epoch": 2.509776039815144,
      "grad_norm": 1.8898404836654663,
      "learning_rate": 9.960895840739424e-05,
      "loss": 0.4642,
      "step": 7060
    },
    {
      "epoch": 2.5133309633842873,
      "grad_norm": 6.947741508483887,
      "learning_rate": 9.94667614646285e-05,
      "loss": 0.4942,
      "step": 7070
    },
    {
      "epoch": 2.5168858869534305,
      "grad_norm": 2.3095130920410156,
      "learning_rate": 9.932456452186278e-05,
      "loss": 0.4984,
      "step": 7080
    },
    {
      "epoch": 2.5204408105225737,
      "grad_norm": 3.9841623306274414,
      "learning_rate": 9.918236757909706e-05,
      "loss": 0.4536,
      "step": 7090
    },
    {
      "epoch": 2.523995734091717,
      "grad_norm": 1.1967072486877441,
      "learning_rate": 9.904017063633132e-05,
      "loss": 0.3542,
      "step": 7100
    },
    {
      "epoch": 2.52755065766086,
      "grad_norm": 3.784261703491211,
      "learning_rate": 9.88979736935656e-05,
      "loss": 0.3868,
      "step": 7110
    },
    {
      "epoch": 2.5311055812300034,
      "grad_norm": 1.6934748888015747,
      "learning_rate": 9.875577675079987e-05,
      "loss": 0.4134,
      "step": 7120
    },
    {
      "epoch": 2.534660504799147,
      "grad_norm": 1.5741368532180786,
      "learning_rate": 9.861357980803413e-05,
      "loss": 0.3693,
      "step": 7130
    },
    {
      "epoch": 2.5382154283682903,
      "grad_norm": 3.8352832794189453,
      "learning_rate": 9.84713828652684e-05,
      "loss": 0.3762,
      "step": 7140
    },
    {
      "epoch": 2.5417703519374335,
      "grad_norm": 1.517220377922058,
      "learning_rate": 9.832918592250268e-05,
      "loss": 0.3353,
      "step": 7150
    },
    {
      "epoch": 2.5453252755065767,
      "grad_norm": 2.330721616744995,
      "learning_rate": 9.818698897973694e-05,
      "loss": 0.3784,
      "step": 7160
    },
    {
      "epoch": 2.54888019907572,
      "grad_norm": 2.5558533668518066,
      "learning_rate": 9.804479203697122e-05,
      "loss": 0.3783,
      "step": 7170
    },
    {
      "epoch": 2.552435122644863,
      "grad_norm": 2.0636210441589355,
      "learning_rate": 9.790259509420548e-05,
      "loss": 0.3592,
      "step": 7180
    },
    {
      "epoch": 2.5559900462140064,
      "grad_norm": 6.598829746246338,
      "learning_rate": 9.776039815143974e-05,
      "loss": 0.3314,
      "step": 7190
    },
    {
      "epoch": 2.5595449697831496,
      "grad_norm": 0.8983827233314514,
      "learning_rate": 9.761820120867402e-05,
      "loss": 0.3731,
      "step": 7200
    },
    {
      "epoch": 2.563099893352293,
      "grad_norm": 3.715437173843384,
      "learning_rate": 9.747600426590828e-05,
      "loss": 0.3724,
      "step": 7210
    },
    {
      "epoch": 2.566654816921436,
      "grad_norm": 2.8143930435180664,
      "learning_rate": 9.733380732314255e-05,
      "loss": 0.3565,
      "step": 7220
    },
    {
      "epoch": 2.5702097404905793,
      "grad_norm": 3.5969741344451904,
      "learning_rate": 9.719161038037683e-05,
      "loss": 0.4016,
      "step": 7230
    },
    {
      "epoch": 2.5737646640597225,
      "grad_norm": 4.0104241371154785,
      "learning_rate": 9.704941343761109e-05,
      "loss": 0.4661,
      "step": 7240
    },
    {
      "epoch": 2.5773195876288657,
      "grad_norm": 2.8546559810638428,
      "learning_rate": 9.690721649484537e-05,
      "loss": 0.4694,
      "step": 7250
    },
    {
      "epoch": 2.5808745111980094,
      "grad_norm": 2.106584310531616,
      "learning_rate": 9.676501955207964e-05,
      "loss": 0.3044,
      "step": 7260
    },
    {
      "epoch": 2.5844294347671526,
      "grad_norm": 0.6858622431755066,
      "learning_rate": 9.66228226093139e-05,
      "loss": 0.3314,
      "step": 7270
    },
    {
      "epoch": 2.587984358336296,
      "grad_norm": 1.5054020881652832,
      "learning_rate": 9.648062566654818e-05,
      "loss": 0.3533,
      "step": 7280
    },
    {
      "epoch": 2.591539281905439,
      "grad_norm": 4.102948188781738,
      "learning_rate": 9.633842872378245e-05,
      "loss": 0.4857,
      "step": 7290
    },
    {
      "epoch": 2.5950942054745822,
      "grad_norm": 0.9583965539932251,
      "learning_rate": 9.619623178101671e-05,
      "loss": 0.3667,
      "step": 7300
    },
    {
      "epoch": 2.5986491290437255,
      "grad_norm": 2.3440182209014893,
      "learning_rate": 9.605403483825099e-05,
      "loss": 0.4275,
      "step": 7310
    },
    {
      "epoch": 2.6022040526128687,
      "grad_norm": 5.264316558837891,
      "learning_rate": 9.591183789548525e-05,
      "loss": 0.4467,
      "step": 7320
    },
    {
      "epoch": 2.6057589761820124,
      "grad_norm": 2.9896819591522217,
      "learning_rate": 9.576964095271951e-05,
      "loss": 0.4342,
      "step": 7330
    },
    {
      "epoch": 2.6093138997511556,
      "grad_norm": 1.3950968980789185,
      "learning_rate": 9.562744400995379e-05,
      "loss": 0.4465,
      "step": 7340
    },
    {
      "epoch": 2.612868823320299,
      "grad_norm": 2.2903430461883545,
      "learning_rate": 9.548524706718805e-05,
      "loss": 0.4223,
      "step": 7350
    },
    {
      "epoch": 2.616423746889442,
      "grad_norm": 2.0241475105285645,
      "learning_rate": 9.534305012442232e-05,
      "loss": 0.4542,
      "step": 7360
    },
    {
      "epoch": 2.6199786704585852,
      "grad_norm": 1.3303109407424927,
      "learning_rate": 9.52008531816566e-05,
      "loss": 0.3465,
      "step": 7370
    },
    {
      "epoch": 2.6235335940277285,
      "grad_norm": 1.3908766508102417,
      "learning_rate": 9.505865623889086e-05,
      "loss": 0.4127,
      "step": 7380
    },
    {
      "epoch": 2.6270885175968717,
      "grad_norm": 3.5291337966918945,
      "learning_rate": 9.491645929612514e-05,
      "loss": 0.4296,
      "step": 7390
    },
    {
      "epoch": 2.630643441166015,
      "grad_norm": 1.4407633543014526,
      "learning_rate": 9.477426235335941e-05,
      "loss": 0.3681,
      "step": 7400
    },
    {
      "epoch": 2.634198364735158,
      "grad_norm": 3.735556125640869,
      "learning_rate": 9.463206541059367e-05,
      "loss": 0.3867,
      "step": 7410
    },
    {
      "epoch": 2.6377532883043013,
      "grad_norm": 6.144986152648926,
      "learning_rate": 9.448986846782795e-05,
      "loss": 0.3678,
      "step": 7420
    },
    {
      "epoch": 2.6413082118734446,
      "grad_norm": 2.6688807010650635,
      "learning_rate": 9.434767152506222e-05,
      "loss": 0.3441,
      "step": 7430
    },
    {
      "epoch": 2.6448631354425878,
      "grad_norm": 4.838056564331055,
      "learning_rate": 9.420547458229649e-05,
      "loss": 0.4537,
      "step": 7440
    },
    {
      "epoch": 2.648418059011731,
      "grad_norm": 3.086300849914551,
      "learning_rate": 9.406327763953076e-05,
      "loss": 0.3952,
      "step": 7450
    },
    {
      "epoch": 2.6519729825808747,
      "grad_norm": 1.292366862297058,
      "learning_rate": 9.392108069676502e-05,
      "loss": 0.4083,
      "step": 7460
    },
    {
      "epoch": 2.655527906150018,
      "grad_norm": 3.0059421062469482,
      "learning_rate": 9.37788837539993e-05,
      "loss": 0.5151,
      "step": 7470
    },
    {
      "epoch": 2.659082829719161,
      "grad_norm": 1.1851760149002075,
      "learning_rate": 9.363668681123356e-05,
      "loss": 0.3191,
      "step": 7480
    },
    {
      "epoch": 2.6626377532883043,
      "grad_norm": 2.414851427078247,
      "learning_rate": 9.349448986846782e-05,
      "loss": 0.4146,
      "step": 7490
    },
    {
      "epoch": 2.6661926768574475,
      "grad_norm": 1.4544895887374878,
      "learning_rate": 9.33522929257021e-05,
      "loss": 0.3936,
      "step": 7500
    },
    {
      "epoch": 2.6697476004265908,
      "grad_norm": 1.1628495454788208,
      "learning_rate": 9.321009598293637e-05,
      "loss": 0.3671,
      "step": 7510
    },
    {
      "epoch": 2.673302523995734,
      "grad_norm": 1.3897345066070557,
      "learning_rate": 9.306789904017063e-05,
      "loss": 0.4831,
      "step": 7520
    },
    {
      "epoch": 2.676857447564877,
      "grad_norm": 3.384948968887329,
      "learning_rate": 9.292570209740491e-05,
      "loss": 0.4101,
      "step": 7530
    },
    {
      "epoch": 2.680412371134021,
      "grad_norm": 4.80650520324707,
      "learning_rate": 9.278350515463918e-05,
      "loss": 0.4037,
      "step": 7540
    },
    {
      "epoch": 2.683967294703164,
      "grad_norm": 1.613157868385315,
      "learning_rate": 9.264130821187345e-05,
      "loss": 0.4157,
      "step": 7550
    },
    {
      "epoch": 2.6875222182723073,
      "grad_norm": 1.509613275527954,
      "learning_rate": 9.249911126910772e-05,
      "loss": 0.2841,
      "step": 7560
    },
    {
      "epoch": 2.6910771418414505,
      "grad_norm": 4.924310207366943,
      "learning_rate": 9.2356914326342e-05,
      "loss": 0.4135,
      "step": 7570
    },
    {
      "epoch": 2.6946320654105937,
      "grad_norm": 2.4687728881835938,
      "learning_rate": 9.221471738357626e-05,
      "loss": 0.5059,
      "step": 7580
    },
    {
      "epoch": 2.698186988979737,
      "grad_norm": 5.407236576080322,
      "learning_rate": 9.207252044081053e-05,
      "loss": 0.4098,
      "step": 7590
    },
    {
      "epoch": 2.70174191254888,
      "grad_norm": 3.5231211185455322,
      "learning_rate": 9.193032349804481e-05,
      "loss": 0.4006,
      "step": 7600
    },
    {
      "epoch": 2.7052968361180234,
      "grad_norm": 1.2072120904922485,
      "learning_rate": 9.178812655527907e-05,
      "loss": 0.421,
      "step": 7610
    },
    {
      "epoch": 2.7088517596871666,
      "grad_norm": 1.7283014059066772,
      "learning_rate": 9.164592961251333e-05,
      "loss": 0.391,
      "step": 7620
    },
    {
      "epoch": 2.71240668325631,
      "grad_norm": 1.3753299713134766,
      "learning_rate": 9.150373266974759e-05,
      "loss": 0.3861,
      "step": 7630
    },
    {
      "epoch": 2.715961606825453,
      "grad_norm": 2.1742184162139893,
      "learning_rate": 9.136153572698187e-05,
      "loss": 0.3629,
      "step": 7640
    },
    {
      "epoch": 2.7195165303945963,
      "grad_norm": 1.3022942543029785,
      "learning_rate": 9.121933878421614e-05,
      "loss": 0.4715,
      "step": 7650
    },
    {
      "epoch": 2.7230714539637395,
      "grad_norm": 1.5448172092437744,
      "learning_rate": 9.10771418414504e-05,
      "loss": 0.387,
      "step": 7660
    },
    {
      "epoch": 2.726626377532883,
      "grad_norm": 3.739229917526245,
      "learning_rate": 9.093494489868468e-05,
      "loss": 0.4149,
      "step": 7670
    },
    {
      "epoch": 2.7301813011020264,
      "grad_norm": 0.971530556678772,
      "learning_rate": 9.079274795591896e-05,
      "loss": 0.3259,
      "step": 7680
    },
    {
      "epoch": 2.7337362246711696,
      "grad_norm": 3.732107400894165,
      "learning_rate": 9.065055101315322e-05,
      "loss": 0.4107,
      "step": 7690
    },
    {
      "epoch": 2.737291148240313,
      "grad_norm": 3.4967730045318604,
      "learning_rate": 9.050835407038749e-05,
      "loss": 0.4353,
      "step": 7700
    },
    {
      "epoch": 2.740846071809456,
      "grad_norm": 1.429868221282959,
      "learning_rate": 9.036615712762177e-05,
      "loss": 0.3692,
      "step": 7710
    },
    {
      "epoch": 2.7444009953785993,
      "grad_norm": 1.081580400466919,
      "learning_rate": 9.022396018485603e-05,
      "loss": 0.3182,
      "step": 7720
    },
    {
      "epoch": 2.7479559189477425,
      "grad_norm": 2.227130651473999,
      "learning_rate": 9.00817632420903e-05,
      "loss": 0.3287,
      "step": 7730
    },
    {
      "epoch": 2.751510842516886,
      "grad_norm": 1.236730694770813,
      "learning_rate": 8.993956629932458e-05,
      "loss": 0.3961,
      "step": 7740
    },
    {
      "epoch": 2.7550657660860294,
      "grad_norm": 4.0049147605896,
      "learning_rate": 8.979736935655884e-05,
      "loss": 0.3366,
      "step": 7750
    },
    {
      "epoch": 2.7586206896551726,
      "grad_norm": 1.3042631149291992,
      "learning_rate": 8.96551724137931e-05,
      "loss": 0.3735,
      "step": 7760
    },
    {
      "epoch": 2.762175613224316,
      "grad_norm": 1.2735215425491333,
      "learning_rate": 8.951297547102738e-05,
      "loss": 0.3584,
      "step": 7770
    },
    {
      "epoch": 2.765730536793459,
      "grad_norm": 3.306567907333374,
      "learning_rate": 8.937077852826164e-05,
      "loss": 0.3502,
      "step": 7780
    },
    {
      "epoch": 2.7692854603626023,
      "grad_norm": 0.4445367157459259,
      "learning_rate": 8.922858158549592e-05,
      "loss": 0.2894,
      "step": 7790
    },
    {
      "epoch": 2.7728403839317455,
      "grad_norm": 1.6353836059570312,
      "learning_rate": 8.908638464273018e-05,
      "loss": 0.3868,
      "step": 7800
    },
    {
      "epoch": 2.7763953075008887,
      "grad_norm": 2.699547052383423,
      "learning_rate": 8.894418769996445e-05,
      "loss": 0.3709,
      "step": 7810
    },
    {
      "epoch": 2.779950231070032,
      "grad_norm": 4.125883102416992,
      "learning_rate": 8.880199075719873e-05,
      "loss": 0.3862,
      "step": 7820
    },
    {
      "epoch": 2.783505154639175,
      "grad_norm": 4.43751859664917,
      "learning_rate": 8.865979381443299e-05,
      "loss": 0.3455,
      "step": 7830
    },
    {
      "epoch": 2.7870600782083184,
      "grad_norm": 3.878887891769409,
      "learning_rate": 8.851759687166726e-05,
      "loss": 0.3145,
      "step": 7840
    },
    {
      "epoch": 2.7906150017774616,
      "grad_norm": 0.6816956996917725,
      "learning_rate": 8.837539992890154e-05,
      "loss": 0.3628,
      "step": 7850
    },
    {
      "epoch": 2.794169925346605,
      "grad_norm": 0.7058656215667725,
      "learning_rate": 8.82332029861358e-05,
      "loss": 0.3787,
      "step": 7860
    },
    {
      "epoch": 2.7977248489157485,
      "grad_norm": 1.0716603994369507,
      "learning_rate": 8.809100604337008e-05,
      "loss": 0.4149,
      "step": 7870
    },
    {
      "epoch": 2.8012797724848917,
      "grad_norm": 1.1220866441726685,
      "learning_rate": 8.794880910060435e-05,
      "loss": 0.4384,
      "step": 7880
    },
    {
      "epoch": 2.804834696054035,
      "grad_norm": 0.5500094294548035,
      "learning_rate": 8.780661215783861e-05,
      "loss": 0.3306,
      "step": 7890
    },
    {
      "epoch": 2.808389619623178,
      "grad_norm": 1.9749702215194702,
      "learning_rate": 8.766441521507287e-05,
      "loss": 0.492,
      "step": 7900
    },
    {
      "epoch": 2.8119445431923213,
      "grad_norm": 3.1814677715301514,
      "learning_rate": 8.752221827230715e-05,
      "loss": 0.3947,
      "step": 7910
    },
    {
      "epoch": 2.8154994667614646,
      "grad_norm": 2.673434019088745,
      "learning_rate": 8.738002132954141e-05,
      "loss": 0.3033,
      "step": 7920
    },
    {
      "epoch": 2.819054390330608,
      "grad_norm": 1.0623767375946045,
      "learning_rate": 8.723782438677569e-05,
      "loss": 0.3249,
      "step": 7930
    },
    {
      "epoch": 2.822609313899751,
      "grad_norm": 1.5781071186065674,
      "learning_rate": 8.709562744400995e-05,
      "loss": 0.3671,
      "step": 7940
    },
    {
      "epoch": 2.8261642374688947,
      "grad_norm": 1.8068199157714844,
      "learning_rate": 8.695343050124422e-05,
      "loss": 0.4336,
      "step": 7950
    },
    {
      "epoch": 2.829719161038038,
      "grad_norm": 1.3282811641693115,
      "learning_rate": 8.68112335584785e-05,
      "loss": 0.3935,
      "step": 7960
    },
    {
      "epoch": 2.833274084607181,
      "grad_norm": 3.5395567417144775,
      "learning_rate": 8.666903661571276e-05,
      "loss": 0.4577,
      "step": 7970
    },
    {
      "epoch": 2.8368290081763243,
      "grad_norm": 1.972395420074463,
      "learning_rate": 8.652683967294704e-05,
      "loss": 0.4027,
      "step": 7980
    },
    {
      "epoch": 2.8403839317454675,
      "grad_norm": 1.3630162477493286,
      "learning_rate": 8.638464273018131e-05,
      "loss": 0.3831,
      "step": 7990
    },
    {
      "epoch": 2.8439388553146108,
      "grad_norm": 1.1377322673797607,
      "learning_rate": 8.624244578741557e-05,
      "loss": 0.3824,
      "step": 8000
    },
    {
      "epoch": 2.847493778883754,
      "grad_norm": 4.7244462966918945,
      "learning_rate": 8.610024884464985e-05,
      "loss": 0.4837,
      "step": 8010
    },
    {
      "epoch": 2.851048702452897,
      "grad_norm": 1.2224221229553223,
      "learning_rate": 8.595805190188412e-05,
      "loss": 0.3857,
      "step": 8020
    },
    {
      "epoch": 2.8546036260220404,
      "grad_norm": 1.3198227882385254,
      "learning_rate": 8.581585495911838e-05,
      "loss": 0.4482,
      "step": 8030
    },
    {
      "epoch": 2.8581585495911837,
      "grad_norm": 4.097764015197754,
      "learning_rate": 8.567365801635266e-05,
      "loss": 0.354,
      "step": 8040
    },
    {
      "epoch": 2.861713473160327,
      "grad_norm": 1.4986647367477417,
      "learning_rate": 8.553146107358692e-05,
      "loss": 0.3823,
      "step": 8050
    },
    {
      "epoch": 2.86526839672947,
      "grad_norm": 4.129611968994141,
      "learning_rate": 8.538926413082118e-05,
      "loss": 0.4418,
      "step": 8060
    },
    {
      "epoch": 2.8688233202986133,
      "grad_norm": 0.9618481993675232,
      "learning_rate": 8.524706718805546e-05,
      "loss": 0.3083,
      "step": 8070
    },
    {
      "epoch": 2.872378243867757,
      "grad_norm": 2.698390007019043,
      "learning_rate": 8.510487024528972e-05,
      "loss": 0.4756,
      "step": 8080
    },
    {
      "epoch": 2.8759331674369,
      "grad_norm": 1.0807243585586548,
      "learning_rate": 8.4962673302524e-05,
      "loss": 0.3685,
      "step": 8090
    },
    {
      "epoch": 2.8794880910060434,
      "grad_norm": 1.907535195350647,
      "learning_rate": 8.482047635975827e-05,
      "loss": 0.2747,
      "step": 8100
    },
    {
      "epoch": 2.8830430145751866,
      "grad_norm": 0.9207631349563599,
      "learning_rate": 8.467827941699253e-05,
      "loss": 0.4667,
      "step": 8110
    },
    {
      "epoch": 2.88659793814433,
      "grad_norm": 1.7853574752807617,
      "learning_rate": 8.453608247422681e-05,
      "loss": 0.3884,
      "step": 8120
    },
    {
      "epoch": 2.890152861713473,
      "grad_norm": 2.2495925426483154,
      "learning_rate": 8.439388553146108e-05,
      "loss": 0.4913,
      "step": 8130
    },
    {
      "epoch": 2.8937077852826163,
      "grad_norm": 5.394918441772461,
      "learning_rate": 8.425168858869534e-05,
      "loss": 0.4048,
      "step": 8140
    },
    {
      "epoch": 2.89726270885176,
      "grad_norm": 1.5212703943252563,
      "learning_rate": 8.410949164592962e-05,
      "loss": 0.3933,
      "step": 8150
    },
    {
      "epoch": 2.900817632420903,
      "grad_norm": 3.6853981018066406,
      "learning_rate": 8.39672947031639e-05,
      "loss": 0.353,
      "step": 8160
    },
    {
      "epoch": 2.9043725559900464,
      "grad_norm": 1.4996205568313599,
      "learning_rate": 8.382509776039816e-05,
      "loss": 0.2828,
      "step": 8170
    },
    {
      "epoch": 2.9079274795591896,
      "grad_norm": 0.7836193442344666,
      "learning_rate": 8.368290081763243e-05,
      "loss": 0.4303,
      "step": 8180
    },
    {
      "epoch": 2.911482403128333,
      "grad_norm": 6.284509658813477,
      "learning_rate": 8.35407038748667e-05,
      "loss": 0.4831,
      "step": 8190
    },
    {
      "epoch": 2.915037326697476,
      "grad_norm": 1.6559768915176392,
      "learning_rate": 8.339850693210095e-05,
      "loss": 0.4439,
      "step": 8200
    },
    {
      "epoch": 2.9185922502666193,
      "grad_norm": 3.3030178546905518,
      "learning_rate": 8.325630998933523e-05,
      "loss": 0.3603,
      "step": 8210
    },
    {
      "epoch": 2.9221471738357625,
      "grad_norm": 3.4331674575805664,
      "learning_rate": 8.311411304656949e-05,
      "loss": 0.4662,
      "step": 8220
    },
    {
      "epoch": 2.9257020974049057,
      "grad_norm": 0.9943974018096924,
      "learning_rate": 8.297191610380377e-05,
      "loss": 0.4526,
      "step": 8230
    },
    {
      "epoch": 2.929257020974049,
      "grad_norm": 1.486001968383789,
      "learning_rate": 8.282971916103804e-05,
      "loss": 0.5157,
      "step": 8240
    },
    {
      "epoch": 2.932811944543192,
      "grad_norm": 4.793610572814941,
      "learning_rate": 8.26875222182723e-05,
      "loss": 0.4131,
      "step": 8250
    },
    {
      "epoch": 2.9363668681123354,
      "grad_norm": 3.5230867862701416,
      "learning_rate": 8.254532527550658e-05,
      "loss": 0.4036,
      "step": 8260
    },
    {
      "epoch": 2.9399217916814786,
      "grad_norm": 2.5432448387145996,
      "learning_rate": 8.240312833274085e-05,
      "loss": 0.4884,
      "step": 8270
    },
    {
      "epoch": 2.9434767152506223,
      "grad_norm": 1.1889322996139526,
      "learning_rate": 8.226093138997512e-05,
      "loss": 0.3771,
      "step": 8280
    },
    {
      "epoch": 2.9470316388197655,
      "grad_norm": 0.8827773332595825,
      "learning_rate": 8.211873444720939e-05,
      "loss": 0.3318,
      "step": 8290
    },
    {
      "epoch": 2.9505865623889087,
      "grad_norm": 2.5294361114501953,
      "learning_rate": 8.197653750444367e-05,
      "loss": 0.4343,
      "step": 8300
    },
    {
      "epoch": 2.954141485958052,
      "grad_norm": 1.0012933015823364,
      "learning_rate": 8.183434056167793e-05,
      "loss": 0.3447,
      "step": 8310
    },
    {
      "epoch": 2.957696409527195,
      "grad_norm": 1.0948197841644287,
      "learning_rate": 8.16921436189122e-05,
      "loss": 0.3094,
      "step": 8320
    },
    {
      "epoch": 2.9612513330963384,
      "grad_norm": 2.1941301822662354,
      "learning_rate": 8.154994667614646e-05,
      "loss": 0.4165,
      "step": 8330
    },
    {
      "epoch": 2.9648062566654816,
      "grad_norm": 1.0286225080490112,
      "learning_rate": 8.140774973338074e-05,
      "loss": 0.3274,
      "step": 8340
    },
    {
      "epoch": 2.968361180234625,
      "grad_norm": 2.8543872833251953,
      "learning_rate": 8.1265552790615e-05,
      "loss": 0.5179,
      "step": 8350
    },
    {
      "epoch": 2.9719161038037685,
      "grad_norm": 1.3666783571243286,
      "learning_rate": 8.112335584784928e-05,
      "loss": 0.4096,
      "step": 8360
    },
    {
      "epoch": 2.9754710273729117,
      "grad_norm": 1.4205999374389648,
      "learning_rate": 8.098115890508354e-05,
      "loss": 0.3415,
      "step": 8370
    },
    {
      "epoch": 2.979025950942055,
      "grad_norm": 1.47357177734375,
      "learning_rate": 8.083896196231781e-05,
      "loss": 0.4181,
      "step": 8380
    },
    {
      "epoch": 2.982580874511198,
      "grad_norm": 2.8320364952087402,
      "learning_rate": 8.069676501955208e-05,
      "loss": 0.456,
      "step": 8390
    },
    {
      "epoch": 2.9861357980803414,
      "grad_norm": 1.3975865840911865,
      "learning_rate": 8.055456807678635e-05,
      "loss": 0.4396,
      "step": 8400
    },
    {
      "epoch": 2.9896907216494846,
      "grad_norm": 2.5636532306671143,
      "learning_rate": 8.041237113402063e-05,
      "loss": 0.3476,
      "step": 8410
    },
    {
      "epoch": 2.993245645218628,
      "grad_norm": 1.1095280647277832,
      "learning_rate": 8.027017419125489e-05,
      "loss": 0.3785,
      "step": 8420
    },
    {
      "epoch": 2.996800568787771,
      "grad_norm": 2.1175999641418457,
      "learning_rate": 8.012797724848916e-05,
      "loss": 0.4579,
      "step": 8430
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.8304,
      "eval_f1_macro": 0.8303995658228885,
      "eval_f1_neg": 0.8301282051282052,
      "eval_f1_pos": 0.8306709265175719,
      "eval_loss": 0.39433369040489197,
      "eval_precision_global": 0.8304086231536694,
      "eval_precision_neg": 0.8268156424581006,
      "eval_precision_pos": 0.8340016038492382,
      "eval_recall_global": 0.8304170818796877,
      "eval_recall_neg": 0.833467417538214,
      "eval_recall_pos": 0.8273667462211615,
      "eval_runtime": 56.8769,
      "eval_samples_per_second": 43.955,
      "eval_steps_per_second": 5.503,
      "step": 8439
    },
    {
      "epoch": 3.0003554923569142,
      "grad_norm": 2.904878616333008,
      "learning_rate": 7.998578030572344e-05,
      "loss": 0.4283,
      "step": 8440
    },
    {
      "epoch": 3.0039104159260575,
      "grad_norm": 1.077917456626892,
      "learning_rate": 7.98435833629577e-05,
      "loss": 0.2635,
      "step": 8450
    },
    {
      "epoch": 3.0074653394952007,
      "grad_norm": 1.6751521825790405,
      "learning_rate": 7.970138642019198e-05,
      "loss": 0.4279,
      "step": 8460
    },
    {
      "epoch": 3.0110202630643443,
      "grad_norm": 2.6588315963745117,
      "learning_rate": 7.955918947742625e-05,
      "loss": 0.3228,
      "step": 8470
    },
    {
      "epoch": 3.0145751866334876,
      "grad_norm": 5.052703857421875,
      "learning_rate": 7.941699253466051e-05,
      "loss": 0.4417,
      "step": 8480
    },
    {
      "epoch": 3.018130110202631,
      "grad_norm": 1.9428311586380005,
      "learning_rate": 7.927479559189477e-05,
      "loss": 0.2827,
      "step": 8490
    },
    {
      "epoch": 3.021685033771774,
      "grad_norm": 0.780826985836029,
      "learning_rate": 7.913259864912905e-05,
      "loss": 0.4111,
      "step": 8500
    },
    {
      "epoch": 3.025239957340917,
      "grad_norm": 7.594499588012695,
      "learning_rate": 7.899040170636331e-05,
      "loss": 0.3413,
      "step": 8510
    },
    {
      "epoch": 3.0287948809100604,
      "grad_norm": 3.7461421489715576,
      "learning_rate": 7.884820476359759e-05,
      "loss": 0.4956,
      "step": 8520
    },
    {
      "epoch": 3.0323498044792037,
      "grad_norm": 3.4044952392578125,
      "learning_rate": 7.870600782083185e-05,
      "loss": 0.4315,
      "step": 8530
    },
    {
      "epoch": 3.035904728048347,
      "grad_norm": 0.9088969230651855,
      "learning_rate": 7.856381087806612e-05,
      "loss": 0.3542,
      "step": 8540
    },
    {
      "epoch": 3.03945965161749,
      "grad_norm": 3.4766488075256348,
      "learning_rate": 7.84216139353004e-05,
      "loss": 0.3998,
      "step": 8550
    },
    {
      "epoch": 3.0430145751866333,
      "grad_norm": 2.0868399143218994,
      "learning_rate": 7.827941699253466e-05,
      "loss": 0.3084,
      "step": 8560
    },
    {
      "epoch": 3.0465694987557765,
      "grad_norm": 1.3547022342681885,
      "learning_rate": 7.813722004976893e-05,
      "loss": 0.3826,
      "step": 8570
    },
    {
      "epoch": 3.05012442232492,
      "grad_norm": 1.050795316696167,
      "learning_rate": 7.799502310700321e-05,
      "loss": 0.341,
      "step": 8580
    },
    {
      "epoch": 3.0536793458940634,
      "grad_norm": 1.1654796600341797,
      "learning_rate": 7.785282616423747e-05,
      "loss": 0.4483,
      "step": 8590
    },
    {
      "epoch": 3.0572342694632066,
      "grad_norm": 1.8038525581359863,
      "learning_rate": 7.771062922147175e-05,
      "loss": 0.4096,
      "step": 8600
    },
    {
      "epoch": 3.06078919303235,
      "grad_norm": 1.4752488136291504,
      "learning_rate": 7.756843227870602e-05,
      "loss": 0.3901,
      "step": 8610
    },
    {
      "epoch": 3.064344116601493,
      "grad_norm": 3.0377213954925537,
      "learning_rate": 7.742623533594028e-05,
      "loss": 0.4815,
      "step": 8620
    },
    {
      "epoch": 3.0678990401706363,
      "grad_norm": 1.6137640476226807,
      "learning_rate": 7.728403839317455e-05,
      "loss": 0.4403,
      "step": 8630
    },
    {
      "epoch": 3.0714539637397795,
      "grad_norm": 2.1406502723693848,
      "learning_rate": 7.714184145040882e-05,
      "loss": 0.3788,
      "step": 8640
    },
    {
      "epoch": 3.0750088873089227,
      "grad_norm": 1.6354188919067383,
      "learning_rate": 7.699964450764308e-05,
      "loss": 0.447,
      "step": 8650
    },
    {
      "epoch": 3.078563810878066,
      "grad_norm": 1.035581350326538,
      "learning_rate": 7.685744756487736e-05,
      "loss": 0.4146,
      "step": 8660
    },
    {
      "epoch": 3.082118734447209,
      "grad_norm": 3.215437650680542,
      "learning_rate": 7.671525062211162e-05,
      "loss": 0.4148,
      "step": 8670
    },
    {
      "epoch": 3.085673658016353,
      "grad_norm": 3.2685747146606445,
      "learning_rate": 7.65730536793459e-05,
      "loss": 0.4646,
      "step": 8680
    },
    {
      "epoch": 3.089228581585496,
      "grad_norm": 0.7516098022460938,
      "learning_rate": 7.643085673658017e-05,
      "loss": 0.3982,
      "step": 8690
    },
    {
      "epoch": 3.0927835051546393,
      "grad_norm": 2.4524290561676025,
      "learning_rate": 7.628865979381443e-05,
      "loss": 0.3297,
      "step": 8700
    },
    {
      "epoch": 3.0963384287237825,
      "grad_norm": 1.6426200866699219,
      "learning_rate": 7.61464628510487e-05,
      "loss": 0.4861,
      "step": 8710
    },
    {
      "epoch": 3.0998933522929257,
      "grad_norm": 2.88734769821167,
      "learning_rate": 7.600426590828298e-05,
      "loss": 0.3924,
      "step": 8720
    },
    {
      "epoch": 3.103448275862069,
      "grad_norm": 2.761134386062622,
      "learning_rate": 7.586206896551724e-05,
      "loss": 0.4864,
      "step": 8730
    },
    {
      "epoch": 3.107003199431212,
      "grad_norm": 1.609295129776001,
      "learning_rate": 7.571987202275152e-05,
      "loss": 0.4146,
      "step": 8740
    },
    {
      "epoch": 3.1105581230003554,
      "grad_norm": 1.859438419342041,
      "learning_rate": 7.55776750799858e-05,
      "loss": 0.3018,
      "step": 8750
    },
    {
      "epoch": 3.1141130465694986,
      "grad_norm": 1.895728588104248,
      "learning_rate": 7.543547813722006e-05,
      "loss": 0.3677,
      "step": 8760
    },
    {
      "epoch": 3.117667970138642,
      "grad_norm": 1.6460893154144287,
      "learning_rate": 7.529328119445433e-05,
      "loss": 0.4602,
      "step": 8770
    },
    {
      "epoch": 3.1212228937077855,
      "grad_norm": 1.7103724479675293,
      "learning_rate": 7.515108425168859e-05,
      "loss": 0.3433,
      "step": 8780
    },
    {
      "epoch": 3.1247778172769287,
      "grad_norm": 1.148797631263733,
      "learning_rate": 7.500888730892285e-05,
      "loss": 0.3776,
      "step": 8790
    },
    {
      "epoch": 3.128332740846072,
      "grad_norm": 0.8434626460075378,
      "learning_rate": 7.486669036615713e-05,
      "loss": 0.3968,
      "step": 8800
    },
    {
      "epoch": 3.131887664415215,
      "grad_norm": 3.8506011962890625,
      "learning_rate": 7.472449342339139e-05,
      "loss": 0.5364,
      "step": 8810
    },
    {
      "epoch": 3.1354425879843584,
      "grad_norm": 0.8494074940681458,
      "learning_rate": 7.458229648062567e-05,
      "loss": 0.3274,
      "step": 8820
    },
    {
      "epoch": 3.1389975115535016,
      "grad_norm": 3.177764892578125,
      "learning_rate": 7.444009953785994e-05,
      "loss": 0.46,
      "step": 8830
    },
    {
      "epoch": 3.142552435122645,
      "grad_norm": 4.9563751220703125,
      "learning_rate": 7.42979025950942e-05,
      "loss": 0.3508,
      "step": 8840
    },
    {
      "epoch": 3.146107358691788,
      "grad_norm": 1.180750846862793,
      "learning_rate": 7.415570565232848e-05,
      "loss": 0.4173,
      "step": 8850
    },
    {
      "epoch": 3.1496622822609313,
      "grad_norm": 1.623831868171692,
      "learning_rate": 7.401350870956275e-05,
      "loss": 0.4027,
      "step": 8860
    },
    {
      "epoch": 3.1532172058300745,
      "grad_norm": 0.7234892845153809,
      "learning_rate": 7.387131176679701e-05,
      "loss": 0.3716,
      "step": 8870
    },
    {
      "epoch": 3.1567721293992177,
      "grad_norm": 1.0595039129257202,
      "learning_rate": 7.372911482403129e-05,
      "loss": 0.3285,
      "step": 8880
    },
    {
      "epoch": 3.1603270529683614,
      "grad_norm": 2.59775447845459,
      "learning_rate": 7.358691788126557e-05,
      "loss": 0.354,
      "step": 8890
    },
    {
      "epoch": 3.1638819765375046,
      "grad_norm": 1.9821498394012451,
      "learning_rate": 7.344472093849983e-05,
      "loss": 0.3269,
      "step": 8900
    },
    {
      "epoch": 3.167436900106648,
      "grad_norm": 1.28731107711792,
      "learning_rate": 7.33025239957341e-05,
      "loss": 0.4379,
      "step": 8910
    },
    {
      "epoch": 3.170991823675791,
      "grad_norm": 1.9398829936981201,
      "learning_rate": 7.316032705296836e-05,
      "loss": 0.4912,
      "step": 8920
    },
    {
      "epoch": 3.1745467472449342,
      "grad_norm": 2.8548693656921387,
      "learning_rate": 7.301813011020263e-05,
      "loss": 0.4405,
      "step": 8930
    },
    {
      "epoch": 3.1781016708140775,
      "grad_norm": 4.846094608306885,
      "learning_rate": 7.28759331674369e-05,
      "loss": 0.4148,
      "step": 8940
    },
    {
      "epoch": 3.1816565943832207,
      "grad_norm": 1.8558158874511719,
      "learning_rate": 7.273373622467118e-05,
      "loss": 0.5444,
      "step": 8950
    },
    {
      "epoch": 3.185211517952364,
      "grad_norm": 2.4874181747436523,
      "learning_rate": 7.259153928190544e-05,
      "loss": 0.4211,
      "step": 8960
    },
    {
      "epoch": 3.188766441521507,
      "grad_norm": 3.902548313140869,
      "learning_rate": 7.244934233913971e-05,
      "loss": 0.4103,
      "step": 8970
    },
    {
      "epoch": 3.192321365090651,
      "grad_norm": 2.3036906719207764,
      "learning_rate": 7.230714539637397e-05,
      "loss": 0.4004,
      "step": 8980
    },
    {
      "epoch": 3.195876288659794,
      "grad_norm": 4.396113872528076,
      "learning_rate": 7.216494845360825e-05,
      "loss": 0.3755,
      "step": 8990
    },
    {
      "epoch": 3.1994312122289372,
      "grad_norm": 2.462993621826172,
      "learning_rate": 7.202275151084252e-05,
      "loss": 0.3348,
      "step": 9000
    },
    {
      "epoch": 3.2029861357980804,
      "grad_norm": 1.7393025159835815,
      "learning_rate": 7.188055456807679e-05,
      "loss": 0.3208,
      "step": 9010
    },
    {
      "epoch": 3.2065410593672237,
      "grad_norm": 0.7843826413154602,
      "learning_rate": 7.173835762531106e-05,
      "loss": 0.3224,
      "step": 9020
    },
    {
      "epoch": 3.210095982936367,
      "grad_norm": 1.0368733406066895,
      "learning_rate": 7.159616068254534e-05,
      "loss": 0.3109,
      "step": 9030
    },
    {
      "epoch": 3.21365090650551,
      "grad_norm": 2.571885824203491,
      "learning_rate": 7.14539637397796e-05,
      "loss": 0.3506,
      "step": 9040
    },
    {
      "epoch": 3.2172058300746533,
      "grad_norm": 2.767112970352173,
      "learning_rate": 7.131176679701387e-05,
      "loss": 0.3312,
      "step": 9050
    },
    {
      "epoch": 3.2207607536437965,
      "grad_norm": 0.6228271722793579,
      "learning_rate": 7.116956985424814e-05,
      "loss": 0.4133,
      "step": 9060
    },
    {
      "epoch": 3.2243156772129398,
      "grad_norm": 2.738466501235962,
      "learning_rate": 7.10273729114824e-05,
      "loss": 0.4736,
      "step": 9070
    },
    {
      "epoch": 3.227870600782083,
      "grad_norm": 1.8431960344314575,
      "learning_rate": 7.088517596871667e-05,
      "loss": 0.375,
      "step": 9080
    },
    {
      "epoch": 3.2314255243512267,
      "grad_norm": 3.8363630771636963,
      "learning_rate": 7.074297902595095e-05,
      "loss": 0.4244,
      "step": 9090
    },
    {
      "epoch": 3.23498044792037,
      "grad_norm": 1.1960492134094238,
      "learning_rate": 7.060078208318521e-05,
      "loss": 0.421,
      "step": 9100
    },
    {
      "epoch": 3.238535371489513,
      "grad_norm": 4.641552448272705,
      "learning_rate": 7.045858514041948e-05,
      "loss": 0.3723,
      "step": 9110
    },
    {
      "epoch": 3.2420902950586563,
      "grad_norm": 2.553812265396118,
      "learning_rate": 7.031638819765375e-05,
      "loss": 0.3581,
      "step": 9120
    },
    {
      "epoch": 3.2456452186277995,
      "grad_norm": 2.8874104022979736,
      "learning_rate": 7.017419125488802e-05,
      "loss": 0.306,
      "step": 9130
    },
    {
      "epoch": 3.2492001421969428,
      "grad_norm": 2.898719072341919,
      "learning_rate": 7.00319943121223e-05,
      "loss": 0.4784,
      "step": 9140
    },
    {
      "epoch": 3.252755065766086,
      "grad_norm": 2.010065793991089,
      "learning_rate": 6.988979736935656e-05,
      "loss": 0.3969,
      "step": 9150
    },
    {
      "epoch": 3.256309989335229,
      "grad_norm": 1.1339024305343628,
      "learning_rate": 6.974760042659083e-05,
      "loss": 0.416,
      "step": 9160
    },
    {
      "epoch": 3.2598649129043724,
      "grad_norm": 1.0418339967727661,
      "learning_rate": 6.960540348382511e-05,
      "loss": 0.3456,
      "step": 9170
    },
    {
      "epoch": 3.2634198364735156,
      "grad_norm": 1.3420706987380981,
      "learning_rate": 6.946320654105937e-05,
      "loss": 0.2955,
      "step": 9180
    },
    {
      "epoch": 3.2669747600426593,
      "grad_norm": 4.1877522468566895,
      "learning_rate": 6.932100959829365e-05,
      "loss": 0.4078,
      "step": 9190
    },
    {
      "epoch": 3.2705296836118025,
      "grad_norm": 0.5708364844322205,
      "learning_rate": 6.917881265552791e-05,
      "loss": 0.3401,
      "step": 9200
    },
    {
      "epoch": 3.2740846071809457,
      "grad_norm": 4.43261194229126,
      "learning_rate": 6.903661571276218e-05,
      "loss": 0.3994,
      "step": 9210
    },
    {
      "epoch": 3.277639530750089,
      "grad_norm": 1.338051199913025,
      "learning_rate": 6.889441876999644e-05,
      "loss": 0.4008,
      "step": 9220
    },
    {
      "epoch": 3.281194454319232,
      "grad_norm": 1.2899547815322876,
      "learning_rate": 6.875222182723072e-05,
      "loss": 0.3591,
      "step": 9230
    },
    {
      "epoch": 3.2847493778883754,
      "grad_norm": 2.5991523265838623,
      "learning_rate": 6.861002488446498e-05,
      "loss": 0.3398,
      "step": 9240
    },
    {
      "epoch": 3.2883043014575186,
      "grad_norm": 4.1426239013671875,
      "learning_rate": 6.846782794169926e-05,
      "loss": 0.3616,
      "step": 9250
    },
    {
      "epoch": 3.291859225026662,
      "grad_norm": 9.464312553405762,
      "learning_rate": 6.832563099893352e-05,
      "loss": 0.4666,
      "step": 9260
    },
    {
      "epoch": 3.295414148595805,
      "grad_norm": 5.401423931121826,
      "learning_rate": 6.818343405616779e-05,
      "loss": 0.4223,
      "step": 9270
    },
    {
      "epoch": 3.2989690721649483,
      "grad_norm": 2.0760231018066406,
      "learning_rate": 6.804123711340207e-05,
      "loss": 0.3592,
      "step": 9280
    },
    {
      "epoch": 3.3025239957340915,
      "grad_norm": 1.0978097915649414,
      "learning_rate": 6.789904017063633e-05,
      "loss": 0.3592,
      "step": 9290
    },
    {
      "epoch": 3.306078919303235,
      "grad_norm": 1.6214163303375244,
      "learning_rate": 6.77568432278706e-05,
      "loss": 0.4496,
      "step": 9300
    },
    {
      "epoch": 3.3096338428723784,
      "grad_norm": 0.9935455918312073,
      "learning_rate": 6.761464628510488e-05,
      "loss": 0.3116,
      "step": 9310
    },
    {
      "epoch": 3.3131887664415216,
      "grad_norm": 1.9466606378555298,
      "learning_rate": 6.747244934233914e-05,
      "loss": 0.3726,
      "step": 9320
    },
    {
      "epoch": 3.316743690010665,
      "grad_norm": 1.108750343322754,
      "learning_rate": 6.733025239957342e-05,
      "loss": 0.3636,
      "step": 9330
    },
    {
      "epoch": 3.320298613579808,
      "grad_norm": 3.927797555923462,
      "learning_rate": 6.718805545680769e-05,
      "loss": 0.4131,
      "step": 9340
    },
    {
      "epoch": 3.3238535371489513,
      "grad_norm": 3.1799309253692627,
      "learning_rate": 6.704585851404195e-05,
      "loss": 0.4032,
      "step": 9350
    },
    {
      "epoch": 3.3274084607180945,
      "grad_norm": 3.353276491165161,
      "learning_rate": 6.690366157127622e-05,
      "loss": 0.312,
      "step": 9360
    },
    {
      "epoch": 3.3309633842872377,
      "grad_norm": 6.299170970916748,
      "learning_rate": 6.676146462851049e-05,
      "loss": 0.3768,
      "step": 9370
    },
    {
      "epoch": 3.334518307856381,
      "grad_norm": 1.0507986545562744,
      "learning_rate": 6.661926768574475e-05,
      "loss": 0.4682,
      "step": 9380
    },
    {
      "epoch": 3.3380732314255246,
      "grad_norm": 2.369075298309326,
      "learning_rate": 6.647707074297903e-05,
      "loss": 0.3775,
      "step": 9390
    },
    {
      "epoch": 3.341628154994668,
      "grad_norm": 2.3168230056762695,
      "learning_rate": 6.63348738002133e-05,
      "loss": 0.3786,
      "step": 9400
    },
    {
      "epoch": 3.345183078563811,
      "grad_norm": 3.51485538482666,
      "learning_rate": 6.619267685744756e-05,
      "loss": 0.4657,
      "step": 9410
    },
    {
      "epoch": 3.3487380021329543,
      "grad_norm": 0.6236200332641602,
      "learning_rate": 6.605047991468184e-05,
      "loss": 0.3544,
      "step": 9420
    },
    {
      "epoch": 3.3522929257020975,
      "grad_norm": 2.9922358989715576,
      "learning_rate": 6.59082829719161e-05,
      "loss": 0.3715,
      "step": 9430
    },
    {
      "epoch": 3.3558478492712407,
      "grad_norm": 2.3765618801116943,
      "learning_rate": 6.576608602915038e-05,
      "loss": 0.3676,
      "step": 9440
    },
    {
      "epoch": 3.359402772840384,
      "grad_norm": 1.321534276008606,
      "learning_rate": 6.562388908638465e-05,
      "loss": 0.4085,
      "step": 9450
    },
    {
      "epoch": 3.362957696409527,
      "grad_norm": 0.9152252674102783,
      "learning_rate": 6.548169214361891e-05,
      "loss": 0.407,
      "step": 9460
    },
    {
      "epoch": 3.3665126199786704,
      "grad_norm": 2.8115437030792236,
      "learning_rate": 6.533949520085319e-05,
      "loss": 0.3564,
      "step": 9470
    },
    {
      "epoch": 3.3700675435478136,
      "grad_norm": 1.738379955291748,
      "learning_rate": 6.519729825808746e-05,
      "loss": 0.5301,
      "step": 9480
    },
    {
      "epoch": 3.373622467116957,
      "grad_norm": 1.0378295183181763,
      "learning_rate": 6.505510131532173e-05,
      "loss": 0.3319,
      "step": 9490
    },
    {
      "epoch": 3.3771773906861,
      "grad_norm": 1.4679099321365356,
      "learning_rate": 6.491290437255599e-05,
      "loss": 0.4127,
      "step": 9500
    },
    {
      "epoch": 3.3807323142552437,
      "grad_norm": 2.6128270626068115,
      "learning_rate": 6.477070742979026e-05,
      "loss": 0.4701,
      "step": 9510
    },
    {
      "epoch": 3.384287237824387,
      "grad_norm": 2.6626806259155273,
      "learning_rate": 6.462851048702452e-05,
      "loss": 0.4155,
      "step": 9520
    },
    {
      "epoch": 3.38784216139353,
      "grad_norm": 2.1353695392608643,
      "learning_rate": 6.44863135442588e-05,
      "loss": 0.4047,
      "step": 9530
    },
    {
      "epoch": 3.3913970849626733,
      "grad_norm": 2.872572898864746,
      "learning_rate": 6.434411660149307e-05,
      "loss": 0.3389,
      "step": 9540
    },
    {
      "epoch": 3.3949520085318166,
      "grad_norm": 1.3029711246490479,
      "learning_rate": 6.420191965872734e-05,
      "loss": 0.3603,
      "step": 9550
    },
    {
      "epoch": 3.3985069321009598,
      "grad_norm": 1.4950783252716064,
      "learning_rate": 6.405972271596161e-05,
      "loss": 0.3762,
      "step": 9560
    },
    {
      "epoch": 3.402061855670103,
      "grad_norm": 3.458385705947876,
      "learning_rate": 6.391752577319587e-05,
      "loss": 0.3556,
      "step": 9570
    },
    {
      "epoch": 3.405616779239246,
      "grad_norm": 1.547417402267456,
      "learning_rate": 6.377532883043015e-05,
      "loss": 0.4146,
      "step": 9580
    },
    {
      "epoch": 3.4091717028083894,
      "grad_norm": 1.229134202003479,
      "learning_rate": 6.363313188766442e-05,
      "loss": 0.4053,
      "step": 9590
    },
    {
      "epoch": 3.412726626377533,
      "grad_norm": 0.6464443802833557,
      "learning_rate": 6.349093494489869e-05,
      "loss": 0.3841,
      "step": 9600
    },
    {
      "epoch": 3.4162815499466763,
      "grad_norm": 0.6337963938713074,
      "learning_rate": 6.334873800213296e-05,
      "loss": 0.433,
      "step": 9610
    },
    {
      "epoch": 3.4198364735158195,
      "grad_norm": 1.4186855554580688,
      "learning_rate": 6.320654105936724e-05,
      "loss": 0.4234,
      "step": 9620
    },
    {
      "epoch": 3.4233913970849628,
      "grad_norm": 0.9378098249435425,
      "learning_rate": 6.30643441166015e-05,
      "loss": 0.3962,
      "step": 9630
    },
    {
      "epoch": 3.426946320654106,
      "grad_norm": 1.3425337076187134,
      "learning_rate": 6.292214717383577e-05,
      "loss": 0.3565,
      "step": 9640
    },
    {
      "epoch": 3.430501244223249,
      "grad_norm": 1.022175669670105,
      "learning_rate": 6.277995023107003e-05,
      "loss": 0.2454,
      "step": 9650
    },
    {
      "epoch": 3.4340561677923924,
      "grad_norm": 5.915073394775391,
      "learning_rate": 6.26377532883043e-05,
      "loss": 0.4305,
      "step": 9660
    },
    {
      "epoch": 3.4376110913615356,
      "grad_norm": 0.6860565543174744,
      "learning_rate": 6.249555634553857e-05,
      "loss": 0.506,
      "step": 9670
    },
    {
      "epoch": 3.441166014930679,
      "grad_norm": 2.4150660037994385,
      "learning_rate": 6.235335940277285e-05,
      "loss": 0.4776,
      "step": 9680
    },
    {
      "epoch": 3.444720938499822,
      "grad_norm": 3.5441341400146484,
      "learning_rate": 6.221116246000711e-05,
      "loss": 0.4438,
      "step": 9690
    },
    {
      "epoch": 3.4482758620689653,
      "grad_norm": 2.1261065006256104,
      "learning_rate": 6.206896551724138e-05,
      "loss": 0.308,
      "step": 9700
    },
    {
      "epoch": 3.451830785638109,
      "grad_norm": 1.9123247861862183,
      "learning_rate": 6.192676857447564e-05,
      "loss": 0.3691,
      "step": 9710
    },
    {
      "epoch": 3.455385709207252,
      "grad_norm": 2.79682993888855,
      "learning_rate": 6.178457163170992e-05,
      "loss": 0.3324,
      "step": 9720
    },
    {
      "epoch": 3.4589406327763954,
      "grad_norm": 1.7168766260147095,
      "learning_rate": 6.16423746889442e-05,
      "loss": 0.36,
      "step": 9730
    },
    {
      "epoch": 3.4624955563455386,
      "grad_norm": 1.5087943077087402,
      "learning_rate": 6.150017774617846e-05,
      "loss": 0.4189,
      "step": 9740
    },
    {
      "epoch": 3.466050479914682,
      "grad_norm": 1.1348586082458496,
      "learning_rate": 6.135798080341273e-05,
      "loss": 0.4195,
      "step": 9750
    },
    {
      "epoch": 3.469605403483825,
      "grad_norm": 1.288109540939331,
      "learning_rate": 6.121578386064701e-05,
      "loss": 0.3262,
      "step": 9760
    },
    {
      "epoch": 3.4731603270529683,
      "grad_norm": 2.581171989440918,
      "learning_rate": 6.107358691788127e-05,
      "loss": 0.4526,
      "step": 9770
    },
    {
      "epoch": 3.4767152506221115,
      "grad_norm": 2.5150465965270996,
      "learning_rate": 6.093138997511554e-05,
      "loss": 0.3691,
      "step": 9780
    },
    {
      "epoch": 3.4802701741912547,
      "grad_norm": 2.8858883380889893,
      "learning_rate": 6.078919303234981e-05,
      "loss": 0.4262,
      "step": 9790
    },
    {
      "epoch": 3.4838250977603984,
      "grad_norm": 1.9503780603408813,
      "learning_rate": 6.0646996089584074e-05,
      "loss": 0.3884,
      "step": 9800
    },
    {
      "epoch": 3.4873800213295416,
      "grad_norm": 1.1031875610351562,
      "learning_rate": 6.050479914681835e-05,
      "loss": 0.347,
      "step": 9810
    },
    {
      "epoch": 3.490934944898685,
      "grad_norm": 1.5046648979187012,
      "learning_rate": 6.036260220405262e-05,
      "loss": 0.3944,
      "step": 9820
    },
    {
      "epoch": 3.494489868467828,
      "grad_norm": 1.1312620639801025,
      "learning_rate": 6.022040526128688e-05,
      "loss": 0.2968,
      "step": 9830
    },
    {
      "epoch": 3.4980447920369713,
      "grad_norm": 3.6867096424102783,
      "learning_rate": 6.0078208318521155e-05,
      "loss": 0.3539,
      "step": 9840
    },
    {
      "epoch": 3.5015997156061145,
      "grad_norm": 1.622599482536316,
      "learning_rate": 5.9936011375755417e-05,
      "loss": 0.3258,
      "step": 9850
    },
    {
      "epoch": 3.5051546391752577,
      "grad_norm": 2.9354801177978516,
      "learning_rate": 5.979381443298969e-05,
      "loss": 0.4287,
      "step": 9860
    },
    {
      "epoch": 3.508709562744401,
      "grad_norm": 3.611701488494873,
      "learning_rate": 5.965161749022397e-05,
      "loss": 0.3633,
      "step": 9870
    },
    {
      "epoch": 3.512264486313544,
      "grad_norm": 3.03193736076355,
      "learning_rate": 5.950942054745823e-05,
      "loss": 0.3626,
      "step": 9880
    },
    {
      "epoch": 3.5158194098826874,
      "grad_norm": 1.6054623126983643,
      "learning_rate": 5.9367223604692504e-05,
      "loss": 0.4235,
      "step": 9890
    },
    {
      "epoch": 3.5193743334518306,
      "grad_norm": 1.5550864934921265,
      "learning_rate": 5.922502666192677e-05,
      "loss": 0.4785,
      "step": 9900
    },
    {
      "epoch": 3.522929257020974,
      "grad_norm": 4.889039516448975,
      "learning_rate": 5.9082829719161034e-05,
      "loss": 0.3459,
      "step": 9910
    },
    {
      "epoch": 3.5264841805901175,
      "grad_norm": 2.256932020187378,
      "learning_rate": 5.894063277639531e-05,
      "loss": 0.36,
      "step": 9920
    },
    {
      "epoch": 3.5300391041592607,
      "grad_norm": 2.5926928520202637,
      "learning_rate": 5.8798435833629584e-05,
      "loss": 0.4403,
      "step": 9930
    },
    {
      "epoch": 3.533594027728404,
      "grad_norm": 1.24857497215271,
      "learning_rate": 5.8656238890863846e-05,
      "loss": 0.3504,
      "step": 9940
    },
    {
      "epoch": 3.537148951297547,
      "grad_norm": 1.8928724527359009,
      "learning_rate": 5.851404194809812e-05,
      "loss": 0.4632,
      "step": 9950
    },
    {
      "epoch": 3.5407038748666904,
      "grad_norm": 1.9153492450714111,
      "learning_rate": 5.837184500533239e-05,
      "loss": 0.4952,
      "step": 9960
    },
    {
      "epoch": 3.5442587984358336,
      "grad_norm": 2.3824427127838135,
      "learning_rate": 5.822964806256666e-05,
      "loss": 0.3575,
      "step": 9970
    },
    {
      "epoch": 3.547813722004977,
      "grad_norm": 1.533184289932251,
      "learning_rate": 5.8087451119800927e-05,
      "loss": 0.4236,
      "step": 9980
    },
    {
      "epoch": 3.55136864557412,
      "grad_norm": 1.7333029508590698,
      "learning_rate": 5.79452541770352e-05,
      "loss": 0.2885,
      "step": 9990
    },
    {
      "epoch": 3.5549235691432637,
      "grad_norm": 1.089228630065918,
      "learning_rate": 5.7803057234269463e-05,
      "loss": 0.415,
      "step": 10000
    },
    {
      "epoch": 3.558478492712407,
      "grad_norm": 2.95732045173645,
      "learning_rate": 5.766086029150374e-05,
      "loss": 0.4188,
      "step": 10010
    },
    {
      "epoch": 3.56203341628155,
      "grad_norm": 4.756927013397217,
      "learning_rate": 5.7518663348738e-05,
      "loss": 0.4258,
      "step": 10020
    },
    {
      "epoch": 3.5655883398506933,
      "grad_norm": 1.3557541370391846,
      "learning_rate": 5.7376466405972276e-05,
      "loss": 0.2845,
      "step": 10030
    },
    {
      "epoch": 3.5691432634198366,
      "grad_norm": 1.0937870740890503,
      "learning_rate": 5.7234269463206544e-05,
      "loss": 0.3353,
      "step": 10040
    },
    {
      "epoch": 3.57269818698898,
      "grad_norm": 2.5492162704467773,
      "learning_rate": 5.709207252044081e-05,
      "loss": 0.3576,
      "step": 10050
    },
    {
      "epoch": 3.576253110558123,
      "grad_norm": 1.29802668094635,
      "learning_rate": 5.694987557767508e-05,
      "loss": 0.2869,
      "step": 10060
    },
    {
      "epoch": 3.5798080341272662,
      "grad_norm": 1.845753788948059,
      "learning_rate": 5.6807678634909356e-05,
      "loss": 0.3926,
      "step": 10070
    },
    {
      "epoch": 3.5833629576964094,
      "grad_norm": 2.927309513092041,
      "learning_rate": 5.666548169214362e-05,
      "loss": 0.2938,
      "step": 10080
    },
    {
      "epoch": 3.5869178812655527,
      "grad_norm": 1.9453901052474976,
      "learning_rate": 5.652328474937789e-05,
      "loss": 0.4225,
      "step": 10090
    },
    {
      "epoch": 3.590472804834696,
      "grad_norm": 1.485055685043335,
      "learning_rate": 5.638108780661217e-05,
      "loss": 0.347,
      "step": 10100
    },
    {
      "epoch": 3.594027728403839,
      "grad_norm": 3.1072182655334473,
      "learning_rate": 5.623889086384643e-05,
      "loss": 0.341,
      "step": 10110
    },
    {
      "epoch": 3.5975826519729823,
      "grad_norm": 2.321288585662842,
      "learning_rate": 5.60966939210807e-05,
      "loss": 0.4811,
      "step": 10120
    },
    {
      "epoch": 3.601137575542126,
      "grad_norm": 2.6745357513427734,
      "learning_rate": 5.5954496978314973e-05,
      "loss": 0.3676,
      "step": 10130
    },
    {
      "epoch": 3.604692499111269,
      "grad_norm": 1.585580825805664,
      "learning_rate": 5.5812300035549235e-05,
      "loss": 0.3458,
      "step": 10140
    },
    {
      "epoch": 3.6082474226804124,
      "grad_norm": 1.150804877281189,
      "learning_rate": 5.567010309278351e-05,
      "loss": 0.3132,
      "step": 10150
    },
    {
      "epoch": 3.6118023462495557,
      "grad_norm": 2.953601837158203,
      "learning_rate": 5.552790615001777e-05,
      "loss": 0.4263,
      "step": 10160
    },
    {
      "epoch": 3.615357269818699,
      "grad_norm": 3.747802495956421,
      "learning_rate": 5.538570920725205e-05,
      "loss": 0.3868,
      "step": 10170
    },
    {
      "epoch": 3.618912193387842,
      "grad_norm": 0.9915607571601868,
      "learning_rate": 5.5243512264486316e-05,
      "loss": 0.4153,
      "step": 10180
    },
    {
      "epoch": 3.6224671169569853,
      "grad_norm": 1.0303573608398438,
      "learning_rate": 5.5101315321720584e-05,
      "loss": 0.3317,
      "step": 10190
    },
    {
      "epoch": 3.6260220405261285,
      "grad_norm": 1.035593032836914,
      "learning_rate": 5.495911837895485e-05,
      "loss": 0.3848,
      "step": 10200
    },
    {
      "epoch": 3.629576964095272,
      "grad_norm": 2.902449131011963,
      "learning_rate": 5.481692143618913e-05,
      "loss": 0.3811,
      "step": 10210
    },
    {
      "epoch": 3.6331318876644154,
      "grad_norm": 5.237423896789551,
      "learning_rate": 5.467472449342339e-05,
      "loss": 0.2998,
      "step": 10220
    },
    {
      "epoch": 3.6366868112335586,
      "grad_norm": 1.6525412797927856,
      "learning_rate": 5.4532527550657665e-05,
      "loss": 0.39,
      "step": 10230
    },
    {
      "epoch": 3.640241734802702,
      "grad_norm": 1.2304760217666626,
      "learning_rate": 5.439033060789194e-05,
      "loss": 0.4147,
      "step": 10240
    },
    {
      "epoch": 3.643796658371845,
      "grad_norm": 1.9165832996368408,
      "learning_rate": 5.42481336651262e-05,
      "loss": 0.2891,
      "step": 10250
    },
    {
      "epoch": 3.6473515819409883,
      "grad_norm": 5.140467643737793,
      "learning_rate": 5.410593672236047e-05,
      "loss": 0.445,
      "step": 10260
    },
    {
      "epoch": 3.6509065055101315,
      "grad_norm": 3.5859920978546143,
      "learning_rate": 5.3963739779594745e-05,
      "loss": 0.4807,
      "step": 10270
    },
    {
      "epoch": 3.6544614290792747,
      "grad_norm": 1.650202989578247,
      "learning_rate": 5.382154283682901e-05,
      "loss": 0.3633,
      "step": 10280
    },
    {
      "epoch": 3.658016352648418,
      "grad_norm": 0.9593248963356018,
      "learning_rate": 5.367934589406328e-05,
      "loss": 0.4372,
      "step": 10290
    },
    {
      "epoch": 3.661571276217561,
      "grad_norm": 1.8746387958526611,
      "learning_rate": 5.3537148951297544e-05,
      "loss": 0.3441,
      "step": 10300
    },
    {
      "epoch": 3.6651261997867044,
      "grad_norm": 2.2975428104400635,
      "learning_rate": 5.339495200853182e-05,
      "loss": 0.3348,
      "step": 10310
    },
    {
      "epoch": 3.6686811233558476,
      "grad_norm": 1.9703340530395508,
      "learning_rate": 5.3252755065766094e-05,
      "loss": 0.3939,
      "step": 10320
    },
    {
      "epoch": 3.672236046924991,
      "grad_norm": 2.944546699523926,
      "learning_rate": 5.3110558123000356e-05,
      "loss": 0.5233,
      "step": 10330
    },
    {
      "epoch": 3.6757909704941345,
      "grad_norm": 3.7473397254943848,
      "learning_rate": 5.2968361180234624e-05,
      "loss": 0.3322,
      "step": 10340
    },
    {
      "epoch": 3.6793458940632777,
      "grad_norm": 1.2155373096466064,
      "learning_rate": 5.28261642374689e-05,
      "loss": 0.4114,
      "step": 10350
    },
    {
      "epoch": 3.682900817632421,
      "grad_norm": 2.6125681400299072,
      "learning_rate": 5.268396729470316e-05,
      "loss": 0.5509,
      "step": 10360
    },
    {
      "epoch": 3.686455741201564,
      "grad_norm": 2.575827121734619,
      "learning_rate": 5.2541770351937436e-05,
      "loss": 0.387,
      "step": 10370
    },
    {
      "epoch": 3.6900106647707074,
      "grad_norm": 0.6447463631629944,
      "learning_rate": 5.239957340917171e-05,
      "loss": 0.4758,
      "step": 10380
    },
    {
      "epoch": 3.6935655883398506,
      "grad_norm": 3.637387752532959,
      "learning_rate": 5.225737646640597e-05,
      "loss": 0.4192,
      "step": 10390
    },
    {
      "epoch": 3.697120511908994,
      "grad_norm": 5.033977031707764,
      "learning_rate": 5.211517952364025e-05,
      "loss": 0.3797,
      "step": 10400
    },
    {
      "epoch": 3.7006754354781375,
      "grad_norm": 2.8189003467559814,
      "learning_rate": 5.197298258087452e-05,
      "loss": 0.3896,
      "step": 10410
    },
    {
      "epoch": 3.7042303590472807,
      "grad_norm": 3.1963510513305664,
      "learning_rate": 5.183078563810878e-05,
      "loss": 0.4312,
      "step": 10420
    },
    {
      "epoch": 3.707785282616424,
      "grad_norm": 4.958958625793457,
      "learning_rate": 5.1688588695343054e-05,
      "loss": 0.4232,
      "step": 10430
    },
    {
      "epoch": 3.711340206185567,
      "grad_norm": 1.4999263286590576,
      "learning_rate": 5.1546391752577315e-05,
      "loss": 0.3853,
      "step": 10440
    },
    {
      "epoch": 3.7148951297547104,
      "grad_norm": 1.8474879264831543,
      "learning_rate": 5.140419480981159e-05,
      "loss": 0.3417,
      "step": 10450
    },
    {
      "epoch": 3.7184500533238536,
      "grad_norm": 2.780515670776367,
      "learning_rate": 5.1261997867045866e-05,
      "loss": 0.4232,
      "step": 10460
    },
    {
      "epoch": 3.722004976892997,
      "grad_norm": 1.3744457960128784,
      "learning_rate": 5.111980092428013e-05,
      "loss": 0.338,
      "step": 10470
    },
    {
      "epoch": 3.72555990046214,
      "grad_norm": 1.6949714422225952,
      "learning_rate": 5.0977603981514396e-05,
      "loss": 0.3861,
      "step": 10480
    },
    {
      "epoch": 3.7291148240312832,
      "grad_norm": 0.7991328239440918,
      "learning_rate": 5.083540703874867e-05,
      "loss": 0.4404,
      "step": 10490
    },
    {
      "epoch": 3.7326697476004265,
      "grad_norm": 1.547674536705017,
      "learning_rate": 5.069321009598293e-05,
      "loss": 0.2953,
      "step": 10500
    },
    {
      "epoch": 3.7362246711695697,
      "grad_norm": 6.051143646240234,
      "learning_rate": 5.055101315321721e-05,
      "loss": 0.4481,
      "step": 10510
    },
    {
      "epoch": 3.739779594738713,
      "grad_norm": 1.913223385810852,
      "learning_rate": 5.040881621045148e-05,
      "loss": 0.4753,
      "step": 10520
    },
    {
      "epoch": 3.743334518307856,
      "grad_norm": 1.975264072418213,
      "learning_rate": 5.0266619267685745e-05,
      "loss": 0.4172,
      "step": 10530
    },
    {
      "epoch": 3.746889441877,
      "grad_norm": 3.5466601848602295,
      "learning_rate": 5.012442232492002e-05,
      "loss": 0.2736,
      "step": 10540
    },
    {
      "epoch": 3.750444365446143,
      "grad_norm": 4.32005500793457,
      "learning_rate": 4.998222538215429e-05,
      "loss": 0.3962,
      "step": 10550
    },
    {
      "epoch": 3.7539992890152862,
      "grad_norm": 3.2569165229797363,
      "learning_rate": 4.984002843938855e-05,
      "loss": 0.4296,
      "step": 10560
    },
    {
      "epoch": 3.7575542125844295,
      "grad_norm": 2.707894802093506,
      "learning_rate": 4.9697831496622825e-05,
      "loss": 0.4529,
      "step": 10570
    },
    {
      "epoch": 3.7611091361535727,
      "grad_norm": 2.454164981842041,
      "learning_rate": 4.9555634553857094e-05,
      "loss": 0.3776,
      "step": 10580
    },
    {
      "epoch": 3.764664059722716,
      "grad_norm": 1.8366259336471558,
      "learning_rate": 4.941343761109136e-05,
      "loss": 0.4089,
      "step": 10590
    },
    {
      "epoch": 3.768218983291859,
      "grad_norm": 5.944333076477051,
      "learning_rate": 4.927124066832564e-05,
      "loss": 0.4289,
      "step": 10600
    },
    {
      "epoch": 3.7717739068610023,
      "grad_norm": 2.3752474784851074,
      "learning_rate": 4.9129043725559906e-05,
      "loss": 0.3275,
      "step": 10610
    },
    {
      "epoch": 3.775328830430146,
      "grad_norm": 3.3194260597229004,
      "learning_rate": 4.8986846782794174e-05,
      "loss": 0.4724,
      "step": 10620
    },
    {
      "epoch": 3.778883753999289,
      "grad_norm": 4.019346714019775,
      "learning_rate": 4.8844649840028436e-05,
      "loss": 0.3391,
      "step": 10630
    },
    {
      "epoch": 3.7824386775684324,
      "grad_norm": 5.908036708831787,
      "learning_rate": 4.870245289726271e-05,
      "loss": 0.4106,
      "step": 10640
    },
    {
      "epoch": 3.7859936011375757,
      "grad_norm": 1.0762481689453125,
      "learning_rate": 4.856025595449698e-05,
      "loss": 0.3063,
      "step": 10650
    },
    {
      "epoch": 3.789548524706719,
      "grad_norm": 2.5287113189697266,
      "learning_rate": 4.841805901173125e-05,
      "loss": 0.3948,
      "step": 10660
    },
    {
      "epoch": 3.793103448275862,
      "grad_norm": 3.4215011596679688,
      "learning_rate": 4.827586206896552e-05,
      "loss": 0.3942,
      "step": 10670
    },
    {
      "epoch": 3.7966583718450053,
      "grad_norm": 1.5653674602508545,
      "learning_rate": 4.813366512619979e-05,
      "loss": 0.3555,
      "step": 10680
    },
    {
      "epoch": 3.8002132954141485,
      "grad_norm": 1.7926684617996216,
      "learning_rate": 4.799146818343406e-05,
      "loss": 0.3705,
      "step": 10690
    },
    {
      "epoch": 3.8037682189832918,
      "grad_norm": 9.811566352844238,
      "learning_rate": 4.784927124066833e-05,
      "loss": 0.4712,
      "step": 10700
    },
    {
      "epoch": 3.807323142552435,
      "grad_norm": 1.057242751121521,
      "learning_rate": 4.77070742979026e-05,
      "loss": 0.358,
      "step": 10710
    },
    {
      "epoch": 3.810878066121578,
      "grad_norm": 1.623126745223999,
      "learning_rate": 4.7564877355136865e-05,
      "loss": 0.3747,
      "step": 10720
    },
    {
      "epoch": 3.8144329896907214,
      "grad_norm": 2.3146657943725586,
      "learning_rate": 4.7422680412371134e-05,
      "loss": 0.3153,
      "step": 10730
    },
    {
      "epoch": 3.8179879132598646,
      "grad_norm": 0.509503185749054,
      "learning_rate": 4.728048346960541e-05,
      "loss": 0.3248,
      "step": 10740
    },
    {
      "epoch": 3.8215428368290083,
      "grad_norm": 4.724321365356445,
      "learning_rate": 4.713828652683968e-05,
      "loss": 0.4329,
      "step": 10750
    },
    {
      "epoch": 3.8250977603981515,
      "grad_norm": 2.9779129028320312,
      "learning_rate": 4.6996089584073946e-05,
      "loss": 0.3836,
      "step": 10760
    },
    {
      "epoch": 3.8286526839672947,
      "grad_norm": 0.5160874724388123,
      "learning_rate": 4.6853892641308214e-05,
      "loss": 0.3973,
      "step": 10770
    },
    {
      "epoch": 3.832207607536438,
      "grad_norm": 3.472350597381592,
      "learning_rate": 4.671169569854248e-05,
      "loss": 0.4888,
      "step": 10780
    },
    {
      "epoch": 3.835762531105581,
      "grad_norm": 2.5550456047058105,
      "learning_rate": 4.656949875577675e-05,
      "loss": 0.3492,
      "step": 10790
    },
    {
      "epoch": 3.8393174546747244,
      "grad_norm": 1.2634307146072388,
      "learning_rate": 4.642730181301102e-05,
      "loss": 0.4146,
      "step": 10800
    },
    {
      "epoch": 3.8428723782438676,
      "grad_norm": 3.524564027786255,
      "learning_rate": 4.6285104870245295e-05,
      "loss": 0.381,
      "step": 10810
    },
    {
      "epoch": 3.8464273018130113,
      "grad_norm": 1.5264148712158203,
      "learning_rate": 4.6142907927479563e-05,
      "loss": 0.4491,
      "step": 10820
    },
    {
      "epoch": 3.8499822253821545,
      "grad_norm": 2.256599187850952,
      "learning_rate": 4.600071098471383e-05,
      "loss": 0.4104,
      "step": 10830
    },
    {
      "epoch": 3.8535371489512977,
      "grad_norm": 3.1614041328430176,
      "learning_rate": 4.58585140419481e-05,
      "loss": 0.4012,
      "step": 10840
    },
    {
      "epoch": 3.857092072520441,
      "grad_norm": 1.4844751358032227,
      "learning_rate": 4.571631709918237e-05,
      "loss": 0.3773,
      "step": 10850
    },
    {
      "epoch": 3.860646996089584,
      "grad_norm": 1.0465482473373413,
      "learning_rate": 4.557412015641664e-05,
      "loss": 0.3909,
      "step": 10860
    },
    {
      "epoch": 3.8642019196587274,
      "grad_norm": 2.5786147117614746,
      "learning_rate": 4.5431923213650906e-05,
      "loss": 0.4641,
      "step": 10870
    },
    {
      "epoch": 3.8677568432278706,
      "grad_norm": 1.792092204093933,
      "learning_rate": 4.528972627088518e-05,
      "loss": 0.4322,
      "step": 10880
    },
    {
      "epoch": 3.871311766797014,
      "grad_norm": 0.9687771797180176,
      "learning_rate": 4.514752932811945e-05,
      "loss": 0.2967,
      "step": 10890
    },
    {
      "epoch": 3.874866690366157,
      "grad_norm": 1.107487678527832,
      "learning_rate": 4.500533238535372e-05,
      "loss": 0.3767,
      "step": 10900
    },
    {
      "epoch": 3.8784216139353003,
      "grad_norm": 3.418389320373535,
      "learning_rate": 4.4863135442587986e-05,
      "loss": 0.3923,
      "step": 10910
    },
    {
      "epoch": 3.8819765375044435,
      "grad_norm": 1.324615716934204,
      "learning_rate": 4.4720938499822255e-05,
      "loss": 0.4036,
      "step": 10920
    },
    {
      "epoch": 3.8855314610735867,
      "grad_norm": 2.8717775344848633,
      "learning_rate": 4.457874155705652e-05,
      "loss": 0.252,
      "step": 10930
    },
    {
      "epoch": 3.88908638464273,
      "grad_norm": 0.8373494148254395,
      "learning_rate": 4.443654461429079e-05,
      "loss": 0.4379,
      "step": 10940
    },
    {
      "epoch": 3.8926413082118736,
      "grad_norm": 1.7007808685302734,
      "learning_rate": 4.429434767152507e-05,
      "loss": 0.276,
      "step": 10950
    },
    {
      "epoch": 3.896196231781017,
      "grad_norm": 1.2281700372695923,
      "learning_rate": 4.4152150728759335e-05,
      "loss": 0.4434,
      "step": 10960
    },
    {
      "epoch": 3.89975115535016,
      "grad_norm": 6.7838921546936035,
      "learning_rate": 4.4009953785993604e-05,
      "loss": 0.3017,
      "step": 10970
    },
    {
      "epoch": 3.9033060789193033,
      "grad_norm": 1.578415870666504,
      "learning_rate": 4.386775684322787e-05,
      "loss": 0.3999,
      "step": 10980
    },
    {
      "epoch": 3.9068610024884465,
      "grad_norm": 4.165004253387451,
      "learning_rate": 4.372555990046214e-05,
      "loss": 0.3751,
      "step": 10990
    },
    {
      "epoch": 3.9104159260575897,
      "grad_norm": 2.554697036743164,
      "learning_rate": 4.358336295769641e-05,
      "loss": 0.3838,
      "step": 11000
    },
    {
      "epoch": 3.913970849626733,
      "grad_norm": 2.848695993423462,
      "learning_rate": 4.344116601493068e-05,
      "loss": 0.3187,
      "step": 11010
    },
    {
      "epoch": 3.917525773195876,
      "grad_norm": 6.939241409301758,
      "learning_rate": 4.329896907216495e-05,
      "loss": 0.3602,
      "step": 11020
    },
    {
      "epoch": 3.92108069676502,
      "grad_norm": 1.2197827100753784,
      "learning_rate": 4.315677212939922e-05,
      "loss": 0.4823,
      "step": 11030
    },
    {
      "epoch": 3.924635620334163,
      "grad_norm": 2.5626933574676514,
      "learning_rate": 4.301457518663349e-05,
      "loss": 0.4416,
      "step": 11040
    },
    {
      "epoch": 3.9281905439033062,
      "grad_norm": 3.7976901531219482,
      "learning_rate": 4.2872378243867765e-05,
      "loss": 0.4547,
      "step": 11050
    },
    {
      "epoch": 3.9317454674724495,
      "grad_norm": 0.9861619472503662,
      "learning_rate": 4.2730181301102026e-05,
      "loss": 0.3503,
      "step": 11060
    },
    {
      "epoch": 3.9353003910415927,
      "grad_norm": 4.4146552085876465,
      "learning_rate": 4.2587984358336295e-05,
      "loss": 0.3973,
      "step": 11070
    },
    {
      "epoch": 3.938855314610736,
      "grad_norm": 5.515569686889648,
      "learning_rate": 4.244578741557056e-05,
      "loss": 0.4838,
      "step": 11080
    },
    {
      "epoch": 3.942410238179879,
      "grad_norm": 2.472043752670288,
      "learning_rate": 4.230359047280484e-05,
      "loss": 0.482,
      "step": 11090
    },
    {
      "epoch": 3.9459651617490223,
      "grad_norm": 1.3441437482833862,
      "learning_rate": 4.216139353003911e-05,
      "loss": 0.3093,
      "step": 11100
    },
    {
      "epoch": 3.9495200853181656,
      "grad_norm": 1.1696438789367676,
      "learning_rate": 4.2019196587273375e-05,
      "loss": 0.4231,
      "step": 11110
    },
    {
      "epoch": 3.953075008887309,
      "grad_norm": 1.9584709405899048,
      "learning_rate": 4.187699964450765e-05,
      "loss": 0.3447,
      "step": 11120
    },
    {
      "epoch": 3.956629932456452,
      "grad_norm": 0.6345128417015076,
      "learning_rate": 4.173480270174191e-05,
      "loss": 0.2842,
      "step": 11130
    },
    {
      "epoch": 3.9601848560255952,
      "grad_norm": 4.566887378692627,
      "learning_rate": 4.159260575897618e-05,
      "loss": 0.4865,
      "step": 11140
    },
    {
      "epoch": 3.9637397795947384,
      "grad_norm": 3.060037136077881,
      "learning_rate": 4.145040881621045e-05,
      "loss": 0.3631,
      "step": 11150
    },
    {
      "epoch": 3.967294703163882,
      "grad_norm": 2.1806867122650146,
      "learning_rate": 4.1308211873444724e-05,
      "loss": 0.3564,
      "step": 11160
    },
    {
      "epoch": 3.9708496267330253,
      "grad_norm": 2.017314910888672,
      "learning_rate": 4.116601493067899e-05,
      "loss": 0.2835,
      "step": 11170
    },
    {
      "epoch": 3.9744045503021685,
      "grad_norm": 1.5429232120513916,
      "learning_rate": 4.102381798791326e-05,
      "loss": 0.3554,
      "step": 11180
    },
    {
      "epoch": 3.9779594738713118,
      "grad_norm": 1.2868183851242065,
      "learning_rate": 4.0881621045147536e-05,
      "loss": 0.3556,
      "step": 11190
    },
    {
      "epoch": 3.981514397440455,
      "grad_norm": 1.4696528911590576,
      "learning_rate": 4.07394241023818e-05,
      "loss": 0.5079,
      "step": 11200
    },
    {
      "epoch": 3.985069321009598,
      "grad_norm": 1.12599778175354,
      "learning_rate": 4.0597227159616066e-05,
      "loss": 0.3451,
      "step": 11210
    },
    {
      "epoch": 3.9886242445787414,
      "grad_norm": 0.9623573422431946,
      "learning_rate": 4.0455030216850335e-05,
      "loss": 0.4827,
      "step": 11220
    },
    {
      "epoch": 3.992179168147885,
      "grad_norm": 4.741085529327393,
      "learning_rate": 4.031283327408461e-05,
      "loss": 0.3655,
      "step": 11230
    },
    {
      "epoch": 3.9957340917170283,
      "grad_norm": 7.04385232925415,
      "learning_rate": 4.017063633131888e-05,
      "loss": 0.5562,
      "step": 11240
    },
    {
      "epoch": 3.9992890152861715,
      "grad_norm": 0.8874197006225586,
      "learning_rate": 4.002843938855315e-05,
      "loss": 0.3583,
      "step": 11250
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.8348,
      "eval_f1_macro": 0.8347015879373945,
      "eval_f1_neg": 0.838734869191722,
      "eval_f1_pos": 0.8306683066830668,
      "eval_loss": 0.3900667130947113,
      "eval_precision_global": 0.8359465066539313,
      "eval_precision_neg": 0.8148710166919575,
      "eval_precision_pos": 0.8570219966159053,
      "eval_recall_global": 0.8349628244341742,
      "eval_recall_neg": 0.8640386162510056,
      "eval_recall_pos": 0.8058870326173428,
      "eval_runtime": 56.8452,
      "eval_samples_per_second": 43.979,
      "eval_steps_per_second": 5.506,
      "step": 11252
    },
    {
      "epoch": 4.002843938855315,
      "grad_norm": 4.482727527618408,
      "learning_rate": 3.988624244578742e-05,
      "loss": 0.5094,
      "step": 11260
    },
    {
      "epoch": 4.006398862424458,
      "grad_norm": 2.243558168411255,
      "learning_rate": 3.974404550302169e-05,
      "loss": 0.4645,
      "step": 11270
    },
    {
      "epoch": 4.009953785993601,
      "grad_norm": 1.9191237688064575,
      "learning_rate": 3.960184856025595e-05,
      "loss": 0.3879,
      "step": 11280
    },
    {
      "epoch": 4.013508709562744,
      "grad_norm": 1.0607131719589233,
      "learning_rate": 3.945965161749022e-05,
      "loss": 0.3748,
      "step": 11290
    },
    {
      "epoch": 4.017063633131888,
      "grad_norm": 0.8800438642501831,
      "learning_rate": 3.9317454674724496e-05,
      "loss": 0.4169,
      "step": 11300
    },
    {
      "epoch": 4.020618556701031,
      "grad_norm": 2.3417344093322754,
      "learning_rate": 3.9175257731958764e-05,
      "loss": 0.3249,
      "step": 11310
    },
    {
      "epoch": 4.024173480270174,
      "grad_norm": 2.2563867568969727,
      "learning_rate": 3.903306078919303e-05,
      "loss": 0.2375,
      "step": 11320
    },
    {
      "epoch": 4.027728403839317,
      "grad_norm": 2.176501989364624,
      "learning_rate": 3.889086384642731e-05,
      "loss": 0.3568,
      "step": 11330
    },
    {
      "epoch": 4.0312833274084605,
      "grad_norm": 1.9770076274871826,
      "learning_rate": 3.8748666903661576e-05,
      "loss": 0.4806,
      "step": 11340
    },
    {
      "epoch": 4.034838250977604,
      "grad_norm": 3.7870240211486816,
      "learning_rate": 3.860646996089584e-05,
      "loss": 0.4375,
      "step": 11350
    },
    {
      "epoch": 4.038393174546747,
      "grad_norm": 1.2532479763031006,
      "learning_rate": 3.846427301813011e-05,
      "loss": 0.4799,
      "step": 11360
    },
    {
      "epoch": 4.04194809811589,
      "grad_norm": 7.6114912033081055,
      "learning_rate": 3.832207607536438e-05,
      "loss": 0.4305,
      "step": 11370
    },
    {
      "epoch": 4.045503021685033,
      "grad_norm": 3.4832141399383545,
      "learning_rate": 3.817987913259865e-05,
      "loss": 0.428,
      "step": 11380
    },
    {
      "epoch": 4.049057945254177,
      "grad_norm": 2.6667428016662598,
      "learning_rate": 3.803768218983292e-05,
      "loss": 0.2625,
      "step": 11390
    },
    {
      "epoch": 4.052612868823321,
      "grad_norm": 2.4752469062805176,
      "learning_rate": 3.7895485247067194e-05,
      "loss": 0.4087,
      "step": 11400
    },
    {
      "epoch": 4.056167792392464,
      "grad_norm": 4.9574809074401855,
      "learning_rate": 3.775328830430146e-05,
      "loss": 0.3384,
      "step": 11410
    },
    {
      "epoch": 4.059722715961607,
      "grad_norm": 3.302814245223999,
      "learning_rate": 3.761109136153573e-05,
      "loss": 0.4405,
      "step": 11420
    },
    {
      "epoch": 4.06327763953075,
      "grad_norm": 2.078439712524414,
      "learning_rate": 3.746889441877e-05,
      "loss": 0.3517,
      "step": 11430
    },
    {
      "epoch": 4.066832563099894,
      "grad_norm": 1.295878291130066,
      "learning_rate": 3.732669747600427e-05,
      "loss": 0.4432,
      "step": 11440
    },
    {
      "epoch": 4.070387486669037,
      "grad_norm": 1.4191910028457642,
      "learning_rate": 3.7184500533238536e-05,
      "loss": 0.3702,
      "step": 11450
    },
    {
      "epoch": 4.07394241023818,
      "grad_norm": 6.973834991455078,
      "learning_rate": 3.7042303590472804e-05,
      "loss": 0.3807,
      "step": 11460
    },
    {
      "epoch": 4.077497333807323,
      "grad_norm": 1.7930617332458496,
      "learning_rate": 3.690010664770708e-05,
      "loss": 0.379,
      "step": 11470
    },
    {
      "epoch": 4.0810522573764665,
      "grad_norm": 1.7204389572143555,
      "learning_rate": 3.675790970494135e-05,
      "loss": 0.4059,
      "step": 11480
    },
    {
      "epoch": 4.08460718094561,
      "grad_norm": 1.049856424331665,
      "learning_rate": 3.6615712762175616e-05,
      "loss": 0.2983,
      "step": 11490
    },
    {
      "epoch": 4.088162104514753,
      "grad_norm": 1.3217023611068726,
      "learning_rate": 3.6473515819409885e-05,
      "loss": 0.4496,
      "step": 11500
    },
    {
      "epoch": 4.091717028083896,
      "grad_norm": 0.688823401927948,
      "learning_rate": 3.633131887664415e-05,
      "loss": 0.4359,
      "step": 11510
    },
    {
      "epoch": 4.095271951653039,
      "grad_norm": 0.5169585943222046,
      "learning_rate": 3.618912193387842e-05,
      "loss": 0.3472,
      "step": 11520
    },
    {
      "epoch": 4.098826875222183,
      "grad_norm": 1.9973233938217163,
      "learning_rate": 3.604692499111269e-05,
      "loss": 0.327,
      "step": 11530
    },
    {
      "epoch": 4.102381798791326,
      "grad_norm": 0.9015929102897644,
      "learning_rate": 3.5904728048346965e-05,
      "loss": 0.3192,
      "step": 11540
    },
    {
      "epoch": 4.105936722360469,
      "grad_norm": 2.89997935295105,
      "learning_rate": 3.5762531105581234e-05,
      "loss": 0.3558,
      "step": 11550
    },
    {
      "epoch": 4.109491645929612,
      "grad_norm": 1.2472575902938843,
      "learning_rate": 3.56203341628155e-05,
      "loss": 0.3926,
      "step": 11560
    },
    {
      "epoch": 4.1130465694987555,
      "grad_norm": 1.5465236902236938,
      "learning_rate": 3.547813722004977e-05,
      "loss": 0.3258,
      "step": 11570
    },
    {
      "epoch": 4.116601493067899,
      "grad_norm": 1.764448881149292,
      "learning_rate": 3.533594027728404e-05,
      "loss": 0.3646,
      "step": 11580
    },
    {
      "epoch": 4.120156416637042,
      "grad_norm": 2.3594448566436768,
      "learning_rate": 3.519374333451831e-05,
      "loss": 0.4499,
      "step": 11590
    },
    {
      "epoch": 4.123711340206185,
      "grad_norm": 2.2894318103790283,
      "learning_rate": 3.5051546391752576e-05,
      "loss": 0.4028,
      "step": 11600
    },
    {
      "epoch": 4.127266263775329,
      "grad_norm": 2.9868850708007812,
      "learning_rate": 3.490934944898685e-05,
      "loss": 0.4246,
      "step": 11610
    },
    {
      "epoch": 4.1308211873444725,
      "grad_norm": 1.5682588815689087,
      "learning_rate": 3.476715250622112e-05,
      "loss": 0.3579,
      "step": 11620
    },
    {
      "epoch": 4.134376110913616,
      "grad_norm": 0.8394826054573059,
      "learning_rate": 3.462495556345539e-05,
      "loss": 0.3701,
      "step": 11630
    },
    {
      "epoch": 4.137931034482759,
      "grad_norm": 2.3581607341766357,
      "learning_rate": 3.4482758620689657e-05,
      "loss": 0.3631,
      "step": 11640
    },
    {
      "epoch": 4.141485958051902,
      "grad_norm": 1.3914484977722168,
      "learning_rate": 3.4340561677923925e-05,
      "loss": 0.306,
      "step": 11650
    },
    {
      "epoch": 4.145040881621045,
      "grad_norm": 1.8340739011764526,
      "learning_rate": 3.4198364735158193e-05,
      "loss": 0.4208,
      "step": 11660
    },
    {
      "epoch": 4.148595805190189,
      "grad_norm": 1.2279220819473267,
      "learning_rate": 3.405616779239246e-05,
      "loss": 0.4271,
      "step": 11670
    },
    {
      "epoch": 4.152150728759332,
      "grad_norm": 2.8292479515075684,
      "learning_rate": 3.391397084962674e-05,
      "loss": 0.3478,
      "step": 11680
    },
    {
      "epoch": 4.155705652328475,
      "grad_norm": 2.028303384780884,
      "learning_rate": 3.3771773906861006e-05,
      "loss": 0.2981,
      "step": 11690
    },
    {
      "epoch": 4.159260575897618,
      "grad_norm": 4.023552894592285,
      "learning_rate": 3.3629576964095274e-05,
      "loss": 0.319,
      "step": 11700
    },
    {
      "epoch": 4.162815499466761,
      "grad_norm": 3.7622194290161133,
      "learning_rate": 3.348738002132954e-05,
      "loss": 0.3806,
      "step": 11710
    },
    {
      "epoch": 4.166370423035905,
      "grad_norm": 3.5768423080444336,
      "learning_rate": 3.334518307856381e-05,
      "loss": 0.5676,
      "step": 11720
    },
    {
      "epoch": 4.169925346605048,
      "grad_norm": 1.1680411100387573,
      "learning_rate": 3.320298613579808e-05,
      "loss": 0.3125,
      "step": 11730
    },
    {
      "epoch": 4.173480270174191,
      "grad_norm": 0.8456751704216003,
      "learning_rate": 3.306078919303235e-05,
      "loss": 0.363,
      "step": 11740
    },
    {
      "epoch": 4.177035193743334,
      "grad_norm": 3.1017708778381348,
      "learning_rate": 3.291859225026662e-05,
      "loss": 0.3946,
      "step": 11750
    },
    {
      "epoch": 4.1805901173124775,
      "grad_norm": 3.6057074069976807,
      "learning_rate": 3.277639530750089e-05,
      "loss": 0.3811,
      "step": 11760
    },
    {
      "epoch": 4.184145040881621,
      "grad_norm": 0.8271917700767517,
      "learning_rate": 3.263419836473516e-05,
      "loss": 0.3867,
      "step": 11770
    },
    {
      "epoch": 4.187699964450764,
      "grad_norm": 2.5430588722229004,
      "learning_rate": 3.249200142196943e-05,
      "loss": 0.3905,
      "step": 11780
    },
    {
      "epoch": 4.191254888019907,
      "grad_norm": 1.2691044807434082,
      "learning_rate": 3.23498044792037e-05,
      "loss": 0.4375,
      "step": 11790
    },
    {
      "epoch": 4.19480981158905,
      "grad_norm": 1.9555659294128418,
      "learning_rate": 3.2207607536437965e-05,
      "loss": 0.3429,
      "step": 11800
    },
    {
      "epoch": 4.1983647351581945,
      "grad_norm": 3.650033712387085,
      "learning_rate": 3.2065410593672234e-05,
      "loss": 0.4689,
      "step": 11810
    },
    {
      "epoch": 4.201919658727338,
      "grad_norm": 2.588026523590088,
      "learning_rate": 3.192321365090651e-05,
      "loss": 0.2495,
      "step": 11820
    },
    {
      "epoch": 4.205474582296481,
      "grad_norm": 1.805153489112854,
      "learning_rate": 3.178101670814078e-05,
      "loss": 0.3925,
      "step": 11830
    },
    {
      "epoch": 4.209029505865624,
      "grad_norm": 3.699232578277588,
      "learning_rate": 3.1638819765375046e-05,
      "loss": 0.3492,
      "step": 11840
    },
    {
      "epoch": 4.212584429434767,
      "grad_norm": 1.9068142175674438,
      "learning_rate": 3.1496622822609314e-05,
      "loss": 0.4067,
      "step": 11850
    },
    {
      "epoch": 4.216139353003911,
      "grad_norm": 3.81229829788208,
      "learning_rate": 3.135442587984358e-05,
      "loss": 0.452,
      "step": 11860
    },
    {
      "epoch": 4.219694276573054,
      "grad_norm": 2.5215179920196533,
      "learning_rate": 3.121222893707785e-05,
      "loss": 0.4694,
      "step": 11870
    },
    {
      "epoch": 4.223249200142197,
      "grad_norm": 3.943091869354248,
      "learning_rate": 3.1070031994312126e-05,
      "loss": 0.4496,
      "step": 11880
    },
    {
      "epoch": 4.22680412371134,
      "grad_norm": 2.0884652137756348,
      "learning_rate": 3.0927835051546395e-05,
      "loss": 0.4233,
      "step": 11890
    },
    {
      "epoch": 4.2303590472804835,
      "grad_norm": 1.602554440498352,
      "learning_rate": 3.078563810878066e-05,
      "loss": 0.479,
      "step": 11900
    },
    {
      "epoch": 4.233913970849627,
      "grad_norm": 1.076300859451294,
      "learning_rate": 3.064344116601493e-05,
      "loss": 0.3721,
      "step": 11910
    },
    {
      "epoch": 4.23746889441877,
      "grad_norm": 1.2452601194381714,
      "learning_rate": 3.0501244223249203e-05,
      "loss": 0.433,
      "step": 11920
    },
    {
      "epoch": 4.241023817987913,
      "grad_norm": 1.5346603393554688,
      "learning_rate": 3.0359047280483472e-05,
      "loss": 0.3452,
      "step": 11930
    },
    {
      "epoch": 4.244578741557056,
      "grad_norm": 2.6548428535461426,
      "learning_rate": 3.021685033771774e-05,
      "loss": 0.3473,
      "step": 11940
    },
    {
      "epoch": 4.2481336651262,
      "grad_norm": 1.1731586456298828,
      "learning_rate": 3.0074653394952012e-05,
      "loss": 0.3267,
      "step": 11950
    },
    {
      "epoch": 4.251688588695343,
      "grad_norm": 1.8288486003875732,
      "learning_rate": 2.993245645218628e-05,
      "loss": 0.3077,
      "step": 11960
    },
    {
      "epoch": 4.255243512264486,
      "grad_norm": 0.9997895956039429,
      "learning_rate": 2.979025950942055e-05,
      "loss": 0.3789,
      "step": 11970
    },
    {
      "epoch": 4.258798435833629,
      "grad_norm": 6.1164069175720215,
      "learning_rate": 2.9648062566654817e-05,
      "loss": 0.5109,
      "step": 11980
    },
    {
      "epoch": 4.2623533594027725,
      "grad_norm": 2.9238011837005615,
      "learning_rate": 2.950586562388909e-05,
      "loss": 0.3808,
      "step": 11990
    },
    {
      "epoch": 4.265908282971916,
      "grad_norm": 2.1396191120147705,
      "learning_rate": 2.9363668681123358e-05,
      "loss": 0.2728,
      "step": 12000
    },
    {
      "epoch": 4.26946320654106,
      "grad_norm": 2.1046316623687744,
      "learning_rate": 2.9221471738357626e-05,
      "loss": 0.3817,
      "step": 12010
    },
    {
      "epoch": 4.273018130110203,
      "grad_norm": 5.702977657318115,
      "learning_rate": 2.9079274795591898e-05,
      "loss": 0.4619,
      "step": 12020
    },
    {
      "epoch": 4.276573053679346,
      "grad_norm": 1.9148809909820557,
      "learning_rate": 2.8937077852826166e-05,
      "loss": 0.2551,
      "step": 12030
    },
    {
      "epoch": 4.2801279772484895,
      "grad_norm": 1.4952020645141602,
      "learning_rate": 2.8794880910060435e-05,
      "loss": 0.4324,
      "step": 12040
    },
    {
      "epoch": 4.283682900817633,
      "grad_norm": 1.26325261592865,
      "learning_rate": 2.8652683967294703e-05,
      "loss": 0.4042,
      "step": 12050
    },
    {
      "epoch": 4.287237824386776,
      "grad_norm": 3.653970241546631,
      "learning_rate": 2.8510487024528975e-05,
      "loss": 0.3559,
      "step": 12060
    },
    {
      "epoch": 4.290792747955919,
      "grad_norm": 1.7942535877227783,
      "learning_rate": 2.8368290081763243e-05,
      "loss": 0.5569,
      "step": 12070
    },
    {
      "epoch": 4.294347671525062,
      "grad_norm": 5.303649425506592,
      "learning_rate": 2.8226093138997512e-05,
      "loss": 0.4906,
      "step": 12080
    },
    {
      "epoch": 4.297902595094206,
      "grad_norm": 3.1843833923339844,
      "learning_rate": 2.8083896196231784e-05,
      "loss": 0.2586,
      "step": 12090
    },
    {
      "epoch": 4.301457518663349,
      "grad_norm": 2.1325132846832275,
      "learning_rate": 2.7941699253466052e-05,
      "loss": 0.3248,
      "step": 12100
    },
    {
      "epoch": 4.305012442232492,
      "grad_norm": 4.7168288230896,
      "learning_rate": 2.779950231070032e-05,
      "loss": 0.4563,
      "step": 12110
    },
    {
      "epoch": 4.308567365801635,
      "grad_norm": 2.2817907333374023,
      "learning_rate": 2.765730536793459e-05,
      "loss": 0.4116,
      "step": 12120
    },
    {
      "epoch": 4.3121222893707785,
      "grad_norm": 2.431086301803589,
      "learning_rate": 2.751510842516886e-05,
      "loss": 0.4357,
      "step": 12130
    },
    {
      "epoch": 4.315677212939922,
      "grad_norm": 1.4191771745681763,
      "learning_rate": 2.737291148240313e-05,
      "loss": 0.4029,
      "step": 12140
    },
    {
      "epoch": 4.319232136509065,
      "grad_norm": 1.3108164072036743,
      "learning_rate": 2.7230714539637398e-05,
      "loss": 0.2995,
      "step": 12150
    },
    {
      "epoch": 4.322787060078208,
      "grad_norm": 5.518639087677002,
      "learning_rate": 2.708851759687167e-05,
      "loss": 0.4251,
      "step": 12160
    },
    {
      "epoch": 4.326341983647351,
      "grad_norm": 3.1215407848358154,
      "learning_rate": 2.6946320654105938e-05,
      "loss": 0.3212,
      "step": 12170
    },
    {
      "epoch": 4.329896907216495,
      "grad_norm": 4.848907947540283,
      "learning_rate": 2.6804123711340206e-05,
      "loss": 0.4291,
      "step": 12180
    },
    {
      "epoch": 4.333451830785638,
      "grad_norm": 5.88396692276001,
      "learning_rate": 2.6661926768574475e-05,
      "loss": 0.5784,
      "step": 12190
    },
    {
      "epoch": 4.337006754354781,
      "grad_norm": 1.1787595748901367,
      "learning_rate": 2.6519729825808747e-05,
      "loss": 0.4187,
      "step": 12200
    },
    {
      "epoch": 4.340561677923924,
      "grad_norm": 5.242917060852051,
      "learning_rate": 2.6377532883043015e-05,
      "loss": 0.3898,
      "step": 12210
    },
    {
      "epoch": 4.344116601493068,
      "grad_norm": 1.8356218338012695,
      "learning_rate": 2.6235335940277284e-05,
      "loss": 0.2779,
      "step": 12220
    },
    {
      "epoch": 4.3476715250622116,
      "grad_norm": 1.8274809122085571,
      "learning_rate": 2.6093138997511555e-05,
      "loss": 0.3878,
      "step": 12230
    },
    {
      "epoch": 4.351226448631355,
      "grad_norm": 1.180050253868103,
      "learning_rate": 2.5950942054745824e-05,
      "loss": 0.293,
      "step": 12240
    },
    {
      "epoch": 4.354781372200498,
      "grad_norm": 5.195744037628174,
      "learning_rate": 2.5808745111980092e-05,
      "loss": 0.3886,
      "step": 12250
    },
    {
      "epoch": 4.358336295769641,
      "grad_norm": 2.10921573638916,
      "learning_rate": 2.566654816921436e-05,
      "loss": 0.357,
      "step": 12260
    },
    {
      "epoch": 4.361891219338784,
      "grad_norm": 1.547573208808899,
      "learning_rate": 2.5524351226448633e-05,
      "loss": 0.3182,
      "step": 12270
    },
    {
      "epoch": 4.365446142907928,
      "grad_norm": 2.0758748054504395,
      "learning_rate": 2.53821542836829e-05,
      "loss": 0.5234,
      "step": 12280
    },
    {
      "epoch": 4.369001066477071,
      "grad_norm": 1.2228742837905884,
      "learning_rate": 2.523995734091717e-05,
      "loss": 0.2817,
      "step": 12290
    },
    {
      "epoch": 4.372555990046214,
      "grad_norm": 0.7024641036987305,
      "learning_rate": 2.5097760398151445e-05,
      "loss": 0.3318,
      "step": 12300
    },
    {
      "epoch": 4.376110913615357,
      "grad_norm": 0.8695378303527832,
      "learning_rate": 2.495556345538571e-05,
      "loss": 0.4314,
      "step": 12310
    },
    {
      "epoch": 4.3796658371845005,
      "grad_norm": 2.634610891342163,
      "learning_rate": 2.4813366512619978e-05,
      "loss": 0.4074,
      "step": 12320
    },
    {
      "epoch": 4.383220760753644,
      "grad_norm": 1.635660171508789,
      "learning_rate": 2.467116956985425e-05,
      "loss": 0.4588,
      "step": 12330
    },
    {
      "epoch": 4.386775684322787,
      "grad_norm": 2.3243229389190674,
      "learning_rate": 2.4528972627088522e-05,
      "loss": 0.381,
      "step": 12340
    },
    {
      "epoch": 4.39033060789193,
      "grad_norm": 1.474327564239502,
      "learning_rate": 2.4386775684322787e-05,
      "loss": 0.3418,
      "step": 12350
    },
    {
      "epoch": 4.393885531461073,
      "grad_norm": 3.485180616378784,
      "learning_rate": 2.424457874155706e-05,
      "loss": 0.3529,
      "step": 12360
    },
    {
      "epoch": 4.397440455030217,
      "grad_norm": 1.865649938583374,
      "learning_rate": 2.4102381798791327e-05,
      "loss": 0.4281,
      "step": 12370
    },
    {
      "epoch": 4.40099537859936,
      "grad_norm": 2.3626725673675537,
      "learning_rate": 2.3960184856025595e-05,
      "loss": 0.2675,
      "step": 12380
    },
    {
      "epoch": 4.404550302168503,
      "grad_norm": 2.532829761505127,
      "learning_rate": 2.3817987913259864e-05,
      "loss": 0.4162,
      "step": 12390
    },
    {
      "epoch": 4.408105225737646,
      "grad_norm": 2.2049973011016846,
      "learning_rate": 2.3675790970494136e-05,
      "loss": 0.3979,
      "step": 12400
    },
    {
      "epoch": 4.4116601493067895,
      "grad_norm": 0.8435302972793579,
      "learning_rate": 2.3533594027728408e-05,
      "loss": 0.3231,
      "step": 12410
    },
    {
      "epoch": 4.415215072875933,
      "grad_norm": 1.5834702253341675,
      "learning_rate": 2.3391397084962673e-05,
      "loss": 0.3506,
      "step": 12420
    },
    {
      "epoch": 4.418769996445077,
      "grad_norm": 2.8169116973876953,
      "learning_rate": 2.3249200142196944e-05,
      "loss": 0.4758,
      "step": 12430
    },
    {
      "epoch": 4.42232492001422,
      "grad_norm": 2.298321008682251,
      "learning_rate": 2.3107003199431213e-05,
      "loss": 0.4764,
      "step": 12440
    },
    {
      "epoch": 4.425879843583363,
      "grad_norm": 1.119953989982605,
      "learning_rate": 2.2964806256665485e-05,
      "loss": 0.4336,
      "step": 12450
    },
    {
      "epoch": 4.4294347671525065,
      "grad_norm": 2.4979002475738525,
      "learning_rate": 2.282260931389975e-05,
      "loss": 0.3356,
      "step": 12460
    },
    {
      "epoch": 4.43298969072165,
      "grad_norm": 4.187160968780518,
      "learning_rate": 2.268041237113402e-05,
      "loss": 0.4507,
      "step": 12470
    },
    {
      "epoch": 4.436544614290793,
      "grad_norm": 0.8579984307289124,
      "learning_rate": 2.2538215428368293e-05,
      "loss": 0.3567,
      "step": 12480
    },
    {
      "epoch": 4.440099537859936,
      "grad_norm": 2.58282208442688,
      "learning_rate": 2.239601848560256e-05,
      "loss": 0.3169,
      "step": 12490
    },
    {
      "epoch": 4.443654461429079,
      "grad_norm": 2.282254934310913,
      "learning_rate": 2.225382154283683e-05,
      "loss": 0.4445,
      "step": 12500
    },
    {
      "epoch": 4.447209384998223,
      "grad_norm": 1.3945157527923584,
      "learning_rate": 2.21116246000711e-05,
      "loss": 0.4173,
      "step": 12510
    },
    {
      "epoch": 4.450764308567366,
      "grad_norm": 1.4753810167312622,
      "learning_rate": 2.196942765730537e-05,
      "loss": 0.3575,
      "step": 12520
    },
    {
      "epoch": 4.454319232136509,
      "grad_norm": 1.0105087757110596,
      "learning_rate": 2.1827230714539636e-05,
      "loss": 0.3109,
      "step": 12530
    },
    {
      "epoch": 4.457874155705652,
      "grad_norm": 3.3093199729919434,
      "learning_rate": 2.1685033771773907e-05,
      "loss": 0.4079,
      "step": 12540
    },
    {
      "epoch": 4.4614290792747955,
      "grad_norm": 1.349610686302185,
      "learning_rate": 2.154283682900818e-05,
      "loss": 0.4162,
      "step": 12550
    },
    {
      "epoch": 4.464984002843939,
      "grad_norm": 3.6977367401123047,
      "learning_rate": 2.1400639886242448e-05,
      "loss": 0.3463,
      "step": 12560
    },
    {
      "epoch": 4.468538926413082,
      "grad_norm": 2.1439871788024902,
      "learning_rate": 2.1258442943476716e-05,
      "loss": 0.4795,
      "step": 12570
    },
    {
      "epoch": 4.472093849982225,
      "grad_norm": 1.2710070610046387,
      "learning_rate": 2.1116246000710985e-05,
      "loss": 0.4532,
      "step": 12580
    },
    {
      "epoch": 4.475648773551368,
      "grad_norm": 1.2543964385986328,
      "learning_rate": 2.0974049057945256e-05,
      "loss": 0.4369,
      "step": 12590
    },
    {
      "epoch": 4.479203697120512,
      "grad_norm": 3.3784878253936768,
      "learning_rate": 2.0831852115179525e-05,
      "loss": 0.3564,
      "step": 12600
    },
    {
      "epoch": 4.482758620689655,
      "grad_norm": 2.5527405738830566,
      "learning_rate": 2.0689655172413793e-05,
      "loss": 0.4164,
      "step": 12610
    },
    {
      "epoch": 4.486313544258798,
      "grad_norm": 6.649290561676025,
      "learning_rate": 2.0547458229648065e-05,
      "loss": 0.416,
      "step": 12620
    },
    {
      "epoch": 4.489868467827941,
      "grad_norm": 3.0918173789978027,
      "learning_rate": 2.0405261286882334e-05,
      "loss": 0.3674,
      "step": 12630
    },
    {
      "epoch": 4.493423391397085,
      "grad_norm": 2.2793660163879395,
      "learning_rate": 2.0263064344116602e-05,
      "loss": 0.3336,
      "step": 12640
    },
    {
      "epoch": 4.496978314966229,
      "grad_norm": 0.8164825439453125,
      "learning_rate": 2.012086740135087e-05,
      "loss": 0.3688,
      "step": 12650
    },
    {
      "epoch": 4.500533238535372,
      "grad_norm": 1.1311969757080078,
      "learning_rate": 1.9978670458585142e-05,
      "loss": 0.4047,
      "step": 12660
    },
    {
      "epoch": 4.504088162104515,
      "grad_norm": 3.8837077617645264,
      "learning_rate": 1.983647351581941e-05,
      "loss": 0.3661,
      "step": 12670
    },
    {
      "epoch": 4.507643085673658,
      "grad_norm": 0.5487209558486938,
      "learning_rate": 1.969427657305368e-05,
      "loss": 0.4497,
      "step": 12680
    },
    {
      "epoch": 4.5111980092428015,
      "grad_norm": 5.418594837188721,
      "learning_rate": 1.955207963028795e-05,
      "loss": 0.4163,
      "step": 12690
    },
    {
      "epoch": 4.514752932811945,
      "grad_norm": 2.1940653324127197,
      "learning_rate": 1.940988268752222e-05,
      "loss": 0.3994,
      "step": 12700
    },
    {
      "epoch": 4.518307856381088,
      "grad_norm": 1.3877061605453491,
      "learning_rate": 1.9267685744756488e-05,
      "loss": 0.3642,
      "step": 12710
    },
    {
      "epoch": 4.521862779950231,
      "grad_norm": 1.297600269317627,
      "learning_rate": 1.9125488801990756e-05,
      "loss": 0.3344,
      "step": 12720
    },
    {
      "epoch": 4.525417703519374,
      "grad_norm": 2.4710335731506348,
      "learning_rate": 1.8983291859225028e-05,
      "loss": 0.3148,
      "step": 12730
    },
    {
      "epoch": 4.5289726270885176,
      "grad_norm": 1.5750187635421753,
      "learning_rate": 1.8841094916459297e-05,
      "loss": 0.4276,
      "step": 12740
    },
    {
      "epoch": 4.532527550657661,
      "grad_norm": 4.309848308563232,
      "learning_rate": 1.8698897973693565e-05,
      "loss": 0.3527,
      "step": 12750
    },
    {
      "epoch": 4.536082474226804,
      "grad_norm": 2.852015733718872,
      "learning_rate": 1.8556701030927837e-05,
      "loss": 0.3501,
      "step": 12760
    },
    {
      "epoch": 4.539637397795947,
      "grad_norm": 2.525238513946533,
      "learning_rate": 1.8414504088162105e-05,
      "loss": 0.3538,
      "step": 12770
    },
    {
      "epoch": 4.54319232136509,
      "grad_norm": 1.6718974113464355,
      "learning_rate": 1.8272307145396374e-05,
      "loss": 0.3178,
      "step": 12780
    },
    {
      "epoch": 4.546747244934234,
      "grad_norm": 2.27109694480896,
      "learning_rate": 1.8130110202630642e-05,
      "loss": 0.4168,
      "step": 12790
    },
    {
      "epoch": 4.550302168503377,
      "grad_norm": 2.8640544414520264,
      "learning_rate": 1.7987913259864914e-05,
      "loss": 0.4258,
      "step": 12800
    },
    {
      "epoch": 4.55385709207252,
      "grad_norm": 0.41622862219810486,
      "learning_rate": 1.7845716317099186e-05,
      "loss": 0.4108,
      "step": 12810
    },
    {
      "epoch": 4.557412015641663,
      "grad_norm": 1.4299360513687134,
      "learning_rate": 1.770351937433345e-05,
      "loss": 0.4859,
      "step": 12820
    },
    {
      "epoch": 4.560966939210807,
      "grad_norm": 1.5860098600387573,
      "learning_rate": 1.7561322431567723e-05,
      "loss": 0.3565,
      "step": 12830
    },
    {
      "epoch": 4.56452186277995,
      "grad_norm": 3.287452459335327,
      "learning_rate": 1.741912548880199e-05,
      "loss": 0.449,
      "step": 12840
    },
    {
      "epoch": 4.568076786349094,
      "grad_norm": 0.8932592868804932,
      "learning_rate": 1.7276928546036263e-05,
      "loss": 0.3129,
      "step": 12850
    },
    {
      "epoch": 4.571631709918237,
      "grad_norm": 2.443288803100586,
      "learning_rate": 1.713473160327053e-05,
      "loss": 0.4193,
      "step": 12860
    },
    {
      "epoch": 4.57518663348738,
      "grad_norm": 1.600775122642517,
      "learning_rate": 1.69925346605048e-05,
      "loss": 0.354,
      "step": 12870
    },
    {
      "epoch": 4.5787415570565235,
      "grad_norm": 4.008594036102295,
      "learning_rate": 1.685033771773907e-05,
      "loss": 0.4573,
      "step": 12880
    },
    {
      "epoch": 4.582296480625667,
      "grad_norm": 1.9787524938583374,
      "learning_rate": 1.6708140774973337e-05,
      "loss": 0.4063,
      "step": 12890
    },
    {
      "epoch": 4.58585140419481,
      "grad_norm": 1.456809639930725,
      "learning_rate": 1.656594383220761e-05,
      "loss": 0.3466,
      "step": 12900
    },
    {
      "epoch": 4.589406327763953,
      "grad_norm": 3.0374515056610107,
      "learning_rate": 1.6423746889441877e-05,
      "loss": 0.3587,
      "step": 12910
    },
    {
      "epoch": 4.592961251333096,
      "grad_norm": 1.119954228401184,
      "learning_rate": 1.628154994667615e-05,
      "loss": 0.443,
      "step": 12920
    },
    {
      "epoch": 4.59651617490224,
      "grad_norm": 1.6727313995361328,
      "learning_rate": 1.6139353003910417e-05,
      "loss": 0.3388,
      "step": 12930
    },
    {
      "epoch": 4.600071098471383,
      "grad_norm": 4.388779640197754,
      "learning_rate": 1.5997156061144686e-05,
      "loss": 0.3552,
      "step": 12940
    },
    {
      "epoch": 4.603626022040526,
      "grad_norm": 1.713442325592041,
      "learning_rate": 1.5854959118378957e-05,
      "loss": 0.2882,
      "step": 12950
    },
    {
      "epoch": 4.607180945609669,
      "grad_norm": 1.487037181854248,
      "learning_rate": 1.5712762175613226e-05,
      "loss": 0.3116,
      "step": 12960
    },
    {
      "epoch": 4.6107358691788125,
      "grad_norm": 3.1378908157348633,
      "learning_rate": 1.5570565232847494e-05,
      "loss": 0.344,
      "step": 12970
    },
    {
      "epoch": 4.614290792747956,
      "grad_norm": 2.5177242755889893,
      "learning_rate": 1.5428368290081763e-05,
      "loss": 0.3301,
      "step": 12980
    },
    {
      "epoch": 4.617845716317099,
      "grad_norm": 2.0728726387023926,
      "learning_rate": 1.5286171347316035e-05,
      "loss": 0.4614,
      "step": 12990
    },
    {
      "epoch": 4.621400639886242,
      "grad_norm": 2.2356650829315186,
      "learning_rate": 1.5143974404550305e-05,
      "loss": 0.3994,
      "step": 13000
    },
    {
      "epoch": 4.624955563455385,
      "grad_norm": 4.669815540313721,
      "learning_rate": 1.5001777461784571e-05,
      "loss": 0.3725,
      "step": 13010
    },
    {
      "epoch": 4.628510487024529,
      "grad_norm": 0.785077691078186,
      "learning_rate": 1.4859580519018843e-05,
      "loss": 0.2832,
      "step": 13020
    },
    {
      "epoch": 4.632065410593672,
      "grad_norm": 3.7027668952941895,
      "learning_rate": 1.471738357625311e-05,
      "loss": 0.3792,
      "step": 13030
    },
    {
      "epoch": 4.635620334162816,
      "grad_norm": 1.5166007280349731,
      "learning_rate": 1.4575186633487382e-05,
      "loss": 0.4481,
      "step": 13040
    },
    {
      "epoch": 4.639175257731958,
      "grad_norm": 0.7004179954528809,
      "learning_rate": 1.4432989690721649e-05,
      "loss": 0.3752,
      "step": 13050
    },
    {
      "epoch": 4.642730181301102,
      "grad_norm": 4.300911903381348,
      "learning_rate": 1.4290792747955919e-05,
      "loss": 0.4257,
      "step": 13060
    },
    {
      "epoch": 4.646285104870246,
      "grad_norm": 1.5741183757781982,
      "learning_rate": 1.414859580519019e-05,
      "loss": 0.3405,
      "step": 13070
    },
    {
      "epoch": 4.649840028439389,
      "grad_norm": 2.22267484664917,
      "learning_rate": 1.4006398862424457e-05,
      "loss": 0.4051,
      "step": 13080
    },
    {
      "epoch": 4.653394952008532,
      "grad_norm": 1.5552380084991455,
      "learning_rate": 1.3864201919658729e-05,
      "loss": 0.4485,
      "step": 13090
    },
    {
      "epoch": 4.656949875577675,
      "grad_norm": 2.2427518367767334,
      "learning_rate": 1.3722004976892996e-05,
      "loss": 0.379,
      "step": 13100
    },
    {
      "epoch": 4.6605047991468185,
      "grad_norm": 1.8826439380645752,
      "learning_rate": 1.3579808034127268e-05,
      "loss": 0.4292,
      "step": 13110
    },
    {
      "epoch": 4.664059722715962,
      "grad_norm": 2.1198647022247314,
      "learning_rate": 1.3437611091361538e-05,
      "loss": 0.4555,
      "step": 13120
    },
    {
      "epoch": 4.667614646285105,
      "grad_norm": 2.0669679641723633,
      "learning_rate": 1.3295414148595806e-05,
      "loss": 0.339,
      "step": 13130
    },
    {
      "epoch": 4.671169569854248,
      "grad_norm": 2.68119478225708,
      "learning_rate": 1.3153217205830076e-05,
      "loss": 0.3809,
      "step": 13140
    },
    {
      "epoch": 4.674724493423391,
      "grad_norm": 2.0469634532928467,
      "learning_rate": 1.3011020263064345e-05,
      "loss": 0.382,
      "step": 13150
    },
    {
      "epoch": 4.678279416992535,
      "grad_norm": 1.6503583192825317,
      "learning_rate": 1.2868823320298615e-05,
      "loss": 0.4721,
      "step": 13160
    },
    {
      "epoch": 4.681834340561678,
      "grad_norm": 1.5377247333526611,
      "learning_rate": 1.2726626377532883e-05,
      "loss": 0.5088,
      "step": 13170
    },
    {
      "epoch": 4.685389264130821,
      "grad_norm": 3.6964478492736816,
      "learning_rate": 1.2584429434767154e-05,
      "loss": 0.399,
      "step": 13180
    },
    {
      "epoch": 4.688944187699964,
      "grad_norm": 2.7341816425323486,
      "learning_rate": 1.2442232492001422e-05,
      "loss": 0.4669,
      "step": 13190
    },
    {
      "epoch": 4.6924991112691075,
      "grad_norm": 1.4067548513412476,
      "learning_rate": 1.2300035549235692e-05,
      "loss": 0.3731,
      "step": 13200
    },
    {
      "epoch": 4.696054034838251,
      "grad_norm": 3.357285261154175,
      "learning_rate": 1.215783860646996e-05,
      "loss": 0.3755,
      "step": 13210
    },
    {
      "epoch": 4.699608958407394,
      "grad_norm": 2.4608538150787354,
      "learning_rate": 1.2015641663704232e-05,
      "loss": 0.4474,
      "step": 13220
    },
    {
      "epoch": 4.703163881976537,
      "grad_norm": 2.501497268676758,
      "learning_rate": 1.18734447209385e-05,
      "loss": 0.3789,
      "step": 13230
    },
    {
      "epoch": 4.70671880554568,
      "grad_norm": 0.7204046249389648,
      "learning_rate": 1.173124777817277e-05,
      "loss": 0.3229,
      "step": 13240
    },
    {
      "epoch": 4.7102737291148244,
      "grad_norm": 1.7982326745986938,
      "learning_rate": 1.158905083540704e-05,
      "loss": 0.3891,
      "step": 13250
    },
    {
      "epoch": 4.713828652683968,
      "grad_norm": 4.1616435050964355,
      "learning_rate": 1.1446853892641308e-05,
      "loss": 0.329,
      "step": 13260
    },
    {
      "epoch": 4.717383576253111,
      "grad_norm": 0.9853293299674988,
      "learning_rate": 1.1304656949875578e-05,
      "loss": 0.3287,
      "step": 13270
    },
    {
      "epoch": 4.720938499822254,
      "grad_norm": 3.246239423751831,
      "learning_rate": 1.1162460007109846e-05,
      "loss": 0.4029,
      "step": 13280
    },
    {
      "epoch": 4.724493423391397,
      "grad_norm": 1.4519808292388916,
      "learning_rate": 1.1020263064344118e-05,
      "loss": 0.312,
      "step": 13290
    },
    {
      "epoch": 4.7280483469605405,
      "grad_norm": 3.808684825897217,
      "learning_rate": 1.0878066121578387e-05,
      "loss": 0.4353,
      "step": 13300
    },
    {
      "epoch": 4.731603270529684,
      "grad_norm": 3.904050588607788,
      "learning_rate": 1.0735869178812657e-05,
      "loss": 0.5482,
      "step": 13310
    },
    {
      "epoch": 4.735158194098827,
      "grad_norm": 0.7487955093383789,
      "learning_rate": 1.0593672236046925e-05,
      "loss": 0.4407,
      "step": 13320
    },
    {
      "epoch": 4.73871311766797,
      "grad_norm": 1.5543863773345947,
      "learning_rate": 1.0451475293281195e-05,
      "loss": 0.3749,
      "step": 13330
    },
    {
      "epoch": 4.742268041237113,
      "grad_norm": 1.8487235307693481,
      "learning_rate": 1.0309278350515464e-05,
      "loss": 0.3763,
      "step": 13340
    },
    {
      "epoch": 4.745822964806257,
      "grad_norm": 3.4624810218811035,
      "learning_rate": 1.0167081407749734e-05,
      "loss": 0.3484,
      "step": 13350
    },
    {
      "epoch": 4.7493778883754,
      "grad_norm": 1.9416565895080566,
      "learning_rate": 1.0024884464984004e-05,
      "loss": 0.3705,
      "step": 13360
    },
    {
      "epoch": 4.752932811944543,
      "grad_norm": 4.183261394500732,
      "learning_rate": 9.882687522218272e-06,
      "loss": 0.4007,
      "step": 13370
    },
    {
      "epoch": 4.756487735513686,
      "grad_norm": 4.098232269287109,
      "learning_rate": 9.740490579452543e-06,
      "loss": 0.387,
      "step": 13380
    },
    {
      "epoch": 4.7600426590828295,
      "grad_norm": 3.062514543533325,
      "learning_rate": 9.598293636686811e-06,
      "loss": 0.4858,
      "step": 13390
    },
    {
      "epoch": 4.763597582651973,
      "grad_norm": 5.097332954406738,
      "learning_rate": 9.456096693921081e-06,
      "loss": 0.396,
      "step": 13400
    },
    {
      "epoch": 4.767152506221116,
      "grad_norm": 2.6299002170562744,
      "learning_rate": 9.31389975115535e-06,
      "loss": 0.3908,
      "step": 13410
    },
    {
      "epoch": 4.770707429790259,
      "grad_norm": 1.4125412702560425,
      "learning_rate": 9.171702808389621e-06,
      "loss": 0.3567,
      "step": 13420
    },
    {
      "epoch": 4.774262353359402,
      "grad_norm": 2.6927967071533203,
      "learning_rate": 9.02950586562389e-06,
      "loss": 0.2886,
      "step": 13430
    },
    {
      "epoch": 4.7778172769285465,
      "grad_norm": 1.2905349731445312,
      "learning_rate": 8.887308922858158e-06,
      "loss": 0.4331,
      "step": 13440
    },
    {
      "epoch": 4.781372200497689,
      "grad_norm": 1.7328100204467773,
      "learning_rate": 8.745111980092428e-06,
      "loss": 0.4172,
      "step": 13450
    },
    {
      "epoch": 4.784927124066833,
      "grad_norm": 5.296794414520264,
      "learning_rate": 8.602915037326697e-06,
      "loss": 0.4276,
      "step": 13460
    },
    {
      "epoch": 4.788482047635976,
      "grad_norm": 1.726806640625,
      "learning_rate": 8.460718094560967e-06,
      "loss": 0.355,
      "step": 13470
    },
    {
      "epoch": 4.792036971205119,
      "grad_norm": 1.0282511711120605,
      "learning_rate": 8.318521151795237e-06,
      "loss": 0.4398,
      "step": 13480
    },
    {
      "epoch": 4.795591894774263,
      "grad_norm": 1.468692660331726,
      "learning_rate": 8.176324209029507e-06,
      "loss": 0.3557,
      "step": 13490
    },
    {
      "epoch": 4.799146818343406,
      "grad_norm": 1.665874719619751,
      "learning_rate": 8.034127266263776e-06,
      "loss": 0.3667,
      "step": 13500
    },
    {
      "epoch": 4.802701741912549,
      "grad_norm": 7.39600944519043,
      "learning_rate": 7.891930323498046e-06,
      "loss": 0.3649,
      "step": 13510
    },
    {
      "epoch": 4.806256665481692,
      "grad_norm": 4.311175346374512,
      "learning_rate": 7.749733380732314e-06,
      "loss": 0.3548,
      "step": 13520
    },
    {
      "epoch": 4.8098115890508355,
      "grad_norm": 2.6091115474700928,
      "learning_rate": 7.6075364379665835e-06,
      "loss": 0.5281,
      "step": 13530
    },
    {
      "epoch": 4.813366512619979,
      "grad_norm": 1.7773782014846802,
      "learning_rate": 7.465339495200853e-06,
      "loss": 0.3704,
      "step": 13540
    },
    {
      "epoch": 4.816921436189122,
      "grad_norm": 0.40164217352867126,
      "learning_rate": 7.323142552435124e-06,
      "loss": 0.4359,
      "step": 13550
    },
    {
      "epoch": 4.820476359758265,
      "grad_norm": 2.5258305072784424,
      "learning_rate": 7.180945609669393e-06,
      "loss": 0.4587,
      "step": 13560
    },
    {
      "epoch": 4.824031283327408,
      "grad_norm": 2.7837135791778564,
      "learning_rate": 7.038748666903662e-06,
      "loss": 0.4657,
      "step": 13570
    },
    {
      "epoch": 4.827586206896552,
      "grad_norm": 1.8023940324783325,
      "learning_rate": 6.896551724137932e-06,
      "loss": 0.36,
      "step": 13580
    },
    {
      "epoch": 4.831141130465695,
      "grad_norm": 1.6066914796829224,
      "learning_rate": 6.7543547813722e-06,
      "loss": 0.4379,
      "step": 13590
    },
    {
      "epoch": 4.834696054034838,
      "grad_norm": 1.7212557792663574,
      "learning_rate": 6.612157838606469e-06,
      "loss": 0.2607,
      "step": 13600
    },
    {
      "epoch": 4.838250977603981,
      "grad_norm": 1.8180160522460938,
      "learning_rate": 6.46996089584074e-06,
      "loss": 0.3066,
      "step": 13610
    },
    {
      "epoch": 4.8418059011731245,
      "grad_norm": 1.6885900497436523,
      "learning_rate": 6.32776395307501e-06,
      "loss": 0.3901,
      "step": 13620
    },
    {
      "epoch": 4.845360824742268,
      "grad_norm": 3.476595878601074,
      "learning_rate": 6.185567010309279e-06,
      "loss": 0.3905,
      "step": 13630
    },
    {
      "epoch": 4.848915748311411,
      "grad_norm": 2.3217110633850098,
      "learning_rate": 6.043370067543548e-06,
      "loss": 0.3756,
      "step": 13640
    },
    {
      "epoch": 4.852470671880555,
      "grad_norm": 3.250972032546997,
      "learning_rate": 5.9011731247778175e-06,
      "loss": 0.3863,
      "step": 13650
    },
    {
      "epoch": 4.856025595449697,
      "grad_norm": 2.0331623554229736,
      "learning_rate": 5.758976182012088e-06,
      "loss": 0.3402,
      "step": 13660
    },
    {
      "epoch": 4.8595805190188415,
      "grad_norm": 1.0572353601455688,
      "learning_rate": 5.616779239246357e-06,
      "loss": 0.3934,
      "step": 13670
    },
    {
      "epoch": 4.863135442587985,
      "grad_norm": 1.233379602432251,
      "learning_rate": 5.474582296480626e-06,
      "loss": 0.3286,
      "step": 13680
    },
    {
      "epoch": 4.866690366157128,
      "grad_norm": 3.3782126903533936,
      "learning_rate": 5.3323853537148955e-06,
      "loss": 0.4915,
      "step": 13690
    },
    {
      "epoch": 4.870245289726271,
      "grad_norm": 1.4955247640609741,
      "learning_rate": 5.190188410949165e-06,
      "loss": 0.2703,
      "step": 13700
    },
    {
      "epoch": 4.873800213295414,
      "grad_norm": 4.679915904998779,
      "learning_rate": 5.047991468183434e-06,
      "loss": 0.3656,
      "step": 13710
    },
    {
      "epoch": 4.877355136864558,
      "grad_norm": 0.807009756565094,
      "learning_rate": 4.905794525417703e-06,
      "loss": 0.3452,
      "step": 13720
    },
    {
      "epoch": 4.880910060433701,
      "grad_norm": 1.6971193552017212,
      "learning_rate": 4.7635975826519735e-06,
      "loss": 0.3795,
      "step": 13730
    },
    {
      "epoch": 4.884464984002844,
      "grad_norm": 2.6934635639190674,
      "learning_rate": 4.621400639886243e-06,
      "loss": 0.3687,
      "step": 13740
    },
    {
      "epoch": 4.888019907571987,
      "grad_norm": 2.3178799152374268,
      "learning_rate": 4.479203697120512e-06,
      "loss": 0.3823,
      "step": 13750
    },
    {
      "epoch": 4.8915748311411305,
      "grad_norm": 1.065805435180664,
      "learning_rate": 4.337006754354782e-06,
      "loss": 0.329,
      "step": 13760
    },
    {
      "epoch": 4.895129754710274,
      "grad_norm": 2.3662736415863037,
      "learning_rate": 4.1948098115890514e-06,
      "loss": 0.3418,
      "step": 13770
    },
    {
      "epoch": 4.898684678279417,
      "grad_norm": 1.3689218759536743,
      "learning_rate": 4.05261286882332e-06,
      "loss": 0.3304,
      "step": 13780
    },
    {
      "epoch": 4.90223960184856,
      "grad_norm": 3.5608081817626953,
      "learning_rate": 3.91041592605759e-06,
      "loss": 0.4862,
      "step": 13790
    },
    {
      "epoch": 4.905794525417703,
      "grad_norm": 0.9637213349342346,
      "learning_rate": 3.7682189832918593e-06,
      "loss": 0.3864,
      "step": 13800
    },
    {
      "epoch": 4.9093494489868466,
      "grad_norm": 1.5451884269714355,
      "learning_rate": 3.6260220405261286e-06,
      "loss": 0.4209,
      "step": 13810
    },
    {
      "epoch": 4.91290437255599,
      "grad_norm": 2.168637275695801,
      "learning_rate": 3.483825097760398e-06,
      "loss": 0.4314,
      "step": 13820
    },
    {
      "epoch": 4.916459296125133,
      "grad_norm": 3.911609649658203,
      "learning_rate": 3.341628154994668e-06,
      "loss": 0.3431,
      "step": 13830
    },
    {
      "epoch": 4.920014219694276,
      "grad_norm": 0.8374506235122681,
      "learning_rate": 3.1994312122289373e-06,
      "loss": 0.3808,
      "step": 13840
    },
    {
      "epoch": 4.923569143263419,
      "grad_norm": 1.8809927701950073,
      "learning_rate": 3.0572342694632066e-06,
      "loss": 0.332,
      "step": 13850
    },
    {
      "epoch": 4.9271240668325635,
      "grad_norm": 0.600085973739624,
      "learning_rate": 2.915037326697476e-06,
      "loss": 0.3642,
      "step": 13860
    },
    {
      "epoch": 4.930678990401706,
      "grad_norm": 0.655680775642395,
      "learning_rate": 2.7728403839317456e-06,
      "loss": 0.3787,
      "step": 13870
    },
    {
      "epoch": 4.93423391397085,
      "grad_norm": 1.4089800119400024,
      "learning_rate": 2.6306434411660153e-06,
      "loss": 0.3486,
      "step": 13880
    },
    {
      "epoch": 4.937788837539993,
      "grad_norm": 0.2988053858280182,
      "learning_rate": 2.4884464984002846e-06,
      "loss": 0.3782,
      "step": 13890
    },
    {
      "epoch": 4.941343761109136,
      "grad_norm": 2.571345090866089,
      "learning_rate": 2.346249555634554e-06,
      "loss": 0.3801,
      "step": 13900
    },
    {
      "epoch": 4.94489868467828,
      "grad_norm": 2.422424554824829,
      "learning_rate": 2.2040526128688235e-06,
      "loss": 0.4104,
      "step": 13910
    },
    {
      "epoch": 4.948453608247423,
      "grad_norm": 2.3801965713500977,
      "learning_rate": 2.061855670103093e-06,
      "loss": 0.3102,
      "step": 13920
    },
    {
      "epoch": 4.952008531816566,
      "grad_norm": 1.2963708639144897,
      "learning_rate": 1.9196587273373625e-06,
      "loss": 0.4134,
      "step": 13930
    },
    {
      "epoch": 4.955563455385709,
      "grad_norm": 1.0957800149917603,
      "learning_rate": 1.7774617845716316e-06,
      "loss": 0.38,
      "step": 13940
    },
    {
      "epoch": 4.9591183789548525,
      "grad_norm": 3.758470296859741,
      "learning_rate": 1.6352648418059013e-06,
      "loss": 0.2988,
      "step": 13950
    },
    {
      "epoch": 4.962673302523996,
      "grad_norm": 1.6572120189666748,
      "learning_rate": 1.4930678990401706e-06,
      "loss": 0.3907,
      "step": 13960
    },
    {
      "epoch": 4.966228226093139,
      "grad_norm": 5.218579292297363,
      "learning_rate": 1.35087095627444e-06,
      "loss": 0.3928,
      "step": 13970
    },
    {
      "epoch": 4.969783149662282,
      "grad_norm": 1.3753541707992554,
      "learning_rate": 1.2086740135087096e-06,
      "loss": 0.3767,
      "step": 13980
    },
    {
      "epoch": 4.973338073231425,
      "grad_norm": 1.0231395959854126,
      "learning_rate": 1.066477070742979e-06,
      "loss": 0.4023,
      "step": 13990
    },
    {
      "epoch": 4.976892996800569,
      "grad_norm": 2.4242043495178223,
      "learning_rate": 9.242801279772485e-07,
      "loss": 0.3773,
      "step": 14000
    },
    {
      "epoch": 4.980447920369712,
      "grad_norm": 2.456693172454834,
      "learning_rate": 7.82083185211518e-07,
      "loss": 0.3211,
      "step": 14010
    },
    {
      "epoch": 4.984002843938855,
      "grad_norm": 1.2410904169082642,
      "learning_rate": 6.398862424457875e-07,
      "loss": 0.5105,
      "step": 14020
    },
    {
      "epoch": 4.987557767507998,
      "grad_norm": 1.203725814819336,
      "learning_rate": 4.976892996800569e-07,
      "loss": 0.2847,
      "step": 14030
    },
    {
      "epoch": 4.9911126910771415,
      "grad_norm": 2.139524221420288,
      "learning_rate": 3.5549235691432637e-07,
      "loss": 0.4258,
      "step": 14040
    },
    {
      "epoch": 4.994667614646285,
      "grad_norm": 1.4655201435089111,
      "learning_rate": 2.1329541414859584e-07,
      "loss": 0.3859,
      "step": 14050
    },
    {
      "epoch": 4.998222538215428,
      "grad_norm": 4.473796844482422,
      "learning_rate": 7.109847138286527e-08,
      "loss": 0.3135,
      "step": 14060
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.8344,
      "eval_f1_macro": 0.834238641262951,
      "eval_f1_neg": 0.839410395655547,
      "eval_f1_pos": 0.8290668868703551,
      "eval_loss": 0.3895866572856903,
      "eval_precision_global": 0.8361447332465319,
      "eval_precision_neg": 0.8104868913857678,
      "eval_precision_pos": 0.8618025751072962,
      "eval_recall_global": 0.8346008930840072,
      "eval_recall_neg": 0.8704746580852776,
      "eval_recall_pos": 0.7987271280827367,
      "eval_runtime": 56.8578,
      "eval_samples_per_second": 43.969,
      "eval_steps_per_second": 5.505,
      "step": 14065
    }
  ],
  "logging_steps": 10,
  "max_steps": 14065,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.94598574848e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
