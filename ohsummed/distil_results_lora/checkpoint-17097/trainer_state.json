{
  "best_global_step": 17097,
  "best_metric": 0.40286659726333457,
  "best_model_checkpoint": "./results_lora/checkpoint-17097",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 17097,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001754693805930865,
      "grad_norm": 2.423769950866699,
      "learning_rate": 4.997075510323449e-05,
      "loss": 3.121,
      "step": 10
    },
    {
      "epoch": 0.00350938761186173,
      "grad_norm": 2.540194272994995,
      "learning_rate": 4.9941510206468974e-05,
      "loss": 3.0885,
      "step": 20
    },
    {
      "epoch": 0.005264081417792595,
      "grad_norm": 2.7628469467163086,
      "learning_rate": 4.991226530970346e-05,
      "loss": 3.0345,
      "step": 30
    },
    {
      "epoch": 0.00701877522372346,
      "grad_norm": 2.635322332382202,
      "learning_rate": 4.9883020412937945e-05,
      "loss": 2.9186,
      "step": 40
    },
    {
      "epoch": 0.008773469029654325,
      "grad_norm": 3.1455459594726562,
      "learning_rate": 4.985377551617243e-05,
      "loss": 2.9473,
      "step": 50
    },
    {
      "epoch": 0.01052816283558519,
      "grad_norm": 3.4859650135040283,
      "learning_rate": 4.9824530619406916e-05,
      "loss": 2.9637,
      "step": 60
    },
    {
      "epoch": 0.012282856641516056,
      "grad_norm": 2.975558280944824,
      "learning_rate": 4.97952857226414e-05,
      "loss": 2.9241,
      "step": 70
    },
    {
      "epoch": 0.01403755044744692,
      "grad_norm": 3.0196328163146973,
      "learning_rate": 4.976604082587589e-05,
      "loss": 2.9338,
      "step": 80
    },
    {
      "epoch": 0.015792244253377784,
      "grad_norm": 3.601076602935791,
      "learning_rate": 4.973679592911037e-05,
      "loss": 2.8492,
      "step": 90
    },
    {
      "epoch": 0.01754693805930865,
      "grad_norm": 3.98695707321167,
      "learning_rate": 4.970755103234486e-05,
      "loss": 2.9535,
      "step": 100
    },
    {
      "epoch": 0.019301631865239515,
      "grad_norm": 4.517102241516113,
      "learning_rate": 4.9678306135579344e-05,
      "loss": 2.8056,
      "step": 110
    },
    {
      "epoch": 0.02105632567117038,
      "grad_norm": 4.123042106628418,
      "learning_rate": 4.964906123881383e-05,
      "loss": 2.8744,
      "step": 120
    },
    {
      "epoch": 0.022811019477101246,
      "grad_norm": 3.1744558811187744,
      "learning_rate": 4.9619816342048315e-05,
      "loss": 2.8716,
      "step": 130
    },
    {
      "epoch": 0.02456571328303211,
      "grad_norm": 3.319650411605835,
      "learning_rate": 4.95905714452828e-05,
      "loss": 2.9183,
      "step": 140
    },
    {
      "epoch": 0.026320407088962976,
      "grad_norm": 3.948974609375,
      "learning_rate": 4.956132654851729e-05,
      "loss": 2.7331,
      "step": 150
    },
    {
      "epoch": 0.02807510089489384,
      "grad_norm": 4.233358860015869,
      "learning_rate": 4.953208165175177e-05,
      "loss": 2.7544,
      "step": 160
    },
    {
      "epoch": 0.029829794700824707,
      "grad_norm": 4.633305072784424,
      "learning_rate": 4.950283675498626e-05,
      "loss": 2.9315,
      "step": 170
    },
    {
      "epoch": 0.03158448850675557,
      "grad_norm": 4.164665699005127,
      "learning_rate": 4.9473591858220744e-05,
      "loss": 2.7262,
      "step": 180
    },
    {
      "epoch": 0.033339182312686434,
      "grad_norm": 4.1212568283081055,
      "learning_rate": 4.944434696145523e-05,
      "loss": 2.5988,
      "step": 190
    },
    {
      "epoch": 0.0350938761186173,
      "grad_norm": 3.9582581520080566,
      "learning_rate": 4.9415102064689715e-05,
      "loss": 2.6844,
      "step": 200
    },
    {
      "epoch": 0.036848569924548165,
      "grad_norm": 3.901001214981079,
      "learning_rate": 4.93858571679242e-05,
      "loss": 2.6795,
      "step": 210
    },
    {
      "epoch": 0.03860326373047903,
      "grad_norm": 5.367015838623047,
      "learning_rate": 4.9356612271158686e-05,
      "loss": 2.9009,
      "step": 220
    },
    {
      "epoch": 0.040357957536409896,
      "grad_norm": 4.321787357330322,
      "learning_rate": 4.932736737439317e-05,
      "loss": 2.8844,
      "step": 230
    },
    {
      "epoch": 0.04211265134234076,
      "grad_norm": 4.9411444664001465,
      "learning_rate": 4.929812247762766e-05,
      "loss": 2.9255,
      "step": 240
    },
    {
      "epoch": 0.043867345148271626,
      "grad_norm": 4.732158660888672,
      "learning_rate": 4.926887758086214e-05,
      "loss": 2.8648,
      "step": 250
    },
    {
      "epoch": 0.04562203895420249,
      "grad_norm": 4.8323163986206055,
      "learning_rate": 4.923963268409663e-05,
      "loss": 2.7057,
      "step": 260
    },
    {
      "epoch": 0.04737673276013336,
      "grad_norm": 4.0787858963012695,
      "learning_rate": 4.9210387787331114e-05,
      "loss": 2.704,
      "step": 270
    },
    {
      "epoch": 0.04913142656606422,
      "grad_norm": 3.6190483570098877,
      "learning_rate": 4.91811428905656e-05,
      "loss": 2.6957,
      "step": 280
    },
    {
      "epoch": 0.05088612037199509,
      "grad_norm": 4.241422176361084,
      "learning_rate": 4.9151897993800085e-05,
      "loss": 2.745,
      "step": 290
    },
    {
      "epoch": 0.05264081417792595,
      "grad_norm": 4.2424421310424805,
      "learning_rate": 4.912265309703457e-05,
      "loss": 2.5346,
      "step": 300
    },
    {
      "epoch": 0.05439550798385682,
      "grad_norm": 4.03532600402832,
      "learning_rate": 4.909340820026905e-05,
      "loss": 2.6105,
      "step": 310
    },
    {
      "epoch": 0.05615020178978768,
      "grad_norm": 4.8678083419799805,
      "learning_rate": 4.906416330350354e-05,
      "loss": 2.5711,
      "step": 320
    },
    {
      "epoch": 0.05790489559571855,
      "grad_norm": 5.46809196472168,
      "learning_rate": 4.903491840673803e-05,
      "loss": 2.5501,
      "step": 330
    },
    {
      "epoch": 0.059659589401649414,
      "grad_norm": 4.805027484893799,
      "learning_rate": 4.9005673509972514e-05,
      "loss": 2.5294,
      "step": 340
    },
    {
      "epoch": 0.06141428320758028,
      "grad_norm": 4.39687967300415,
      "learning_rate": 4.8976428613207e-05,
      "loss": 2.3125,
      "step": 350
    },
    {
      "epoch": 0.06316897701351114,
      "grad_norm": 5.7667236328125,
      "learning_rate": 4.8947183716441485e-05,
      "loss": 2.6905,
      "step": 360
    },
    {
      "epoch": 0.064923670819442,
      "grad_norm": 4.740568161010742,
      "learning_rate": 4.891793881967597e-05,
      "loss": 2.5494,
      "step": 370
    },
    {
      "epoch": 0.06667836462537287,
      "grad_norm": 4.652160167694092,
      "learning_rate": 4.8888693922910456e-05,
      "loss": 2.5683,
      "step": 380
    },
    {
      "epoch": 0.06843305843130373,
      "grad_norm": 4.880370140075684,
      "learning_rate": 4.8859449026144935e-05,
      "loss": 2.486,
      "step": 390
    },
    {
      "epoch": 0.0701877522372346,
      "grad_norm": 4.731980323791504,
      "learning_rate": 4.883020412937943e-05,
      "loss": 2.498,
      "step": 400
    },
    {
      "epoch": 0.07194244604316546,
      "grad_norm": 5.875381946563721,
      "learning_rate": 4.880095923261391e-05,
      "loss": 2.4158,
      "step": 410
    },
    {
      "epoch": 0.07369713984909633,
      "grad_norm": 5.946038722991943,
      "learning_rate": 4.87717143358484e-05,
      "loss": 2.2425,
      "step": 420
    },
    {
      "epoch": 0.0754518336550272,
      "grad_norm": 5.597408771514893,
      "learning_rate": 4.8742469439082884e-05,
      "loss": 2.5385,
      "step": 430
    },
    {
      "epoch": 0.07720652746095806,
      "grad_norm": 5.0181169509887695,
      "learning_rate": 4.871322454231737e-05,
      "loss": 2.2141,
      "step": 440
    },
    {
      "epoch": 0.07896122126688893,
      "grad_norm": 6.2567667961120605,
      "learning_rate": 4.8683979645551855e-05,
      "loss": 2.5779,
      "step": 450
    },
    {
      "epoch": 0.08071591507281979,
      "grad_norm": 5.020555019378662,
      "learning_rate": 4.865473474878634e-05,
      "loss": 2.2905,
      "step": 460
    },
    {
      "epoch": 0.08247060887875066,
      "grad_norm": 5.232471466064453,
      "learning_rate": 4.862548985202082e-05,
      "loss": 2.2966,
      "step": 470
    },
    {
      "epoch": 0.08422530268468152,
      "grad_norm": 5.163954257965088,
      "learning_rate": 4.859624495525531e-05,
      "loss": 2.2215,
      "step": 480
    },
    {
      "epoch": 0.08597999649061239,
      "grad_norm": 5.791988372802734,
      "learning_rate": 4.85670000584898e-05,
      "loss": 2.6019,
      "step": 490
    },
    {
      "epoch": 0.08773469029654325,
      "grad_norm": 5.674750804901123,
      "learning_rate": 4.853775516172428e-05,
      "loss": 2.3576,
      "step": 500
    },
    {
      "epoch": 0.08948938410247412,
      "grad_norm": 4.776401042938232,
      "learning_rate": 4.850851026495877e-05,
      "loss": 2.4136,
      "step": 510
    },
    {
      "epoch": 0.09124407790840498,
      "grad_norm": 6.462698936462402,
      "learning_rate": 4.8479265368193255e-05,
      "loss": 2.3737,
      "step": 520
    },
    {
      "epoch": 0.09299877171433585,
      "grad_norm": 6.2902607917785645,
      "learning_rate": 4.845002047142774e-05,
      "loss": 2.6015,
      "step": 530
    },
    {
      "epoch": 0.09475346552026671,
      "grad_norm": 5.280025482177734,
      "learning_rate": 4.8420775574662226e-05,
      "loss": 2.1736,
      "step": 540
    },
    {
      "epoch": 0.09650815932619758,
      "grad_norm": 7.747463703155518,
      "learning_rate": 4.8391530677896705e-05,
      "loss": 2.2915,
      "step": 550
    },
    {
      "epoch": 0.09826285313212844,
      "grad_norm": 5.325371742248535,
      "learning_rate": 4.83622857811312e-05,
      "loss": 2.6708,
      "step": 560
    },
    {
      "epoch": 0.10001754693805931,
      "grad_norm": 7.858338356018066,
      "learning_rate": 4.833304088436568e-05,
      "loss": 2.2366,
      "step": 570
    },
    {
      "epoch": 0.10177224074399017,
      "grad_norm": 5.150753021240234,
      "learning_rate": 4.830379598760016e-05,
      "loss": 2.4832,
      "step": 580
    },
    {
      "epoch": 0.10352693454992104,
      "grad_norm": 5.091195583343506,
      "learning_rate": 4.8274551090834654e-05,
      "loss": 2.1114,
      "step": 590
    },
    {
      "epoch": 0.1052816283558519,
      "grad_norm": 5.3727240562438965,
      "learning_rate": 4.824530619406914e-05,
      "loss": 2.2948,
      "step": 600
    },
    {
      "epoch": 0.10703632216178277,
      "grad_norm": 6.456594467163086,
      "learning_rate": 4.821606129730362e-05,
      "loss": 2.3632,
      "step": 610
    },
    {
      "epoch": 0.10879101596771364,
      "grad_norm": 6.524171829223633,
      "learning_rate": 4.818681640053811e-05,
      "loss": 2.3614,
      "step": 620
    },
    {
      "epoch": 0.1105457097736445,
      "grad_norm": 5.566040515899658,
      "learning_rate": 4.815757150377259e-05,
      "loss": 2.333,
      "step": 630
    },
    {
      "epoch": 0.11230040357957537,
      "grad_norm": 6.347842693328857,
      "learning_rate": 4.812832660700708e-05,
      "loss": 2.1789,
      "step": 640
    },
    {
      "epoch": 0.11405509738550623,
      "grad_norm": 5.464089393615723,
      "learning_rate": 4.809908171024157e-05,
      "loss": 2.2382,
      "step": 650
    },
    {
      "epoch": 0.1158097911914371,
      "grad_norm": 5.27803373336792,
      "learning_rate": 4.806983681347605e-05,
      "loss": 2.1717,
      "step": 660
    },
    {
      "epoch": 0.11756448499736796,
      "grad_norm": 5.666835784912109,
      "learning_rate": 4.804059191671054e-05,
      "loss": 2.3508,
      "step": 670
    },
    {
      "epoch": 0.11931917880329883,
      "grad_norm": 6.344057083129883,
      "learning_rate": 4.8011347019945025e-05,
      "loss": 2.1117,
      "step": 680
    },
    {
      "epoch": 0.1210738726092297,
      "grad_norm": 5.276747226715088,
      "learning_rate": 4.7982102123179503e-05,
      "loss": 2.1452,
      "step": 690
    },
    {
      "epoch": 0.12282856641516056,
      "grad_norm": 5.644641399383545,
      "learning_rate": 4.7952857226413996e-05,
      "loss": 2.2161,
      "step": 700
    },
    {
      "epoch": 0.12458326022109142,
      "grad_norm": 6.884969711303711,
      "learning_rate": 4.7923612329648475e-05,
      "loss": 2.3953,
      "step": 710
    },
    {
      "epoch": 0.12633795402702228,
      "grad_norm": 6.4679059982299805,
      "learning_rate": 4.789436743288297e-05,
      "loss": 2.2185,
      "step": 720
    },
    {
      "epoch": 0.12809264783295315,
      "grad_norm": 6.56328821182251,
      "learning_rate": 4.786512253611745e-05,
      "loss": 2.2026,
      "step": 730
    },
    {
      "epoch": 0.129847341638884,
      "grad_norm": 6.462314605712891,
      "learning_rate": 4.783587763935193e-05,
      "loss": 2.052,
      "step": 740
    },
    {
      "epoch": 0.13160203544481489,
      "grad_norm": 9.29555892944336,
      "learning_rate": 4.7806632742586424e-05,
      "loss": 2.3103,
      "step": 750
    },
    {
      "epoch": 0.13335672925074574,
      "grad_norm": 5.972268104553223,
      "learning_rate": 4.777738784582091e-05,
      "loss": 2.1858,
      "step": 760
    },
    {
      "epoch": 0.13511142305667662,
      "grad_norm": 7.089939117431641,
      "learning_rate": 4.774814294905539e-05,
      "loss": 2.1542,
      "step": 770
    },
    {
      "epoch": 0.13686611686260747,
      "grad_norm": 6.303804874420166,
      "learning_rate": 4.771889805228988e-05,
      "loss": 2.3907,
      "step": 780
    },
    {
      "epoch": 0.13862081066853835,
      "grad_norm": 6.047572612762451,
      "learning_rate": 4.768965315552436e-05,
      "loss": 1.8752,
      "step": 790
    },
    {
      "epoch": 0.1403755044744692,
      "grad_norm": 5.769197463989258,
      "learning_rate": 4.7660408258758845e-05,
      "loss": 2.3075,
      "step": 800
    },
    {
      "epoch": 0.14213019828040008,
      "grad_norm": 7.924705505371094,
      "learning_rate": 4.763116336199334e-05,
      "loss": 2.237,
      "step": 810
    },
    {
      "epoch": 0.14388489208633093,
      "grad_norm": 5.610526084899902,
      "learning_rate": 4.7601918465227817e-05,
      "loss": 2.3116,
      "step": 820
    },
    {
      "epoch": 0.1456395858922618,
      "grad_norm": 6.459797382354736,
      "learning_rate": 4.757267356846231e-05,
      "loss": 2.03,
      "step": 830
    },
    {
      "epoch": 0.14739427969819266,
      "grad_norm": 5.565415382385254,
      "learning_rate": 4.7543428671696795e-05,
      "loss": 1.8252,
      "step": 840
    },
    {
      "epoch": 0.14914897350412354,
      "grad_norm": 5.726858139038086,
      "learning_rate": 4.7514183774931273e-05,
      "loss": 2.0692,
      "step": 850
    },
    {
      "epoch": 0.1509036673100544,
      "grad_norm": 5.892488956451416,
      "learning_rate": 4.7484938878165766e-05,
      "loss": 1.9426,
      "step": 860
    },
    {
      "epoch": 0.15265836111598527,
      "grad_norm": 7.593120574951172,
      "learning_rate": 4.7455693981400245e-05,
      "loss": 2.0554,
      "step": 870
    },
    {
      "epoch": 0.15441305492191612,
      "grad_norm": 7.3250322341918945,
      "learning_rate": 4.742644908463473e-05,
      "loss": 2.0744,
      "step": 880
    },
    {
      "epoch": 0.156167748727847,
      "grad_norm": 7.564825057983398,
      "learning_rate": 4.739720418786922e-05,
      "loss": 1.888,
      "step": 890
    },
    {
      "epoch": 0.15792244253377785,
      "grad_norm": 5.444169521331787,
      "learning_rate": 4.73679592911037e-05,
      "loss": 2.0199,
      "step": 900
    },
    {
      "epoch": 0.15967713633970873,
      "grad_norm": 5.259507179260254,
      "learning_rate": 4.7338714394338194e-05,
      "loss": 2.1763,
      "step": 910
    },
    {
      "epoch": 0.16143183014563958,
      "grad_norm": 6.295555591583252,
      "learning_rate": 4.730946949757268e-05,
      "loss": 1.9795,
      "step": 920
    },
    {
      "epoch": 0.16318652395157046,
      "grad_norm": 6.0496625900268555,
      "learning_rate": 4.728022460080716e-05,
      "loss": 1.9413,
      "step": 930
    },
    {
      "epoch": 0.1649412177575013,
      "grad_norm": 6.041101932525635,
      "learning_rate": 4.725097970404165e-05,
      "loss": 2.358,
      "step": 940
    },
    {
      "epoch": 0.1666959115634322,
      "grad_norm": 6.928005218505859,
      "learning_rate": 4.722173480727613e-05,
      "loss": 2.1101,
      "step": 950
    },
    {
      "epoch": 0.16845060536936304,
      "grad_norm": 5.25537109375,
      "learning_rate": 4.7192489910510615e-05,
      "loss": 1.9705,
      "step": 960
    },
    {
      "epoch": 0.17020529917529392,
      "grad_norm": 6.655152320861816,
      "learning_rate": 4.716324501374511e-05,
      "loss": 2.3704,
      "step": 970
    },
    {
      "epoch": 0.17195999298122477,
      "grad_norm": 6.585590362548828,
      "learning_rate": 4.7134000116979587e-05,
      "loss": 2.1819,
      "step": 980
    },
    {
      "epoch": 0.17371468678715565,
      "grad_norm": 8.06493091583252,
      "learning_rate": 4.710475522021407e-05,
      "loss": 2.0458,
      "step": 990
    },
    {
      "epoch": 0.1754693805930865,
      "grad_norm": 7.633573055267334,
      "learning_rate": 4.7075510323448565e-05,
      "loss": 2.2358,
      "step": 1000
    },
    {
      "epoch": 0.17722407439901738,
      "grad_norm": 8.043906211853027,
      "learning_rate": 4.704626542668304e-05,
      "loss": 1.892,
      "step": 1010
    },
    {
      "epoch": 0.17897876820494824,
      "grad_norm": 7.911675930023193,
      "learning_rate": 4.7017020529917536e-05,
      "loss": 2.0418,
      "step": 1020
    },
    {
      "epoch": 0.18073346201087911,
      "grad_norm": 11.586776733398438,
      "learning_rate": 4.6987775633152015e-05,
      "loss": 2.1059,
      "step": 1030
    },
    {
      "epoch": 0.18248815581680997,
      "grad_norm": 6.588846683502197,
      "learning_rate": 4.69585307363865e-05,
      "loss": 2.1529,
      "step": 1040
    },
    {
      "epoch": 0.18424284962274085,
      "grad_norm": 5.458082675933838,
      "learning_rate": 4.692928583962099e-05,
      "loss": 2.0185,
      "step": 1050
    },
    {
      "epoch": 0.1859975434286717,
      "grad_norm": 6.91018533706665,
      "learning_rate": 4.690004094285547e-05,
      "loss": 2.0663,
      "step": 1060
    },
    {
      "epoch": 0.18775223723460255,
      "grad_norm": 7.160170555114746,
      "learning_rate": 4.687079604608996e-05,
      "loss": 2.3277,
      "step": 1070
    },
    {
      "epoch": 0.18950693104053343,
      "grad_norm": 6.000794887542725,
      "learning_rate": 4.684155114932445e-05,
      "loss": 1.9563,
      "step": 1080
    },
    {
      "epoch": 0.19126162484646428,
      "grad_norm": 7.83563232421875,
      "learning_rate": 4.681230625255893e-05,
      "loss": 2.246,
      "step": 1090
    },
    {
      "epoch": 0.19301631865239516,
      "grad_norm": 7.539337635040283,
      "learning_rate": 4.6783061355793414e-05,
      "loss": 2.0546,
      "step": 1100
    },
    {
      "epoch": 0.194771012458326,
      "grad_norm": 7.0733795166015625,
      "learning_rate": 4.67538164590279e-05,
      "loss": 2.0481,
      "step": 1110
    },
    {
      "epoch": 0.1965257062642569,
      "grad_norm": 5.9677019119262695,
      "learning_rate": 4.6724571562262385e-05,
      "loss": 2.0665,
      "step": 1120
    },
    {
      "epoch": 0.19828040007018774,
      "grad_norm": 5.294163227081299,
      "learning_rate": 4.669532666549688e-05,
      "loss": 2.1357,
      "step": 1130
    },
    {
      "epoch": 0.20003509387611862,
      "grad_norm": 6.675804615020752,
      "learning_rate": 4.6666081768731356e-05,
      "loss": 2.1537,
      "step": 1140
    },
    {
      "epoch": 0.20178978768204947,
      "grad_norm": 8.543306350708008,
      "learning_rate": 4.663683687196584e-05,
      "loss": 2.0359,
      "step": 1150
    },
    {
      "epoch": 0.20354448148798035,
      "grad_norm": 5.092161178588867,
      "learning_rate": 4.6607591975200334e-05,
      "loss": 1.8937,
      "step": 1160
    },
    {
      "epoch": 0.2052991752939112,
      "grad_norm": 6.729983806610107,
      "learning_rate": 4.657834707843481e-05,
      "loss": 1.8722,
      "step": 1170
    },
    {
      "epoch": 0.20705386909984208,
      "grad_norm": 7.164217948913574,
      "learning_rate": 4.65491021816693e-05,
      "loss": 2.0078,
      "step": 1180
    },
    {
      "epoch": 0.20880856290577293,
      "grad_norm": 7.409936428070068,
      "learning_rate": 4.6519857284903785e-05,
      "loss": 1.9753,
      "step": 1190
    },
    {
      "epoch": 0.2105632567117038,
      "grad_norm": 6.882501125335693,
      "learning_rate": 4.649061238813827e-05,
      "loss": 2.1418,
      "step": 1200
    },
    {
      "epoch": 0.21231795051763466,
      "grad_norm": 7.212266445159912,
      "learning_rate": 4.646136749137276e-05,
      "loss": 2.0645,
      "step": 1210
    },
    {
      "epoch": 0.21407264432356554,
      "grad_norm": 8.26136589050293,
      "learning_rate": 4.643212259460724e-05,
      "loss": 1.8474,
      "step": 1220
    },
    {
      "epoch": 0.2158273381294964,
      "grad_norm": 7.828616619110107,
      "learning_rate": 4.640287769784173e-05,
      "loss": 2.0431,
      "step": 1230
    },
    {
      "epoch": 0.21758203193542727,
      "grad_norm": 6.562122821807861,
      "learning_rate": 4.637363280107622e-05,
      "loss": 2.0685,
      "step": 1240
    },
    {
      "epoch": 0.21933672574135812,
      "grad_norm": 8.054193496704102,
      "learning_rate": 4.63443879043107e-05,
      "loss": 1.9969,
      "step": 1250
    },
    {
      "epoch": 0.221091419547289,
      "grad_norm": 6.446579933166504,
      "learning_rate": 4.6315143007545184e-05,
      "loss": 1.8922,
      "step": 1260
    },
    {
      "epoch": 0.22284611335321985,
      "grad_norm": 6.227848052978516,
      "learning_rate": 4.628589811077967e-05,
      "loss": 2.1733,
      "step": 1270
    },
    {
      "epoch": 0.22460080715915073,
      "grad_norm": 5.9696831703186035,
      "learning_rate": 4.6256653214014155e-05,
      "loss": 1.951,
      "step": 1280
    },
    {
      "epoch": 0.22635550096508159,
      "grad_norm": 7.208510875701904,
      "learning_rate": 4.622740831724864e-05,
      "loss": 1.9743,
      "step": 1290
    },
    {
      "epoch": 0.22811019477101246,
      "grad_norm": 6.984467506408691,
      "learning_rate": 4.6198163420483126e-05,
      "loss": 2.213,
      "step": 1300
    },
    {
      "epoch": 0.22986488857694332,
      "grad_norm": 7.05916166305542,
      "learning_rate": 4.616891852371761e-05,
      "loss": 2.0852,
      "step": 1310
    },
    {
      "epoch": 0.2316195823828742,
      "grad_norm": 6.776137828826904,
      "learning_rate": 4.6139673626952104e-05,
      "loss": 1.8208,
      "step": 1320
    },
    {
      "epoch": 0.23337427618880505,
      "grad_norm": 5.208398342132568,
      "learning_rate": 4.611042873018658e-05,
      "loss": 2.174,
      "step": 1330
    },
    {
      "epoch": 0.23512896999473593,
      "grad_norm": 7.051685333251953,
      "learning_rate": 4.608118383342107e-05,
      "loss": 1.7297,
      "step": 1340
    },
    {
      "epoch": 0.23688366380066678,
      "grad_norm": 6.138790130615234,
      "learning_rate": 4.6051938936655554e-05,
      "loss": 1.863,
      "step": 1350
    },
    {
      "epoch": 0.23863835760659766,
      "grad_norm": 6.811829566955566,
      "learning_rate": 4.602269403989004e-05,
      "loss": 2.0416,
      "step": 1360
    },
    {
      "epoch": 0.2403930514125285,
      "grad_norm": 6.538064002990723,
      "learning_rate": 4.5993449143124526e-05,
      "loss": 2.0413,
      "step": 1370
    },
    {
      "epoch": 0.2421477452184594,
      "grad_norm": 8.014226913452148,
      "learning_rate": 4.596420424635901e-05,
      "loss": 2.2351,
      "step": 1380
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 8.014841079711914,
      "learning_rate": 4.59349593495935e-05,
      "loss": 2.0432,
      "step": 1390
    },
    {
      "epoch": 0.24565713283032112,
      "grad_norm": 8.932219505310059,
      "learning_rate": 4.590571445282798e-05,
      "loss": 2.0761,
      "step": 1400
    },
    {
      "epoch": 0.24741182663625197,
      "grad_norm": 6.978327751159668,
      "learning_rate": 4.587646955606247e-05,
      "loss": 1.8432,
      "step": 1410
    },
    {
      "epoch": 0.24916652044218285,
      "grad_norm": 6.77755880355835,
      "learning_rate": 4.5847224659296954e-05,
      "loss": 2.1316,
      "step": 1420
    },
    {
      "epoch": 0.2509212142481137,
      "grad_norm": 8.647007942199707,
      "learning_rate": 4.581797976253144e-05,
      "loss": 1.9126,
      "step": 1430
    },
    {
      "epoch": 0.25267590805404455,
      "grad_norm": 8.218179702758789,
      "learning_rate": 4.5788734865765925e-05,
      "loss": 2.1317,
      "step": 1440
    },
    {
      "epoch": 0.25443060185997546,
      "grad_norm": 6.494162559509277,
      "learning_rate": 4.575948996900041e-05,
      "loss": 1.9361,
      "step": 1450
    },
    {
      "epoch": 0.2561852956659063,
      "grad_norm": 6.994071960449219,
      "learning_rate": 4.5730245072234896e-05,
      "loss": 2.0203,
      "step": 1460
    },
    {
      "epoch": 0.25793998947183716,
      "grad_norm": 7.650722026824951,
      "learning_rate": 4.570100017546938e-05,
      "loss": 1.955,
      "step": 1470
    },
    {
      "epoch": 0.259694683277768,
      "grad_norm": 8.709532737731934,
      "learning_rate": 4.567175527870387e-05,
      "loss": 1.9106,
      "step": 1480
    },
    {
      "epoch": 0.2614493770836989,
      "grad_norm": 6.631239414215088,
      "learning_rate": 4.564251038193835e-05,
      "loss": 1.8672,
      "step": 1490
    },
    {
      "epoch": 0.26320407088962977,
      "grad_norm": 7.681665420532227,
      "learning_rate": 4.561326548517284e-05,
      "loss": 1.9381,
      "step": 1500
    },
    {
      "epoch": 0.2649587646955606,
      "grad_norm": 5.801487922668457,
      "learning_rate": 4.5584020588407324e-05,
      "loss": 1.9061,
      "step": 1510
    },
    {
      "epoch": 0.2667134585014915,
      "grad_norm": 8.518288612365723,
      "learning_rate": 4.555477569164181e-05,
      "loss": 1.8219,
      "step": 1520
    },
    {
      "epoch": 0.2684681523074224,
      "grad_norm": 8.176392555236816,
      "learning_rate": 4.5525530794876296e-05,
      "loss": 1.8244,
      "step": 1530
    },
    {
      "epoch": 0.27022284611335323,
      "grad_norm": 7.042206764221191,
      "learning_rate": 4.549628589811078e-05,
      "loss": 1.5753,
      "step": 1540
    },
    {
      "epoch": 0.2719775399192841,
      "grad_norm": 7.682450294494629,
      "learning_rate": 4.546704100134527e-05,
      "loss": 2.0128,
      "step": 1550
    },
    {
      "epoch": 0.27373223372521494,
      "grad_norm": 7.243950843811035,
      "learning_rate": 4.543779610457975e-05,
      "loss": 1.8867,
      "step": 1560
    },
    {
      "epoch": 0.27548692753114584,
      "grad_norm": 7.341374397277832,
      "learning_rate": 4.540855120781424e-05,
      "loss": 1.837,
      "step": 1570
    },
    {
      "epoch": 0.2772416213370767,
      "grad_norm": 7.340439319610596,
      "learning_rate": 4.5379306311048724e-05,
      "loss": 1.855,
      "step": 1580
    },
    {
      "epoch": 0.27899631514300754,
      "grad_norm": 3.9261224269866943,
      "learning_rate": 4.535006141428321e-05,
      "loss": 1.7932,
      "step": 1590
    },
    {
      "epoch": 0.2807510089489384,
      "grad_norm": 7.310398101806641,
      "learning_rate": 4.5320816517517695e-05,
      "loss": 1.9273,
      "step": 1600
    },
    {
      "epoch": 0.28250570275486925,
      "grad_norm": 10.087886810302734,
      "learning_rate": 4.529157162075218e-05,
      "loss": 2.0536,
      "step": 1610
    },
    {
      "epoch": 0.28426039656080015,
      "grad_norm": 7.433969497680664,
      "learning_rate": 4.5262326723986666e-05,
      "loss": 1.8333,
      "step": 1620
    },
    {
      "epoch": 0.286015090366731,
      "grad_norm": 6.5494704246521,
      "learning_rate": 4.523308182722115e-05,
      "loss": 1.8001,
      "step": 1630
    },
    {
      "epoch": 0.28776978417266186,
      "grad_norm": 7.43143892288208,
      "learning_rate": 4.520383693045564e-05,
      "loss": 2.1551,
      "step": 1640
    },
    {
      "epoch": 0.2895244779785927,
      "grad_norm": 10.472461700439453,
      "learning_rate": 4.517459203369012e-05,
      "loss": 2.0798,
      "step": 1650
    },
    {
      "epoch": 0.2912791717845236,
      "grad_norm": 7.9048566818237305,
      "learning_rate": 4.514534713692461e-05,
      "loss": 1.8524,
      "step": 1660
    },
    {
      "epoch": 0.29303386559045447,
      "grad_norm": 7.848968029022217,
      "learning_rate": 4.5116102240159094e-05,
      "loss": 1.8926,
      "step": 1670
    },
    {
      "epoch": 0.2947885593963853,
      "grad_norm": 8.776379585266113,
      "learning_rate": 4.508685734339358e-05,
      "loss": 1.8706,
      "step": 1680
    },
    {
      "epoch": 0.29654325320231617,
      "grad_norm": 7.014822006225586,
      "learning_rate": 4.5057612446628066e-05,
      "loss": 1.9171,
      "step": 1690
    },
    {
      "epoch": 0.2982979470082471,
      "grad_norm": 7.659574508666992,
      "learning_rate": 4.502836754986255e-05,
      "loss": 1.8819,
      "step": 1700
    },
    {
      "epoch": 0.30005264081417793,
      "grad_norm": 8.793721199035645,
      "learning_rate": 4.499912265309704e-05,
      "loss": 1.885,
      "step": 1710
    },
    {
      "epoch": 0.3018073346201088,
      "grad_norm": 10.033446311950684,
      "learning_rate": 4.496987775633152e-05,
      "loss": 1.9469,
      "step": 1720
    },
    {
      "epoch": 0.30356202842603963,
      "grad_norm": 5.697812080383301,
      "learning_rate": 4.494063285956601e-05,
      "loss": 2.0618,
      "step": 1730
    },
    {
      "epoch": 0.30531672223197054,
      "grad_norm": 8.643402099609375,
      "learning_rate": 4.4911387962800494e-05,
      "loss": 1.84,
      "step": 1740
    },
    {
      "epoch": 0.3070714160379014,
      "grad_norm": 7.761040210723877,
      "learning_rate": 4.488214306603498e-05,
      "loss": 2.1585,
      "step": 1750
    },
    {
      "epoch": 0.30882610984383224,
      "grad_norm": 6.329005718231201,
      "learning_rate": 4.4852898169269465e-05,
      "loss": 1.7189,
      "step": 1760
    },
    {
      "epoch": 0.3105808036497631,
      "grad_norm": 7.2328081130981445,
      "learning_rate": 4.482365327250395e-05,
      "loss": 1.9567,
      "step": 1770
    },
    {
      "epoch": 0.312335497455694,
      "grad_norm": 6.665572166442871,
      "learning_rate": 4.4794408375738436e-05,
      "loss": 2.0919,
      "step": 1780
    },
    {
      "epoch": 0.31409019126162485,
      "grad_norm": 7.6847052574157715,
      "learning_rate": 4.476516347897292e-05,
      "loss": 1.9615,
      "step": 1790
    },
    {
      "epoch": 0.3158448850675557,
      "grad_norm": 7.0101318359375,
      "learning_rate": 4.473591858220741e-05,
      "loss": 1.9673,
      "step": 1800
    },
    {
      "epoch": 0.31759957887348655,
      "grad_norm": 6.718395709991455,
      "learning_rate": 4.470667368544189e-05,
      "loss": 1.9167,
      "step": 1810
    },
    {
      "epoch": 0.31935427267941746,
      "grad_norm": 6.983694553375244,
      "learning_rate": 4.467742878867638e-05,
      "loss": 1.9936,
      "step": 1820
    },
    {
      "epoch": 0.3211089664853483,
      "grad_norm": 7.4565110206604,
      "learning_rate": 4.4648183891910864e-05,
      "loss": 2.022,
      "step": 1830
    },
    {
      "epoch": 0.32286366029127916,
      "grad_norm": 8.18420124053955,
      "learning_rate": 4.461893899514535e-05,
      "loss": 1.8773,
      "step": 1840
    },
    {
      "epoch": 0.32461835409721,
      "grad_norm": 6.562978744506836,
      "learning_rate": 4.4589694098379836e-05,
      "loss": 1.9661,
      "step": 1850
    },
    {
      "epoch": 0.3263730479031409,
      "grad_norm": 7.349366188049316,
      "learning_rate": 4.456044920161432e-05,
      "loss": 1.7517,
      "step": 1860
    },
    {
      "epoch": 0.3281277417090718,
      "grad_norm": 7.51874303817749,
      "learning_rate": 4.453120430484881e-05,
      "loss": 1.9718,
      "step": 1870
    },
    {
      "epoch": 0.3298824355150026,
      "grad_norm": 7.179809093475342,
      "learning_rate": 4.450195940808329e-05,
      "loss": 1.9834,
      "step": 1880
    },
    {
      "epoch": 0.3316371293209335,
      "grad_norm": 7.828216552734375,
      "learning_rate": 4.447271451131778e-05,
      "loss": 1.904,
      "step": 1890
    },
    {
      "epoch": 0.3333918231268644,
      "grad_norm": 7.898566246032715,
      "learning_rate": 4.4443469614552264e-05,
      "loss": 1.8591,
      "step": 1900
    },
    {
      "epoch": 0.33514651693279524,
      "grad_norm": 5.954827785491943,
      "learning_rate": 4.441422471778675e-05,
      "loss": 1.5903,
      "step": 1910
    },
    {
      "epoch": 0.3369012107387261,
      "grad_norm": 7.213908672332764,
      "learning_rate": 4.4384979821021235e-05,
      "loss": 1.927,
      "step": 1920
    },
    {
      "epoch": 0.33865590454465694,
      "grad_norm": 8.244013786315918,
      "learning_rate": 4.435573492425572e-05,
      "loss": 2.0291,
      "step": 1930
    },
    {
      "epoch": 0.34041059835058785,
      "grad_norm": 5.607027053833008,
      "learning_rate": 4.4326490027490206e-05,
      "loss": 1.9884,
      "step": 1940
    },
    {
      "epoch": 0.3421652921565187,
      "grad_norm": 7.869271278381348,
      "learning_rate": 4.429724513072469e-05,
      "loss": 1.9684,
      "step": 1950
    },
    {
      "epoch": 0.34391998596244955,
      "grad_norm": 8.629063606262207,
      "learning_rate": 4.426800023395918e-05,
      "loss": 2.1234,
      "step": 1960
    },
    {
      "epoch": 0.3456746797683804,
      "grad_norm": 7.087287425994873,
      "learning_rate": 4.423875533719366e-05,
      "loss": 1.7779,
      "step": 1970
    },
    {
      "epoch": 0.3474293735743113,
      "grad_norm": 6.776772975921631,
      "learning_rate": 4.420951044042815e-05,
      "loss": 2.0728,
      "step": 1980
    },
    {
      "epoch": 0.34918406738024216,
      "grad_norm": 8.234354972839355,
      "learning_rate": 4.4180265543662634e-05,
      "loss": 2.1309,
      "step": 1990
    },
    {
      "epoch": 0.350938761186173,
      "grad_norm": 6.896507740020752,
      "learning_rate": 4.415102064689712e-05,
      "loss": 1.8801,
      "step": 2000
    },
    {
      "epoch": 0.35269345499210386,
      "grad_norm": 7.816993713378906,
      "learning_rate": 4.4124700239808154e-05,
      "loss": 1.8046,
      "step": 2010
    },
    {
      "epoch": 0.35444814879803477,
      "grad_norm": 7.9490814208984375,
      "learning_rate": 4.409545534304264e-05,
      "loss": 2.0458,
      "step": 2020
    },
    {
      "epoch": 0.3562028426039656,
      "grad_norm": 7.278040885925293,
      "learning_rate": 4.4066210446277125e-05,
      "loss": 1.9674,
      "step": 2030
    },
    {
      "epoch": 0.35795753640989647,
      "grad_norm": 7.157069206237793,
      "learning_rate": 4.403696554951161e-05,
      "loss": 1.925,
      "step": 2040
    },
    {
      "epoch": 0.3597122302158273,
      "grad_norm": 7.503618240356445,
      "learning_rate": 4.40077206527461e-05,
      "loss": 1.6427,
      "step": 2050
    },
    {
      "epoch": 0.36146692402175823,
      "grad_norm": 7.658382892608643,
      "learning_rate": 4.397847575598059e-05,
      "loss": 2.2211,
      "step": 2060
    },
    {
      "epoch": 0.3632216178276891,
      "grad_norm": 7.7635908126831055,
      "learning_rate": 4.394923085921507e-05,
      "loss": 1.7182,
      "step": 2070
    },
    {
      "epoch": 0.36497631163361993,
      "grad_norm": 5.956923484802246,
      "learning_rate": 4.3919985962449554e-05,
      "loss": 1.8051,
      "step": 2080
    },
    {
      "epoch": 0.3667310054395508,
      "grad_norm": 6.401308536529541,
      "learning_rate": 4.389074106568404e-05,
      "loss": 1.7984,
      "step": 2090
    },
    {
      "epoch": 0.3684856992454817,
      "grad_norm": 6.460728645324707,
      "learning_rate": 4.3861496168918525e-05,
      "loss": 1.7614,
      "step": 2100
    },
    {
      "epoch": 0.37024039305141254,
      "grad_norm": 7.375851631164551,
      "learning_rate": 4.383225127215301e-05,
      "loss": 1.8875,
      "step": 2110
    },
    {
      "epoch": 0.3719950868573434,
      "grad_norm": 6.831369876861572,
      "learning_rate": 4.3803006375387496e-05,
      "loss": 1.8527,
      "step": 2120
    },
    {
      "epoch": 0.37374978066327424,
      "grad_norm": 7.141661167144775,
      "learning_rate": 4.377376147862198e-05,
      "loss": 2.0444,
      "step": 2130
    },
    {
      "epoch": 0.3755044744692051,
      "grad_norm": 9.600095748901367,
      "learning_rate": 4.374451658185647e-05,
      "loss": 2.0946,
      "step": 2140
    },
    {
      "epoch": 0.377259168275136,
      "grad_norm": 6.725779056549072,
      "learning_rate": 4.371527168509095e-05,
      "loss": 1.8432,
      "step": 2150
    },
    {
      "epoch": 0.37901386208106685,
      "grad_norm": 7.815572738647461,
      "learning_rate": 4.368602678832544e-05,
      "loss": 1.9664,
      "step": 2160
    },
    {
      "epoch": 0.3807685558869977,
      "grad_norm": 7.3833723068237305,
      "learning_rate": 4.3656781891559924e-05,
      "loss": 1.9658,
      "step": 2170
    },
    {
      "epoch": 0.38252324969292856,
      "grad_norm": 5.617852687835693,
      "learning_rate": 4.362753699479441e-05,
      "loss": 1.8811,
      "step": 2180
    },
    {
      "epoch": 0.38427794349885946,
      "grad_norm": 8.50537109375,
      "learning_rate": 4.3598292098028895e-05,
      "loss": 1.932,
      "step": 2190
    },
    {
      "epoch": 0.3860326373047903,
      "grad_norm": 7.7940592765808105,
      "learning_rate": 4.356904720126338e-05,
      "loss": 1.7548,
      "step": 2200
    },
    {
      "epoch": 0.38778733111072117,
      "grad_norm": 10.31884765625,
      "learning_rate": 4.353980230449787e-05,
      "loss": 2.0526,
      "step": 2210
    },
    {
      "epoch": 0.389542024916652,
      "grad_norm": 7.8374834060668945,
      "learning_rate": 4.351055740773235e-05,
      "loss": 2.0783,
      "step": 2220
    },
    {
      "epoch": 0.3912967187225829,
      "grad_norm": 7.21628999710083,
      "learning_rate": 4.348131251096684e-05,
      "loss": 1.8012,
      "step": 2230
    },
    {
      "epoch": 0.3930514125285138,
      "grad_norm": 7.663694381713867,
      "learning_rate": 4.3452067614201324e-05,
      "loss": 1.9891,
      "step": 2240
    },
    {
      "epoch": 0.39480610633444463,
      "grad_norm": 9.416566848754883,
      "learning_rate": 4.342282271743581e-05,
      "loss": 1.9593,
      "step": 2250
    },
    {
      "epoch": 0.3965608001403755,
      "grad_norm": 7.691897392272949,
      "learning_rate": 4.3393577820670295e-05,
      "loss": 2.043,
      "step": 2260
    },
    {
      "epoch": 0.3983154939463064,
      "grad_norm": 6.99429178237915,
      "learning_rate": 4.336433292390478e-05,
      "loss": 1.7397,
      "step": 2270
    },
    {
      "epoch": 0.40007018775223724,
      "grad_norm": 8.260778427124023,
      "learning_rate": 4.3335088027139266e-05,
      "loss": 1.9725,
      "step": 2280
    },
    {
      "epoch": 0.4018248815581681,
      "grad_norm": 7.433472633361816,
      "learning_rate": 4.330584313037375e-05,
      "loss": 2.1051,
      "step": 2290
    },
    {
      "epoch": 0.40357957536409894,
      "grad_norm": 6.2681169509887695,
      "learning_rate": 4.327659823360824e-05,
      "loss": 2.0272,
      "step": 2300
    },
    {
      "epoch": 0.40533426917002985,
      "grad_norm": 8.679910659790039,
      "learning_rate": 4.324735333684272e-05,
      "loss": 2.0141,
      "step": 2310
    },
    {
      "epoch": 0.4070889629759607,
      "grad_norm": 8.907126426696777,
      "learning_rate": 4.321810844007721e-05,
      "loss": 2.1523,
      "step": 2320
    },
    {
      "epoch": 0.40884365678189155,
      "grad_norm": 8.340994834899902,
      "learning_rate": 4.3188863543311694e-05,
      "loss": 1.8191,
      "step": 2330
    },
    {
      "epoch": 0.4105983505878224,
      "grad_norm": 5.876894474029541,
      "learning_rate": 4.315961864654618e-05,
      "loss": 1.8123,
      "step": 2340
    },
    {
      "epoch": 0.4123530443937533,
      "grad_norm": 8.025657653808594,
      "learning_rate": 4.3130373749780665e-05,
      "loss": 1.7485,
      "step": 2350
    },
    {
      "epoch": 0.41410773819968416,
      "grad_norm": 6.677215576171875,
      "learning_rate": 4.310112885301515e-05,
      "loss": 2.0247,
      "step": 2360
    },
    {
      "epoch": 0.415862432005615,
      "grad_norm": 7.651278018951416,
      "learning_rate": 4.3071883956249637e-05,
      "loss": 1.6923,
      "step": 2370
    },
    {
      "epoch": 0.41761712581154586,
      "grad_norm": 7.661524772644043,
      "learning_rate": 4.304263905948412e-05,
      "loss": 1.9186,
      "step": 2380
    },
    {
      "epoch": 0.41937181961747677,
      "grad_norm": 8.968270301818848,
      "learning_rate": 4.301339416271861e-05,
      "loss": 2.0227,
      "step": 2390
    },
    {
      "epoch": 0.4211265134234076,
      "grad_norm": 7.879574775695801,
      "learning_rate": 4.2984149265953093e-05,
      "loss": 2.109,
      "step": 2400
    },
    {
      "epoch": 0.4228812072293385,
      "grad_norm": 6.580073833465576,
      "learning_rate": 4.295490436918758e-05,
      "loss": 1.8808,
      "step": 2410
    },
    {
      "epoch": 0.4246359010352693,
      "grad_norm": 7.40146541595459,
      "learning_rate": 4.2925659472422065e-05,
      "loss": 2.128,
      "step": 2420
    },
    {
      "epoch": 0.42639059484120023,
      "grad_norm": 8.630766868591309,
      "learning_rate": 4.289641457565655e-05,
      "loss": 1.8625,
      "step": 2430
    },
    {
      "epoch": 0.4281452886471311,
      "grad_norm": 6.897637367248535,
      "learning_rate": 4.2867169678891036e-05,
      "loss": 1.9946,
      "step": 2440
    },
    {
      "epoch": 0.42989998245306194,
      "grad_norm": 7.45115852355957,
      "learning_rate": 4.283792478212552e-05,
      "loss": 1.9595,
      "step": 2450
    },
    {
      "epoch": 0.4316546762589928,
      "grad_norm": 8.170005798339844,
      "learning_rate": 4.280867988536001e-05,
      "loss": 1.9411,
      "step": 2460
    },
    {
      "epoch": 0.4334093700649237,
      "grad_norm": 7.993865013122559,
      "learning_rate": 4.277943498859449e-05,
      "loss": 1.8357,
      "step": 2470
    },
    {
      "epoch": 0.43516406387085454,
      "grad_norm": 7.385665416717529,
      "learning_rate": 4.275019009182898e-05,
      "loss": 1.8209,
      "step": 2480
    },
    {
      "epoch": 0.4369187576767854,
      "grad_norm": 6.0670084953308105,
      "learning_rate": 4.2720945195063464e-05,
      "loss": 2.0379,
      "step": 2490
    },
    {
      "epoch": 0.43867345148271625,
      "grad_norm": 9.507941246032715,
      "learning_rate": 4.269170029829795e-05,
      "loss": 1.6881,
      "step": 2500
    },
    {
      "epoch": 0.44042814528864715,
      "grad_norm": 12.51726245880127,
      "learning_rate": 4.2662455401532435e-05,
      "loss": 1.934,
      "step": 2510
    },
    {
      "epoch": 0.442182839094578,
      "grad_norm": 9.431156158447266,
      "learning_rate": 4.263321050476692e-05,
      "loss": 1.9674,
      "step": 2520
    },
    {
      "epoch": 0.44393753290050886,
      "grad_norm": 7.5674614906311035,
      "learning_rate": 4.2603965608001407e-05,
      "loss": 1.7882,
      "step": 2530
    },
    {
      "epoch": 0.4456922267064397,
      "grad_norm": 8.671258926391602,
      "learning_rate": 4.257472071123589e-05,
      "loss": 1.7111,
      "step": 2540
    },
    {
      "epoch": 0.4474469205123706,
      "grad_norm": 8.305204391479492,
      "learning_rate": 4.254547581447038e-05,
      "loss": 1.5682,
      "step": 2550
    },
    {
      "epoch": 0.44920161431830147,
      "grad_norm": 8.055242538452148,
      "learning_rate": 4.2516230917704863e-05,
      "loss": 2.0473,
      "step": 2560
    },
    {
      "epoch": 0.4509563081242323,
      "grad_norm": 6.3704938888549805,
      "learning_rate": 4.248698602093934e-05,
      "loss": 1.7202,
      "step": 2570
    },
    {
      "epoch": 0.45271100193016317,
      "grad_norm": 6.965793132781982,
      "learning_rate": 4.2457741124173835e-05,
      "loss": 1.6939,
      "step": 2580
    },
    {
      "epoch": 0.4544656957360941,
      "grad_norm": 7.5125555992126465,
      "learning_rate": 4.242849622740832e-05,
      "loss": 1.9352,
      "step": 2590
    },
    {
      "epoch": 0.45622038954202493,
      "grad_norm": 8.448831558227539,
      "learning_rate": 4.2399251330642806e-05,
      "loss": 1.9989,
      "step": 2600
    },
    {
      "epoch": 0.4579750833479558,
      "grad_norm": 9.093822479248047,
      "learning_rate": 4.237000643387729e-05,
      "loss": 1.8417,
      "step": 2610
    },
    {
      "epoch": 0.45972977715388663,
      "grad_norm": 7.824317932128906,
      "learning_rate": 4.234076153711178e-05,
      "loss": 2.0909,
      "step": 2620
    },
    {
      "epoch": 0.46148447095981754,
      "grad_norm": 7.035995960235596,
      "learning_rate": 4.231151664034626e-05,
      "loss": 1.8818,
      "step": 2630
    },
    {
      "epoch": 0.4632391647657484,
      "grad_norm": 9.596044540405273,
      "learning_rate": 4.228227174358075e-05,
      "loss": 1.8148,
      "step": 2640
    },
    {
      "epoch": 0.46499385857167924,
      "grad_norm": 8.415526390075684,
      "learning_rate": 4.225302684681523e-05,
      "loss": 2.0579,
      "step": 2650
    },
    {
      "epoch": 0.4667485523776101,
      "grad_norm": 8.81614875793457,
      "learning_rate": 4.222378195004972e-05,
      "loss": 1.7117,
      "step": 2660
    },
    {
      "epoch": 0.46850324618354094,
      "grad_norm": 6.854835033416748,
      "learning_rate": 4.2194537053284205e-05,
      "loss": 1.9477,
      "step": 2670
    },
    {
      "epoch": 0.47025793998947185,
      "grad_norm": 9.381402969360352,
      "learning_rate": 4.216529215651869e-05,
      "loss": 2.0881,
      "step": 2680
    },
    {
      "epoch": 0.4720126337954027,
      "grad_norm": 7.2936577796936035,
      "learning_rate": 4.2136047259753176e-05,
      "loss": 2.048,
      "step": 2690
    },
    {
      "epoch": 0.47376732760133355,
      "grad_norm": 9.604708671569824,
      "learning_rate": 4.210680236298766e-05,
      "loss": 1.9555,
      "step": 2700
    },
    {
      "epoch": 0.4755220214072644,
      "grad_norm": 7.324658393859863,
      "learning_rate": 4.207755746622215e-05,
      "loss": 1.95,
      "step": 2710
    },
    {
      "epoch": 0.4772767152131953,
      "grad_norm": 7.580718994140625,
      "learning_rate": 4.204831256945663e-05,
      "loss": 1.7914,
      "step": 2720
    },
    {
      "epoch": 0.47903140901912616,
      "grad_norm": 10.072842597961426,
      "learning_rate": 4.201906767269111e-05,
      "loss": 2.1006,
      "step": 2730
    },
    {
      "epoch": 0.480786102825057,
      "grad_norm": 7.350348472595215,
      "learning_rate": 4.1989822775925605e-05,
      "loss": 1.7745,
      "step": 2740
    },
    {
      "epoch": 0.48254079663098787,
      "grad_norm": 7.047656536102295,
      "learning_rate": 4.196057787916009e-05,
      "loss": 2.0972,
      "step": 2750
    },
    {
      "epoch": 0.4842954904369188,
      "grad_norm": 8.358116149902344,
      "learning_rate": 4.193133298239457e-05,
      "loss": 1.9845,
      "step": 2760
    },
    {
      "epoch": 0.4860501842428496,
      "grad_norm": 8.961874008178711,
      "learning_rate": 4.190208808562906e-05,
      "loss": 2.0324,
      "step": 2770
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 6.203963279724121,
      "learning_rate": 4.187284318886355e-05,
      "loss": 1.9364,
      "step": 2780
    },
    {
      "epoch": 0.48955957185471133,
      "grad_norm": 6.398298740386963,
      "learning_rate": 4.184359829209803e-05,
      "loss": 2.0537,
      "step": 2790
    },
    {
      "epoch": 0.49131426566064224,
      "grad_norm": 8.36142349243164,
      "learning_rate": 4.181435339533252e-05,
      "loss": 1.6445,
      "step": 2800
    },
    {
      "epoch": 0.4930689594665731,
      "grad_norm": 9.695984840393066,
      "learning_rate": 4.1785108498567e-05,
      "loss": 1.9671,
      "step": 2810
    },
    {
      "epoch": 0.49482365327250394,
      "grad_norm": 6.95733642578125,
      "learning_rate": 4.175586360180149e-05,
      "loss": 1.8939,
      "step": 2820
    },
    {
      "epoch": 0.4965783470784348,
      "grad_norm": 6.353109359741211,
      "learning_rate": 4.1726618705035975e-05,
      "loss": 1.8373,
      "step": 2830
    },
    {
      "epoch": 0.4983330408843657,
      "grad_norm": 8.447498321533203,
      "learning_rate": 4.1697373808270454e-05,
      "loss": 1.847,
      "step": 2840
    },
    {
      "epoch": 0.5000877346902965,
      "grad_norm": 8.65880298614502,
      "learning_rate": 4.1668128911504946e-05,
      "loss": 1.8106,
      "step": 2850
    },
    {
      "epoch": 0.5018424284962274,
      "grad_norm": 9.344013214111328,
      "learning_rate": 4.163888401473943e-05,
      "loss": 1.9911,
      "step": 2860
    },
    {
      "epoch": 0.5035971223021583,
      "grad_norm": 8.611393928527832,
      "learning_rate": 4.160963911797391e-05,
      "loss": 1.7966,
      "step": 2870
    },
    {
      "epoch": 0.5053518161080891,
      "grad_norm": 7.5934367179870605,
      "learning_rate": 4.15803942212084e-05,
      "loss": 1.8569,
      "step": 2880
    },
    {
      "epoch": 0.50710650991402,
      "grad_norm": 6.393404483795166,
      "learning_rate": 4.155114932444288e-05,
      "loss": 1.8366,
      "step": 2890
    },
    {
      "epoch": 0.5088612037199509,
      "grad_norm": 8.2294340133667,
      "learning_rate": 4.1521904427677375e-05,
      "loss": 1.789,
      "step": 2900
    },
    {
      "epoch": 0.5106158975258818,
      "grad_norm": 8.022841453552246,
      "learning_rate": 4.149265953091186e-05,
      "loss": 1.7249,
      "step": 2910
    },
    {
      "epoch": 0.5123705913318126,
      "grad_norm": 8.695769309997559,
      "learning_rate": 4.146341463414634e-05,
      "loss": 1.9487,
      "step": 2920
    },
    {
      "epoch": 0.5141252851377435,
      "grad_norm": 8.999140739440918,
      "learning_rate": 4.143416973738083e-05,
      "loss": 2.1093,
      "step": 2930
    },
    {
      "epoch": 0.5158799789436743,
      "grad_norm": 8.917503356933594,
      "learning_rate": 4.140492484061532e-05,
      "loss": 1.7027,
      "step": 2940
    },
    {
      "epoch": 0.5176346727496052,
      "grad_norm": 9.770017623901367,
      "learning_rate": 4.1375679943849796e-05,
      "loss": 2.1378,
      "step": 2950
    },
    {
      "epoch": 0.519389366555536,
      "grad_norm": 8.161751747131348,
      "learning_rate": 4.134643504708429e-05,
      "loss": 1.667,
      "step": 2960
    },
    {
      "epoch": 0.5211440603614669,
      "grad_norm": 10.167064666748047,
      "learning_rate": 4.131719015031877e-05,
      "loss": 1.7161,
      "step": 2970
    },
    {
      "epoch": 0.5228987541673978,
      "grad_norm": 7.685187339782715,
      "learning_rate": 4.128794525355326e-05,
      "loss": 1.9429,
      "step": 2980
    },
    {
      "epoch": 0.5246534479733287,
      "grad_norm": 7.98021936416626,
      "learning_rate": 4.1258700356787745e-05,
      "loss": 1.9612,
      "step": 2990
    },
    {
      "epoch": 0.5264081417792595,
      "grad_norm": 8.374429702758789,
      "learning_rate": 4.1229455460022224e-05,
      "loss": 1.9874,
      "step": 3000
    },
    {
      "epoch": 0.5281628355851904,
      "grad_norm": 9.77852725982666,
      "learning_rate": 4.1200210563256716e-05,
      "loss": 2.0918,
      "step": 3010
    },
    {
      "epoch": 0.5299175293911212,
      "grad_norm": 7.288815975189209,
      "learning_rate": 4.11709656664912e-05,
      "loss": 1.543,
      "step": 3020
    },
    {
      "epoch": 0.5316722231970521,
      "grad_norm": 7.029747486114502,
      "learning_rate": 4.114172076972568e-05,
      "loss": 2.0692,
      "step": 3030
    },
    {
      "epoch": 0.533426917002983,
      "grad_norm": 8.318429946899414,
      "learning_rate": 4.111247587296017e-05,
      "loss": 1.9402,
      "step": 3040
    },
    {
      "epoch": 0.5351816108089138,
      "grad_norm": 8.640129089355469,
      "learning_rate": 4.108323097619466e-05,
      "loss": 1.8658,
      "step": 3050
    },
    {
      "epoch": 0.5369363046148448,
      "grad_norm": 7.294399261474609,
      "learning_rate": 4.105398607942914e-05,
      "loss": 1.814,
      "step": 3060
    },
    {
      "epoch": 0.5386909984207756,
      "grad_norm": 5.995456695556641,
      "learning_rate": 4.102474118266363e-05,
      "loss": 2.0364,
      "step": 3070
    },
    {
      "epoch": 0.5404456922267065,
      "grad_norm": 7.011064052581787,
      "learning_rate": 4.099549628589811e-05,
      "loss": 1.7983,
      "step": 3080
    },
    {
      "epoch": 0.5422003860326373,
      "grad_norm": 7.672762393951416,
      "learning_rate": 4.09662513891326e-05,
      "loss": 1.8702,
      "step": 3090
    },
    {
      "epoch": 0.5439550798385682,
      "grad_norm": 9.283306121826172,
      "learning_rate": 4.093700649236709e-05,
      "loss": 1.934,
      "step": 3100
    },
    {
      "epoch": 0.545709773644499,
      "grad_norm": 9.957877159118652,
      "learning_rate": 4.0907761595601566e-05,
      "loss": 1.7963,
      "step": 3110
    },
    {
      "epoch": 0.5474644674504299,
      "grad_norm": 7.3725152015686035,
      "learning_rate": 4.087851669883606e-05,
      "loss": 1.9198,
      "step": 3120
    },
    {
      "epoch": 0.5492191612563607,
      "grad_norm": 4.9166717529296875,
      "learning_rate": 4.0849271802070544e-05,
      "loss": 1.6745,
      "step": 3130
    },
    {
      "epoch": 0.5509738550622917,
      "grad_norm": 10.604667663574219,
      "learning_rate": 4.082002690530502e-05,
      "loss": 1.8508,
      "step": 3140
    },
    {
      "epoch": 0.5527285488682225,
      "grad_norm": 7.6501054763793945,
      "learning_rate": 4.0790782008539515e-05,
      "loss": 1.7287,
      "step": 3150
    },
    {
      "epoch": 0.5544832426741534,
      "grad_norm": 8.134082794189453,
      "learning_rate": 4.0761537111773994e-05,
      "loss": 1.6959,
      "step": 3160
    },
    {
      "epoch": 0.5562379364800842,
      "grad_norm": 7.513731956481934,
      "learning_rate": 4.0732292215008486e-05,
      "loss": 1.7709,
      "step": 3170
    },
    {
      "epoch": 0.5579926302860151,
      "grad_norm": 7.613053321838379,
      "learning_rate": 4.070304731824297e-05,
      "loss": 1.8676,
      "step": 3180
    },
    {
      "epoch": 0.5597473240919459,
      "grad_norm": 6.304816246032715,
      "learning_rate": 4.067380242147745e-05,
      "loss": 1.9311,
      "step": 3190
    },
    {
      "epoch": 0.5615020178978768,
      "grad_norm": 7.694853782653809,
      "learning_rate": 4.064455752471194e-05,
      "loss": 2.0581,
      "step": 3200
    },
    {
      "epoch": 0.5632567117038076,
      "grad_norm": 7.379085063934326,
      "learning_rate": 4.061531262794643e-05,
      "loss": 1.7356,
      "step": 3210
    },
    {
      "epoch": 0.5650114055097385,
      "grad_norm": 7.4439802169799805,
      "learning_rate": 4.058606773118091e-05,
      "loss": 1.8662,
      "step": 3220
    },
    {
      "epoch": 0.5667660993156695,
      "grad_norm": 9.305098533630371,
      "learning_rate": 4.05568228344154e-05,
      "loss": 2.0034,
      "step": 3230
    },
    {
      "epoch": 0.5685207931216003,
      "grad_norm": 9.977211952209473,
      "learning_rate": 4.052757793764988e-05,
      "loss": 1.8881,
      "step": 3240
    },
    {
      "epoch": 0.5702754869275312,
      "grad_norm": 9.549229621887207,
      "learning_rate": 4.0498333040884364e-05,
      "loss": 2.0403,
      "step": 3250
    },
    {
      "epoch": 0.572030180733462,
      "grad_norm": 7.973468780517578,
      "learning_rate": 4.046908814411886e-05,
      "loss": 1.8873,
      "step": 3260
    },
    {
      "epoch": 0.5737848745393929,
      "grad_norm": 7.671144485473633,
      "learning_rate": 4.0439843247353336e-05,
      "loss": 1.9887,
      "step": 3270
    },
    {
      "epoch": 0.5755395683453237,
      "grad_norm": 7.791079044342041,
      "learning_rate": 4.041059835058783e-05,
      "loss": 2.1075,
      "step": 3280
    },
    {
      "epoch": 0.5772942621512546,
      "grad_norm": 8.478113174438477,
      "learning_rate": 4.0381353453822314e-05,
      "loss": 1.9062,
      "step": 3290
    },
    {
      "epoch": 0.5790489559571854,
      "grad_norm": 6.03745174407959,
      "learning_rate": 4.035210855705679e-05,
      "loss": 1.9328,
      "step": 3300
    },
    {
      "epoch": 0.5808036497631164,
      "grad_norm": 8.968368530273438,
      "learning_rate": 4.0322863660291285e-05,
      "loss": 2.0725,
      "step": 3310
    },
    {
      "epoch": 0.5825583435690472,
      "grad_norm": 8.674917221069336,
      "learning_rate": 4.0293618763525764e-05,
      "loss": 1.8987,
      "step": 3320
    },
    {
      "epoch": 0.5843130373749781,
      "grad_norm": 6.9590744972229,
      "learning_rate": 4.026437386676025e-05,
      "loss": 1.8397,
      "step": 3330
    },
    {
      "epoch": 0.5860677311809089,
      "grad_norm": 6.951268196105957,
      "learning_rate": 4.023512896999474e-05,
      "loss": 2.0462,
      "step": 3340
    },
    {
      "epoch": 0.5878224249868398,
      "grad_norm": 7.864795684814453,
      "learning_rate": 4.020588407322922e-05,
      "loss": 1.8216,
      "step": 3350
    },
    {
      "epoch": 0.5895771187927706,
      "grad_norm": 8.960466384887695,
      "learning_rate": 4.0176639176463706e-05,
      "loss": 1.8753,
      "step": 3360
    },
    {
      "epoch": 0.5913318125987015,
      "grad_norm": 8.708386421203613,
      "learning_rate": 4.01473942796982e-05,
      "loss": 2.0646,
      "step": 3370
    },
    {
      "epoch": 0.5930865064046323,
      "grad_norm": 7.187032222747803,
      "learning_rate": 4.011814938293268e-05,
      "loss": 1.8904,
      "step": 3380
    },
    {
      "epoch": 0.5948412002105633,
      "grad_norm": 9.41211986541748,
      "learning_rate": 4.008890448616717e-05,
      "loss": 2.0183,
      "step": 3390
    },
    {
      "epoch": 0.5965958940164942,
      "grad_norm": 7.859705924987793,
      "learning_rate": 4.005965958940165e-05,
      "loss": 2.0117,
      "step": 3400
    },
    {
      "epoch": 0.598350587822425,
      "grad_norm": 5.965307235717773,
      "learning_rate": 4.0030414692636134e-05,
      "loss": 1.8851,
      "step": 3410
    },
    {
      "epoch": 0.6001052816283559,
      "grad_norm": 8.57575511932373,
      "learning_rate": 4.000116979587063e-05,
      "loss": 1.8875,
      "step": 3420
    },
    {
      "epoch": 0.6018599754342867,
      "grad_norm": 7.8649983406066895,
      "learning_rate": 3.9971924899105106e-05,
      "loss": 1.88,
      "step": 3430
    },
    {
      "epoch": 0.6036146692402176,
      "grad_norm": 8.146160125732422,
      "learning_rate": 3.994268000233959e-05,
      "loss": 1.8914,
      "step": 3440
    },
    {
      "epoch": 0.6053693630461484,
      "grad_norm": 6.634668350219727,
      "learning_rate": 3.9913435105574084e-05,
      "loss": 1.8737,
      "step": 3450
    },
    {
      "epoch": 0.6071240568520793,
      "grad_norm": 8.187376976013184,
      "learning_rate": 3.988419020880856e-05,
      "loss": 1.8106,
      "step": 3460
    },
    {
      "epoch": 0.6088787506580102,
      "grad_norm": 8.274352073669434,
      "learning_rate": 3.9854945312043055e-05,
      "loss": 1.6886,
      "step": 3470
    },
    {
      "epoch": 0.6106334444639411,
      "grad_norm": 6.624914646148682,
      "learning_rate": 3.9825700415277534e-05,
      "loss": 1.51,
      "step": 3480
    },
    {
      "epoch": 0.6123881382698719,
      "grad_norm": 9.228864669799805,
      "learning_rate": 3.979645551851202e-05,
      "loss": 1.7883,
      "step": 3490
    },
    {
      "epoch": 0.6141428320758028,
      "grad_norm": 9.044224739074707,
      "learning_rate": 3.976721062174651e-05,
      "loss": 1.98,
      "step": 3500
    },
    {
      "epoch": 0.6158975258817336,
      "grad_norm": 9.82866382598877,
      "learning_rate": 3.973796572498099e-05,
      "loss": 1.942,
      "step": 3510
    },
    {
      "epoch": 0.6176522196876645,
      "grad_norm": 7.3290534019470215,
      "learning_rate": 3.9708720828215476e-05,
      "loss": 1.9175,
      "step": 3520
    },
    {
      "epoch": 0.6194069134935953,
      "grad_norm": 6.991813659667969,
      "learning_rate": 3.967947593144997e-05,
      "loss": 1.9013,
      "step": 3530
    },
    {
      "epoch": 0.6211616072995262,
      "grad_norm": 8.161505699157715,
      "learning_rate": 3.965023103468445e-05,
      "loss": 1.6013,
      "step": 3540
    },
    {
      "epoch": 0.6229163011054571,
      "grad_norm": 8.86521053314209,
      "learning_rate": 3.962098613791893e-05,
      "loss": 2.0583,
      "step": 3550
    },
    {
      "epoch": 0.624670994911388,
      "grad_norm": 8.015758514404297,
      "learning_rate": 3.959174124115342e-05,
      "loss": 2.0787,
      "step": 3560
    },
    {
      "epoch": 0.6264256887173189,
      "grad_norm": 9.343063354492188,
      "learning_rate": 3.9562496344387904e-05,
      "loss": 1.6295,
      "step": 3570
    },
    {
      "epoch": 0.6281803825232497,
      "grad_norm": 9.556726455688477,
      "learning_rate": 3.95332514476224e-05,
      "loss": 1.6739,
      "step": 3580
    },
    {
      "epoch": 0.6299350763291806,
      "grad_norm": 10.527349472045898,
      "learning_rate": 3.9504006550856876e-05,
      "loss": 1.8286,
      "step": 3590
    },
    {
      "epoch": 0.6316897701351114,
      "grad_norm": 7.577964782714844,
      "learning_rate": 3.947476165409136e-05,
      "loss": 1.6713,
      "step": 3600
    },
    {
      "epoch": 0.6334444639410423,
      "grad_norm": 10.46111011505127,
      "learning_rate": 3.9445516757325854e-05,
      "loss": 1.8095,
      "step": 3610
    },
    {
      "epoch": 0.6351991577469731,
      "grad_norm": 8.275674819946289,
      "learning_rate": 3.941627186056033e-05,
      "loss": 1.8846,
      "step": 3620
    },
    {
      "epoch": 0.6369538515529041,
      "grad_norm": 6.9671711921691895,
      "learning_rate": 3.938702696379482e-05,
      "loss": 1.7177,
      "step": 3630
    },
    {
      "epoch": 0.6387085453588349,
      "grad_norm": 8.347223281860352,
      "learning_rate": 3.9357782067029304e-05,
      "loss": 1.8157,
      "step": 3640
    },
    {
      "epoch": 0.6404632391647658,
      "grad_norm": 8.260639190673828,
      "learning_rate": 3.932853717026379e-05,
      "loss": 1.9688,
      "step": 3650
    },
    {
      "epoch": 0.6422179329706966,
      "grad_norm": 10.067076683044434,
      "learning_rate": 3.9299292273498275e-05,
      "loss": 2.0209,
      "step": 3660
    },
    {
      "epoch": 0.6439726267766275,
      "grad_norm": 9.883514404296875,
      "learning_rate": 3.927004737673276e-05,
      "loss": 2.0497,
      "step": 3670
    },
    {
      "epoch": 0.6457273205825583,
      "grad_norm": 10.498370170593262,
      "learning_rate": 3.9240802479967246e-05,
      "loss": 1.8299,
      "step": 3680
    },
    {
      "epoch": 0.6474820143884892,
      "grad_norm": 9.28521728515625,
      "learning_rate": 3.921155758320174e-05,
      "loss": 1.7029,
      "step": 3690
    },
    {
      "epoch": 0.64923670819442,
      "grad_norm": 7.368404865264893,
      "learning_rate": 3.918231268643622e-05,
      "loss": 1.651,
      "step": 3700
    },
    {
      "epoch": 0.650991402000351,
      "grad_norm": 6.890046119689941,
      "learning_rate": 3.91530677896707e-05,
      "loss": 2.0097,
      "step": 3710
    },
    {
      "epoch": 0.6527460958062818,
      "grad_norm": 10.364001274108887,
      "learning_rate": 3.912382289290519e-05,
      "loss": 1.6493,
      "step": 3720
    },
    {
      "epoch": 0.6545007896122127,
      "grad_norm": 8.306217193603516,
      "learning_rate": 3.9094577996139674e-05,
      "loss": 1.9154,
      "step": 3730
    },
    {
      "epoch": 0.6562554834181435,
      "grad_norm": 8.513472557067871,
      "learning_rate": 3.906533309937416e-05,
      "loss": 2.0009,
      "step": 3740
    },
    {
      "epoch": 0.6580101772240744,
      "grad_norm": 7.27830696105957,
      "learning_rate": 3.9036088202608646e-05,
      "loss": 1.794,
      "step": 3750
    },
    {
      "epoch": 0.6597648710300053,
      "grad_norm": 9.468120574951172,
      "learning_rate": 3.900684330584313e-05,
      "loss": 1.8851,
      "step": 3760
    },
    {
      "epoch": 0.6615195648359361,
      "grad_norm": 9.89455795288086,
      "learning_rate": 3.8977598409077624e-05,
      "loss": 1.6726,
      "step": 3770
    },
    {
      "epoch": 0.663274258641867,
      "grad_norm": 10.56911563873291,
      "learning_rate": 3.89483535123121e-05,
      "loss": 1.7707,
      "step": 3780
    },
    {
      "epoch": 0.6650289524477978,
      "grad_norm": 6.911468505859375,
      "learning_rate": 3.891910861554659e-05,
      "loss": 1.6796,
      "step": 3790
    },
    {
      "epoch": 0.6667836462537288,
      "grad_norm": 8.758173942565918,
      "learning_rate": 3.8889863718781074e-05,
      "loss": 1.8495,
      "step": 3800
    },
    {
      "epoch": 0.6685383400596596,
      "grad_norm": 9.560717582702637,
      "learning_rate": 3.886061882201556e-05,
      "loss": 1.8314,
      "step": 3810
    },
    {
      "epoch": 0.6702930338655905,
      "grad_norm": 6.690609931945801,
      "learning_rate": 3.8831373925250045e-05,
      "loss": 1.8555,
      "step": 3820
    },
    {
      "epoch": 0.6720477276715213,
      "grad_norm": 9.056072235107422,
      "learning_rate": 3.880212902848453e-05,
      "loss": 2.0032,
      "step": 3830
    },
    {
      "epoch": 0.6738024214774522,
      "grad_norm": 7.7766242027282715,
      "learning_rate": 3.8772884131719016e-05,
      "loss": 1.6885,
      "step": 3840
    },
    {
      "epoch": 0.675557115283383,
      "grad_norm": 10.46932315826416,
      "learning_rate": 3.87436392349535e-05,
      "loss": 2.103,
      "step": 3850
    },
    {
      "epoch": 0.6773118090893139,
      "grad_norm": 8.35351848602295,
      "learning_rate": 3.871439433818799e-05,
      "loss": 1.828,
      "step": 3860
    },
    {
      "epoch": 0.6790665028952447,
      "grad_norm": 9.100356101989746,
      "learning_rate": 3.868514944142247e-05,
      "loss": 1.9225,
      "step": 3870
    },
    {
      "epoch": 0.6808211967011757,
      "grad_norm": 9.93741512298584,
      "learning_rate": 3.865590454465696e-05,
      "loss": 1.8122,
      "step": 3880
    },
    {
      "epoch": 0.6825758905071065,
      "grad_norm": 8.934633255004883,
      "learning_rate": 3.8626659647891444e-05,
      "loss": 2.0905,
      "step": 3890
    },
    {
      "epoch": 0.6843305843130374,
      "grad_norm": 7.076767444610596,
      "learning_rate": 3.859741475112593e-05,
      "loss": 1.7494,
      "step": 3900
    },
    {
      "epoch": 0.6860852781189682,
      "grad_norm": 10.294880867004395,
      "learning_rate": 3.8568169854360415e-05,
      "loss": 1.7674,
      "step": 3910
    },
    {
      "epoch": 0.6878399719248991,
      "grad_norm": 9.538376808166504,
      "learning_rate": 3.85389249575949e-05,
      "loss": 2.0038,
      "step": 3920
    },
    {
      "epoch": 0.68959466573083,
      "grad_norm": 7.487983226776123,
      "learning_rate": 3.850968006082939e-05,
      "loss": 1.6568,
      "step": 3930
    },
    {
      "epoch": 0.6913493595367608,
      "grad_norm": 9.325736045837402,
      "learning_rate": 3.848335965374043e-05,
      "loss": 1.9465,
      "step": 3940
    },
    {
      "epoch": 0.6931040533426917,
      "grad_norm": 11.616250038146973,
      "learning_rate": 3.845411475697491e-05,
      "loss": 1.6282,
      "step": 3950
    },
    {
      "epoch": 0.6948587471486226,
      "grad_norm": 6.94856071472168,
      "learning_rate": 3.842486986020939e-05,
      "loss": 1.9042,
      "step": 3960
    },
    {
      "epoch": 0.6966134409545535,
      "grad_norm": 9.21236801147461,
      "learning_rate": 3.8395624963443885e-05,
      "loss": 1.8905,
      "step": 3970
    },
    {
      "epoch": 0.6983681347604843,
      "grad_norm": 8.483938217163086,
      "learning_rate": 3.8366380066678364e-05,
      "loss": 1.9647,
      "step": 3980
    },
    {
      "epoch": 0.7001228285664152,
      "grad_norm": 7.835909366607666,
      "learning_rate": 3.833713516991285e-05,
      "loss": 1.9099,
      "step": 3990
    },
    {
      "epoch": 0.701877522372346,
      "grad_norm": 8.707355499267578,
      "learning_rate": 3.830789027314734e-05,
      "loss": 2.0125,
      "step": 4000
    },
    {
      "epoch": 0.7036322161782769,
      "grad_norm": 10.020755767822266,
      "learning_rate": 3.827864537638182e-05,
      "loss": 1.9969,
      "step": 4010
    },
    {
      "epoch": 0.7053869099842077,
      "grad_norm": 7.607214450836182,
      "learning_rate": 3.824940047961631e-05,
      "loss": 1.8918,
      "step": 4020
    },
    {
      "epoch": 0.7071416037901386,
      "grad_norm": 7.22476863861084,
      "learning_rate": 3.822015558285079e-05,
      "loss": 1.6463,
      "step": 4030
    },
    {
      "epoch": 0.7088962975960695,
      "grad_norm": 8.870501518249512,
      "learning_rate": 3.819091068608528e-05,
      "loss": 2.0248,
      "step": 4040
    },
    {
      "epoch": 0.7106509914020004,
      "grad_norm": 7.99105167388916,
      "learning_rate": 3.816166578931977e-05,
      "loss": 1.6417,
      "step": 4050
    },
    {
      "epoch": 0.7124056852079312,
      "grad_norm": 9.101513862609863,
      "learning_rate": 3.813242089255425e-05,
      "loss": 1.8246,
      "step": 4060
    },
    {
      "epoch": 0.7141603790138621,
      "grad_norm": 7.096899032592773,
      "learning_rate": 3.8103175995788734e-05,
      "loss": 1.9882,
      "step": 4070
    },
    {
      "epoch": 0.7159150728197929,
      "grad_norm": 7.3488640785217285,
      "learning_rate": 3.8073931099023227e-05,
      "loss": 1.9137,
      "step": 4080
    },
    {
      "epoch": 0.7176697666257238,
      "grad_norm": 8.757529258728027,
      "learning_rate": 3.8044686202257705e-05,
      "loss": 1.9266,
      "step": 4090
    },
    {
      "epoch": 0.7194244604316546,
      "grad_norm": 6.867065906524658,
      "learning_rate": 3.801544130549219e-05,
      "loss": 1.8448,
      "step": 4100
    },
    {
      "epoch": 0.7211791542375855,
      "grad_norm": 9.974079132080078,
      "learning_rate": 3.798619640872668e-05,
      "loss": 1.8247,
      "step": 4110
    },
    {
      "epoch": 0.7229338480435165,
      "grad_norm": 9.911099433898926,
      "learning_rate": 3.795695151196116e-05,
      "loss": 1.8535,
      "step": 4120
    },
    {
      "epoch": 0.7246885418494473,
      "grad_norm": 9.355168342590332,
      "learning_rate": 3.7927706615195655e-05,
      "loss": 1.6845,
      "step": 4130
    },
    {
      "epoch": 0.7264432356553782,
      "grad_norm": 8.126165390014648,
      "learning_rate": 3.7898461718430134e-05,
      "loss": 1.7654,
      "step": 4140
    },
    {
      "epoch": 0.728197929461309,
      "grad_norm": 8.371764183044434,
      "learning_rate": 3.786921682166462e-05,
      "loss": 1.7867,
      "step": 4150
    },
    {
      "epoch": 0.7299526232672399,
      "grad_norm": 7.062753677368164,
      "learning_rate": 3.783997192489911e-05,
      "loss": 1.892,
      "step": 4160
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 6.923789978027344,
      "learning_rate": 3.781072702813359e-05,
      "loss": 1.8109,
      "step": 4170
    },
    {
      "epoch": 0.7334620108791016,
      "grad_norm": 6.223722457885742,
      "learning_rate": 3.7781482131368076e-05,
      "loss": 1.6455,
      "step": 4180
    },
    {
      "epoch": 0.7352167046850324,
      "grad_norm": 5.830484867095947,
      "learning_rate": 3.775223723460256e-05,
      "loss": 1.686,
      "step": 4190
    },
    {
      "epoch": 0.7369713984909634,
      "grad_norm": 10.013045310974121,
      "learning_rate": 3.772299233783705e-05,
      "loss": 1.7021,
      "step": 4200
    },
    {
      "epoch": 0.7387260922968942,
      "grad_norm": 9.207067489624023,
      "learning_rate": 3.769374744107154e-05,
      "loss": 1.9218,
      "step": 4210
    },
    {
      "epoch": 0.7404807861028251,
      "grad_norm": 7.951423645019531,
      "learning_rate": 3.766450254430602e-05,
      "loss": 1.8032,
      "step": 4220
    },
    {
      "epoch": 0.7422354799087559,
      "grad_norm": 8.398551940917969,
      "learning_rate": 3.7635257647540504e-05,
      "loss": 1.7698,
      "step": 4230
    },
    {
      "epoch": 0.7439901737146868,
      "grad_norm": 6.784214973449707,
      "learning_rate": 3.7606012750774997e-05,
      "loss": 1.7555,
      "step": 4240
    },
    {
      "epoch": 0.7457448675206176,
      "grad_norm": 7.526875972747803,
      "learning_rate": 3.7576767854009475e-05,
      "loss": 1.8188,
      "step": 4250
    },
    {
      "epoch": 0.7474995613265485,
      "grad_norm": 6.4329514503479,
      "learning_rate": 3.754752295724396e-05,
      "loss": 1.7349,
      "step": 4260
    },
    {
      "epoch": 0.7492542551324793,
      "grad_norm": 8.032578468322754,
      "learning_rate": 3.7518278060478447e-05,
      "loss": 1.7326,
      "step": 4270
    },
    {
      "epoch": 0.7510089489384102,
      "grad_norm": 7.745704650878906,
      "learning_rate": 3.748903316371293e-05,
      "loss": 1.8532,
      "step": 4280
    },
    {
      "epoch": 0.7527636427443412,
      "grad_norm": 7.716867446899414,
      "learning_rate": 3.745978826694742e-05,
      "loss": 1.5829,
      "step": 4290
    },
    {
      "epoch": 0.754518336550272,
      "grad_norm": 5.450503349304199,
      "learning_rate": 3.7430543370181903e-05,
      "loss": 1.6292,
      "step": 4300
    },
    {
      "epoch": 0.7562730303562029,
      "grad_norm": 9.317574501037598,
      "learning_rate": 3.740129847341639e-05,
      "loss": 2.1626,
      "step": 4310
    },
    {
      "epoch": 0.7580277241621337,
      "grad_norm": 10.760480880737305,
      "learning_rate": 3.737205357665088e-05,
      "loss": 1.8306,
      "step": 4320
    },
    {
      "epoch": 0.7597824179680646,
      "grad_norm": 9.146780967712402,
      "learning_rate": 3.734280867988536e-05,
      "loss": 1.7637,
      "step": 4330
    },
    {
      "epoch": 0.7615371117739954,
      "grad_norm": 8.056571006774902,
      "learning_rate": 3.7313563783119846e-05,
      "loss": 1.6176,
      "step": 4340
    },
    {
      "epoch": 0.7632918055799263,
      "grad_norm": 7.266781806945801,
      "learning_rate": 3.728431888635433e-05,
      "loss": 2.0151,
      "step": 4350
    },
    {
      "epoch": 0.7650464993858571,
      "grad_norm": 8.789555549621582,
      "learning_rate": 3.725507398958882e-05,
      "loss": 1.7615,
      "step": 4360
    },
    {
      "epoch": 0.7668011931917881,
      "grad_norm": 9.444474220275879,
      "learning_rate": 3.72258290928233e-05,
      "loss": 1.9695,
      "step": 4370
    },
    {
      "epoch": 0.7685558869977189,
      "grad_norm": 9.950580596923828,
      "learning_rate": 3.719658419605779e-05,
      "loss": 1.9049,
      "step": 4380
    },
    {
      "epoch": 0.7703105808036498,
      "grad_norm": 9.392492294311523,
      "learning_rate": 3.7167339299292274e-05,
      "loss": 1.7712,
      "step": 4390
    },
    {
      "epoch": 0.7720652746095806,
      "grad_norm": 6.827998638153076,
      "learning_rate": 3.713809440252676e-05,
      "loss": 2.0086,
      "step": 4400
    },
    {
      "epoch": 0.7738199684155115,
      "grad_norm": 10.144564628601074,
      "learning_rate": 3.7108849505761245e-05,
      "loss": 1.9091,
      "step": 4410
    },
    {
      "epoch": 0.7755746622214423,
      "grad_norm": 8.945558547973633,
      "learning_rate": 3.707960460899573e-05,
      "loss": 1.9595,
      "step": 4420
    },
    {
      "epoch": 0.7773293560273732,
      "grad_norm": 8.723885536193848,
      "learning_rate": 3.7050359712230217e-05,
      "loss": 1.9826,
      "step": 4430
    },
    {
      "epoch": 0.779084049833304,
      "grad_norm": 9.628294944763184,
      "learning_rate": 3.70211148154647e-05,
      "loss": 1.7763,
      "step": 4440
    },
    {
      "epoch": 0.780838743639235,
      "grad_norm": 8.290728569030762,
      "learning_rate": 3.699186991869919e-05,
      "loss": 2.0147,
      "step": 4450
    },
    {
      "epoch": 0.7825934374451659,
      "grad_norm": 7.2254109382629395,
      "learning_rate": 3.6962625021933673e-05,
      "loss": 1.8007,
      "step": 4460
    },
    {
      "epoch": 0.7843481312510967,
      "grad_norm": 8.5567045211792,
      "learning_rate": 3.693338012516816e-05,
      "loss": 1.7937,
      "step": 4470
    },
    {
      "epoch": 0.7861028250570276,
      "grad_norm": 8.920160293579102,
      "learning_rate": 3.6904135228402645e-05,
      "loss": 1.8181,
      "step": 4480
    },
    {
      "epoch": 0.7878575188629584,
      "grad_norm": 8.630278587341309,
      "learning_rate": 3.687489033163713e-05,
      "loss": 1.6798,
      "step": 4490
    },
    {
      "epoch": 0.7896122126688893,
      "grad_norm": 7.978768825531006,
      "learning_rate": 3.6845645434871616e-05,
      "loss": 1.65,
      "step": 4500
    },
    {
      "epoch": 0.7913669064748201,
      "grad_norm": 7.171823501586914,
      "learning_rate": 3.681640053810611e-05,
      "loss": 1.6276,
      "step": 4510
    },
    {
      "epoch": 0.793121600280751,
      "grad_norm": 7.822957515716553,
      "learning_rate": 3.678715564134059e-05,
      "loss": 1.9436,
      "step": 4520
    },
    {
      "epoch": 0.7948762940866819,
      "grad_norm": 9.427062034606934,
      "learning_rate": 3.675791074457507e-05,
      "loss": 1.7202,
      "step": 4530
    },
    {
      "epoch": 0.7966309878926128,
      "grad_norm": 9.03616714477539,
      "learning_rate": 3.672866584780956e-05,
      "loss": 1.6052,
      "step": 4540
    },
    {
      "epoch": 0.7983856816985436,
      "grad_norm": 9.232610702514648,
      "learning_rate": 3.6699420951044044e-05,
      "loss": 1.9137,
      "step": 4550
    },
    {
      "epoch": 0.8001403755044745,
      "grad_norm": 9.484803199768066,
      "learning_rate": 3.667017605427853e-05,
      "loss": 1.6786,
      "step": 4560
    },
    {
      "epoch": 0.8018950693104053,
      "grad_norm": 7.180175304412842,
      "learning_rate": 3.6640931157513015e-05,
      "loss": 1.9579,
      "step": 4570
    },
    {
      "epoch": 0.8036497631163362,
      "grad_norm": 9.729513168334961,
      "learning_rate": 3.66116862607475e-05,
      "loss": 1.9951,
      "step": 4580
    },
    {
      "epoch": 0.805404456922267,
      "grad_norm": 7.5664448738098145,
      "learning_rate": 3.6582441363981986e-05,
      "loss": 1.5976,
      "step": 4590
    },
    {
      "epoch": 0.8071591507281979,
      "grad_norm": 9.8851318359375,
      "learning_rate": 3.655319646721647e-05,
      "loss": 1.7808,
      "step": 4600
    },
    {
      "epoch": 0.8089138445341288,
      "grad_norm": 8.433416366577148,
      "learning_rate": 3.652395157045096e-05,
      "loss": 1.756,
      "step": 4610
    },
    {
      "epoch": 0.8106685383400597,
      "grad_norm": 8.758637428283691,
      "learning_rate": 3.649470667368544e-05,
      "loss": 1.9199,
      "step": 4620
    },
    {
      "epoch": 0.8124232321459905,
      "grad_norm": 8.43545913696289,
      "learning_rate": 3.646546177691993e-05,
      "loss": 1.8709,
      "step": 4630
    },
    {
      "epoch": 0.8141779259519214,
      "grad_norm": 9.140243530273438,
      "learning_rate": 3.6436216880154415e-05,
      "loss": 1.8414,
      "step": 4640
    },
    {
      "epoch": 0.8159326197578523,
      "grad_norm": 8.187261581420898,
      "learning_rate": 3.64069719833889e-05,
      "loss": 2.0442,
      "step": 4650
    },
    {
      "epoch": 0.8176873135637831,
      "grad_norm": 7.548014163970947,
      "learning_rate": 3.6377727086623386e-05,
      "loss": 1.6601,
      "step": 4660
    },
    {
      "epoch": 0.819442007369714,
      "grad_norm": 8.671483993530273,
      "learning_rate": 3.634848218985787e-05,
      "loss": 1.8577,
      "step": 4670
    },
    {
      "epoch": 0.8211967011756448,
      "grad_norm": 7.798547744750977,
      "learning_rate": 3.631923729309236e-05,
      "loss": 1.6155,
      "step": 4680
    },
    {
      "epoch": 0.8229513949815758,
      "grad_norm": 8.652077674865723,
      "learning_rate": 3.628999239632684e-05,
      "loss": 1.9157,
      "step": 4690
    },
    {
      "epoch": 0.8247060887875066,
      "grad_norm": 9.389532089233398,
      "learning_rate": 3.626074749956133e-05,
      "loss": 1.8667,
      "step": 4700
    },
    {
      "epoch": 0.8264607825934375,
      "grad_norm": 9.031167984008789,
      "learning_rate": 3.6231502602795814e-05,
      "loss": 1.7985,
      "step": 4710
    },
    {
      "epoch": 0.8282154763993683,
      "grad_norm": 7.5857133865356445,
      "learning_rate": 3.62022577060303e-05,
      "loss": 1.6578,
      "step": 4720
    },
    {
      "epoch": 0.8299701702052992,
      "grad_norm": 6.625002384185791,
      "learning_rate": 3.6173012809264785e-05,
      "loss": 1.9184,
      "step": 4730
    },
    {
      "epoch": 0.83172486401123,
      "grad_norm": 8.53068733215332,
      "learning_rate": 3.614376791249927e-05,
      "loss": 1.6065,
      "step": 4740
    },
    {
      "epoch": 0.8334795578171609,
      "grad_norm": 9.38802719116211,
      "learning_rate": 3.6114523015733756e-05,
      "loss": 1.6776,
      "step": 4750
    },
    {
      "epoch": 0.8352342516230917,
      "grad_norm": 7.819828510284424,
      "learning_rate": 3.608527811896824e-05,
      "loss": 1.6316,
      "step": 4760
    },
    {
      "epoch": 0.8369889454290227,
      "grad_norm": 7.457601547241211,
      "learning_rate": 3.605603322220273e-05,
      "loss": 1.8165,
      "step": 4770
    },
    {
      "epoch": 0.8387436392349535,
      "grad_norm": 9.62600326538086,
      "learning_rate": 3.602678832543721e-05,
      "loss": 1.6734,
      "step": 4780
    },
    {
      "epoch": 0.8404983330408844,
      "grad_norm": 9.425342559814453,
      "learning_rate": 3.59975434286717e-05,
      "loss": 1.9277,
      "step": 4790
    },
    {
      "epoch": 0.8422530268468152,
      "grad_norm": 9.500102043151855,
      "learning_rate": 3.5968298531906185e-05,
      "loss": 1.6917,
      "step": 4800
    },
    {
      "epoch": 0.8440077206527461,
      "grad_norm": 7.567027568817139,
      "learning_rate": 3.593905363514067e-05,
      "loss": 1.914,
      "step": 4810
    },
    {
      "epoch": 0.845762414458677,
      "grad_norm": 11.583364486694336,
      "learning_rate": 3.5909808738375156e-05,
      "loss": 1.7641,
      "step": 4820
    },
    {
      "epoch": 0.8475171082646078,
      "grad_norm": 10.022360801696777,
      "learning_rate": 3.588056384160964e-05,
      "loss": 1.8175,
      "step": 4830
    },
    {
      "epoch": 0.8492718020705387,
      "grad_norm": 7.051565647125244,
      "learning_rate": 3.585131894484413e-05,
      "loss": 1.9582,
      "step": 4840
    },
    {
      "epoch": 0.8510264958764695,
      "grad_norm": 8.791182518005371,
      "learning_rate": 3.582207404807861e-05,
      "loss": 1.6674,
      "step": 4850
    },
    {
      "epoch": 0.8527811896824005,
      "grad_norm": 10.499293327331543,
      "learning_rate": 3.57928291513131e-05,
      "loss": 2.037,
      "step": 4860
    },
    {
      "epoch": 0.8545358834883313,
      "grad_norm": 6.385371685028076,
      "learning_rate": 3.5763584254547584e-05,
      "loss": 1.6635,
      "step": 4870
    },
    {
      "epoch": 0.8562905772942622,
      "grad_norm": 7.630228519439697,
      "learning_rate": 3.573433935778207e-05,
      "loss": 1.5931,
      "step": 4880
    },
    {
      "epoch": 0.858045271100193,
      "grad_norm": 9.193973541259766,
      "learning_rate": 3.5705094461016555e-05,
      "loss": 1.8006,
      "step": 4890
    },
    {
      "epoch": 0.8597999649061239,
      "grad_norm": 6.531421184539795,
      "learning_rate": 3.567584956425104e-05,
      "loss": 1.7499,
      "step": 4900
    },
    {
      "epoch": 0.8615546587120547,
      "grad_norm": 8.464767456054688,
      "learning_rate": 3.5646604667485526e-05,
      "loss": 1.7513,
      "step": 4910
    },
    {
      "epoch": 0.8633093525179856,
      "grad_norm": 7.643024444580078,
      "learning_rate": 3.561735977072001e-05,
      "loss": 1.8786,
      "step": 4920
    },
    {
      "epoch": 0.8650640463239164,
      "grad_norm": 6.813136577606201,
      "learning_rate": 3.55881148739545e-05,
      "loss": 1.9556,
      "step": 4930
    },
    {
      "epoch": 0.8668187401298474,
      "grad_norm": 8.794146537780762,
      "learning_rate": 3.555886997718898e-05,
      "loss": 1.9242,
      "step": 4940
    },
    {
      "epoch": 0.8685734339357782,
      "grad_norm": 7.870793342590332,
      "learning_rate": 3.552962508042347e-05,
      "loss": 1.6966,
      "step": 4950
    },
    {
      "epoch": 0.8703281277417091,
      "grad_norm": 7.155563831329346,
      "learning_rate": 3.5500380183657954e-05,
      "loss": 1.6284,
      "step": 4960
    },
    {
      "epoch": 0.8720828215476399,
      "grad_norm": 9.28737735748291,
      "learning_rate": 3.547113528689244e-05,
      "loss": 1.6664,
      "step": 4970
    },
    {
      "epoch": 0.8738375153535708,
      "grad_norm": 7.8445258140563965,
      "learning_rate": 3.5441890390126926e-05,
      "loss": 1.8597,
      "step": 4980
    },
    {
      "epoch": 0.8755922091595016,
      "grad_norm": 5.712126731872559,
      "learning_rate": 3.541264549336141e-05,
      "loss": 1.689,
      "step": 4990
    },
    {
      "epoch": 0.8773469029654325,
      "grad_norm": 12.078822135925293,
      "learning_rate": 3.53834005965959e-05,
      "loss": 1.9807,
      "step": 5000
    },
    {
      "epoch": 0.8791015967713633,
      "grad_norm": 8.17280387878418,
      "learning_rate": 3.535415569983038e-05,
      "loss": 1.8295,
      "step": 5010
    },
    {
      "epoch": 0.8808562905772943,
      "grad_norm": 9.58312702178955,
      "learning_rate": 3.532491080306486e-05,
      "loss": 1.7267,
      "step": 5020
    },
    {
      "epoch": 0.8826109843832252,
      "grad_norm": 9.236628532409668,
      "learning_rate": 3.5295665906299354e-05,
      "loss": 1.7322,
      "step": 5030
    },
    {
      "epoch": 0.884365678189156,
      "grad_norm": 7.602957725524902,
      "learning_rate": 3.526642100953384e-05,
      "loss": 1.9673,
      "step": 5040
    },
    {
      "epoch": 0.8861203719950869,
      "grad_norm": 9.511564254760742,
      "learning_rate": 3.5237176112768325e-05,
      "loss": 1.6217,
      "step": 5050
    },
    {
      "epoch": 0.8878750658010177,
      "grad_norm": 7.412097930908203,
      "learning_rate": 3.520793121600281e-05,
      "loss": 1.8219,
      "step": 5060
    },
    {
      "epoch": 0.8896297596069486,
      "grad_norm": 8.63351821899414,
      "learning_rate": 3.5178686319237296e-05,
      "loss": 1.7013,
      "step": 5070
    },
    {
      "epoch": 0.8913844534128794,
      "grad_norm": 9.086199760437012,
      "learning_rate": 3.514944142247178e-05,
      "loss": 1.9434,
      "step": 5080
    },
    {
      "epoch": 0.8931391472188103,
      "grad_norm": 8.522062301635742,
      "learning_rate": 3.512019652570627e-05,
      "loss": 1.6654,
      "step": 5090
    },
    {
      "epoch": 0.8948938410247412,
      "grad_norm": 8.070709228515625,
      "learning_rate": 3.5090951628940746e-05,
      "loss": 2.1004,
      "step": 5100
    },
    {
      "epoch": 0.8966485348306721,
      "grad_norm": 9.091981887817383,
      "learning_rate": 3.506170673217524e-05,
      "loss": 2.0266,
      "step": 5110
    },
    {
      "epoch": 0.8984032286366029,
      "grad_norm": 11.500965118408203,
      "learning_rate": 3.5032461835409724e-05,
      "loss": 1.7744,
      "step": 5120
    },
    {
      "epoch": 0.9001579224425338,
      "grad_norm": 8.734110832214355,
      "learning_rate": 3.50032169386442e-05,
      "loss": 1.9007,
      "step": 5130
    },
    {
      "epoch": 0.9019126162484646,
      "grad_norm": 9.257922172546387,
      "learning_rate": 3.4973972041878696e-05,
      "loss": 1.8958,
      "step": 5140
    },
    {
      "epoch": 0.9036673100543955,
      "grad_norm": 9.750519752502441,
      "learning_rate": 3.494472714511318e-05,
      "loss": 1.6352,
      "step": 5150
    },
    {
      "epoch": 0.9054220038603263,
      "grad_norm": 9.701977729797363,
      "learning_rate": 3.491548224834767e-05,
      "loss": 1.6972,
      "step": 5160
    },
    {
      "epoch": 0.9071766976662572,
      "grad_norm": 9.488678932189941,
      "learning_rate": 3.488623735158215e-05,
      "loss": 1.798,
      "step": 5170
    },
    {
      "epoch": 0.9089313914721882,
      "grad_norm": 8.629952430725098,
      "learning_rate": 3.485699245481663e-05,
      "loss": 1.7953,
      "step": 5180
    },
    {
      "epoch": 0.910686085278119,
      "grad_norm": 7.498021602630615,
      "learning_rate": 3.4827747558051124e-05,
      "loss": 1.6086,
      "step": 5190
    },
    {
      "epoch": 0.9124407790840499,
      "grad_norm": 11.358619689941406,
      "learning_rate": 3.479850266128561e-05,
      "loss": 2.0148,
      "step": 5200
    },
    {
      "epoch": 0.9141954728899807,
      "grad_norm": 6.238211154937744,
      "learning_rate": 3.476925776452009e-05,
      "loss": 1.9079,
      "step": 5210
    },
    {
      "epoch": 0.9159501666959116,
      "grad_norm": 8.594352722167969,
      "learning_rate": 3.474001286775458e-05,
      "loss": 1.9388,
      "step": 5220
    },
    {
      "epoch": 0.9177048605018424,
      "grad_norm": 9.93667984008789,
      "learning_rate": 3.4710767970989066e-05,
      "loss": 1.8404,
      "step": 5230
    },
    {
      "epoch": 0.9194595543077733,
      "grad_norm": 7.155617713928223,
      "learning_rate": 3.468152307422355e-05,
      "loss": 1.7616,
      "step": 5240
    },
    {
      "epoch": 0.9212142481137041,
      "grad_norm": 8.546019554138184,
      "learning_rate": 3.465227817745804e-05,
      "loss": 1.8869,
      "step": 5250
    },
    {
      "epoch": 0.9229689419196351,
      "grad_norm": 8.501314163208008,
      "learning_rate": 3.4623033280692516e-05,
      "loss": 1.7068,
      "step": 5260
    },
    {
      "epoch": 0.9247236357255659,
      "grad_norm": 8.0296630859375,
      "learning_rate": 3.459378838392701e-05,
      "loss": 1.8056,
      "step": 5270
    },
    {
      "epoch": 0.9264783295314968,
      "grad_norm": 7.784960746765137,
      "learning_rate": 3.4564543487161494e-05,
      "loss": 1.7029,
      "step": 5280
    },
    {
      "epoch": 0.9282330233374276,
      "grad_norm": 7.341885566711426,
      "learning_rate": 3.453529859039597e-05,
      "loss": 1.7577,
      "step": 5290
    },
    {
      "epoch": 0.9299877171433585,
      "grad_norm": 8.34261417388916,
      "learning_rate": 3.4506053693630466e-05,
      "loss": 1.9418,
      "step": 5300
    },
    {
      "epoch": 0.9317424109492893,
      "grad_norm": 9.454113960266113,
      "learning_rate": 3.447680879686495e-05,
      "loss": 1.8669,
      "step": 5310
    },
    {
      "epoch": 0.9334971047552202,
      "grad_norm": 9.443458557128906,
      "learning_rate": 3.444756390009943e-05,
      "loss": 2.0426,
      "step": 5320
    },
    {
      "epoch": 0.935251798561151,
      "grad_norm": 9.556172370910645,
      "learning_rate": 3.441831900333392e-05,
      "loss": 1.7469,
      "step": 5330
    },
    {
      "epoch": 0.9370064923670819,
      "grad_norm": 8.98470401763916,
      "learning_rate": 3.43890741065684e-05,
      "loss": 1.8677,
      "step": 5340
    },
    {
      "epoch": 0.9387611861730129,
      "grad_norm": 5.58733606338501,
      "learning_rate": 3.4359829209802894e-05,
      "loss": 1.4452,
      "step": 5350
    },
    {
      "epoch": 0.9405158799789437,
      "grad_norm": 8.130080223083496,
      "learning_rate": 3.433058431303738e-05,
      "loss": 1.6258,
      "step": 5360
    },
    {
      "epoch": 0.9422705737848746,
      "grad_norm": 6.944863319396973,
      "learning_rate": 3.430133941627186e-05,
      "loss": 1.7015,
      "step": 5370
    },
    {
      "epoch": 0.9440252675908054,
      "grad_norm": 9.842438697814941,
      "learning_rate": 3.427209451950635e-05,
      "loss": 1.8363,
      "step": 5380
    },
    {
      "epoch": 0.9457799613967363,
      "grad_norm": 7.363970756530762,
      "learning_rate": 3.4242849622740836e-05,
      "loss": 1.7154,
      "step": 5390
    },
    {
      "epoch": 0.9475346552026671,
      "grad_norm": 9.950494766235352,
      "learning_rate": 3.4213604725975315e-05,
      "loss": 1.8461,
      "step": 5400
    },
    {
      "epoch": 0.949289349008598,
      "grad_norm": 6.6153059005737305,
      "learning_rate": 3.418435982920981e-05,
      "loss": 1.8368,
      "step": 5410
    },
    {
      "epoch": 0.9510440428145288,
      "grad_norm": 7.841240406036377,
      "learning_rate": 3.4155114932444286e-05,
      "loss": 1.6788,
      "step": 5420
    },
    {
      "epoch": 0.9527987366204598,
      "grad_norm": 9.29143238067627,
      "learning_rate": 3.412587003567878e-05,
      "loss": 1.903,
      "step": 5430
    },
    {
      "epoch": 0.9545534304263906,
      "grad_norm": 7.527097225189209,
      "learning_rate": 3.4096625138913264e-05,
      "loss": 1.7107,
      "step": 5440
    },
    {
      "epoch": 0.9563081242323215,
      "grad_norm": 7.783794403076172,
      "learning_rate": 3.406738024214774e-05,
      "loss": 2.0215,
      "step": 5450
    },
    {
      "epoch": 0.9580628180382523,
      "grad_norm": 7.714066028594971,
      "learning_rate": 3.4038135345382236e-05,
      "loss": 1.9092,
      "step": 5460
    },
    {
      "epoch": 0.9598175118441832,
      "grad_norm": 9.204014778137207,
      "learning_rate": 3.400889044861672e-05,
      "loss": 1.7232,
      "step": 5470
    },
    {
      "epoch": 0.961572205650114,
      "grad_norm": 7.438296318054199,
      "learning_rate": 3.39796455518512e-05,
      "loss": 1.6949,
      "step": 5480
    },
    {
      "epoch": 0.9633268994560449,
      "grad_norm": 8.00752067565918,
      "learning_rate": 3.395040065508569e-05,
      "loss": 1.6911,
      "step": 5490
    },
    {
      "epoch": 0.9650815932619757,
      "grad_norm": 4.880671977996826,
      "learning_rate": 3.392115575832017e-05,
      "loss": 1.6134,
      "step": 5500
    },
    {
      "epoch": 0.9668362870679067,
      "grad_norm": 8.937060356140137,
      "learning_rate": 3.389191086155466e-05,
      "loss": 1.9388,
      "step": 5510
    },
    {
      "epoch": 0.9685909808738375,
      "grad_norm": 9.35396671295166,
      "learning_rate": 3.386266596478915e-05,
      "loss": 1.8123,
      "step": 5520
    },
    {
      "epoch": 0.9703456746797684,
      "grad_norm": 7.669732570648193,
      "learning_rate": 3.383342106802363e-05,
      "loss": 1.7401,
      "step": 5530
    },
    {
      "epoch": 0.9721003684856993,
      "grad_norm": 12.252663612365723,
      "learning_rate": 3.380417617125812e-05,
      "loss": 1.8268,
      "step": 5540
    },
    {
      "epoch": 0.9738550622916301,
      "grad_norm": 7.090322017669678,
      "learning_rate": 3.3774931274492606e-05,
      "loss": 1.7198,
      "step": 5550
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 8.443917274475098,
      "learning_rate": 3.3745686377727085e-05,
      "loss": 1.8864,
      "step": 5560
    },
    {
      "epoch": 0.9773644499034918,
      "grad_norm": 10.160118103027344,
      "learning_rate": 3.371644148096158e-05,
      "loss": 1.8519,
      "step": 5570
    },
    {
      "epoch": 0.9791191437094227,
      "grad_norm": 7.633236885070801,
      "learning_rate": 3.3687196584196056e-05,
      "loss": 1.9304,
      "step": 5580
    },
    {
      "epoch": 0.9808738375153536,
      "grad_norm": 9.012197494506836,
      "learning_rate": 3.365795168743054e-05,
      "loss": 1.8326,
      "step": 5590
    },
    {
      "epoch": 0.9826285313212845,
      "grad_norm": 8.584856033325195,
      "learning_rate": 3.3628706790665034e-05,
      "loss": 1.5868,
      "step": 5600
    },
    {
      "epoch": 0.9843832251272153,
      "grad_norm": 9.470207214355469,
      "learning_rate": 3.359946189389951e-05,
      "loss": 1.5048,
      "step": 5610
    },
    {
      "epoch": 0.9861379189331462,
      "grad_norm": 10.125364303588867,
      "learning_rate": 3.3570216997134e-05,
      "loss": 1.7115,
      "step": 5620
    },
    {
      "epoch": 0.987892612739077,
      "grad_norm": 9.605165481567383,
      "learning_rate": 3.354097210036849e-05,
      "loss": 2.0384,
      "step": 5630
    },
    {
      "epoch": 0.9896473065450079,
      "grad_norm": 6.588136196136475,
      "learning_rate": 3.351172720360297e-05,
      "loss": 1.8887,
      "step": 5640
    },
    {
      "epoch": 0.9914020003509387,
      "grad_norm": 10.909399032592773,
      "learning_rate": 3.348248230683746e-05,
      "loss": 1.8426,
      "step": 5650
    },
    {
      "epoch": 0.9931566941568696,
      "grad_norm": 10.385293960571289,
      "learning_rate": 3.345323741007194e-05,
      "loss": 1.9052,
      "step": 5660
    },
    {
      "epoch": 0.9949113879628005,
      "grad_norm": 9.30502986907959,
      "learning_rate": 3.342399251330643e-05,
      "loss": 2.0959,
      "step": 5670
    },
    {
      "epoch": 0.9966660817687314,
      "grad_norm": 7.281422138214111,
      "learning_rate": 3.339474761654092e-05,
      "loss": 1.6948,
      "step": 5680
    },
    {
      "epoch": 0.9984207755746622,
      "grad_norm": 8.164470672607422,
      "learning_rate": 3.33655027197754e-05,
      "loss": 1.8165,
      "step": 5690
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.43967710801088006,
      "eval_f1_C01": 0.37748344370860926,
      "eval_f1_C02": 0.12794612794612795,
      "eval_f1_C03": 0.43548387096774194,
      "eval_f1_C04": 0.5858383433533734,
      "eval_f1_C05": 0.35542168674698793,
      "eval_f1_C06": 0.4704073789392775,
      "eval_f1_C07": 0.2962962962962963,
      "eval_f1_C08": 0.4515021459227468,
      "eval_f1_C09": 0.35251798561151076,
      "eval_f1_C10": 0.45072115384615385,
      "eval_f1_C11": 0.49528301886792453,
      "eval_f1_C12": 0.5133689839572193,
      "eval_f1_C13": 0.4361948955916473,
      "eval_f1_C14": 0.6238078417520311,
      "eval_f1_C15": 0.1884498480243161,
      "eval_f1_C16": 0.20670391061452514,
      "eval_f1_C17": 0.3505747126436782,
      "eval_f1_C18": 0.46846846846846846,
      "eval_f1_C19": 0.23481781376518218,
      "eval_f1_C20": 0.4604501607717042,
      "eval_f1_C21": 0.5521367521367522,
      "eval_f1_C22": 0.0,
      "eval_f1_C23": 0.17058185760751718,
      "eval_f1_macro": 0.3741068129365126,
      "eval_loss": 1.7496334314346313,
      "eval_precision_C01": 0.4296482412060301,
      "eval_precision_C02": 0.30158730158730157,
      "eval_precision_C03": 0.6923076923076923,
      "eval_precision_C04": 0.5075231481481481,
      "eval_precision_C05": 0.3597560975609756,
      "eval_precision_C06": 0.4352773826458037,
      "eval_precision_C07": 0.42105263157894735,
      "eval_precision_C08": 0.40649149922720246,
      "eval_precision_C09": 0.362962962962963,
      "eval_precision_C10": 0.41946308724832215,
      "eval_precision_C11": 0.46875,
      "eval_precision_C12": 0.46601941747572817,
      "eval_precision_C13": 0.3500931098696462,
      "eval_precision_C14": 0.548106765983861,
      "eval_precision_C15": 0.4189189189189189,
      "eval_precision_C16": 0.2624113475177305,
      "eval_precision_C17": 0.32707774798927614,
      "eval_precision_C18": 0.4631043256997455,
      "eval_precision_C19": 0.3918918918918919,
      "eval_precision_C20": 0.38412017167381973,
      "eval_precision_C21": 0.5540308747855918,
      "eval_precision_C22": 1.0,
      "eval_precision_C23": 0.27928994082840236,
      "eval_precision_global": 0.4456471546568695,
      "eval_recall_C01": 0.33661417322834647,
      "eval_recall_C02": 0.0811965811965812,
      "eval_recall_C03": 0.3176470588235294,
      "eval_recall_C04": 0.6927330173775671,
      "eval_recall_C05": 0.35119047619047616,
      "eval_recall_C06": 0.5117056856187291,
      "eval_recall_C07": 0.22857142857142856,
      "eval_recall_C08": 0.5077220077220077,
      "eval_recall_C09": 0.34265734265734266,
      "eval_recall_C10": 0.487012987012987,
      "eval_recall_C11": 0.525,
      "eval_recall_C12": 0.5714285714285714,
      "eval_recall_C13": 0.5784615384615385,
      "eval_recall_C14": 0.7237704918032787,
      "eval_recall_C15": 0.12156862745098039,
      "eval_recall_C16": 0.17050691244239632,
      "eval_recall_C17": 0.37770897832817335,
      "eval_recall_C18": 0.4739583333333333,
      "eval_recall_C19": 0.1676300578034682,
      "eval_recall_C20": 0.5746388443017657,
      "eval_recall_C21": 0.5502555366269165,
      "eval_recall_C22": 0.0,
      "eval_recall_C23": 0.12278876170655567,
      "eval_recall_global": 0.3832507570472163,
      "eval_runtime": 51.5878,
      "eval_samples_per_second": 220.924,
      "eval_steps_per_second": 27.623,
      "step": 5699
    },
    {
      "epoch": 1.000175469380593,
      "grad_norm": 10.88360595703125,
      "learning_rate": 3.3336257823009884e-05,
      "loss": 1.9483,
      "step": 5700
    },
    {
      "epoch": 1.001930163186524,
      "grad_norm": 7.78026008605957,
      "learning_rate": 3.3307012926244376e-05,
      "loss": 1.6334,
      "step": 5710
    },
    {
      "epoch": 1.0036848569924548,
      "grad_norm": 8.038515090942383,
      "learning_rate": 3.3277768029478855e-05,
      "loss": 1.7239,
      "step": 5720
    },
    {
      "epoch": 1.0054395507983858,
      "grad_norm": 10.546990394592285,
      "learning_rate": 3.324852313271335e-05,
      "loss": 1.7607,
      "step": 5730
    },
    {
      "epoch": 1.0071942446043165,
      "grad_norm": 6.972897052764893,
      "learning_rate": 3.3219278235947826e-05,
      "loss": 1.6981,
      "step": 5740
    },
    {
      "epoch": 1.0089489384102475,
      "grad_norm": 6.044368743896484,
      "learning_rate": 3.319003333918231e-05,
      "loss": 1.7081,
      "step": 5750
    },
    {
      "epoch": 1.0107036322161782,
      "grad_norm": 8.188117027282715,
      "learning_rate": 3.3160788442416804e-05,
      "loss": 1.6653,
      "step": 5760
    },
    {
      "epoch": 1.0124583260221092,
      "grad_norm": 12.675460815429688,
      "learning_rate": 3.313154354565128e-05,
      "loss": 1.959,
      "step": 5770
    },
    {
      "epoch": 1.01421301982804,
      "grad_norm": 7.281296730041504,
      "learning_rate": 3.310229864888577e-05,
      "loss": 1.8214,
      "step": 5780
    },
    {
      "epoch": 1.0159677136339709,
      "grad_norm": 8.577159881591797,
      "learning_rate": 3.307305375212026e-05,
      "loss": 1.7668,
      "step": 5790
    },
    {
      "epoch": 1.0177224074399018,
      "grad_norm": 8.478657722473145,
      "learning_rate": 3.304380885535474e-05,
      "loss": 1.7778,
      "step": 5800
    },
    {
      "epoch": 1.0194771012458326,
      "grad_norm": 8.949118614196777,
      "learning_rate": 3.3014563958589225e-05,
      "loss": 2.0452,
      "step": 5810
    },
    {
      "epoch": 1.0212317950517635,
      "grad_norm": 8.11253547668457,
      "learning_rate": 3.298531906182371e-05,
      "loss": 1.6749,
      "step": 5820
    },
    {
      "epoch": 1.0229864888576943,
      "grad_norm": 5.524095058441162,
      "learning_rate": 3.29560741650582e-05,
      "loss": 1.6773,
      "step": 5830
    },
    {
      "epoch": 1.0247411826636252,
      "grad_norm": 8.616943359375,
      "learning_rate": 3.292682926829269e-05,
      "loss": 1.6812,
      "step": 5840
    },
    {
      "epoch": 1.026495876469556,
      "grad_norm": 9.729182243347168,
      "learning_rate": 3.289758437152717e-05,
      "loss": 1.6873,
      "step": 5850
    },
    {
      "epoch": 1.028250570275487,
      "grad_norm": 6.706664562225342,
      "learning_rate": 3.2868339474761654e-05,
      "loss": 1.4994,
      "step": 5860
    },
    {
      "epoch": 1.0300052640814177,
      "grad_norm": 8.129389762878418,
      "learning_rate": 3.2839094577996146e-05,
      "loss": 1.9427,
      "step": 5870
    },
    {
      "epoch": 1.0317599578873486,
      "grad_norm": 9.51707649230957,
      "learning_rate": 3.2809849681230625e-05,
      "loss": 1.8205,
      "step": 5880
    },
    {
      "epoch": 1.0335146516932796,
      "grad_norm": 8.568910598754883,
      "learning_rate": 3.278060478446511e-05,
      "loss": 1.6053,
      "step": 5890
    },
    {
      "epoch": 1.0352693454992103,
      "grad_norm": 5.660968780517578,
      "learning_rate": 3.2751359887699596e-05,
      "loss": 1.7175,
      "step": 5900
    },
    {
      "epoch": 1.0370240393051413,
      "grad_norm": 9.894271850585938,
      "learning_rate": 3.272211499093408e-05,
      "loss": 1.8439,
      "step": 5910
    },
    {
      "epoch": 1.038778733111072,
      "grad_norm": 7.963089466094971,
      "learning_rate": 3.269287009416857e-05,
      "loss": 1.8274,
      "step": 5920
    },
    {
      "epoch": 1.040533426917003,
      "grad_norm": 10.650036811828613,
      "learning_rate": 3.266362519740305e-05,
      "loss": 1.7832,
      "step": 5930
    },
    {
      "epoch": 1.0422881207229338,
      "grad_norm": 6.796629905700684,
      "learning_rate": 3.263438030063754e-05,
      "loss": 1.8105,
      "step": 5940
    },
    {
      "epoch": 1.0440428145288647,
      "grad_norm": 9.653138160705566,
      "learning_rate": 3.260513540387203e-05,
      "loss": 1.833,
      "step": 5950
    },
    {
      "epoch": 1.0457975083347957,
      "grad_norm": 5.8643412590026855,
      "learning_rate": 3.257589050710651e-05,
      "loss": 1.6105,
      "step": 5960
    },
    {
      "epoch": 1.0475522021407264,
      "grad_norm": 10.134371757507324,
      "learning_rate": 3.2546645610340995e-05,
      "loss": 1.6896,
      "step": 5970
    },
    {
      "epoch": 1.0493068959466574,
      "grad_norm": 7.956132888793945,
      "learning_rate": 3.251740071357548e-05,
      "loss": 1.93,
      "step": 5980
    },
    {
      "epoch": 1.0510615897525881,
      "grad_norm": 7.5971574783325195,
      "learning_rate": 3.248815581680997e-05,
      "loss": 1.6468,
      "step": 5990
    },
    {
      "epoch": 1.052816283558519,
      "grad_norm": 8.781153678894043,
      "learning_rate": 3.245891092004445e-05,
      "loss": 1.8582,
      "step": 6000
    },
    {
      "epoch": 1.0545709773644498,
      "grad_norm": 8.331767082214355,
      "learning_rate": 3.242966602327894e-05,
      "loss": 1.8413,
      "step": 6010
    },
    {
      "epoch": 1.0563256711703808,
      "grad_norm": 7.095063209533691,
      "learning_rate": 3.2400421126513424e-05,
      "loss": 1.8216,
      "step": 6020
    },
    {
      "epoch": 1.0580803649763115,
      "grad_norm": 7.704182147979736,
      "learning_rate": 3.2371176229747916e-05,
      "loss": 1.7233,
      "step": 6030
    },
    {
      "epoch": 1.0598350587822425,
      "grad_norm": 9.54984188079834,
      "learning_rate": 3.2341931332982395e-05,
      "loss": 1.6268,
      "step": 6040
    },
    {
      "epoch": 1.0615897525881735,
      "grad_norm": 7.752352714538574,
      "learning_rate": 3.231268643621688e-05,
      "loss": 1.6938,
      "step": 6050
    },
    {
      "epoch": 1.0633444463941042,
      "grad_norm": 9.924966812133789,
      "learning_rate": 3.2283441539451366e-05,
      "loss": 1.662,
      "step": 6060
    },
    {
      "epoch": 1.0650991402000352,
      "grad_norm": 7.994937896728516,
      "learning_rate": 3.225419664268585e-05,
      "loss": 1.7522,
      "step": 6070
    },
    {
      "epoch": 1.066853834005966,
      "grad_norm": 7.171484470367432,
      "learning_rate": 3.222495174592034e-05,
      "loss": 1.5685,
      "step": 6080
    },
    {
      "epoch": 1.0686085278118969,
      "grad_norm": 10.399428367614746,
      "learning_rate": 3.219570684915482e-05,
      "loss": 1.8559,
      "step": 6090
    },
    {
      "epoch": 1.0703632216178276,
      "grad_norm": 10.003985404968262,
      "learning_rate": 3.216646195238931e-05,
      "loss": 1.8643,
      "step": 6100
    },
    {
      "epoch": 1.0721179154237586,
      "grad_norm": 9.879509925842285,
      "learning_rate": 3.2137217055623794e-05,
      "loss": 1.8108,
      "step": 6110
    },
    {
      "epoch": 1.0738726092296895,
      "grad_norm": 8.140923500061035,
      "learning_rate": 3.210797215885828e-05,
      "loss": 1.7604,
      "step": 6120
    },
    {
      "epoch": 1.0756273030356203,
      "grad_norm": 8.381264686584473,
      "learning_rate": 3.2078727262092765e-05,
      "loss": 1.715,
      "step": 6130
    },
    {
      "epoch": 1.0773819968415512,
      "grad_norm": 9.671202659606934,
      "learning_rate": 3.204948236532726e-05,
      "loss": 1.7556,
      "step": 6140
    },
    {
      "epoch": 1.079136690647482,
      "grad_norm": 9.271567344665527,
      "learning_rate": 3.202023746856174e-05,
      "loss": 1.683,
      "step": 6150
    },
    {
      "epoch": 1.080891384453413,
      "grad_norm": 7.749670505523682,
      "learning_rate": 3.199099257179622e-05,
      "loss": 1.9345,
      "step": 6160
    },
    {
      "epoch": 1.0826460782593437,
      "grad_norm": 7.337915420532227,
      "learning_rate": 3.196174767503071e-05,
      "loss": 1.693,
      "step": 6170
    },
    {
      "epoch": 1.0844007720652746,
      "grad_norm": 8.178081512451172,
      "learning_rate": 3.1932502778265193e-05,
      "loss": 1.6961,
      "step": 6180
    },
    {
      "epoch": 1.0861554658712054,
      "grad_norm": 6.371974945068359,
      "learning_rate": 3.190325788149968e-05,
      "loss": 1.6885,
      "step": 6190
    },
    {
      "epoch": 1.0879101596771363,
      "grad_norm": 9.504640579223633,
      "learning_rate": 3.1874012984734165e-05,
      "loss": 1.8822,
      "step": 6200
    },
    {
      "epoch": 1.0896648534830673,
      "grad_norm": 10.009117126464844,
      "learning_rate": 3.184476808796865e-05,
      "loss": 1.6057,
      "step": 6210
    },
    {
      "epoch": 1.091419547288998,
      "grad_norm": 8.822490692138672,
      "learning_rate": 3.1815523191203136e-05,
      "loss": 1.6978,
      "step": 6220
    },
    {
      "epoch": 1.093174241094929,
      "grad_norm": 9.610496520996094,
      "learning_rate": 3.178627829443762e-05,
      "loss": 1.5532,
      "step": 6230
    },
    {
      "epoch": 1.0949289349008597,
      "grad_norm": 7.343064785003662,
      "learning_rate": 3.175703339767211e-05,
      "loss": 1.6506,
      "step": 6240
    },
    {
      "epoch": 1.0966836287067907,
      "grad_norm": 7.43113899230957,
      "learning_rate": 3.172778850090659e-05,
      "loss": 1.6157,
      "step": 6250
    },
    {
      "epoch": 1.0984383225127214,
      "grad_norm": 8.718634605407715,
      "learning_rate": 3.169854360414108e-05,
      "loss": 1.5427,
      "step": 6260
    },
    {
      "epoch": 1.1001930163186524,
      "grad_norm": 8.68769645690918,
      "learning_rate": 3.1669298707375564e-05,
      "loss": 1.7285,
      "step": 6270
    },
    {
      "epoch": 1.1019477101245831,
      "grad_norm": 9.794647216796875,
      "learning_rate": 3.164005381061005e-05,
      "loss": 1.7321,
      "step": 6280
    },
    {
      "epoch": 1.103702403930514,
      "grad_norm": 8.584277153015137,
      "learning_rate": 3.1610808913844535e-05,
      "loss": 1.9216,
      "step": 6290
    },
    {
      "epoch": 1.105457097736445,
      "grad_norm": 10.673543930053711,
      "learning_rate": 3.158156401707902e-05,
      "loss": 1.8431,
      "step": 6300
    },
    {
      "epoch": 1.1072117915423758,
      "grad_norm": 9.802762985229492,
      "learning_rate": 3.1552319120313507e-05,
      "loss": 1.9973,
      "step": 6310
    },
    {
      "epoch": 1.1089664853483068,
      "grad_norm": 9.961932182312012,
      "learning_rate": 3.152307422354799e-05,
      "loss": 1.8769,
      "step": 6320
    },
    {
      "epoch": 1.1107211791542375,
      "grad_norm": 8.648237228393555,
      "learning_rate": 3.149382932678248e-05,
      "loss": 1.8879,
      "step": 6330
    },
    {
      "epoch": 1.1124758729601685,
      "grad_norm": 8.469182014465332,
      "learning_rate": 3.1464584430016963e-05,
      "loss": 1.8739,
      "step": 6340
    },
    {
      "epoch": 1.1142305667660992,
      "grad_norm": 7.923088550567627,
      "learning_rate": 3.143533953325145e-05,
      "loss": 1.8853,
      "step": 6350
    },
    {
      "epoch": 1.1159852605720302,
      "grad_norm": 9.771221160888672,
      "learning_rate": 3.1406094636485935e-05,
      "loss": 1.8245,
      "step": 6360
    },
    {
      "epoch": 1.1177399543779611,
      "grad_norm": 8.228809356689453,
      "learning_rate": 3.137684973972042e-05,
      "loss": 1.8134,
      "step": 6370
    },
    {
      "epoch": 1.1194946481838919,
      "grad_norm": 10.106749534606934,
      "learning_rate": 3.1347604842954906e-05,
      "loss": 1.7577,
      "step": 6380
    },
    {
      "epoch": 1.1212493419898228,
      "grad_norm": 7.976061820983887,
      "learning_rate": 3.131835994618939e-05,
      "loss": 1.7765,
      "step": 6390
    },
    {
      "epoch": 1.1230040357957536,
      "grad_norm": 9.074728965759277,
      "learning_rate": 3.128911504942388e-05,
      "loss": 1.7068,
      "step": 6400
    },
    {
      "epoch": 1.1247587296016845,
      "grad_norm": 10.404802322387695,
      "learning_rate": 3.125987015265836e-05,
      "loss": 1.8939,
      "step": 6410
    },
    {
      "epoch": 1.1265134234076153,
      "grad_norm": 7.771280288696289,
      "learning_rate": 3.123062525589285e-05,
      "loss": 1.685,
      "step": 6420
    },
    {
      "epoch": 1.1282681172135463,
      "grad_norm": 9.409527778625488,
      "learning_rate": 3.1201380359127334e-05,
      "loss": 1.8416,
      "step": 6430
    },
    {
      "epoch": 1.1300228110194772,
      "grad_norm": 10.04856014251709,
      "learning_rate": 3.117213546236182e-05,
      "loss": 1.7011,
      "step": 6440
    },
    {
      "epoch": 1.131777504825408,
      "grad_norm": 9.16362476348877,
      "learning_rate": 3.1142890565596305e-05,
      "loss": 1.7477,
      "step": 6450
    },
    {
      "epoch": 1.133532198631339,
      "grad_norm": 10.357656478881836,
      "learning_rate": 3.111364566883079e-05,
      "loss": 1.802,
      "step": 6460
    },
    {
      "epoch": 1.1352868924372697,
      "grad_norm": 7.693580150604248,
      "learning_rate": 3.1084400772065276e-05,
      "loss": 1.6748,
      "step": 6470
    },
    {
      "epoch": 1.1370415862432006,
      "grad_norm": 10.94958209991455,
      "learning_rate": 3.105515587529976e-05,
      "loss": 1.6872,
      "step": 6480
    },
    {
      "epoch": 1.1387962800491314,
      "grad_norm": 8.25607681274414,
      "learning_rate": 3.102591097853425e-05,
      "loss": 1.7134,
      "step": 6490
    },
    {
      "epoch": 1.1405509738550623,
      "grad_norm": 8.643848419189453,
      "learning_rate": 3.099666608176873e-05,
      "loss": 1.9589,
      "step": 6500
    },
    {
      "epoch": 1.142305667660993,
      "grad_norm": 8.658177375793457,
      "learning_rate": 3.096742118500322e-05,
      "loss": 1.7392,
      "step": 6510
    },
    {
      "epoch": 1.144060361466924,
      "grad_norm": 9.679213523864746,
      "learning_rate": 3.0938176288237705e-05,
      "loss": 1.7972,
      "step": 6520
    },
    {
      "epoch": 1.145815055272855,
      "grad_norm": 7.389594078063965,
      "learning_rate": 3.090893139147219e-05,
      "loss": 1.725,
      "step": 6530
    },
    {
      "epoch": 1.1475697490787857,
      "grad_norm": 11.0184907913208,
      "learning_rate": 3.0879686494706676e-05,
      "loss": 1.6453,
      "step": 6540
    },
    {
      "epoch": 1.1493244428847167,
      "grad_norm": 7.52233362197876,
      "learning_rate": 3.085044159794116e-05,
      "loss": 1.7077,
      "step": 6550
    },
    {
      "epoch": 1.1510791366906474,
      "grad_norm": 7.710406303405762,
      "learning_rate": 3.082119670117565e-05,
      "loss": 1.6063,
      "step": 6560
    },
    {
      "epoch": 1.1528338304965784,
      "grad_norm": 10.384559631347656,
      "learning_rate": 3.079195180441013e-05,
      "loss": 1.5813,
      "step": 6570
    },
    {
      "epoch": 1.1545885243025091,
      "grad_norm": 9.093302726745605,
      "learning_rate": 3.076270690764462e-05,
      "loss": 1.9556,
      "step": 6580
    },
    {
      "epoch": 1.15634321810844,
      "grad_norm": 8.955965995788574,
      "learning_rate": 3.0733462010879104e-05,
      "loss": 1.56,
      "step": 6590
    },
    {
      "epoch": 1.1580979119143708,
      "grad_norm": 8.946950912475586,
      "learning_rate": 3.070421711411359e-05,
      "loss": 1.5284,
      "step": 6600
    },
    {
      "epoch": 1.1598526057203018,
      "grad_norm": 8.785787582397461,
      "learning_rate": 3.0674972217348075e-05,
      "loss": 1.842,
      "step": 6610
    },
    {
      "epoch": 1.1616072995262328,
      "grad_norm": 11.238736152648926,
      "learning_rate": 3.064572732058256e-05,
      "loss": 1.8992,
      "step": 6620
    },
    {
      "epoch": 1.1633619933321635,
      "grad_norm": 10.583873748779297,
      "learning_rate": 3.0616482423817046e-05,
      "loss": 1.7797,
      "step": 6630
    },
    {
      "epoch": 1.1651166871380945,
      "grad_norm": 9.083460807800293,
      "learning_rate": 3.058723752705153e-05,
      "loss": 1.5832,
      "step": 6640
    },
    {
      "epoch": 1.1668713809440252,
      "grad_norm": 8.073328971862793,
      "learning_rate": 3.055799263028601e-05,
      "loss": 1.7454,
      "step": 6650
    },
    {
      "epoch": 1.1686260747499562,
      "grad_norm": 8.709851264953613,
      "learning_rate": 3.05287477335205e-05,
      "loss": 1.6503,
      "step": 6660
    },
    {
      "epoch": 1.170380768555887,
      "grad_norm": 7.364203929901123,
      "learning_rate": 3.049950283675499e-05,
      "loss": 1.7396,
      "step": 6670
    },
    {
      "epoch": 1.1721354623618179,
      "grad_norm": 10.148611068725586,
      "learning_rate": 3.047025793998947e-05,
      "loss": 1.8669,
      "step": 6680
    },
    {
      "epoch": 1.1738901561677486,
      "grad_norm": 9.653757095336914,
      "learning_rate": 3.044101304322396e-05,
      "loss": 1.8765,
      "step": 6690
    },
    {
      "epoch": 1.1756448499736796,
      "grad_norm": 8.834710121154785,
      "learning_rate": 3.0411768146458446e-05,
      "loss": 1.8684,
      "step": 6700
    },
    {
      "epoch": 1.1773995437796105,
      "grad_norm": 7.787805080413818,
      "learning_rate": 3.0382523249692928e-05,
      "loss": 2.0061,
      "step": 6710
    },
    {
      "epoch": 1.1791542375855413,
      "grad_norm": 5.6243205070495605,
      "learning_rate": 3.0353278352927417e-05,
      "loss": 1.7494,
      "step": 6720
    },
    {
      "epoch": 1.1809089313914722,
      "grad_norm": 11.258072853088379,
      "learning_rate": 3.03240334561619e-05,
      "loss": 1.9429,
      "step": 6730
    },
    {
      "epoch": 1.182663625197403,
      "grad_norm": 9.099547386169434,
      "learning_rate": 3.0294788559396385e-05,
      "loss": 1.7671,
      "step": 6740
    },
    {
      "epoch": 1.184418319003334,
      "grad_norm": 7.996392726898193,
      "learning_rate": 3.0265543662630874e-05,
      "loss": 1.8829,
      "step": 6750
    },
    {
      "epoch": 1.186173012809265,
      "grad_norm": 10.43167495727539,
      "learning_rate": 3.0236298765865356e-05,
      "loss": 1.7302,
      "step": 6760
    },
    {
      "epoch": 1.1879277066151956,
      "grad_norm": 10.134807586669922,
      "learning_rate": 3.0207053869099845e-05,
      "loss": 1.9574,
      "step": 6770
    },
    {
      "epoch": 1.1896824004211266,
      "grad_norm": 9.010648727416992,
      "learning_rate": 3.017780897233433e-05,
      "loss": 1.984,
      "step": 6780
    },
    {
      "epoch": 1.1914370942270573,
      "grad_norm": 8.866518020629883,
      "learning_rate": 3.0148564075568813e-05,
      "loss": 1.4935,
      "step": 6790
    },
    {
      "epoch": 1.1931917880329883,
      "grad_norm": 6.79018497467041,
      "learning_rate": 3.0119319178803302e-05,
      "loss": 1.5566,
      "step": 6800
    },
    {
      "epoch": 1.194946481838919,
      "grad_norm": 7.736849784851074,
      "learning_rate": 3.0090074282037784e-05,
      "loss": 1.6834,
      "step": 6810
    },
    {
      "epoch": 1.19670117564485,
      "grad_norm": 10.817789077758789,
      "learning_rate": 3.006082938527227e-05,
      "loss": 1.7924,
      "step": 6820
    },
    {
      "epoch": 1.1984558694507808,
      "grad_norm": 9.795439720153809,
      "learning_rate": 3.003158448850676e-05,
      "loss": 1.8952,
      "step": 6830
    },
    {
      "epoch": 1.2002105632567117,
      "grad_norm": 9.333942413330078,
      "learning_rate": 3.000233959174124e-05,
      "loss": 1.6153,
      "step": 6840
    },
    {
      "epoch": 1.2019652570626427,
      "grad_norm": 8.418233871459961,
      "learning_rate": 2.997309469497573e-05,
      "loss": 1.7708,
      "step": 6850
    },
    {
      "epoch": 1.2037199508685734,
      "grad_norm": 7.970020294189453,
      "learning_rate": 2.9943849798210216e-05,
      "loss": 1.6198,
      "step": 6860
    },
    {
      "epoch": 1.2054746446745044,
      "grad_norm": 6.335177421569824,
      "learning_rate": 2.9914604901444698e-05,
      "loss": 1.7488,
      "step": 6870
    },
    {
      "epoch": 1.2072293384804351,
      "grad_norm": 9.580672264099121,
      "learning_rate": 2.9885360004679187e-05,
      "loss": 1.8538,
      "step": 6880
    },
    {
      "epoch": 1.208984032286366,
      "grad_norm": 7.314029216766357,
      "learning_rate": 2.985611510791367e-05,
      "loss": 1.5831,
      "step": 6890
    },
    {
      "epoch": 1.2107387260922968,
      "grad_norm": 10.146184921264648,
      "learning_rate": 2.9826870211148155e-05,
      "loss": 1.7408,
      "step": 6900
    },
    {
      "epoch": 1.2124934198982278,
      "grad_norm": 9.025787353515625,
      "learning_rate": 2.9797625314382644e-05,
      "loss": 2.0214,
      "step": 6910
    },
    {
      "epoch": 1.2142481137041585,
      "grad_norm": 8.769129753112793,
      "learning_rate": 2.9768380417617126e-05,
      "loss": 1.5578,
      "step": 6920
    },
    {
      "epoch": 1.2160028075100895,
      "grad_norm": 9.492715835571289,
      "learning_rate": 2.973913552085161e-05,
      "loss": 1.8604,
      "step": 6930
    },
    {
      "epoch": 1.2177575013160205,
      "grad_norm": 8.44747257232666,
      "learning_rate": 2.97098906240861e-05,
      "loss": 1.7648,
      "step": 6940
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 8.929800987243652,
      "learning_rate": 2.9680645727320583e-05,
      "loss": 1.6058,
      "step": 6950
    },
    {
      "epoch": 1.2212668889278822,
      "grad_norm": 9.081157684326172,
      "learning_rate": 2.9651400830555072e-05,
      "loss": 1.6045,
      "step": 6960
    },
    {
      "epoch": 1.223021582733813,
      "grad_norm": 10.00325870513916,
      "learning_rate": 2.9622155933789554e-05,
      "loss": 1.6884,
      "step": 6970
    },
    {
      "epoch": 1.2247762765397439,
      "grad_norm": 8.778572082519531,
      "learning_rate": 2.959291103702404e-05,
      "loss": 1.9258,
      "step": 6980
    },
    {
      "epoch": 1.2265309703456746,
      "grad_norm": 10.334219932556152,
      "learning_rate": 2.956366614025853e-05,
      "loss": 1.9663,
      "step": 6990
    },
    {
      "epoch": 1.2282856641516056,
      "grad_norm": 9.400708198547363,
      "learning_rate": 2.953442124349301e-05,
      "loss": 1.6381,
      "step": 7000
    },
    {
      "epoch": 1.2300403579575363,
      "grad_norm": 8.548462867736816,
      "learning_rate": 2.9505176346727497e-05,
      "loss": 1.9062,
      "step": 7010
    },
    {
      "epoch": 1.2317950517634673,
      "grad_norm": 7.876923084259033,
      "learning_rate": 2.9475931449961986e-05,
      "loss": 1.5617,
      "step": 7020
    },
    {
      "epoch": 1.2335497455693982,
      "grad_norm": 8.503175735473633,
      "learning_rate": 2.9446686553196468e-05,
      "loss": 1.7086,
      "step": 7030
    },
    {
      "epoch": 1.235304439375329,
      "grad_norm": 9.338122367858887,
      "learning_rate": 2.9417441656430954e-05,
      "loss": 1.734,
      "step": 7040
    },
    {
      "epoch": 1.23705913318126,
      "grad_norm": 8.164891242980957,
      "learning_rate": 2.9388196759665436e-05,
      "loss": 1.641,
      "step": 7050
    },
    {
      "epoch": 1.2388138269871907,
      "grad_norm": 9.807415962219238,
      "learning_rate": 2.9358951862899925e-05,
      "loss": 1.91,
      "step": 7060
    },
    {
      "epoch": 1.2405685207931216,
      "grad_norm": 9.59743881225586,
      "learning_rate": 2.9329706966134414e-05,
      "loss": 1.3833,
      "step": 7070
    },
    {
      "epoch": 1.2423232145990524,
      "grad_norm": 6.057799816131592,
      "learning_rate": 2.9300462069368896e-05,
      "loss": 1.3199,
      "step": 7080
    },
    {
      "epoch": 1.2440779084049833,
      "grad_norm": 9.19846248626709,
      "learning_rate": 2.927121717260338e-05,
      "loss": 1.6741,
      "step": 7090
    },
    {
      "epoch": 1.245832602210914,
      "grad_norm": 10.505202293395996,
      "learning_rate": 2.924197227583787e-05,
      "loss": 1.6852,
      "step": 7100
    },
    {
      "epoch": 1.247587296016845,
      "grad_norm": 8.844350814819336,
      "learning_rate": 2.9212727379072353e-05,
      "loss": 1.8322,
      "step": 7110
    },
    {
      "epoch": 1.249341989822776,
      "grad_norm": 9.805160522460938,
      "learning_rate": 2.918348248230684e-05,
      "loss": 1.9628,
      "step": 7120
    },
    {
      "epoch": 1.2510966836287067,
      "grad_norm": 8.073349952697754,
      "learning_rate": 2.915423758554132e-05,
      "loss": 1.6497,
      "step": 7130
    },
    {
      "epoch": 1.2528513774346377,
      "grad_norm": 7.558063983917236,
      "learning_rate": 2.912499268877581e-05,
      "loss": 1.6907,
      "step": 7140
    },
    {
      "epoch": 1.2546060712405684,
      "grad_norm": 7.778947353363037,
      "learning_rate": 2.90957477920103e-05,
      "loss": 1.6266,
      "step": 7150
    },
    {
      "epoch": 1.2563607650464994,
      "grad_norm": 9.794005393981934,
      "learning_rate": 2.906650289524478e-05,
      "loss": 1.6577,
      "step": 7160
    },
    {
      "epoch": 1.2581154588524304,
      "grad_norm": 10.705954551696777,
      "learning_rate": 2.9037257998479267e-05,
      "loss": 1.7755,
      "step": 7170
    },
    {
      "epoch": 1.259870152658361,
      "grad_norm": 9.236709594726562,
      "learning_rate": 2.9008013101713756e-05,
      "loss": 1.7936,
      "step": 7180
    },
    {
      "epoch": 1.2616248464642918,
      "grad_norm": 7.400062561035156,
      "learning_rate": 2.8978768204948238e-05,
      "loss": 1.5441,
      "step": 7190
    },
    {
      "epoch": 1.2633795402702228,
      "grad_norm": 11.0730619430542,
      "learning_rate": 2.8949523308182723e-05,
      "loss": 1.8094,
      "step": 7200
    },
    {
      "epoch": 1.2651342340761538,
      "grad_norm": 7.812792778015137,
      "learning_rate": 2.8920278411417206e-05,
      "loss": 1.5102,
      "step": 7210
    },
    {
      "epoch": 1.2668889278820845,
      "grad_norm": 9.510477066040039,
      "learning_rate": 2.8891033514651695e-05,
      "loss": 1.6361,
      "step": 7220
    },
    {
      "epoch": 1.2686436216880155,
      "grad_norm": 8.66109561920166,
      "learning_rate": 2.886178861788618e-05,
      "loss": 1.6521,
      "step": 7230
    },
    {
      "epoch": 1.2703983154939462,
      "grad_norm": 9.716385841369629,
      "learning_rate": 2.8832543721120663e-05,
      "loss": 1.5732,
      "step": 7240
    },
    {
      "epoch": 1.2721530092998772,
      "grad_norm": 9.044893264770508,
      "learning_rate": 2.880329882435515e-05,
      "loss": 1.7252,
      "step": 7250
    },
    {
      "epoch": 1.2739077031058081,
      "grad_norm": 10.818069458007812,
      "learning_rate": 2.877405392758964e-05,
      "loss": 1.6322,
      "step": 7260
    },
    {
      "epoch": 1.2756623969117389,
      "grad_norm": 8.419571876525879,
      "learning_rate": 2.8744809030824123e-05,
      "loss": 1.6732,
      "step": 7270
    },
    {
      "epoch": 1.2774170907176698,
      "grad_norm": 9.246024131774902,
      "learning_rate": 2.871556413405861e-05,
      "loss": 1.7683,
      "step": 7280
    },
    {
      "epoch": 1.2791717845236006,
      "grad_norm": 5.3566412925720215,
      "learning_rate": 2.868631923729309e-05,
      "loss": 1.6297,
      "step": 7290
    },
    {
      "epoch": 1.2809264783295315,
      "grad_norm": 8.697810173034668,
      "learning_rate": 2.865707434052758e-05,
      "loss": 1.8431,
      "step": 7300
    },
    {
      "epoch": 1.2826811721354623,
      "grad_norm": 8.307839393615723,
      "learning_rate": 2.8627829443762065e-05,
      "loss": 1.8353,
      "step": 7310
    },
    {
      "epoch": 1.2844358659413933,
      "grad_norm": 9.232621192932129,
      "learning_rate": 2.8598584546996548e-05,
      "loss": 1.706,
      "step": 7320
    },
    {
      "epoch": 1.286190559747324,
      "grad_norm": 8.493595123291016,
      "learning_rate": 2.8569339650231037e-05,
      "loss": 1.8706,
      "step": 7330
    },
    {
      "epoch": 1.287945253553255,
      "grad_norm": 8.160470008850098,
      "learning_rate": 2.8540094753465526e-05,
      "loss": 1.6274,
      "step": 7340
    },
    {
      "epoch": 1.289699947359186,
      "grad_norm": Infinity,
      "learning_rate": 2.851377434637656e-05,
      "loss": 1.6425,
      "step": 7350
    },
    {
      "epoch": 1.2914546411651167,
      "grad_norm": 9.837879180908203,
      "learning_rate": 2.8484529449611042e-05,
      "loss": 1.7214,
      "step": 7360
    },
    {
      "epoch": 1.2932093349710476,
      "grad_norm": 8.076955795288086,
      "learning_rate": 2.8455284552845528e-05,
      "loss": 1.5288,
      "step": 7370
    },
    {
      "epoch": 1.2949640287769784,
      "grad_norm": 9.641108512878418,
      "learning_rate": 2.8426039656080017e-05,
      "loss": 1.6722,
      "step": 7380
    },
    {
      "epoch": 1.2967187225829093,
      "grad_norm": 6.541688442230225,
      "learning_rate": 2.83967947593145e-05,
      "loss": 1.5408,
      "step": 7390
    },
    {
      "epoch": 1.2984734163888403,
      "grad_norm": 7.893185138702393,
      "learning_rate": 2.8367549862548988e-05,
      "loss": 1.7239,
      "step": 7400
    },
    {
      "epoch": 1.300228110194771,
      "grad_norm": 9.529661178588867,
      "learning_rate": 2.8338304965783474e-05,
      "loss": 1.8562,
      "step": 7410
    },
    {
      "epoch": 1.3019828040007018,
      "grad_norm": 5.855713844299316,
      "learning_rate": 2.8309060069017956e-05,
      "loss": 1.6458,
      "step": 7420
    },
    {
      "epoch": 1.3037374978066327,
      "grad_norm": 8.965156555175781,
      "learning_rate": 2.8279815172252445e-05,
      "loss": 1.8485,
      "step": 7430
    },
    {
      "epoch": 1.3054921916125637,
      "grad_norm": 9.063048362731934,
      "learning_rate": 2.8250570275486927e-05,
      "loss": 1.7558,
      "step": 7440
    },
    {
      "epoch": 1.3072468854184944,
      "grad_norm": 11.345623970031738,
      "learning_rate": 2.8221325378721413e-05,
      "loss": 1.7983,
      "step": 7450
    },
    {
      "epoch": 1.3090015792244254,
      "grad_norm": 7.812575340270996,
      "learning_rate": 2.8192080481955902e-05,
      "loss": 1.9232,
      "step": 7460
    },
    {
      "epoch": 1.3107562730303561,
      "grad_norm": 9.787243843078613,
      "learning_rate": 2.8162835585190384e-05,
      "loss": 1.8374,
      "step": 7470
    },
    {
      "epoch": 1.312510966836287,
      "grad_norm": 8.056583404541016,
      "learning_rate": 2.813359068842487e-05,
      "loss": 1.9285,
      "step": 7480
    },
    {
      "epoch": 1.314265660642218,
      "grad_norm": 7.352668762207031,
      "learning_rate": 2.810434579165936e-05,
      "loss": 1.7249,
      "step": 7490
    },
    {
      "epoch": 1.3160203544481488,
      "grad_norm": 8.884373664855957,
      "learning_rate": 2.807510089489384e-05,
      "loss": 2.0483,
      "step": 7500
    },
    {
      "epoch": 1.3177750482540795,
      "grad_norm": 7.988064289093018,
      "learning_rate": 2.804585599812833e-05,
      "loss": 1.6103,
      "step": 7510
    },
    {
      "epoch": 1.3195297420600105,
      "grad_norm": 11.724400520324707,
      "learning_rate": 2.8016611101362815e-05,
      "loss": 1.9311,
      "step": 7520
    },
    {
      "epoch": 1.3212844358659415,
      "grad_norm": 7.668720722198486,
      "learning_rate": 2.7987366204597298e-05,
      "loss": 1.8399,
      "step": 7530
    },
    {
      "epoch": 1.3230391296718722,
      "grad_norm": 6.998075008392334,
      "learning_rate": 2.7958121307831787e-05,
      "loss": 1.8411,
      "step": 7540
    },
    {
      "epoch": 1.3247938234778032,
      "grad_norm": 9.115072250366211,
      "learning_rate": 2.792887641106627e-05,
      "loss": 1.7093,
      "step": 7550
    },
    {
      "epoch": 1.326548517283734,
      "grad_norm": 8.255778312683105,
      "learning_rate": 2.7899631514300755e-05,
      "loss": 1.5757,
      "step": 7560
    },
    {
      "epoch": 1.3283032110896649,
      "grad_norm": 8.363343238830566,
      "learning_rate": 2.7870386617535244e-05,
      "loss": 1.6364,
      "step": 7570
    },
    {
      "epoch": 1.3300579048955958,
      "grad_norm": 11.92686653137207,
      "learning_rate": 2.7841141720769726e-05,
      "loss": 1.5907,
      "step": 7580
    },
    {
      "epoch": 1.3318125987015266,
      "grad_norm": 8.730456352233887,
      "learning_rate": 2.7811896824004215e-05,
      "loss": 1.8465,
      "step": 7590
    },
    {
      "epoch": 1.3335672925074573,
      "grad_norm": 10.95567512512207,
      "learning_rate": 2.77826519272387e-05,
      "loss": 1.7253,
      "step": 7600
    },
    {
      "epoch": 1.3353219863133883,
      "grad_norm": 10.513795852661133,
      "learning_rate": 2.7753407030473183e-05,
      "loss": 1.7654,
      "step": 7610
    },
    {
      "epoch": 1.3370766801193192,
      "grad_norm": 10.743880271911621,
      "learning_rate": 2.772416213370767e-05,
      "loss": 1.9529,
      "step": 7620
    },
    {
      "epoch": 1.33883137392525,
      "grad_norm": 10.835306167602539,
      "learning_rate": 2.7694917236942154e-05,
      "loss": 1.7299,
      "step": 7630
    },
    {
      "epoch": 1.340586067731181,
      "grad_norm": 8.39891529083252,
      "learning_rate": 2.766567234017664e-05,
      "loss": 1.8059,
      "step": 7640
    },
    {
      "epoch": 1.3423407615371117,
      "grad_norm": 8.831369400024414,
      "learning_rate": 2.763642744341113e-05,
      "loss": 1.7349,
      "step": 7650
    },
    {
      "epoch": 1.3440954553430426,
      "grad_norm": 11.345325469970703,
      "learning_rate": 2.760718254664561e-05,
      "loss": 1.7039,
      "step": 7660
    },
    {
      "epoch": 1.3458501491489736,
      "grad_norm": 11.083179473876953,
      "learning_rate": 2.7577937649880096e-05,
      "loss": 2.0101,
      "step": 7670
    },
    {
      "epoch": 1.3476048429549043,
      "grad_norm": 8.303655624389648,
      "learning_rate": 2.7548692753114585e-05,
      "loss": 1.7637,
      "step": 7680
    },
    {
      "epoch": 1.3493595367608353,
      "grad_norm": 7.8742876052856445,
      "learning_rate": 2.7519447856349068e-05,
      "loss": 1.496,
      "step": 7690
    },
    {
      "epoch": 1.351114230566766,
      "grad_norm": 9.215046882629395,
      "learning_rate": 2.7490202959583557e-05,
      "loss": 1.7233,
      "step": 7700
    },
    {
      "epoch": 1.352868924372697,
      "grad_norm": 8.010089874267578,
      "learning_rate": 2.746095806281804e-05,
      "loss": 1.9825,
      "step": 7710
    },
    {
      "epoch": 1.3546236181786278,
      "grad_norm": 6.503458023071289,
      "learning_rate": 2.7431713166052524e-05,
      "loss": 1.8821,
      "step": 7720
    },
    {
      "epoch": 1.3563783119845587,
      "grad_norm": 8.4673490524292,
      "learning_rate": 2.7402468269287014e-05,
      "loss": 2.1432,
      "step": 7730
    },
    {
      "epoch": 1.3581330057904895,
      "grad_norm": 10.734889030456543,
      "learning_rate": 2.7373223372521496e-05,
      "loss": 1.7611,
      "step": 7740
    },
    {
      "epoch": 1.3598876995964204,
      "grad_norm": 7.314704418182373,
      "learning_rate": 2.734397847575598e-05,
      "loss": 1.6489,
      "step": 7750
    },
    {
      "epoch": 1.3616423934023514,
      "grad_norm": 9.9847993850708,
      "learning_rate": 2.731473357899047e-05,
      "loss": 1.74,
      "step": 7760
    },
    {
      "epoch": 1.3633970872082821,
      "grad_norm": 9.660277366638184,
      "learning_rate": 2.7285488682224953e-05,
      "loss": 1.8454,
      "step": 7770
    },
    {
      "epoch": 1.365151781014213,
      "grad_norm": 8.808096885681152,
      "learning_rate": 2.7256243785459438e-05,
      "loss": 1.4693,
      "step": 7780
    },
    {
      "epoch": 1.3669064748201438,
      "grad_norm": 10.398080825805664,
      "learning_rate": 2.722699888869392e-05,
      "loss": 1.6511,
      "step": 7790
    },
    {
      "epoch": 1.3686611686260748,
      "grad_norm": 10.630614280700684,
      "learning_rate": 2.719775399192841e-05,
      "loss": 1.9197,
      "step": 7800
    },
    {
      "epoch": 1.3704158624320057,
      "grad_norm": 9.886665344238281,
      "learning_rate": 2.71685090951629e-05,
      "loss": 1.7977,
      "step": 7810
    },
    {
      "epoch": 1.3721705562379365,
      "grad_norm": 10.77206039428711,
      "learning_rate": 2.713926419839738e-05,
      "loss": 1.6641,
      "step": 7820
    },
    {
      "epoch": 1.3739252500438672,
      "grad_norm": 7.08008337020874,
      "learning_rate": 2.7110019301631866e-05,
      "loss": 1.6816,
      "step": 7830
    },
    {
      "epoch": 1.3756799438497982,
      "grad_norm": 12.057748794555664,
      "learning_rate": 2.7080774404866355e-05,
      "loss": 1.6272,
      "step": 7840
    },
    {
      "epoch": 1.3774346376557292,
      "grad_norm": 10.110061645507812,
      "learning_rate": 2.7051529508100838e-05,
      "loss": 1.5588,
      "step": 7850
    },
    {
      "epoch": 1.37918933146166,
      "grad_norm": 8.511232376098633,
      "learning_rate": 2.7022284611335323e-05,
      "loss": 1.749,
      "step": 7860
    },
    {
      "epoch": 1.3809440252675909,
      "grad_norm": 9.853191375732422,
      "learning_rate": 2.6993039714569805e-05,
      "loss": 1.7024,
      "step": 7870
    },
    {
      "epoch": 1.3826987190735216,
      "grad_norm": 10.054722785949707,
      "learning_rate": 2.6963794817804294e-05,
      "loss": 1.7604,
      "step": 7880
    },
    {
      "epoch": 1.3844534128794526,
      "grad_norm": 9.440686225891113,
      "learning_rate": 2.6934549921038783e-05,
      "loss": 1.6281,
      "step": 7890
    },
    {
      "epoch": 1.3862081066853835,
      "grad_norm": 9.13852596282959,
      "learning_rate": 2.6905305024273266e-05,
      "loss": 1.6867,
      "step": 7900
    },
    {
      "epoch": 1.3879628004913143,
      "grad_norm": 6.708744049072266,
      "learning_rate": 2.687606012750775e-05,
      "loss": 1.6898,
      "step": 7910
    },
    {
      "epoch": 1.389717494297245,
      "grad_norm": 6.787959575653076,
      "learning_rate": 2.684681523074224e-05,
      "loss": 1.6548,
      "step": 7920
    },
    {
      "epoch": 1.391472188103176,
      "grad_norm": 6.456562519073486,
      "learning_rate": 2.6817570333976723e-05,
      "loss": 1.5844,
      "step": 7930
    },
    {
      "epoch": 1.393226881909107,
      "grad_norm": 7.806210994720459,
      "learning_rate": 2.6788325437211208e-05,
      "loss": 1.6183,
      "step": 7940
    },
    {
      "epoch": 1.3949815757150377,
      "grad_norm": 11.16041374206543,
      "learning_rate": 2.675908054044569e-05,
      "loss": 1.5564,
      "step": 7950
    },
    {
      "epoch": 1.3967362695209686,
      "grad_norm": 8.879746437072754,
      "learning_rate": 2.672983564368018e-05,
      "loss": 1.7355,
      "step": 7960
    },
    {
      "epoch": 1.3984909633268994,
      "grad_norm": 8.556135177612305,
      "learning_rate": 2.6700590746914665e-05,
      "loss": 1.5725,
      "step": 7970
    },
    {
      "epoch": 1.4002456571328303,
      "grad_norm": 8.949925422668457,
      "learning_rate": 2.6671345850149147e-05,
      "loss": 1.5796,
      "step": 7980
    },
    {
      "epoch": 1.4020003509387613,
      "grad_norm": 11.141033172607422,
      "learning_rate": 2.6642100953383636e-05,
      "loss": 1.999,
      "step": 7990
    },
    {
      "epoch": 1.403755044744692,
      "grad_norm": 7.562778949737549,
      "learning_rate": 2.6612856056618125e-05,
      "loss": 1.727,
      "step": 8000
    },
    {
      "epoch": 1.4055097385506228,
      "grad_norm": 9.845789909362793,
      "learning_rate": 2.6583611159852608e-05,
      "loss": 1.7445,
      "step": 8010
    },
    {
      "epoch": 1.4072644323565537,
      "grad_norm": 7.778336048126221,
      "learning_rate": 2.6554366263087093e-05,
      "loss": 1.7477,
      "step": 8020
    },
    {
      "epoch": 1.4090191261624847,
      "grad_norm": 7.62336540222168,
      "learning_rate": 2.6525121366321575e-05,
      "loss": 1.9201,
      "step": 8030
    },
    {
      "epoch": 1.4107738199684154,
      "grad_norm": 12.513190269470215,
      "learning_rate": 2.6495876469556064e-05,
      "loss": 1.9592,
      "step": 8040
    },
    {
      "epoch": 1.4125285137743464,
      "grad_norm": 7.8113274574279785,
      "learning_rate": 2.646663157279055e-05,
      "loss": 1.6064,
      "step": 8050
    },
    {
      "epoch": 1.4142832075802771,
      "grad_norm": 7.053623199462891,
      "learning_rate": 2.6437386676025032e-05,
      "loss": 1.5228,
      "step": 8060
    },
    {
      "epoch": 1.416037901386208,
      "grad_norm": 10.198701858520508,
      "learning_rate": 2.640814177925952e-05,
      "loss": 1.8408,
      "step": 8070
    },
    {
      "epoch": 1.417792595192139,
      "grad_norm": 8.429994583129883,
      "learning_rate": 2.637889688249401e-05,
      "loss": 1.7553,
      "step": 8080
    },
    {
      "epoch": 1.4195472889980698,
      "grad_norm": 5.931032180786133,
      "learning_rate": 2.634965198572849e-05,
      "loss": 1.8604,
      "step": 8090
    },
    {
      "epoch": 1.4213019828040008,
      "grad_norm": 9.651806831359863,
      "learning_rate": 2.6320407088962978e-05,
      "loss": 1.6459,
      "step": 8100
    },
    {
      "epoch": 1.4230566766099315,
      "grad_norm": 8.70075798034668,
      "learning_rate": 2.629116219219746e-05,
      "loss": 1.5604,
      "step": 8110
    },
    {
      "epoch": 1.4248113704158625,
      "grad_norm": 7.402041912078857,
      "learning_rate": 2.626191729543195e-05,
      "loss": 1.6977,
      "step": 8120
    },
    {
      "epoch": 1.4265660642217932,
      "grad_norm": 11.06889533996582,
      "learning_rate": 2.6232672398666435e-05,
      "loss": 1.4636,
      "step": 8130
    },
    {
      "epoch": 1.4283207580277242,
      "grad_norm": 11.056556701660156,
      "learning_rate": 2.6203427501900917e-05,
      "loss": 1.7583,
      "step": 8140
    },
    {
      "epoch": 1.430075451833655,
      "grad_norm": 7.310647964477539,
      "learning_rate": 2.6174182605135406e-05,
      "loss": 1.7433,
      "step": 8150
    },
    {
      "epoch": 1.4318301456395859,
      "grad_norm": 11.416921615600586,
      "learning_rate": 2.6144937708369892e-05,
      "loss": 1.9921,
      "step": 8160
    },
    {
      "epoch": 1.4335848394455168,
      "grad_norm": 6.874154567718506,
      "learning_rate": 2.6115692811604374e-05,
      "loss": 1.5991,
      "step": 8170
    },
    {
      "epoch": 1.4353395332514476,
      "grad_norm": 8.701770782470703,
      "learning_rate": 2.6086447914838863e-05,
      "loss": 1.7075,
      "step": 8180
    },
    {
      "epoch": 1.4370942270573785,
      "grad_norm": 9.65625286102295,
      "learning_rate": 2.6057203018073345e-05,
      "loss": 1.936,
      "step": 8190
    },
    {
      "epoch": 1.4388489208633093,
      "grad_norm": 11.770441055297852,
      "learning_rate": 2.6027958121307834e-05,
      "loss": 1.8723,
      "step": 8200
    },
    {
      "epoch": 1.4406036146692403,
      "grad_norm": 8.379922866821289,
      "learning_rate": 2.599871322454232e-05,
      "loss": 1.7682,
      "step": 8210
    },
    {
      "epoch": 1.4423583084751712,
      "grad_norm": 9.770635604858398,
      "learning_rate": 2.5969468327776802e-05,
      "loss": 1.5433,
      "step": 8220
    },
    {
      "epoch": 1.444113002281102,
      "grad_norm": 6.686012268066406,
      "learning_rate": 2.594022343101129e-05,
      "loss": 1.541,
      "step": 8230
    },
    {
      "epoch": 1.4458676960870327,
      "grad_norm": 9.608519554138184,
      "learning_rate": 2.5910978534245777e-05,
      "loss": 1.5396,
      "step": 8240
    },
    {
      "epoch": 1.4476223898929637,
      "grad_norm": 9.11095905303955,
      "learning_rate": 2.588173363748026e-05,
      "loss": 1.5398,
      "step": 8250
    },
    {
      "epoch": 1.4493770836988946,
      "grad_norm": 9.021799087524414,
      "learning_rate": 2.5852488740714748e-05,
      "loss": 1.5494,
      "step": 8260
    },
    {
      "epoch": 1.4511317775048254,
      "grad_norm": 9.731439590454102,
      "learning_rate": 2.582324384394923e-05,
      "loss": 1.604,
      "step": 8270
    },
    {
      "epoch": 1.4528864713107563,
      "grad_norm": 9.907973289489746,
      "learning_rate": 2.5793998947183716e-05,
      "loss": 1.5623,
      "step": 8280
    },
    {
      "epoch": 1.454641165116687,
      "grad_norm": 11.458046913146973,
      "learning_rate": 2.5764754050418205e-05,
      "loss": 1.7991,
      "step": 8290
    },
    {
      "epoch": 1.456395858922618,
      "grad_norm": 8.936018943786621,
      "learning_rate": 2.5735509153652687e-05,
      "loss": 1.9621,
      "step": 8300
    },
    {
      "epoch": 1.458150552728549,
      "grad_norm": 11.272555351257324,
      "learning_rate": 2.5706264256887176e-05,
      "loss": 1.7193,
      "step": 8310
    },
    {
      "epoch": 1.4599052465344797,
      "grad_norm": 10.972942352294922,
      "learning_rate": 2.5677019360121662e-05,
      "loss": 1.9804,
      "step": 8320
    },
    {
      "epoch": 1.4616599403404105,
      "grad_norm": 6.715494155883789,
      "learning_rate": 2.5647774463356144e-05,
      "loss": 1.9507,
      "step": 8330
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 9.983080863952637,
      "learning_rate": 2.5618529566590633e-05,
      "loss": 1.7525,
      "step": 8340
    },
    {
      "epoch": 1.4651693279522724,
      "grad_norm": 8.218735694885254,
      "learning_rate": 2.5589284669825115e-05,
      "loss": 1.6147,
      "step": 8350
    },
    {
      "epoch": 1.4669240217582031,
      "grad_norm": 9.292771339416504,
      "learning_rate": 2.55600397730596e-05,
      "loss": 1.611,
      "step": 8360
    },
    {
      "epoch": 1.468678715564134,
      "grad_norm": 7.119628429412842,
      "learning_rate": 2.553079487629409e-05,
      "loss": 1.9662,
      "step": 8370
    },
    {
      "epoch": 1.4704334093700648,
      "grad_norm": 10.094433784484863,
      "learning_rate": 2.5501549979528572e-05,
      "loss": 1.8619,
      "step": 8380
    },
    {
      "epoch": 1.4721881031759958,
      "grad_norm": 11.945516586303711,
      "learning_rate": 2.5472305082763058e-05,
      "loss": 1.9197,
      "step": 8390
    },
    {
      "epoch": 1.4739427969819268,
      "grad_norm": 9.201525688171387,
      "learning_rate": 2.5443060185997547e-05,
      "loss": 1.774,
      "step": 8400
    },
    {
      "epoch": 1.4756974907878575,
      "grad_norm": 8.387240409851074,
      "learning_rate": 2.541381528923203e-05,
      "loss": 1.5921,
      "step": 8410
    },
    {
      "epoch": 1.4774521845937885,
      "grad_norm": 7.498470306396484,
      "learning_rate": 2.5384570392466518e-05,
      "loss": 1.9199,
      "step": 8420
    },
    {
      "epoch": 1.4792068783997192,
      "grad_norm": 12.201348304748535,
      "learning_rate": 2.5355325495701e-05,
      "loss": 1.6324,
      "step": 8430
    },
    {
      "epoch": 1.4809615722056502,
      "grad_norm": 6.975820064544678,
      "learning_rate": 2.5326080598935486e-05,
      "loss": 1.6737,
      "step": 8440
    },
    {
      "epoch": 1.482716266011581,
      "grad_norm": 10.539857864379883,
      "learning_rate": 2.5296835702169975e-05,
      "loss": 1.8893,
      "step": 8450
    },
    {
      "epoch": 1.4844709598175119,
      "grad_norm": 8.808501243591309,
      "learning_rate": 2.5267590805404457e-05,
      "loss": 1.7066,
      "step": 8460
    },
    {
      "epoch": 1.4862256536234426,
      "grad_norm": 9.267457962036133,
      "learning_rate": 2.5238345908638943e-05,
      "loss": 1.9337,
      "step": 8470
    },
    {
      "epoch": 1.4879803474293736,
      "grad_norm": 9.19372844696045,
      "learning_rate": 2.5209101011873432e-05,
      "loss": 1.6309,
      "step": 8480
    },
    {
      "epoch": 1.4897350412353045,
      "grad_norm": 12.690382957458496,
      "learning_rate": 2.5179856115107914e-05,
      "loss": 1.9047,
      "step": 8490
    },
    {
      "epoch": 1.4914897350412353,
      "grad_norm": 8.259078025817871,
      "learning_rate": 2.5150611218342403e-05,
      "loss": 1.7255,
      "step": 8500
    },
    {
      "epoch": 1.4932444288471662,
      "grad_norm": 8.979047775268555,
      "learning_rate": 2.5121366321576885e-05,
      "loss": 1.8517,
      "step": 8510
    },
    {
      "epoch": 1.494999122653097,
      "grad_norm": 7.227600574493408,
      "learning_rate": 2.509212142481137e-05,
      "loss": 1.5503,
      "step": 8520
    },
    {
      "epoch": 1.496753816459028,
      "grad_norm": 6.93419075012207,
      "learning_rate": 2.506287652804586e-05,
      "loss": 1.5893,
      "step": 8530
    },
    {
      "epoch": 1.4985085102649587,
      "grad_norm": 9.202777862548828,
      "learning_rate": 2.5033631631280342e-05,
      "loss": 1.6484,
      "step": 8540
    },
    {
      "epoch": 1.5002632040708896,
      "grad_norm": 8.531203269958496,
      "learning_rate": 2.5004386734514828e-05,
      "loss": 1.5428,
      "step": 8550
    },
    {
      "epoch": 1.5020178978768204,
      "grad_norm": 6.2275800704956055,
      "learning_rate": 2.4975141837749313e-05,
      "loss": 1.4345,
      "step": 8560
    },
    {
      "epoch": 1.5037725916827513,
      "grad_norm": 8.37605094909668,
      "learning_rate": 2.49458969409838e-05,
      "loss": 1.4236,
      "step": 8570
    },
    {
      "epoch": 1.5055272854886823,
      "grad_norm": 8.4143648147583,
      "learning_rate": 2.4916652044218285e-05,
      "loss": 1.8013,
      "step": 8580
    },
    {
      "epoch": 1.507281979294613,
      "grad_norm": 7.3910017013549805,
      "learning_rate": 2.488740714745277e-05,
      "loss": 1.6372,
      "step": 8590
    },
    {
      "epoch": 1.5090366731005438,
      "grad_norm": 10.045811653137207,
      "learning_rate": 2.4858162250687256e-05,
      "loss": 1.5342,
      "step": 8600
    },
    {
      "epoch": 1.5107913669064748,
      "grad_norm": 10.341238975524902,
      "learning_rate": 2.482891735392174e-05,
      "loss": 1.6153,
      "step": 8610
    },
    {
      "epoch": 1.5125460607124057,
      "grad_norm": 11.305864334106445,
      "learning_rate": 2.4799672457156227e-05,
      "loss": 1.8987,
      "step": 8620
    },
    {
      "epoch": 1.5143007545183367,
      "grad_norm": 8.422603607177734,
      "learning_rate": 2.4770427560390713e-05,
      "loss": 1.5962,
      "step": 8630
    },
    {
      "epoch": 1.5160554483242674,
      "grad_norm": 8.67715835571289,
      "learning_rate": 2.4741182663625198e-05,
      "loss": 1.9902,
      "step": 8640
    },
    {
      "epoch": 1.5178101421301982,
      "grad_norm": 10.392130851745605,
      "learning_rate": 2.4711937766859684e-05,
      "loss": 1.7586,
      "step": 8650
    },
    {
      "epoch": 1.5195648359361291,
      "grad_norm": 7.524838924407959,
      "learning_rate": 2.468269287009417e-05,
      "loss": 2.0156,
      "step": 8660
    },
    {
      "epoch": 1.52131952974206,
      "grad_norm": 5.732909202575684,
      "learning_rate": 2.4653447973328655e-05,
      "loss": 1.6489,
      "step": 8670
    },
    {
      "epoch": 1.5230742235479908,
      "grad_norm": 9.45809268951416,
      "learning_rate": 2.462420307656314e-05,
      "loss": 1.8835,
      "step": 8680
    },
    {
      "epoch": 1.5248289173539218,
      "grad_norm": 8.87317180633545,
      "learning_rate": 2.4594958179797626e-05,
      "loss": 1.9112,
      "step": 8690
    },
    {
      "epoch": 1.5265836111598525,
      "grad_norm": 10.07402515411377,
      "learning_rate": 2.4565713283032112e-05,
      "loss": 1.9356,
      "step": 8700
    },
    {
      "epoch": 1.5283383049657835,
      "grad_norm": 11.62497615814209,
      "learning_rate": 2.4536468386266598e-05,
      "loss": 1.6091,
      "step": 8710
    },
    {
      "epoch": 1.5300929987717145,
      "grad_norm": 12.095865249633789,
      "learning_rate": 2.4507223489501083e-05,
      "loss": 1.7948,
      "step": 8720
    },
    {
      "epoch": 1.5318476925776452,
      "grad_norm": 9.288772583007812,
      "learning_rate": 2.447797859273557e-05,
      "loss": 1.9848,
      "step": 8730
    },
    {
      "epoch": 1.533602386383576,
      "grad_norm": 9.248035430908203,
      "learning_rate": 2.4448733695970054e-05,
      "loss": 1.721,
      "step": 8740
    },
    {
      "epoch": 1.535357080189507,
      "grad_norm": 7.704615116119385,
      "learning_rate": 2.441948879920454e-05,
      "loss": 1.7593,
      "step": 8750
    },
    {
      "epoch": 1.5371117739954379,
      "grad_norm": 8.416768074035645,
      "learning_rate": 2.4390243902439026e-05,
      "loss": 1.5371,
      "step": 8760
    },
    {
      "epoch": 1.5388664678013688,
      "grad_norm": 11.972687721252441,
      "learning_rate": 2.436099900567351e-05,
      "loss": 1.7655,
      "step": 8770
    },
    {
      "epoch": 1.5406211616072996,
      "grad_norm": 8.051115989685059,
      "learning_rate": 2.4331754108907997e-05,
      "loss": 1.752,
      "step": 8780
    },
    {
      "epoch": 1.5423758554132303,
      "grad_norm": 7.9340596199035645,
      "learning_rate": 2.4302509212142483e-05,
      "loss": 1.6957,
      "step": 8790
    },
    {
      "epoch": 1.5441305492191613,
      "grad_norm": 9.233275413513184,
      "learning_rate": 2.4273264315376968e-05,
      "loss": 1.7101,
      "step": 8800
    },
    {
      "epoch": 1.5458852430250922,
      "grad_norm": 8.443495750427246,
      "learning_rate": 2.4244019418611454e-05,
      "loss": 1.6362,
      "step": 8810
    },
    {
      "epoch": 1.547639936831023,
      "grad_norm": 7.250217914581299,
      "learning_rate": 2.421477452184594e-05,
      "loss": 1.7079,
      "step": 8820
    },
    {
      "epoch": 1.5493946306369537,
      "grad_norm": 8.539504051208496,
      "learning_rate": 2.4185529625080425e-05,
      "loss": 1.5109,
      "step": 8830
    },
    {
      "epoch": 1.5511493244428847,
      "grad_norm": 8.885162353515625,
      "learning_rate": 2.415628472831491e-05,
      "loss": 1.8459,
      "step": 8840
    },
    {
      "epoch": 1.5529040182488156,
      "grad_norm": 8.175411224365234,
      "learning_rate": 2.4127039831549393e-05,
      "loss": 1.5062,
      "step": 8850
    },
    {
      "epoch": 1.5546587120547466,
      "grad_norm": 8.536043167114258,
      "learning_rate": 2.4097794934783882e-05,
      "loss": 1.6567,
      "step": 8860
    },
    {
      "epoch": 1.5564134058606773,
      "grad_norm": 9.276956558227539,
      "learning_rate": 2.4068550038018368e-05,
      "loss": 1.6781,
      "step": 8870
    },
    {
      "epoch": 1.558168099666608,
      "grad_norm": 9.87969970703125,
      "learning_rate": 2.4039305141252853e-05,
      "loss": 1.7399,
      "step": 8880
    },
    {
      "epoch": 1.559922793472539,
      "grad_norm": 8.221068382263184,
      "learning_rate": 2.4010060244487335e-05,
      "loss": 1.7052,
      "step": 8890
    },
    {
      "epoch": 1.56167748727847,
      "grad_norm": 8.82733154296875,
      "learning_rate": 2.3980815347721824e-05,
      "loss": 1.7617,
      "step": 8900
    },
    {
      "epoch": 1.5634321810844007,
      "grad_norm": 10.448200225830078,
      "learning_rate": 2.395157045095631e-05,
      "loss": 1.8375,
      "step": 8910
    },
    {
      "epoch": 1.5651868748903315,
      "grad_norm": 7.463075637817383,
      "learning_rate": 2.3922325554190796e-05,
      "loss": 1.4937,
      "step": 8920
    },
    {
      "epoch": 1.5669415686962624,
      "grad_norm": 6.680016040802002,
      "learning_rate": 2.3893080657425278e-05,
      "loss": 1.753,
      "step": 8930
    },
    {
      "epoch": 1.5686962625021934,
      "grad_norm": 5.68784761428833,
      "learning_rate": 2.3863835760659767e-05,
      "loss": 1.6853,
      "step": 8940
    },
    {
      "epoch": 1.5704509563081244,
      "grad_norm": 8.175453186035156,
      "learning_rate": 2.3834590863894253e-05,
      "loss": 1.4848,
      "step": 8950
    },
    {
      "epoch": 1.572205650114055,
      "grad_norm": 9.660813331604004,
      "learning_rate": 2.3805345967128738e-05,
      "loss": 2.0483,
      "step": 8960
    },
    {
      "epoch": 1.5739603439199858,
      "grad_norm": 9.294382095336914,
      "learning_rate": 2.377610107036322e-05,
      "loss": 1.6682,
      "step": 8970
    },
    {
      "epoch": 1.5757150377259168,
      "grad_norm": 9.550061225891113,
      "learning_rate": 2.374685617359771e-05,
      "loss": 1.8724,
      "step": 8980
    },
    {
      "epoch": 1.5774697315318478,
      "grad_norm": 8.637335777282715,
      "learning_rate": 2.3717611276832195e-05,
      "loss": 1.8296,
      "step": 8990
    },
    {
      "epoch": 1.5792244253377785,
      "grad_norm": 8.013680458068848,
      "learning_rate": 2.3688366380066677e-05,
      "loss": 1.6448,
      "step": 9000
    },
    {
      "epoch": 1.5809791191437095,
      "grad_norm": 9.00144100189209,
      "learning_rate": 2.3659121483301163e-05,
      "loss": 1.7926,
      "step": 9010
    },
    {
      "epoch": 1.5827338129496402,
      "grad_norm": 9.901595115661621,
      "learning_rate": 2.3629876586535652e-05,
      "loss": 1.55,
      "step": 9020
    },
    {
      "epoch": 1.5844885067555712,
      "grad_norm": 8.775924682617188,
      "learning_rate": 2.3600631689770137e-05,
      "loss": 1.6987,
      "step": 9030
    },
    {
      "epoch": 1.5862432005615021,
      "grad_norm": 10.113936424255371,
      "learning_rate": 2.357138679300462e-05,
      "loss": 1.6321,
      "step": 9040
    },
    {
      "epoch": 1.5879978943674329,
      "grad_norm": 10.778124809265137,
      "learning_rate": 2.3542141896239105e-05,
      "loss": 1.596,
      "step": 9050
    },
    {
      "epoch": 1.5897525881733636,
      "grad_norm": 9.618700981140137,
      "learning_rate": 2.3512896999473594e-05,
      "loss": 1.6316,
      "step": 9060
    },
    {
      "epoch": 1.5915072819792946,
      "grad_norm": 8.998453140258789,
      "learning_rate": 2.348365210270808e-05,
      "loss": 1.8453,
      "step": 9070
    },
    {
      "epoch": 1.5932619757852255,
      "grad_norm": 10.52648639678955,
      "learning_rate": 2.3454407205942562e-05,
      "loss": 1.9293,
      "step": 9080
    },
    {
      "epoch": 1.5950166695911563,
      "grad_norm": 10.043724060058594,
      "learning_rate": 2.3425162309177048e-05,
      "loss": 1.7918,
      "step": 9090
    },
    {
      "epoch": 1.5967713633970873,
      "grad_norm": 9.245977401733398,
      "learning_rate": 2.3395917412411537e-05,
      "loss": 1.8266,
      "step": 9100
    },
    {
      "epoch": 1.598526057203018,
      "grad_norm": 7.943103313446045,
      "learning_rate": 2.3366672515646022e-05,
      "loss": 1.8115,
      "step": 9110
    },
    {
      "epoch": 1.600280751008949,
      "grad_norm": 9.205764770507812,
      "learning_rate": 2.3337427618880505e-05,
      "loss": 1.7835,
      "step": 9120
    },
    {
      "epoch": 1.60203544481488,
      "grad_norm": 6.5858073234558105,
      "learning_rate": 2.3308182722114994e-05,
      "loss": 1.6175,
      "step": 9130
    },
    {
      "epoch": 1.6037901386208107,
      "grad_norm": 7.842273712158203,
      "learning_rate": 2.327893782534948e-05,
      "loss": 1.6623,
      "step": 9140
    },
    {
      "epoch": 1.6055448324267414,
      "grad_norm": 8.931817054748535,
      "learning_rate": 2.324969292858396e-05,
      "loss": 1.6987,
      "step": 9150
    },
    {
      "epoch": 1.6072995262326724,
      "grad_norm": 6.9460673332214355,
      "learning_rate": 2.3220448031818447e-05,
      "loss": 1.4471,
      "step": 9160
    },
    {
      "epoch": 1.6090542200386033,
      "grad_norm": 9.71998119354248,
      "learning_rate": 2.3191203135052936e-05,
      "loss": 1.9964,
      "step": 9170
    },
    {
      "epoch": 1.6108089138445343,
      "grad_norm": 12.929435729980469,
      "learning_rate": 2.3161958238287422e-05,
      "loss": 1.971,
      "step": 9180
    },
    {
      "epoch": 1.612563607650465,
      "grad_norm": 9.27545166015625,
      "learning_rate": 2.3132713341521904e-05,
      "loss": 1.8609,
      "step": 9190
    },
    {
      "epoch": 1.6143183014563958,
      "grad_norm": 9.555233001708984,
      "learning_rate": 2.310346844475639e-05,
      "loss": 1.8694,
      "step": 9200
    },
    {
      "epoch": 1.6160729952623267,
      "grad_norm": 8.268084526062012,
      "learning_rate": 2.307422354799088e-05,
      "loss": 1.8218,
      "step": 9210
    },
    {
      "epoch": 1.6178276890682577,
      "grad_norm": 9.032796859741211,
      "learning_rate": 2.3044978651225364e-05,
      "loss": 2.0711,
      "step": 9220
    },
    {
      "epoch": 1.6195823828741884,
      "grad_norm": 8.217491149902344,
      "learning_rate": 2.3015733754459847e-05,
      "loss": 1.6707,
      "step": 9230
    },
    {
      "epoch": 1.6213370766801192,
      "grad_norm": 9.257885932922363,
      "learning_rate": 2.2986488857694332e-05,
      "loss": 1.8251,
      "step": 9240
    },
    {
      "epoch": 1.6230917704860501,
      "grad_norm": 6.767326354980469,
      "learning_rate": 2.295724396092882e-05,
      "loss": 1.4889,
      "step": 9250
    },
    {
      "epoch": 1.624846464291981,
      "grad_norm": 10.4561128616333,
      "learning_rate": 2.2927999064163307e-05,
      "loss": 1.6854,
      "step": 9260
    },
    {
      "epoch": 1.626601158097912,
      "grad_norm": 10.853896141052246,
      "learning_rate": 2.289875416739779e-05,
      "loss": 1.7062,
      "step": 9270
    },
    {
      "epoch": 1.6283558519038428,
      "grad_norm": 9.988030433654785,
      "learning_rate": 2.2869509270632275e-05,
      "loss": 1.6047,
      "step": 9280
    },
    {
      "epoch": 1.6301105457097735,
      "grad_norm": 9.075523376464844,
      "learning_rate": 2.2840264373866764e-05,
      "loss": 1.6469,
      "step": 9290
    },
    {
      "epoch": 1.6318652395157045,
      "grad_norm": 7.617446422576904,
      "learning_rate": 2.2811019477101246e-05,
      "loss": 1.6162,
      "step": 9300
    },
    {
      "epoch": 1.6336199333216355,
      "grad_norm": 8.848396301269531,
      "learning_rate": 2.278177458033573e-05,
      "loss": 1.5891,
      "step": 9310
    },
    {
      "epoch": 1.6353746271275662,
      "grad_norm": 8.597406387329102,
      "learning_rate": 2.2752529683570217e-05,
      "loss": 1.798,
      "step": 9320
    },
    {
      "epoch": 1.637129320933497,
      "grad_norm": 9.834562301635742,
      "learning_rate": 2.2723284786804706e-05,
      "loss": 1.7737,
      "step": 9330
    },
    {
      "epoch": 1.638884014739428,
      "grad_norm": 7.143153667449951,
      "learning_rate": 2.269403989003919e-05,
      "loss": 1.7393,
      "step": 9340
    },
    {
      "epoch": 1.6406387085453589,
      "grad_norm": 8.62605094909668,
      "learning_rate": 2.2664794993273674e-05,
      "loss": 1.6419,
      "step": 9350
    },
    {
      "epoch": 1.6423934023512898,
      "grad_norm": 9.024752616882324,
      "learning_rate": 2.263555009650816e-05,
      "loss": 1.8539,
      "step": 9360
    },
    {
      "epoch": 1.6441480961572206,
      "grad_norm": 9.575485229492188,
      "learning_rate": 2.260630519974265e-05,
      "loss": 1.8435,
      "step": 9370
    },
    {
      "epoch": 1.6459027899631513,
      "grad_norm": 7.572113513946533,
      "learning_rate": 2.257706030297713e-05,
      "loss": 1.8125,
      "step": 9380
    },
    {
      "epoch": 1.6476574837690823,
      "grad_norm": 8.14900016784668,
      "learning_rate": 2.2547815406211616e-05,
      "loss": 1.6076,
      "step": 9390
    },
    {
      "epoch": 1.6494121775750132,
      "grad_norm": 8.564743041992188,
      "learning_rate": 2.2518570509446102e-05,
      "loss": 1.7791,
      "step": 9400
    },
    {
      "epoch": 1.651166871380944,
      "grad_norm": 9.4814453125,
      "learning_rate": 2.248932561268059e-05,
      "loss": 1.8043,
      "step": 9410
    },
    {
      "epoch": 1.652921565186875,
      "grad_norm": 8.736366271972656,
      "learning_rate": 2.2460080715915073e-05,
      "loss": 1.6379,
      "step": 9420
    },
    {
      "epoch": 1.6546762589928057,
      "grad_norm": 8.857908248901367,
      "learning_rate": 2.243083581914956e-05,
      "loss": 1.9318,
      "step": 9430
    },
    {
      "epoch": 1.6564309527987366,
      "grad_norm": 10.266977310180664,
      "learning_rate": 2.2401590922384045e-05,
      "loss": 1.6492,
      "step": 9440
    },
    {
      "epoch": 1.6581856466046676,
      "grad_norm": 9.40562629699707,
      "learning_rate": 2.237234602561853e-05,
      "loss": 1.7889,
      "step": 9450
    },
    {
      "epoch": 1.6599403404105983,
      "grad_norm": 8.313737869262695,
      "learning_rate": 2.2343101128853016e-05,
      "loss": 1.7585,
      "step": 9460
    },
    {
      "epoch": 1.661695034216529,
      "grad_norm": 8.69680118560791,
      "learning_rate": 2.23138562320875e-05,
      "loss": 2.0042,
      "step": 9470
    },
    {
      "epoch": 1.66344972802246,
      "grad_norm": 7.914908409118652,
      "learning_rate": 2.2284611335321987e-05,
      "loss": 1.4837,
      "step": 9480
    },
    {
      "epoch": 1.665204421828391,
      "grad_norm": 6.191806793212891,
      "learning_rate": 2.2255366438556473e-05,
      "loss": 1.6942,
      "step": 9490
    },
    {
      "epoch": 1.666959115634322,
      "grad_norm": 9.864541053771973,
      "learning_rate": 2.222612154179096e-05,
      "loss": 1.906,
      "step": 9500
    },
    {
      "epoch": 1.6687138094402527,
      "grad_norm": 9.113761901855469,
      "learning_rate": 2.2196876645025444e-05,
      "loss": 1.587,
      "step": 9510
    },
    {
      "epoch": 1.6704685032461835,
      "grad_norm": 9.854214668273926,
      "learning_rate": 2.216763174825993e-05,
      "loss": 1.6842,
      "step": 9520
    },
    {
      "epoch": 1.6722231970521144,
      "grad_norm": 9.501564025878906,
      "learning_rate": 2.2138386851494415e-05,
      "loss": 1.6567,
      "step": 9530
    },
    {
      "epoch": 1.6739778908580454,
      "grad_norm": 9.434721946716309,
      "learning_rate": 2.21091419547289e-05,
      "loss": 1.546,
      "step": 9540
    },
    {
      "epoch": 1.6757325846639761,
      "grad_norm": 9.164314270019531,
      "learning_rate": 2.2079897057963386e-05,
      "loss": 1.7931,
      "step": 9550
    },
    {
      "epoch": 1.6774872784699069,
      "grad_norm": 9.4099760055542,
      "learning_rate": 2.2050652161197872e-05,
      "loss": 1.7848,
      "step": 9560
    },
    {
      "epoch": 1.6792419722758378,
      "grad_norm": 7.964051723480225,
      "learning_rate": 2.2021407264432358e-05,
      "loss": 1.5755,
      "step": 9570
    },
    {
      "epoch": 1.6809966660817688,
      "grad_norm": 6.722257137298584,
      "learning_rate": 2.1992162367666843e-05,
      "loss": 1.9582,
      "step": 9580
    },
    {
      "epoch": 1.6827513598876997,
      "grad_norm": 9.835314750671387,
      "learning_rate": 2.196291747090133e-05,
      "loss": 1.7891,
      "step": 9590
    },
    {
      "epoch": 1.6845060536936305,
      "grad_norm": 7.850229740142822,
      "learning_rate": 2.1933672574135815e-05,
      "loss": 2.1055,
      "step": 9600
    },
    {
      "epoch": 1.6862607474995612,
      "grad_norm": 8.062920570373535,
      "learning_rate": 2.19044276773703e-05,
      "loss": 1.6642,
      "step": 9610
    },
    {
      "epoch": 1.6880154413054922,
      "grad_norm": 6.692263126373291,
      "learning_rate": 2.1875182780604786e-05,
      "loss": 1.8007,
      "step": 9620
    },
    {
      "epoch": 1.6897701351114232,
      "grad_norm": 8.644803047180176,
      "learning_rate": 2.184593788383927e-05,
      "loss": 1.6961,
      "step": 9630
    },
    {
      "epoch": 1.691524828917354,
      "grad_norm": 9.0477933883667,
      "learning_rate": 2.1816692987073757e-05,
      "loss": 1.598,
      "step": 9640
    },
    {
      "epoch": 1.6932795227232846,
      "grad_norm": 7.934834957122803,
      "learning_rate": 2.1787448090308243e-05,
      "loss": 1.6605,
      "step": 9650
    },
    {
      "epoch": 1.6950342165292156,
      "grad_norm": 9.928853034973145,
      "learning_rate": 2.1758203193542728e-05,
      "loss": 1.8102,
      "step": 9660
    },
    {
      "epoch": 1.6967889103351466,
      "grad_norm": 9.200145721435547,
      "learning_rate": 2.1728958296777214e-05,
      "loss": 1.6936,
      "step": 9670
    },
    {
      "epoch": 1.6985436041410775,
      "grad_norm": 8.368598937988281,
      "learning_rate": 2.16997134000117e-05,
      "loss": 1.622,
      "step": 9680
    },
    {
      "epoch": 1.7002982979470083,
      "grad_norm": 7.558322429656982,
      "learning_rate": 2.1670468503246185e-05,
      "loss": 1.6762,
      "step": 9690
    },
    {
      "epoch": 1.702052991752939,
      "grad_norm": 8.096274375915527,
      "learning_rate": 2.164122360648067e-05,
      "loss": 1.7467,
      "step": 9700
    },
    {
      "epoch": 1.70380768555887,
      "grad_norm": 8.080049514770508,
      "learning_rate": 2.1611978709715156e-05,
      "loss": 1.6196,
      "step": 9710
    },
    {
      "epoch": 1.705562379364801,
      "grad_norm": 9.580436706542969,
      "learning_rate": 2.1582733812949642e-05,
      "loss": 1.6069,
      "step": 9720
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 9.519357681274414,
      "learning_rate": 2.1553488916184128e-05,
      "loss": 1.6652,
      "step": 9730
    },
    {
      "epoch": 1.7090717669766624,
      "grad_norm": 9.349991798400879,
      "learning_rate": 2.1524244019418613e-05,
      "loss": 1.5971,
      "step": 9740
    },
    {
      "epoch": 1.7108264607825934,
      "grad_norm": 7.815662860870361,
      "learning_rate": 2.14949991226531e-05,
      "loss": 1.537,
      "step": 9750
    },
    {
      "epoch": 1.7125811545885243,
      "grad_norm": 8.268319129943848,
      "learning_rate": 2.146575422588758e-05,
      "loss": 1.6026,
      "step": 9760
    },
    {
      "epoch": 1.7143358483944553,
      "grad_norm": 8.077012062072754,
      "learning_rate": 2.143650932912207e-05,
      "loss": 1.6283,
      "step": 9770
    },
    {
      "epoch": 1.716090542200386,
      "grad_norm": 10.18610954284668,
      "learning_rate": 2.1407264432356556e-05,
      "loss": 1.8203,
      "step": 9780
    },
    {
      "epoch": 1.7178452360063168,
      "grad_norm": 7.287207126617432,
      "learning_rate": 2.137801953559104e-05,
      "loss": 1.8045,
      "step": 9790
    },
    {
      "epoch": 1.7195999298122477,
      "grad_norm": 10.97497272491455,
      "learning_rate": 2.1348774638825524e-05,
      "loss": 1.6807,
      "step": 9800
    },
    {
      "epoch": 1.7213546236181787,
      "grad_norm": 8.562646865844727,
      "learning_rate": 2.1319529742060013e-05,
      "loss": 1.6615,
      "step": 9810
    },
    {
      "epoch": 1.7231093174241094,
      "grad_norm": 8.989923477172852,
      "learning_rate": 2.1290284845294498e-05,
      "loss": 1.5192,
      "step": 9820
    },
    {
      "epoch": 1.7248640112300404,
      "grad_norm": 8.638311386108398,
      "learning_rate": 2.1261039948528984e-05,
      "loss": 1.971,
      "step": 9830
    },
    {
      "epoch": 1.7266187050359711,
      "grad_norm": 9.640708923339844,
      "learning_rate": 2.1231795051763466e-05,
      "loss": 1.5663,
      "step": 9840
    },
    {
      "epoch": 1.728373398841902,
      "grad_norm": 9.370805740356445,
      "learning_rate": 2.1202550154997955e-05,
      "loss": 2.0158,
      "step": 9850
    },
    {
      "epoch": 1.730128092647833,
      "grad_norm": 9.65734577178955,
      "learning_rate": 2.117330525823244e-05,
      "loss": 1.5535,
      "step": 9860
    },
    {
      "epoch": 1.7318827864537638,
      "grad_norm": 11.7805814743042,
      "learning_rate": 2.1144060361466926e-05,
      "loss": 1.6551,
      "step": 9870
    },
    {
      "epoch": 1.7336374802596946,
      "grad_norm": 8.706502914428711,
      "learning_rate": 2.111481546470141e-05,
      "loss": 1.833,
      "step": 9880
    },
    {
      "epoch": 1.7353921740656255,
      "grad_norm": 7.738933563232422,
      "learning_rate": 2.1085570567935898e-05,
      "loss": 1.5901,
      "step": 9890
    },
    {
      "epoch": 1.7371468678715565,
      "grad_norm": 7.823443412780762,
      "learning_rate": 2.1056325671170383e-05,
      "loss": 1.6414,
      "step": 9900
    },
    {
      "epoch": 1.7389015616774874,
      "grad_norm": 10.247618675231934,
      "learning_rate": 2.1027080774404865e-05,
      "loss": 1.8203,
      "step": 9910
    },
    {
      "epoch": 1.7406562554834182,
      "grad_norm": 8.194443702697754,
      "learning_rate": 2.099783587763935e-05,
      "loss": 1.8239,
      "step": 9920
    },
    {
      "epoch": 1.742410949289349,
      "grad_norm": 11.66917896270752,
      "learning_rate": 2.096859098087384e-05,
      "loss": 1.6075,
      "step": 9930
    },
    {
      "epoch": 1.7441656430952799,
      "grad_norm": 9.082756042480469,
      "learning_rate": 2.0939346084108326e-05,
      "loss": 1.784,
      "step": 9940
    },
    {
      "epoch": 1.7459203369012108,
      "grad_norm": 10.283016204833984,
      "learning_rate": 2.0910101187342808e-05,
      "loss": 1.5682,
      "step": 9950
    },
    {
      "epoch": 1.7476750307071416,
      "grad_norm": 8.841554641723633,
      "learning_rate": 2.0880856290577294e-05,
      "loss": 1.8294,
      "step": 9960
    },
    {
      "epoch": 1.7494297245130723,
      "grad_norm": 11.951284408569336,
      "learning_rate": 2.0851611393811783e-05,
      "loss": 1.6788,
      "step": 9970
    },
    {
      "epoch": 1.7511844183190033,
      "grad_norm": 8.40413761138916,
      "learning_rate": 2.0822366497046268e-05,
      "loss": 1.6358,
      "step": 9980
    },
    {
      "epoch": 1.7529391121249343,
      "grad_norm": 10.618672370910645,
      "learning_rate": 2.079312160028075e-05,
      "loss": 1.8492,
      "step": 9990
    },
    {
      "epoch": 1.7546938059308652,
      "grad_norm": 10.641590118408203,
      "learning_rate": 2.0763876703515236e-05,
      "loss": 1.8354,
      "step": 10000
    },
    {
      "epoch": 1.756448499736796,
      "grad_norm": 10.060880661010742,
      "learning_rate": 2.0734631806749725e-05,
      "loss": 1.6603,
      "step": 10010
    },
    {
      "epoch": 1.7582031935427267,
      "grad_norm": 8.036810874938965,
      "learning_rate": 2.070538690998421e-05,
      "loss": 2.1176,
      "step": 10020
    },
    {
      "epoch": 1.7599578873486577,
      "grad_norm": 9.457965850830078,
      "learning_rate": 2.0676142013218693e-05,
      "loss": 1.675,
      "step": 10030
    },
    {
      "epoch": 1.7617125811545886,
      "grad_norm": 9.843395233154297,
      "learning_rate": 2.064689711645318e-05,
      "loss": 1.766,
      "step": 10040
    },
    {
      "epoch": 1.7634672749605194,
      "grad_norm": 8.944501876831055,
      "learning_rate": 2.0617652219687667e-05,
      "loss": 1.6659,
      "step": 10050
    },
    {
      "epoch": 1.76522196876645,
      "grad_norm": 7.034163475036621,
      "learning_rate": 2.058840732292215e-05,
      "loss": 1.5249,
      "step": 10060
    },
    {
      "epoch": 1.766976662572381,
      "grad_norm": 11.694700241088867,
      "learning_rate": 2.0559162426156635e-05,
      "loss": 1.9094,
      "step": 10070
    },
    {
      "epoch": 1.768731356378312,
      "grad_norm": 8.060380935668945,
      "learning_rate": 2.052991752939112e-05,
      "loss": 1.7293,
      "step": 10080
    },
    {
      "epoch": 1.770486050184243,
      "grad_norm": 9.663496971130371,
      "learning_rate": 2.050067263262561e-05,
      "loss": 2.0131,
      "step": 10090
    },
    {
      "epoch": 1.7722407439901737,
      "grad_norm": 8.874425888061523,
      "learning_rate": 2.0471427735860092e-05,
      "loss": 1.7998,
      "step": 10100
    },
    {
      "epoch": 1.7739954377961045,
      "grad_norm": 9.634437561035156,
      "learning_rate": 2.0442182839094578e-05,
      "loss": 1.9097,
      "step": 10110
    },
    {
      "epoch": 1.7757501316020354,
      "grad_norm": 11.500027656555176,
      "learning_rate": 2.0412937942329063e-05,
      "loss": 1.6776,
      "step": 10120
    },
    {
      "epoch": 1.7775048254079664,
      "grad_norm": 9.846365928649902,
      "learning_rate": 2.0383693045563552e-05,
      "loss": 1.7576,
      "step": 10130
    },
    {
      "epoch": 1.7792595192138971,
      "grad_norm": 7.672818183898926,
      "learning_rate": 2.0354448148798035e-05,
      "loss": 1.4983,
      "step": 10140
    },
    {
      "epoch": 1.7810142130198279,
      "grad_norm": 9.424237251281738,
      "learning_rate": 2.032520325203252e-05,
      "loss": 1.7891,
      "step": 10150
    },
    {
      "epoch": 1.7827689068257588,
      "grad_norm": 10.762140274047852,
      "learning_rate": 2.0295958355267006e-05,
      "loss": 1.8743,
      "step": 10160
    },
    {
      "epoch": 1.7845236006316898,
      "grad_norm": 9.19187068939209,
      "learning_rate": 2.0266713458501495e-05,
      "loss": 1.6886,
      "step": 10170
    },
    {
      "epoch": 1.7862782944376208,
      "grad_norm": 7.995884895324707,
      "learning_rate": 2.0237468561735977e-05,
      "loss": 1.8427,
      "step": 10180
    },
    {
      "epoch": 1.7880329882435515,
      "grad_norm": 8.535965919494629,
      "learning_rate": 2.0208223664970463e-05,
      "loss": 1.6921,
      "step": 10190
    },
    {
      "epoch": 1.7897876820494822,
      "grad_norm": 7.42657470703125,
      "learning_rate": 2.017897876820495e-05,
      "loss": 1.7231,
      "step": 10200
    },
    {
      "epoch": 1.7915423758554132,
      "grad_norm": 7.016211986541748,
      "learning_rate": 2.0149733871439434e-05,
      "loss": 1.7508,
      "step": 10210
    },
    {
      "epoch": 1.7932970696613442,
      "grad_norm": 7.309985160827637,
      "learning_rate": 2.012048897467392e-05,
      "loss": 1.5739,
      "step": 10220
    },
    {
      "epoch": 1.795051763467275,
      "grad_norm": 12.619672775268555,
      "learning_rate": 2.0091244077908405e-05,
      "loss": 2.141,
      "step": 10230
    },
    {
      "epoch": 1.7968064572732059,
      "grad_norm": 8.394786834716797,
      "learning_rate": 2.006199918114289e-05,
      "loss": 1.6543,
      "step": 10240
    },
    {
      "epoch": 1.7985611510791366,
      "grad_norm": 7.950320243835449,
      "learning_rate": 2.0032754284377377e-05,
      "loss": 1.6661,
      "step": 10250
    },
    {
      "epoch": 1.8003158448850676,
      "grad_norm": 13.957151412963867,
      "learning_rate": 2.0003509387611862e-05,
      "loss": 1.6693,
      "step": 10260
    },
    {
      "epoch": 1.8020705386909985,
      "grad_norm": 10.300171852111816,
      "learning_rate": 1.9974264490846348e-05,
      "loss": 1.6985,
      "step": 10270
    },
    {
      "epoch": 1.8038252324969293,
      "grad_norm": 8.981159210205078,
      "learning_rate": 1.9945019594080833e-05,
      "loss": 1.7051,
      "step": 10280
    },
    {
      "epoch": 1.80557992630286,
      "grad_norm": 7.533864974975586,
      "learning_rate": 1.991577469731532e-05,
      "loss": 1.9547,
      "step": 10290
    },
    {
      "epoch": 1.807334620108791,
      "grad_norm": 9.820501327514648,
      "learning_rate": 1.9886529800549805e-05,
      "loss": 1.7597,
      "step": 10300
    },
    {
      "epoch": 1.809089313914722,
      "grad_norm": 8.160554885864258,
      "learning_rate": 1.985728490378429e-05,
      "loss": 1.551,
      "step": 10310
    },
    {
      "epoch": 1.810844007720653,
      "grad_norm": 8.152995109558105,
      "learning_rate": 1.9828040007018776e-05,
      "loss": 1.679,
      "step": 10320
    },
    {
      "epoch": 1.8125987015265836,
      "grad_norm": 8.634624481201172,
      "learning_rate": 1.979879511025326e-05,
      "loss": 1.6215,
      "step": 10330
    },
    {
      "epoch": 1.8143533953325144,
      "grad_norm": 11.378121376037598,
      "learning_rate": 1.9769550213487747e-05,
      "loss": 1.6436,
      "step": 10340
    },
    {
      "epoch": 1.8161080891384453,
      "grad_norm": 6.9265618324279785,
      "learning_rate": 1.9740305316722233e-05,
      "loss": 1.7837,
      "step": 10350
    },
    {
      "epoch": 1.8178627829443763,
      "grad_norm": 10.900626182556152,
      "learning_rate": 1.971106041995672e-05,
      "loss": 1.597,
      "step": 10360
    },
    {
      "epoch": 1.819617476750307,
      "grad_norm": 8.430867195129395,
      "learning_rate": 1.9681815523191204e-05,
      "loss": 1.5557,
      "step": 10370
    },
    {
      "epoch": 1.8213721705562378,
      "grad_norm": 8.427532196044922,
      "learning_rate": 1.965257062642569e-05,
      "loss": 1.647,
      "step": 10380
    },
    {
      "epoch": 1.8231268643621688,
      "grad_norm": 7.717519283294678,
      "learning_rate": 1.9623325729660175e-05,
      "loss": 1.6004,
      "step": 10390
    },
    {
      "epoch": 1.8248815581680997,
      "grad_norm": 8.365843772888184,
      "learning_rate": 1.959408083289466e-05,
      "loss": 1.8389,
      "step": 10400
    },
    {
      "epoch": 1.8266362519740307,
      "grad_norm": 8.887258529663086,
      "learning_rate": 1.9564835936129146e-05,
      "loss": 1.8359,
      "step": 10410
    },
    {
      "epoch": 1.8283909457799614,
      "grad_norm": 8.129110336303711,
      "learning_rate": 1.9535591039363632e-05,
      "loss": 1.6488,
      "step": 10420
    },
    {
      "epoch": 1.8301456395858922,
      "grad_norm": 8.592864990234375,
      "learning_rate": 1.9506346142598118e-05,
      "loss": 1.4563,
      "step": 10430
    },
    {
      "epoch": 1.8319003333918231,
      "grad_norm": 8.536449432373047,
      "learning_rate": 1.9477101245832603e-05,
      "loss": 1.5139,
      "step": 10440
    },
    {
      "epoch": 1.833655027197754,
      "grad_norm": 7.795704364776611,
      "learning_rate": 1.944785634906709e-05,
      "loss": 1.8143,
      "step": 10450
    },
    {
      "epoch": 1.8354097210036848,
      "grad_norm": 7.30137825012207,
      "learning_rate": 1.9418611452301575e-05,
      "loss": 1.8037,
      "step": 10460
    },
    {
      "epoch": 1.8371644148096156,
      "grad_norm": 9.204957962036133,
      "learning_rate": 1.938936655553606e-05,
      "loss": 1.744,
      "step": 10470
    },
    {
      "epoch": 1.8389191086155465,
      "grad_norm": 12.968205451965332,
      "learning_rate": 1.9360121658770546e-05,
      "loss": 1.6169,
      "step": 10480
    },
    {
      "epoch": 1.8406738024214775,
      "grad_norm": 10.006073951721191,
      "learning_rate": 1.933087676200503e-05,
      "loss": 1.7442,
      "step": 10490
    },
    {
      "epoch": 1.8424284962274085,
      "grad_norm": 5.70648193359375,
      "learning_rate": 1.9301631865239517e-05,
      "loss": 1.3731,
      "step": 10500
    },
    {
      "epoch": 1.8441831900333392,
      "grad_norm": 12.51486873626709,
      "learning_rate": 1.9272386968474003e-05,
      "loss": 1.7871,
      "step": 10510
    },
    {
      "epoch": 1.84593788383927,
      "grad_norm": 9.16475772857666,
      "learning_rate": 1.9243142071708485e-05,
      "loss": 1.7401,
      "step": 10520
    },
    {
      "epoch": 1.847692577645201,
      "grad_norm": 7.513489246368408,
      "learning_rate": 1.9213897174942974e-05,
      "loss": 1.5445,
      "step": 10530
    },
    {
      "epoch": 1.8494472714511319,
      "grad_norm": 10.460211753845215,
      "learning_rate": 1.918465227817746e-05,
      "loss": 1.9001,
      "step": 10540
    },
    {
      "epoch": 1.8512019652570626,
      "grad_norm": 8.991838455200195,
      "learning_rate": 1.9155407381411945e-05,
      "loss": 1.9107,
      "step": 10550
    },
    {
      "epoch": 1.8529566590629936,
      "grad_norm": 6.944726943969727,
      "learning_rate": 1.9126162484646427e-05,
      "loss": 1.6766,
      "step": 10560
    },
    {
      "epoch": 1.8547113528689243,
      "grad_norm": 8.767901420593262,
      "learning_rate": 1.9096917587880916e-05,
      "loss": 1.7966,
      "step": 10570
    },
    {
      "epoch": 1.8564660466748553,
      "grad_norm": 8.703125,
      "learning_rate": 1.9067672691115402e-05,
      "loss": 1.8353,
      "step": 10580
    },
    {
      "epoch": 1.8582207404807862,
      "grad_norm": 11.192034721374512,
      "learning_rate": 1.9038427794349888e-05,
      "loss": 1.7807,
      "step": 10590
    },
    {
      "epoch": 1.859975434286717,
      "grad_norm": 7.196017742156982,
      "learning_rate": 1.900918289758437e-05,
      "loss": 1.4794,
      "step": 10600
    },
    {
      "epoch": 1.8617301280926477,
      "grad_norm": 6.669260501861572,
      "learning_rate": 1.897993800081886e-05,
      "loss": 1.7539,
      "step": 10610
    },
    {
      "epoch": 1.8634848218985787,
      "grad_norm": 10.689922332763672,
      "learning_rate": 1.8950693104053345e-05,
      "loss": 1.8787,
      "step": 10620
    },
    {
      "epoch": 1.8652395157045096,
      "grad_norm": 9.583078384399414,
      "learning_rate": 1.892144820728783e-05,
      "loss": 1.9116,
      "step": 10630
    },
    {
      "epoch": 1.8669942095104406,
      "grad_norm": 10.154425621032715,
      "learning_rate": 1.8892203310522312e-05,
      "loss": 1.7166,
      "step": 10640
    },
    {
      "epoch": 1.8687489033163713,
      "grad_norm": 9.696690559387207,
      "learning_rate": 1.88629584137568e-05,
      "loss": 1.7906,
      "step": 10650
    },
    {
      "epoch": 1.870503597122302,
      "grad_norm": 9.986124992370605,
      "learning_rate": 1.8833713516991287e-05,
      "loss": 1.7192,
      "step": 10660
    },
    {
      "epoch": 1.872258290928233,
      "grad_norm": 8.030577659606934,
      "learning_rate": 1.880446862022577e-05,
      "loss": 1.8874,
      "step": 10670
    },
    {
      "epoch": 1.874012984734164,
      "grad_norm": 10.220385551452637,
      "learning_rate": 1.8775223723460255e-05,
      "loss": 1.6323,
      "step": 10680
    },
    {
      "epoch": 1.8757676785400947,
      "grad_norm": 9.754371643066406,
      "learning_rate": 1.8745978826694744e-05,
      "loss": 1.5732,
      "step": 10690
    },
    {
      "epoch": 1.8775223723460255,
      "grad_norm": 11.355546951293945,
      "learning_rate": 1.871673392992923e-05,
      "loss": 1.6331,
      "step": 10700
    },
    {
      "epoch": 1.8792770661519564,
      "grad_norm": 8.549748420715332,
      "learning_rate": 1.8687489033163712e-05,
      "loss": 1.5766,
      "step": 10710
    },
    {
      "epoch": 1.8810317599578874,
      "grad_norm": 7.763460636138916,
      "learning_rate": 1.86582441363982e-05,
      "loss": 1.6553,
      "step": 10720
    },
    {
      "epoch": 1.8827864537638184,
      "grad_norm": 6.81072473526001,
      "learning_rate": 1.8628999239632686e-05,
      "loss": 1.4646,
      "step": 10730
    },
    {
      "epoch": 1.884541147569749,
      "grad_norm": 8.44577693939209,
      "learning_rate": 1.8599754342867172e-05,
      "loss": 1.5442,
      "step": 10740
    },
    {
      "epoch": 1.8862958413756798,
      "grad_norm": 10.246694564819336,
      "learning_rate": 1.8570509446101654e-05,
      "loss": 1.8117,
      "step": 10750
    },
    {
      "epoch": 1.8880505351816108,
      "grad_norm": 7.889489650726318,
      "learning_rate": 1.8541264549336143e-05,
      "loss": 1.7388,
      "step": 10760
    },
    {
      "epoch": 1.8898052289875418,
      "grad_norm": 8.761642456054688,
      "learning_rate": 1.851201965257063e-05,
      "loss": 1.685,
      "step": 10770
    },
    {
      "epoch": 1.8915599227934725,
      "grad_norm": 11.011614799499512,
      "learning_rate": 1.8482774755805114e-05,
      "loss": 1.7278,
      "step": 10780
    },
    {
      "epoch": 1.8933146165994033,
      "grad_norm": 9.400129318237305,
      "learning_rate": 1.8453529859039597e-05,
      "loss": 1.5813,
      "step": 10790
    },
    {
      "epoch": 1.8950693104053342,
      "grad_norm": 9.61010456085205,
      "learning_rate": 1.8424284962274086e-05,
      "loss": 1.7656,
      "step": 10800
    },
    {
      "epoch": 1.8968240042112652,
      "grad_norm": 8.26684284210205,
      "learning_rate": 1.839504006550857e-05,
      "loss": 1.6363,
      "step": 10810
    },
    {
      "epoch": 1.8985786980171961,
      "grad_norm": 9.818415641784668,
      "learning_rate": 1.8365795168743054e-05,
      "loss": 1.7911,
      "step": 10820
    },
    {
      "epoch": 1.9003333918231269,
      "grad_norm": 9.373180389404297,
      "learning_rate": 1.833655027197754e-05,
      "loss": 1.6927,
      "step": 10830
    },
    {
      "epoch": 1.9020880856290576,
      "grad_norm": 8.681254386901855,
      "learning_rate": 1.8307305375212028e-05,
      "loss": 1.7561,
      "step": 10840
    },
    {
      "epoch": 1.9038427794349886,
      "grad_norm": 8.46550464630127,
      "learning_rate": 1.8278060478446514e-05,
      "loss": 1.6635,
      "step": 10850
    },
    {
      "epoch": 1.9055974732409195,
      "grad_norm": 13.6680269241333,
      "learning_rate": 1.8248815581680996e-05,
      "loss": 1.7803,
      "step": 10860
    },
    {
      "epoch": 1.9073521670468503,
      "grad_norm": 10.035748481750488,
      "learning_rate": 1.821957068491548e-05,
      "loss": 1.7479,
      "step": 10870
    },
    {
      "epoch": 1.909106860852781,
      "grad_norm": 7.379312992095947,
      "learning_rate": 1.819032578814997e-05,
      "loss": 1.6058,
      "step": 10880
    },
    {
      "epoch": 1.910861554658712,
      "grad_norm": 9.723173141479492,
      "learning_rate": 1.8161080891384456e-05,
      "loss": 2.0651,
      "step": 10890
    },
    {
      "epoch": 1.912616248464643,
      "grad_norm": 9.513566970825195,
      "learning_rate": 1.813183599461894e-05,
      "loss": 1.6246,
      "step": 10900
    },
    {
      "epoch": 1.914370942270574,
      "grad_norm": 9.278735160827637,
      "learning_rate": 1.8102591097853424e-05,
      "loss": 1.7157,
      "step": 10910
    },
    {
      "epoch": 1.9161256360765047,
      "grad_norm": 9.135763168334961,
      "learning_rate": 1.8073346201087913e-05,
      "loss": 1.7442,
      "step": 10920
    },
    {
      "epoch": 1.9178803298824354,
      "grad_norm": 9.529622077941895,
      "learning_rate": 1.80441013043224e-05,
      "loss": 2.011,
      "step": 10930
    },
    {
      "epoch": 1.9196350236883664,
      "grad_norm": 8.663925170898438,
      "learning_rate": 1.801485640755688e-05,
      "loss": 1.7275,
      "step": 10940
    },
    {
      "epoch": 1.9213897174942973,
      "grad_norm": 8.599933624267578,
      "learning_rate": 1.7985611510791367e-05,
      "loss": 1.5484,
      "step": 10950
    },
    {
      "epoch": 1.923144411300228,
      "grad_norm": 10.37496280670166,
      "learning_rate": 1.7956366614025856e-05,
      "loss": 1.745,
      "step": 10960
    },
    {
      "epoch": 1.924899105106159,
      "grad_norm": 10.407289505004883,
      "learning_rate": 1.7927121717260338e-05,
      "loss": 1.8659,
      "step": 10970
    },
    {
      "epoch": 1.9266537989120898,
      "grad_norm": 10.43406867980957,
      "learning_rate": 1.7897876820494823e-05,
      "loss": 2.0858,
      "step": 10980
    },
    {
      "epoch": 1.9284084927180207,
      "grad_norm": 8.014933586120605,
      "learning_rate": 1.786863192372931e-05,
      "loss": 1.6602,
      "step": 10990
    },
    {
      "epoch": 1.9301631865239517,
      "grad_norm": 8.624707221984863,
      "learning_rate": 1.7839387026963798e-05,
      "loss": 1.8242,
      "step": 11000
    },
    {
      "epoch": 1.9319178803298824,
      "grad_norm": 10.464189529418945,
      "learning_rate": 1.781014213019828e-05,
      "loss": 1.7736,
      "step": 11010
    },
    {
      "epoch": 1.9336725741358132,
      "grad_norm": 10.98193359375,
      "learning_rate": 1.7780897233432766e-05,
      "loss": 1.6835,
      "step": 11020
    },
    {
      "epoch": 1.9354272679417441,
      "grad_norm": 7.861283779144287,
      "learning_rate": 1.775165233666725e-05,
      "loss": 1.6285,
      "step": 11030
    },
    {
      "epoch": 1.937181961747675,
      "grad_norm": 9.107782363891602,
      "learning_rate": 1.772240743990174e-05,
      "loss": 1.9249,
      "step": 11040
    },
    {
      "epoch": 1.938936655553606,
      "grad_norm": 8.331486701965332,
      "learning_rate": 1.7693162543136223e-05,
      "loss": 1.835,
      "step": 11050
    },
    {
      "epoch": 1.9406913493595368,
      "grad_norm": 7.881077766418457,
      "learning_rate": 1.766391764637071e-05,
      "loss": 1.6195,
      "step": 11060
    },
    {
      "epoch": 1.9424460431654675,
      "grad_norm": 10.190935134887695,
      "learning_rate": 1.7634672749605194e-05,
      "loss": 1.8399,
      "step": 11070
    },
    {
      "epoch": 1.9442007369713985,
      "grad_norm": 9.533634185791016,
      "learning_rate": 1.7605427852839683e-05,
      "loss": 1.8325,
      "step": 11080
    },
    {
      "epoch": 1.9459554307773295,
      "grad_norm": 9.784087181091309,
      "learning_rate": 1.7576182956074165e-05,
      "loss": 1.7723,
      "step": 11090
    },
    {
      "epoch": 1.9477101245832602,
      "grad_norm": 7.8527116775512695,
      "learning_rate": 1.754693805930865e-05,
      "loss": 1.6237,
      "step": 11100
    },
    {
      "epoch": 1.949464818389191,
      "grad_norm": 6.742361068725586,
      "learning_rate": 1.7517693162543137e-05,
      "loss": 1.5651,
      "step": 11110
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 7.989025592803955,
      "learning_rate": 1.7488448265777622e-05,
      "loss": 1.5398,
      "step": 11120
    },
    {
      "epoch": 1.9529742060010529,
      "grad_norm": 11.155352592468262,
      "learning_rate": 1.7459203369012108e-05,
      "loss": 1.9954,
      "step": 11130
    },
    {
      "epoch": 1.9547288998069838,
      "grad_norm": 7.552244663238525,
      "learning_rate": 1.7429958472246593e-05,
      "loss": 1.5989,
      "step": 11140
    },
    {
      "epoch": 1.9564835936129146,
      "grad_norm": 7.888658046722412,
      "learning_rate": 1.740071357548108e-05,
      "loss": 1.8815,
      "step": 11150
    },
    {
      "epoch": 1.9582382874188453,
      "grad_norm": 10.102225303649902,
      "learning_rate": 1.7371468678715565e-05,
      "loss": 1.9727,
      "step": 11160
    },
    {
      "epoch": 1.9599929812247763,
      "grad_norm": 11.104996681213379,
      "learning_rate": 1.734222378195005e-05,
      "loss": 2.0307,
      "step": 11170
    },
    {
      "epoch": 1.9617476750307072,
      "grad_norm": 8.436601638793945,
      "learning_rate": 1.7312978885184536e-05,
      "loss": 1.5363,
      "step": 11180
    },
    {
      "epoch": 1.963502368836638,
      "grad_norm": 8.26463508605957,
      "learning_rate": 1.728373398841902e-05,
      "loss": 1.9787,
      "step": 11190
    },
    {
      "epoch": 1.9652570626425687,
      "grad_norm": 10.958367347717285,
      "learning_rate": 1.7254489091653507e-05,
      "loss": 1.7821,
      "step": 11200
    },
    {
      "epoch": 1.9670117564484997,
      "grad_norm": 11.466344833374023,
      "learning_rate": 1.7225244194887993e-05,
      "loss": 1.8266,
      "step": 11210
    },
    {
      "epoch": 1.9687664502544306,
      "grad_norm": 10.110020637512207,
      "learning_rate": 1.719599929812248e-05,
      "loss": 2.0435,
      "step": 11220
    },
    {
      "epoch": 1.9705211440603616,
      "grad_norm": 6.780657768249512,
      "learning_rate": 1.7166754401356964e-05,
      "loss": 1.7129,
      "step": 11230
    },
    {
      "epoch": 1.9722758378662923,
      "grad_norm": 8.387899398803711,
      "learning_rate": 1.713750950459145e-05,
      "loss": 1.8266,
      "step": 11240
    },
    {
      "epoch": 1.974030531672223,
      "grad_norm": 7.729079723358154,
      "learning_rate": 1.7108264607825935e-05,
      "loss": 1.7399,
      "step": 11250
    },
    {
      "epoch": 1.975785225478154,
      "grad_norm": 9.45782470703125,
      "learning_rate": 1.707901971106042e-05,
      "loss": 1.8596,
      "step": 11260
    },
    {
      "epoch": 1.977539919284085,
      "grad_norm": 9.201464653015137,
      "learning_rate": 1.7049774814294907e-05,
      "loss": 1.5564,
      "step": 11270
    },
    {
      "epoch": 1.9792946130900158,
      "grad_norm": 9.44221019744873,
      "learning_rate": 1.7020529917529392e-05,
      "loss": 1.7667,
      "step": 11280
    },
    {
      "epoch": 1.9810493068959465,
      "grad_norm": 9.792330741882324,
      "learning_rate": 1.6991285020763878e-05,
      "loss": 1.7149,
      "step": 11290
    },
    {
      "epoch": 1.9828040007018775,
      "grad_norm": 6.486392974853516,
      "learning_rate": 1.6962040123998363e-05,
      "loss": 1.7827,
      "step": 11300
    },
    {
      "epoch": 1.9845586945078084,
      "grad_norm": 7.9338297843933105,
      "learning_rate": 1.693279522723285e-05,
      "loss": 1.416,
      "step": 11310
    },
    {
      "epoch": 1.9863133883137394,
      "grad_norm": 8.370370864868164,
      "learning_rate": 1.6903550330467335e-05,
      "loss": 1.7702,
      "step": 11320
    },
    {
      "epoch": 1.9880680821196701,
      "grad_norm": 10.23532772064209,
      "learning_rate": 1.687430543370182e-05,
      "loss": 1.6592,
      "step": 11330
    },
    {
      "epoch": 1.9898227759256009,
      "grad_norm": 7.505883693695068,
      "learning_rate": 1.6845060536936306e-05,
      "loss": 1.5553,
      "step": 11340
    },
    {
      "epoch": 1.9915774697315318,
      "grad_norm": 11.009270668029785,
      "learning_rate": 1.681581564017079e-05,
      "loss": 1.664,
      "step": 11350
    },
    {
      "epoch": 1.9933321635374628,
      "grad_norm": 11.048267364501953,
      "learning_rate": 1.678949523308183e-05,
      "loss": 1.8469,
      "step": 11360
    },
    {
      "epoch": 1.9950868573433935,
      "grad_norm": 10.154349327087402,
      "learning_rate": 1.6760250336316315e-05,
      "loss": 1.7223,
      "step": 11370
    },
    {
      "epoch": 1.9968415511493245,
      "grad_norm": 8.604768753051758,
      "learning_rate": 1.6731005439550797e-05,
      "loss": 1.7655,
      "step": 11380
    },
    {
      "epoch": 1.9985962449552552,
      "grad_norm": 11.248263359069824,
      "learning_rate": 1.6701760542785286e-05,
      "loss": 1.7035,
      "step": 11390
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.4504694217776608,
      "eval_f1_C01": 0.43304843304843305,
      "eval_f1_C02": 0.2268041237113402,
      "eval_f1_C03": 0.5555555555555556,
      "eval_f1_C04": 0.5882352941176471,
      "eval_f1_C05": 0.35,
      "eval_f1_C06": 0.48093385214007783,
      "eval_f1_C07": 0.3016759776536313,
      "eval_f1_C08": 0.4322274881516588,
      "eval_f1_C09": 0.35294117647058826,
      "eval_f1_C10": 0.4755332496863237,
      "eval_f1_C11": 0.49282296650717705,
      "eval_f1_C12": 0.5247349823321554,
      "eval_f1_C13": 0.45476190476190476,
      "eval_f1_C14": 0.6203641267700607,
      "eval_f1_C15": 0.2519280205655527,
      "eval_f1_C16": 0.22153846153846155,
      "eval_f1_C17": 0.37261146496815284,
      "eval_f1_C18": 0.4872448979591837,
      "eval_f1_C19": 0.26714801444043323,
      "eval_f1_C20": 0.4859154929577465,
      "eval_f1_C21": 0.5403445149592022,
      "eval_f1_C22": 0.0,
      "eval_f1_C23": 0.1807099318752241,
      "eval_f1_macro": 0.3955252143552395,
      "eval_loss": 1.6828253269195557,
      "eval_precision_C01": 0.41834862385321103,
      "eval_precision_C02": 0.2857142857142857,
      "eval_precision_C03": 0.6779661016949152,
      "eval_precision_C04": 0.5098493626882966,
      "eval_precision_C05": 0.3684210526315789,
      "eval_precision_C06": 0.4497816593886463,
      "eval_precision_C07": 0.36486486486486486,
      "eval_precision_C08": 0.4245810055865922,
      "eval_precision_C09": 0.3493150684931507,
      "eval_precision_C10": 0.45995145631067963,
      "eval_precision_C11": 0.4724770642201835,
      "eval_precision_C12": 0.4729299363057325,
      "eval_precision_C13": 0.37087378640776697,
      "eval_precision_C14": 0.5269186712485682,
      "eval_precision_C15": 0.3656716417910448,
      "eval_precision_C16": 0.3333333333333333,
      "eval_precision_C17": 0.3836065573770492,
      "eval_precision_C18": 0.4775,
      "eval_precision_C19": 0.3557692307692308,
      "eval_precision_C20": 0.4328732747804266,
      "eval_precision_C21": 0.5775193798449613,
      "eval_precision_C22": 0.0,
      "eval_precision_C23": 0.2906574394463668,
      "eval_precision_global": 0.40734451290221235,
      "eval_recall_C01": 0.44881889763779526,
      "eval_recall_C02": 0.18803418803418803,
      "eval_recall_C03": 0.47058823529411764,
      "eval_recall_C04": 0.6951026856240127,
      "eval_recall_C05": 0.3333333333333333,
      "eval_recall_C06": 0.5167224080267558,
      "eval_recall_C07": 0.2571428571428571,
      "eval_recall_C08": 0.44015444015444016,
      "eval_recall_C09": 0.35664335664335667,
      "eval_recall_C10": 0.4922077922077922,
      "eval_recall_C11": 0.515,
      "eval_recall_C12": 0.5892857142857143,
      "eval_recall_C13": 0.5876923076923077,
      "eval_recall_C14": 0.7540983606557377,
      "eval_recall_C15": 0.19215686274509805,
      "eval_recall_C16": 0.16589861751152074,
      "eval_recall_C17": 0.3622291021671827,
      "eval_recall_C18": 0.4973958333333333,
      "eval_recall_C19": 0.2138728323699422,
      "eval_recall_C20": 0.5537720706260032,
      "eval_recall_C21": 0.5076660988074957,
      "eval_recall_C22": 0.0,
      "eval_recall_C23": 0.13111342351716962,
      "eval_recall_global": 0.40299693120913715,
      "eval_runtime": 51.5301,
      "eval_samples_per_second": 221.172,
      "eval_steps_per_second": 27.654,
      "step": 11398
    },
    {
      "epoch": 2.000350938761186,
      "grad_norm": 8.645730972290039,
      "learning_rate": 1.6672515646019772e-05,
      "loss": 1.6655,
      "step": 11400
    },
    {
      "epoch": 2.002105632567117,
      "grad_norm": 9.335371017456055,
      "learning_rate": 1.6643270749254254e-05,
      "loss": 1.3986,
      "step": 11410
    },
    {
      "epoch": 2.003860326373048,
      "grad_norm": 7.365959644317627,
      "learning_rate": 1.661402585248874e-05,
      "loss": 1.8164,
      "step": 11420
    },
    {
      "epoch": 2.0056150201789786,
      "grad_norm": 8.266407012939453,
      "learning_rate": 1.658478095572323e-05,
      "loss": 1.7302,
      "step": 11430
    },
    {
      "epoch": 2.0073697139849096,
      "grad_norm": 7.998568534851074,
      "learning_rate": 1.6555536058957714e-05,
      "loss": 1.5363,
      "step": 11440
    },
    {
      "epoch": 2.0091244077908406,
      "grad_norm": 7.353567123413086,
      "learning_rate": 1.6526291162192196e-05,
      "loss": 1.5419,
      "step": 11450
    },
    {
      "epoch": 2.0108791015967715,
      "grad_norm": 8.998563766479492,
      "learning_rate": 1.6497046265426682e-05,
      "loss": 1.5203,
      "step": 11460
    },
    {
      "epoch": 2.012633795402702,
      "grad_norm": 9.624674797058105,
      "learning_rate": 1.646780136866117e-05,
      "loss": 1.5782,
      "step": 11470
    },
    {
      "epoch": 2.014388489208633,
      "grad_norm": 9.605278968811035,
      "learning_rate": 1.6438556471895657e-05,
      "loss": 1.6398,
      "step": 11480
    },
    {
      "epoch": 2.016143183014564,
      "grad_norm": 7.335468292236328,
      "learning_rate": 1.640931157513014e-05,
      "loss": 1.3369,
      "step": 11490
    },
    {
      "epoch": 2.017897876820495,
      "grad_norm": 10.943595886230469,
      "learning_rate": 1.6380066678364625e-05,
      "loss": 1.9449,
      "step": 11500
    },
    {
      "epoch": 2.019652570626426,
      "grad_norm": 9.12691593170166,
      "learning_rate": 1.6350821781599114e-05,
      "loss": 1.4909,
      "step": 11510
    },
    {
      "epoch": 2.0214072644323564,
      "grad_norm": 10.61489486694336,
      "learning_rate": 1.63215768848336e-05,
      "loss": 1.5232,
      "step": 11520
    },
    {
      "epoch": 2.0231619582382874,
      "grad_norm": 9.531538009643555,
      "learning_rate": 1.629233198806808e-05,
      "loss": 1.5187,
      "step": 11530
    },
    {
      "epoch": 2.0249166520442183,
      "grad_norm": 7.746670722961426,
      "learning_rate": 1.6263087091302567e-05,
      "loss": 1.5271,
      "step": 11540
    },
    {
      "epoch": 2.0266713458501493,
      "grad_norm": 10.164623260498047,
      "learning_rate": 1.6233842194537056e-05,
      "loss": 1.5444,
      "step": 11550
    },
    {
      "epoch": 2.02842603965608,
      "grad_norm": 7.3509087562561035,
      "learning_rate": 1.6204597297771538e-05,
      "loss": 1.6783,
      "step": 11560
    },
    {
      "epoch": 2.030180733462011,
      "grad_norm": 11.278038024902344,
      "learning_rate": 1.6175352401006024e-05,
      "loss": 1.6238,
      "step": 11570
    },
    {
      "epoch": 2.0319354272679417,
      "grad_norm": 7.255465507507324,
      "learning_rate": 1.614610750424051e-05,
      "loss": 1.6909,
      "step": 11580
    },
    {
      "epoch": 2.0336901210738727,
      "grad_norm": 9.715103149414062,
      "learning_rate": 1.6116862607475e-05,
      "loss": 1.7294,
      "step": 11590
    },
    {
      "epoch": 2.0354448148798037,
      "grad_norm": 9.970640182495117,
      "learning_rate": 1.608761771070948e-05,
      "loss": 1.5903,
      "step": 11600
    },
    {
      "epoch": 2.037199508685734,
      "grad_norm": 7.3086466789245605,
      "learning_rate": 1.6058372813943966e-05,
      "loss": 1.6679,
      "step": 11610
    },
    {
      "epoch": 2.038954202491665,
      "grad_norm": 9.696260452270508,
      "learning_rate": 1.6029127917178452e-05,
      "loss": 1.9378,
      "step": 11620
    },
    {
      "epoch": 2.040708896297596,
      "grad_norm": 10.898197174072266,
      "learning_rate": 1.599988302041294e-05,
      "loss": 2.0257,
      "step": 11630
    },
    {
      "epoch": 2.042463590103527,
      "grad_norm": 9.243600845336914,
      "learning_rate": 1.5970638123647423e-05,
      "loss": 1.3977,
      "step": 11640
    },
    {
      "epoch": 2.0442182839094576,
      "grad_norm": 9.074006080627441,
      "learning_rate": 1.594139322688191e-05,
      "loss": 1.6242,
      "step": 11650
    },
    {
      "epoch": 2.0459729777153886,
      "grad_norm": 8.059730529785156,
      "learning_rate": 1.5912148330116394e-05,
      "loss": 1.792,
      "step": 11660
    },
    {
      "epoch": 2.0477276715213195,
      "grad_norm": 8.711724281311035,
      "learning_rate": 1.5882903433350883e-05,
      "loss": 1.9863,
      "step": 11670
    },
    {
      "epoch": 2.0494823653272505,
      "grad_norm": 11.260673522949219,
      "learning_rate": 1.5853658536585366e-05,
      "loss": 1.6878,
      "step": 11680
    },
    {
      "epoch": 2.0512370591331814,
      "grad_norm": 9.179145812988281,
      "learning_rate": 1.582441363981985e-05,
      "loss": 1.4786,
      "step": 11690
    },
    {
      "epoch": 2.052991752939112,
      "grad_norm": 9.609874725341797,
      "learning_rate": 1.5795168743054337e-05,
      "loss": 1.594,
      "step": 11700
    },
    {
      "epoch": 2.054746446745043,
      "grad_norm": 10.055258750915527,
      "learning_rate": 1.5765923846288823e-05,
      "loss": 1.8637,
      "step": 11710
    },
    {
      "epoch": 2.056501140550974,
      "grad_norm": 8.394512176513672,
      "learning_rate": 1.5736678949523308e-05,
      "loss": 1.6988,
      "step": 11720
    },
    {
      "epoch": 2.058255834356905,
      "grad_norm": 8.936501502990723,
      "learning_rate": 1.5707434052757794e-05,
      "loss": 1.692,
      "step": 11730
    },
    {
      "epoch": 2.0600105281628354,
      "grad_norm": 5.927695274353027,
      "learning_rate": 1.567818915599228e-05,
      "loss": 1.3958,
      "step": 11740
    },
    {
      "epoch": 2.0617652219687663,
      "grad_norm": 8.020910263061523,
      "learning_rate": 1.5648944259226765e-05,
      "loss": 1.6383,
      "step": 11750
    },
    {
      "epoch": 2.0635199157746973,
      "grad_norm": 11.268837928771973,
      "learning_rate": 1.561969936246125e-05,
      "loss": 1.9723,
      "step": 11760
    },
    {
      "epoch": 2.0652746095806283,
      "grad_norm": 10.646586418151855,
      "learning_rate": 1.5590454465695736e-05,
      "loss": 1.6928,
      "step": 11770
    },
    {
      "epoch": 2.067029303386559,
      "grad_norm": 8.108657836914062,
      "learning_rate": 1.5561209568930222e-05,
      "loss": 1.4089,
      "step": 11780
    },
    {
      "epoch": 2.0687839971924897,
      "grad_norm": 12.488410949707031,
      "learning_rate": 1.5531964672164708e-05,
      "loss": 1.7879,
      "step": 11790
    },
    {
      "epoch": 2.0705386909984207,
      "grad_norm": 12.868727684020996,
      "learning_rate": 1.5502719775399193e-05,
      "loss": 1.3747,
      "step": 11800
    },
    {
      "epoch": 2.0722933848043517,
      "grad_norm": 8.257102012634277,
      "learning_rate": 1.547347487863368e-05,
      "loss": 1.4142,
      "step": 11810
    },
    {
      "epoch": 2.0740480786102826,
      "grad_norm": 8.04133415222168,
      "learning_rate": 1.5444229981868164e-05,
      "loss": 1.7406,
      "step": 11820
    },
    {
      "epoch": 2.0758027724162136,
      "grad_norm": 6.89880895614624,
      "learning_rate": 1.541498508510265e-05,
      "loss": 1.4569,
      "step": 11830
    },
    {
      "epoch": 2.077557466222144,
      "grad_norm": 6.5768961906433105,
      "learning_rate": 1.5385740188337136e-05,
      "loss": 1.6393,
      "step": 11840
    },
    {
      "epoch": 2.079312160028075,
      "grad_norm": 10.009912490844727,
      "learning_rate": 1.535649529157162e-05,
      "loss": 1.7399,
      "step": 11850
    },
    {
      "epoch": 2.081066853834006,
      "grad_norm": 10.474843978881836,
      "learning_rate": 1.5327250394806107e-05,
      "loss": 1.8396,
      "step": 11860
    },
    {
      "epoch": 2.082821547639937,
      "grad_norm": 11.046741485595703,
      "learning_rate": 1.5298005498040593e-05,
      "loss": 1.9233,
      "step": 11870
    },
    {
      "epoch": 2.0845762414458675,
      "grad_norm": 7.395845890045166,
      "learning_rate": 1.5268760601275078e-05,
      "loss": 1.5921,
      "step": 11880
    },
    {
      "epoch": 2.0863309352517985,
      "grad_norm": 10.952130317687988,
      "learning_rate": 1.5239515704509564e-05,
      "loss": 1.6329,
      "step": 11890
    },
    {
      "epoch": 2.0880856290577294,
      "grad_norm": 8.28233528137207,
      "learning_rate": 1.5210270807744048e-05,
      "loss": 1.3304,
      "step": 11900
    },
    {
      "epoch": 2.0898403228636604,
      "grad_norm": 9.655689239501953,
      "learning_rate": 1.5181025910978535e-05,
      "loss": 1.7892,
      "step": 11910
    },
    {
      "epoch": 2.0915950166695914,
      "grad_norm": 10.252312660217285,
      "learning_rate": 1.515178101421302e-05,
      "loss": 1.6589,
      "step": 11920
    },
    {
      "epoch": 2.093349710475522,
      "grad_norm": 8.006658554077148,
      "learning_rate": 1.5122536117447506e-05,
      "loss": 1.8355,
      "step": 11930
    },
    {
      "epoch": 2.095104404281453,
      "grad_norm": 9.988004684448242,
      "learning_rate": 1.509329122068199e-05,
      "loss": 1.5765,
      "step": 11940
    },
    {
      "epoch": 2.096859098087384,
      "grad_norm": 10.952967643737793,
      "learning_rate": 1.5064046323916477e-05,
      "loss": 1.8298,
      "step": 11950
    },
    {
      "epoch": 2.0986137918933148,
      "grad_norm": 8.829910278320312,
      "learning_rate": 1.5034801427150963e-05,
      "loss": 1.7583,
      "step": 11960
    },
    {
      "epoch": 2.1003684856992453,
      "grad_norm": 7.214174747467041,
      "learning_rate": 1.5005556530385447e-05,
      "loss": 1.871,
      "step": 11970
    },
    {
      "epoch": 2.1021231795051762,
      "grad_norm": 9.014708518981934,
      "learning_rate": 1.4976311633619933e-05,
      "loss": 1.5833,
      "step": 11980
    },
    {
      "epoch": 2.103877873311107,
      "grad_norm": 9.956226348876953,
      "learning_rate": 1.494706673685442e-05,
      "loss": 1.6167,
      "step": 11990
    },
    {
      "epoch": 2.105632567117038,
      "grad_norm": 8.926947593688965,
      "learning_rate": 1.4917821840088906e-05,
      "loss": 1.5923,
      "step": 12000
    },
    {
      "epoch": 2.107387260922969,
      "grad_norm": 9.649175643920898,
      "learning_rate": 1.488857694332339e-05,
      "loss": 1.6759,
      "step": 12010
    },
    {
      "epoch": 2.1091419547288996,
      "grad_norm": 9.87642765045166,
      "learning_rate": 1.4859332046557875e-05,
      "loss": 1.9775,
      "step": 12020
    },
    {
      "epoch": 2.1108966485348306,
      "grad_norm": 12.358518600463867,
      "learning_rate": 1.4830087149792362e-05,
      "loss": 2.0086,
      "step": 12030
    },
    {
      "epoch": 2.1126513423407616,
      "grad_norm": 8.736462593078613,
      "learning_rate": 1.4800842253026848e-05,
      "loss": 1.9034,
      "step": 12040
    },
    {
      "epoch": 2.1144060361466925,
      "grad_norm": 7.029114723205566,
      "learning_rate": 1.4771597356261332e-05,
      "loss": 1.6833,
      "step": 12050
    },
    {
      "epoch": 2.116160729952623,
      "grad_norm": 8.353849411010742,
      "learning_rate": 1.4742352459495818e-05,
      "loss": 1.7709,
      "step": 12060
    },
    {
      "epoch": 2.117915423758554,
      "grad_norm": 8.101387977600098,
      "learning_rate": 1.4713107562730305e-05,
      "loss": 1.7108,
      "step": 12070
    },
    {
      "epoch": 2.119670117564485,
      "grad_norm": 6.296718120574951,
      "learning_rate": 1.468386266596479e-05,
      "loss": 1.8511,
      "step": 12080
    },
    {
      "epoch": 2.121424811370416,
      "grad_norm": 9.301572799682617,
      "learning_rate": 1.4654617769199274e-05,
      "loss": 1.7155,
      "step": 12090
    },
    {
      "epoch": 2.123179505176347,
      "grad_norm": 6.847378253936768,
      "learning_rate": 1.462537287243376e-05,
      "loss": 1.6174,
      "step": 12100
    },
    {
      "epoch": 2.1249341989822774,
      "grad_norm": 10.101666450500488,
      "learning_rate": 1.4596127975668247e-05,
      "loss": 1.9512,
      "step": 12110
    },
    {
      "epoch": 2.1266888927882084,
      "grad_norm": 8.190935134887695,
      "learning_rate": 1.4566883078902731e-05,
      "loss": 1.6535,
      "step": 12120
    },
    {
      "epoch": 2.1284435865941393,
      "grad_norm": 9.13408088684082,
      "learning_rate": 1.4537638182137217e-05,
      "loss": 1.9758,
      "step": 12130
    },
    {
      "epoch": 2.1301982804000703,
      "grad_norm": 10.741504669189453,
      "learning_rate": 1.4508393285371703e-05,
      "loss": 1.7154,
      "step": 12140
    },
    {
      "epoch": 2.1319529742060013,
      "grad_norm": 8.67241096496582,
      "learning_rate": 1.447914838860619e-05,
      "loss": 1.5433,
      "step": 12150
    },
    {
      "epoch": 2.133707668011932,
      "grad_norm": 6.553867816925049,
      "learning_rate": 1.4449903491840674e-05,
      "loss": 1.761,
      "step": 12160
    },
    {
      "epoch": 2.1354623618178628,
      "grad_norm": 8.778273582458496,
      "learning_rate": 1.442065859507516e-05,
      "loss": 2.0982,
      "step": 12170
    },
    {
      "epoch": 2.1372170556237937,
      "grad_norm": 6.088240623474121,
      "learning_rate": 1.4391413698309647e-05,
      "loss": 1.6185,
      "step": 12180
    },
    {
      "epoch": 2.1389717494297247,
      "grad_norm": 8.337028503417969,
      "learning_rate": 1.4362168801544132e-05,
      "loss": 1.6683,
      "step": 12190
    },
    {
      "epoch": 2.140726443235655,
      "grad_norm": 9.327923774719238,
      "learning_rate": 1.4332923904778616e-05,
      "loss": 1.6608,
      "step": 12200
    },
    {
      "epoch": 2.142481137041586,
      "grad_norm": 10.005351066589355,
      "learning_rate": 1.4303679008013102e-05,
      "loss": 1.6794,
      "step": 12210
    },
    {
      "epoch": 2.144235830847517,
      "grad_norm": 10.174509048461914,
      "learning_rate": 1.427443411124759e-05,
      "loss": 1.6931,
      "step": 12220
    },
    {
      "epoch": 2.145990524653448,
      "grad_norm": 8.640349388122559,
      "learning_rate": 1.4245189214482075e-05,
      "loss": 1.9363,
      "step": 12230
    },
    {
      "epoch": 2.147745218459379,
      "grad_norm": 9.862326622009277,
      "learning_rate": 1.4215944317716559e-05,
      "loss": 1.7157,
      "step": 12240
    },
    {
      "epoch": 2.1494999122653096,
      "grad_norm": 7.540772914886475,
      "learning_rate": 1.4186699420951044e-05,
      "loss": 1.5723,
      "step": 12250
    },
    {
      "epoch": 2.1512546060712405,
      "grad_norm": 9.859163284301758,
      "learning_rate": 1.4157454524185532e-05,
      "loss": 1.701,
      "step": 12260
    },
    {
      "epoch": 2.1530092998771715,
      "grad_norm": 8.904380798339844,
      "learning_rate": 1.4128209627420017e-05,
      "loss": 1.6934,
      "step": 12270
    },
    {
      "epoch": 2.1547639936831025,
      "grad_norm": 7.9692535400390625,
      "learning_rate": 1.4098964730654501e-05,
      "loss": 1.8041,
      "step": 12280
    },
    {
      "epoch": 2.156518687489033,
      "grad_norm": 9.879271507263184,
      "learning_rate": 1.4069719833888987e-05,
      "loss": 1.7407,
      "step": 12290
    },
    {
      "epoch": 2.158273381294964,
      "grad_norm": 7.43869686126709,
      "learning_rate": 1.4040474937123474e-05,
      "loss": 1.6793,
      "step": 12300
    },
    {
      "epoch": 2.160028075100895,
      "grad_norm": 7.755650997161865,
      "learning_rate": 1.4011230040357958e-05,
      "loss": 1.5795,
      "step": 12310
    },
    {
      "epoch": 2.161782768906826,
      "grad_norm": 8.794836044311523,
      "learning_rate": 1.3981985143592444e-05,
      "loss": 1.6604,
      "step": 12320
    },
    {
      "epoch": 2.163537462712757,
      "grad_norm": 11.650618553161621,
      "learning_rate": 1.395274024682693e-05,
      "loss": 1.864,
      "step": 12330
    },
    {
      "epoch": 2.1652921565186873,
      "grad_norm": 8.705317497253418,
      "learning_rate": 1.3923495350061417e-05,
      "loss": 1.7231,
      "step": 12340
    },
    {
      "epoch": 2.1670468503246183,
      "grad_norm": 8.837482452392578,
      "learning_rate": 1.38942504532959e-05,
      "loss": 1.6116,
      "step": 12350
    },
    {
      "epoch": 2.1688015441305493,
      "grad_norm": 8.981616020202637,
      "learning_rate": 1.3865005556530386e-05,
      "loss": 1.73,
      "step": 12360
    },
    {
      "epoch": 2.1705562379364802,
      "grad_norm": 6.966322422027588,
      "learning_rate": 1.383576065976487e-05,
      "loss": 1.6112,
      "step": 12370
    },
    {
      "epoch": 2.1723109317424107,
      "grad_norm": 9.583521842956543,
      "learning_rate": 1.380651576299936e-05,
      "loss": 1.7181,
      "step": 12380
    },
    {
      "epoch": 2.1740656255483417,
      "grad_norm": 8.569198608398438,
      "learning_rate": 1.3777270866233843e-05,
      "loss": 1.6954,
      "step": 12390
    },
    {
      "epoch": 2.1758203193542727,
      "grad_norm": 8.558792114257812,
      "learning_rate": 1.3748025969468329e-05,
      "loss": 1.4338,
      "step": 12400
    },
    {
      "epoch": 2.1775750131602036,
      "grad_norm": 10.920587539672852,
      "learning_rate": 1.3718781072702813e-05,
      "loss": 1.8559,
      "step": 12410
    },
    {
      "epoch": 2.1793297069661346,
      "grad_norm": 8.708620071411133,
      "learning_rate": 1.3689536175937302e-05,
      "loss": 1.6411,
      "step": 12420
    },
    {
      "epoch": 2.181084400772065,
      "grad_norm": 9.802124977111816,
      "learning_rate": 1.3660291279171786e-05,
      "loss": 1.625,
      "step": 12430
    },
    {
      "epoch": 2.182839094577996,
      "grad_norm": 10.22231674194336,
      "learning_rate": 1.3631046382406271e-05,
      "loss": 1.8543,
      "step": 12440
    },
    {
      "epoch": 2.184593788383927,
      "grad_norm": 7.9334330558776855,
      "learning_rate": 1.3601801485640755e-05,
      "loss": 1.8333,
      "step": 12450
    },
    {
      "epoch": 2.186348482189858,
      "grad_norm": 7.35725736618042,
      "learning_rate": 1.3572556588875242e-05,
      "loss": 1.3927,
      "step": 12460
    },
    {
      "epoch": 2.1881031759957885,
      "grad_norm": 11.617703437805176,
      "learning_rate": 1.3543311692109728e-05,
      "loss": 1.7285,
      "step": 12470
    },
    {
      "epoch": 2.1898578698017195,
      "grad_norm": 9.978744506835938,
      "learning_rate": 1.3514066795344214e-05,
      "loss": 1.4747,
      "step": 12480
    },
    {
      "epoch": 2.1916125636076504,
      "grad_norm": 13.756932258605957,
      "learning_rate": 1.3484821898578698e-05,
      "loss": 1.7112,
      "step": 12490
    },
    {
      "epoch": 2.1933672574135814,
      "grad_norm": 9.260765075683594,
      "learning_rate": 1.3455577001813185e-05,
      "loss": 1.7976,
      "step": 12500
    },
    {
      "epoch": 2.1951219512195124,
      "grad_norm": 10.189413070678711,
      "learning_rate": 1.342633210504767e-05,
      "loss": 1.546,
      "step": 12510
    },
    {
      "epoch": 2.196876645025443,
      "grad_norm": 10.11962890625,
      "learning_rate": 1.3397087208282155e-05,
      "loss": 1.8836,
      "step": 12520
    },
    {
      "epoch": 2.198631338831374,
      "grad_norm": 10.832877159118652,
      "learning_rate": 1.336784231151664e-05,
      "loss": 1.742,
      "step": 12530
    },
    {
      "epoch": 2.200386032637305,
      "grad_norm": 8.924529075622559,
      "learning_rate": 1.3338597414751127e-05,
      "loss": 1.6468,
      "step": 12540
    },
    {
      "epoch": 2.2021407264432358,
      "grad_norm": 9.890408515930176,
      "learning_rate": 1.3309352517985613e-05,
      "loss": 1.9386,
      "step": 12550
    },
    {
      "epoch": 2.2038954202491663,
      "grad_norm": 9.721549034118652,
      "learning_rate": 1.3280107621220097e-05,
      "loss": 1.6728,
      "step": 12560
    },
    {
      "epoch": 2.2056501140550973,
      "grad_norm": 11.132697105407715,
      "learning_rate": 1.3250862724454583e-05,
      "loss": 1.9039,
      "step": 12570
    },
    {
      "epoch": 2.207404807861028,
      "grad_norm": 7.76243257522583,
      "learning_rate": 1.322161782768907e-05,
      "loss": 1.8085,
      "step": 12580
    },
    {
      "epoch": 2.209159501666959,
      "grad_norm": 7.1481146812438965,
      "learning_rate": 1.3192372930923556e-05,
      "loss": 1.8419,
      "step": 12590
    },
    {
      "epoch": 2.21091419547289,
      "grad_norm": 7.805161476135254,
      "learning_rate": 1.316312803415804e-05,
      "loss": 1.7974,
      "step": 12600
    },
    {
      "epoch": 2.2126688892788207,
      "grad_norm": 10.95549488067627,
      "learning_rate": 1.3133883137392525e-05,
      "loss": 1.5873,
      "step": 12610
    },
    {
      "epoch": 2.2144235830847516,
      "grad_norm": 8.363144874572754,
      "learning_rate": 1.3104638240627012e-05,
      "loss": 1.5021,
      "step": 12620
    },
    {
      "epoch": 2.2161782768906826,
      "grad_norm": 9.027972221374512,
      "learning_rate": 1.3075393343861498e-05,
      "loss": 1.6418,
      "step": 12630
    },
    {
      "epoch": 2.2179329706966135,
      "grad_norm": 7.707304000854492,
      "learning_rate": 1.3046148447095982e-05,
      "loss": 1.8103,
      "step": 12640
    },
    {
      "epoch": 2.2196876645025445,
      "grad_norm": 10.25418758392334,
      "learning_rate": 1.3016903550330468e-05,
      "loss": 1.5874,
      "step": 12650
    },
    {
      "epoch": 2.221442358308475,
      "grad_norm": 7.117744445800781,
      "learning_rate": 1.2987658653564955e-05,
      "loss": 1.3974,
      "step": 12660
    },
    {
      "epoch": 2.223197052114406,
      "grad_norm": 7.994534015655518,
      "learning_rate": 1.2958413756799439e-05,
      "loss": 1.757,
      "step": 12670
    },
    {
      "epoch": 2.224951745920337,
      "grad_norm": 9.170398712158203,
      "learning_rate": 1.2929168860033924e-05,
      "loss": 1.6515,
      "step": 12680
    },
    {
      "epoch": 2.226706439726268,
      "grad_norm": 10.500401496887207,
      "learning_rate": 1.289992396326841e-05,
      "loss": 1.8663,
      "step": 12690
    },
    {
      "epoch": 2.2284611335321984,
      "grad_norm": 8.525074005126953,
      "learning_rate": 1.2870679066502897e-05,
      "loss": 1.8325,
      "step": 12700
    },
    {
      "epoch": 2.2302158273381294,
      "grad_norm": 9.071444511413574,
      "learning_rate": 1.2841434169737381e-05,
      "loss": 1.9163,
      "step": 12710
    },
    {
      "epoch": 2.2319705211440604,
      "grad_norm": 7.2098588943481445,
      "learning_rate": 1.2812189272971867e-05,
      "loss": 1.7197,
      "step": 12720
    },
    {
      "epoch": 2.2337252149499913,
      "grad_norm": 8.595837593078613,
      "learning_rate": 1.2782944376206351e-05,
      "loss": 1.7471,
      "step": 12730
    },
    {
      "epoch": 2.2354799087559223,
      "grad_norm": 7.178496837615967,
      "learning_rate": 1.275369947944084e-05,
      "loss": 1.7236,
      "step": 12740
    },
    {
      "epoch": 2.237234602561853,
      "grad_norm": 7.234546184539795,
      "learning_rate": 1.2724454582675324e-05,
      "loss": 1.4622,
      "step": 12750
    },
    {
      "epoch": 2.2389892963677838,
      "grad_norm": 8.371500968933105,
      "learning_rate": 1.269520968590981e-05,
      "loss": 1.6736,
      "step": 12760
    },
    {
      "epoch": 2.2407439901737147,
      "grad_norm": 8.459022521972656,
      "learning_rate": 1.2665964789144293e-05,
      "loss": 1.6454,
      "step": 12770
    },
    {
      "epoch": 2.2424986839796457,
      "grad_norm": 8.224353790283203,
      "learning_rate": 1.2636719892378782e-05,
      "loss": 1.7374,
      "step": 12780
    },
    {
      "epoch": 2.244253377785576,
      "grad_norm": 7.440688610076904,
      "learning_rate": 1.2607474995613266e-05,
      "loss": 1.5439,
      "step": 12790
    },
    {
      "epoch": 2.246008071591507,
      "grad_norm": 8.8635835647583,
      "learning_rate": 1.2578230098847752e-05,
      "loss": 1.7686,
      "step": 12800
    },
    {
      "epoch": 2.247762765397438,
      "grad_norm": 7.323428153991699,
      "learning_rate": 1.2548985202082236e-05,
      "loss": 1.6772,
      "step": 12810
    },
    {
      "epoch": 2.249517459203369,
      "grad_norm": 9.115263938903809,
      "learning_rate": 1.2519740305316723e-05,
      "loss": 1.7325,
      "step": 12820
    },
    {
      "epoch": 2.2512721530093,
      "grad_norm": 11.872920989990234,
      "learning_rate": 1.2490495408551209e-05,
      "loss": 1.7717,
      "step": 12830
    },
    {
      "epoch": 2.2530268468152306,
      "grad_norm": 11.409167289733887,
      "learning_rate": 1.2461250511785694e-05,
      "loss": 1.5509,
      "step": 12840
    },
    {
      "epoch": 2.2547815406211615,
      "grad_norm": 6.547600746154785,
      "learning_rate": 1.243200561502018e-05,
      "loss": 1.601,
      "step": 12850
    },
    {
      "epoch": 2.2565362344270925,
      "grad_norm": 6.649036884307861,
      "learning_rate": 1.2402760718254666e-05,
      "loss": 1.491,
      "step": 12860
    },
    {
      "epoch": 2.2582909282330235,
      "grad_norm": 7.324649810791016,
      "learning_rate": 1.2373515821489151e-05,
      "loss": 1.7877,
      "step": 12870
    },
    {
      "epoch": 2.2600456220389544,
      "grad_norm": 8.291021347045898,
      "learning_rate": 1.2344270924723635e-05,
      "loss": 1.7395,
      "step": 12880
    },
    {
      "epoch": 2.261800315844885,
      "grad_norm": 9.277664184570312,
      "learning_rate": 1.2315026027958122e-05,
      "loss": 1.7926,
      "step": 12890
    },
    {
      "epoch": 2.263555009650816,
      "grad_norm": 8.55916976928711,
      "learning_rate": 1.2285781131192606e-05,
      "loss": 1.6016,
      "step": 12900
    },
    {
      "epoch": 2.265309703456747,
      "grad_norm": 9.246238708496094,
      "learning_rate": 1.2256536234427094e-05,
      "loss": 1.6146,
      "step": 12910
    },
    {
      "epoch": 2.267064397262678,
      "grad_norm": 12.503203392028809,
      "learning_rate": 1.2227291337661578e-05,
      "loss": 1.7403,
      "step": 12920
    },
    {
      "epoch": 2.2688190910686084,
      "grad_norm": 6.603178024291992,
      "learning_rate": 1.2198046440896065e-05,
      "loss": 1.5411,
      "step": 12930
    },
    {
      "epoch": 2.2705737848745393,
      "grad_norm": 9.544763565063477,
      "learning_rate": 1.2168801544130549e-05,
      "loss": 1.8041,
      "step": 12940
    },
    {
      "epoch": 2.2723284786804703,
      "grad_norm": 10.2672758102417,
      "learning_rate": 1.2139556647365036e-05,
      "loss": 1.7547,
      "step": 12950
    },
    {
      "epoch": 2.2740831724864012,
      "grad_norm": 11.2121000289917,
      "learning_rate": 1.211031175059952e-05,
      "loss": 1.437,
      "step": 12960
    },
    {
      "epoch": 2.275837866292332,
      "grad_norm": 10.900444984436035,
      "learning_rate": 1.2081066853834007e-05,
      "loss": 1.8537,
      "step": 12970
    },
    {
      "epoch": 2.2775925600982627,
      "grad_norm": 9.083601951599121,
      "learning_rate": 1.2051821957068491e-05,
      "loss": 1.7839,
      "step": 12980
    },
    {
      "epoch": 2.2793472539041937,
      "grad_norm": 8.33838176727295,
      "learning_rate": 1.2022577060302979e-05,
      "loss": 1.5515,
      "step": 12990
    },
    {
      "epoch": 2.2811019477101246,
      "grad_norm": 11.009254455566406,
      "learning_rate": 1.1993332163537463e-05,
      "loss": 1.5768,
      "step": 13000
    },
    {
      "epoch": 2.2828566415160556,
      "grad_norm": 10.34360122680664,
      "learning_rate": 1.196408726677195e-05,
      "loss": 1.7108,
      "step": 13010
    },
    {
      "epoch": 2.284611335321986,
      "grad_norm": 11.068998336791992,
      "learning_rate": 1.1934842370006434e-05,
      "loss": 1.6308,
      "step": 13020
    },
    {
      "epoch": 2.286366029127917,
      "grad_norm": 8.90635871887207,
      "learning_rate": 1.190559747324092e-05,
      "loss": 1.5944,
      "step": 13030
    },
    {
      "epoch": 2.288120722933848,
      "grad_norm": 10.154537200927734,
      "learning_rate": 1.1876352576475405e-05,
      "loss": 1.7819,
      "step": 13040
    },
    {
      "epoch": 2.289875416739779,
      "grad_norm": 7.930772304534912,
      "learning_rate": 1.184710767970989e-05,
      "loss": 1.4874,
      "step": 13050
    },
    {
      "epoch": 2.29163011054571,
      "grad_norm": 7.036697864532471,
      "learning_rate": 1.1817862782944376e-05,
      "loss": 1.5077,
      "step": 13060
    },
    {
      "epoch": 2.2933848043516405,
      "grad_norm": 9.700639724731445,
      "learning_rate": 1.1788617886178862e-05,
      "loss": 1.3438,
      "step": 13070
    },
    {
      "epoch": 2.2951394981575715,
      "grad_norm": 9.025567054748535,
      "learning_rate": 1.1759372989413348e-05,
      "loss": 1.6685,
      "step": 13080
    },
    {
      "epoch": 2.2968941919635024,
      "grad_norm": 9.17019271850586,
      "learning_rate": 1.1730128092647833e-05,
      "loss": 1.6408,
      "step": 13090
    },
    {
      "epoch": 2.2986488857694334,
      "grad_norm": 9.814010620117188,
      "learning_rate": 1.1700883195882319e-05,
      "loss": 1.9345,
      "step": 13100
    },
    {
      "epoch": 2.300403579575364,
      "grad_norm": 7.960486888885498,
      "learning_rate": 1.1671638299116804e-05,
      "loss": 1.5751,
      "step": 13110
    },
    {
      "epoch": 2.302158273381295,
      "grad_norm": 10.427828788757324,
      "learning_rate": 1.164239340235129e-05,
      "loss": 1.68,
      "step": 13120
    },
    {
      "epoch": 2.303912967187226,
      "grad_norm": 9.060832977294922,
      "learning_rate": 1.1613148505585776e-05,
      "loss": 1.5288,
      "step": 13130
    },
    {
      "epoch": 2.305667660993157,
      "grad_norm": 7.635639667510986,
      "learning_rate": 1.1583903608820261e-05,
      "loss": 1.6315,
      "step": 13140
    },
    {
      "epoch": 2.3074223547990877,
      "grad_norm": 7.502178192138672,
      "learning_rate": 1.1554658712054747e-05,
      "loss": 1.5683,
      "step": 13150
    },
    {
      "epoch": 2.3091770486050183,
      "grad_norm": 7.565356731414795,
      "learning_rate": 1.1525413815289233e-05,
      "loss": 1.6461,
      "step": 13160
    },
    {
      "epoch": 2.3109317424109492,
      "grad_norm": 7.2569966316223145,
      "learning_rate": 1.1496168918523718e-05,
      "loss": 1.7245,
      "step": 13170
    },
    {
      "epoch": 2.31268643621688,
      "grad_norm": 8.865644454956055,
      "learning_rate": 1.1466924021758204e-05,
      "loss": 1.4892,
      "step": 13180
    },
    {
      "epoch": 2.314441130022811,
      "grad_norm": 10.401628494262695,
      "learning_rate": 1.143767912499269e-05,
      "loss": 1.8125,
      "step": 13190
    },
    {
      "epoch": 2.3161958238287417,
      "grad_norm": 7.4731245040893555,
      "learning_rate": 1.1408434228227175e-05,
      "loss": 1.7665,
      "step": 13200
    },
    {
      "epoch": 2.3179505176346726,
      "grad_norm": 9.566397666931152,
      "learning_rate": 1.137918933146166e-05,
      "loss": 1.705,
      "step": 13210
    },
    {
      "epoch": 2.3197052114406036,
      "grad_norm": 9.20560073852539,
      "learning_rate": 1.1349944434696146e-05,
      "loss": 1.527,
      "step": 13220
    },
    {
      "epoch": 2.3214599052465346,
      "grad_norm": 6.2053093910217285,
      "learning_rate": 1.1320699537930632e-05,
      "loss": 1.6527,
      "step": 13230
    },
    {
      "epoch": 2.3232145990524655,
      "grad_norm": 10.22378158569336,
      "learning_rate": 1.1291454641165118e-05,
      "loss": 1.6343,
      "step": 13240
    },
    {
      "epoch": 2.324969292858396,
      "grad_norm": 9.797647476196289,
      "learning_rate": 1.1262209744399603e-05,
      "loss": 1.731,
      "step": 13250
    },
    {
      "epoch": 2.326723986664327,
      "grad_norm": 7.476931095123291,
      "learning_rate": 1.1232964847634087e-05,
      "loss": 1.5874,
      "step": 13260
    },
    {
      "epoch": 2.328478680470258,
      "grad_norm": 6.96316385269165,
      "learning_rate": 1.1203719950868574e-05,
      "loss": 1.602,
      "step": 13270
    },
    {
      "epoch": 2.330233374276189,
      "grad_norm": 9.68134593963623,
      "learning_rate": 1.1174475054103058e-05,
      "loss": 1.787,
      "step": 13280
    },
    {
      "epoch": 2.3319880680821194,
      "grad_norm": 10.932463645935059,
      "learning_rate": 1.1145230157337546e-05,
      "loss": 1.5705,
      "step": 13290
    },
    {
      "epoch": 2.3337427618880504,
      "grad_norm": 10.57918643951416,
      "learning_rate": 1.111598526057203e-05,
      "loss": 1.7963,
      "step": 13300
    },
    {
      "epoch": 2.3354974556939814,
      "grad_norm": 9.873281478881836,
      "learning_rate": 1.1086740363806517e-05,
      "loss": 1.6172,
      "step": 13310
    },
    {
      "epoch": 2.3372521494999123,
      "grad_norm": 9.928609848022461,
      "learning_rate": 1.1057495467041e-05,
      "loss": 1.884,
      "step": 13320
    },
    {
      "epoch": 2.3390068433058433,
      "grad_norm": 8.97584342956543,
      "learning_rate": 1.1028250570275488e-05,
      "loss": 1.4404,
      "step": 13330
    },
    {
      "epoch": 2.340761537111774,
      "grad_norm": 8.48840618133545,
      "learning_rate": 1.0999005673509972e-05,
      "loss": 1.5122,
      "step": 13340
    },
    {
      "epoch": 2.342516230917705,
      "grad_norm": 11.290078163146973,
      "learning_rate": 1.096976077674446e-05,
      "loss": 1.8265,
      "step": 13350
    },
    {
      "epoch": 2.3442709247236357,
      "grad_norm": 8.300474166870117,
      "learning_rate": 1.0940515879978943e-05,
      "loss": 1.5218,
      "step": 13360
    },
    {
      "epoch": 2.3460256185295667,
      "grad_norm": 10.972785949707031,
      "learning_rate": 1.0914195472889981e-05,
      "loss": 1.6825,
      "step": 13370
    },
    {
      "epoch": 2.347780312335497,
      "grad_norm": 10.39194107055664,
      "learning_rate": 1.0884950576124467e-05,
      "loss": 1.5738,
      "step": 13380
    },
    {
      "epoch": 2.349535006141428,
      "grad_norm": 10.5264892578125,
      "learning_rate": 1.0855705679358952e-05,
      "loss": 1.6454,
      "step": 13390
    },
    {
      "epoch": 2.351289699947359,
      "grad_norm": 6.204387664794922,
      "learning_rate": 1.0826460782593438e-05,
      "loss": 1.6407,
      "step": 13400
    },
    {
      "epoch": 2.35304439375329,
      "grad_norm": 6.9248504638671875,
      "learning_rate": 1.0797215885827924e-05,
      "loss": 1.7859,
      "step": 13410
    },
    {
      "epoch": 2.354799087559221,
      "grad_norm": 8.09138011932373,
      "learning_rate": 1.0767970989062409e-05,
      "loss": 1.6211,
      "step": 13420
    },
    {
      "epoch": 2.3565537813651516,
      "grad_norm": 7.64255428314209,
      "learning_rate": 1.0738726092296895e-05,
      "loss": 1.5914,
      "step": 13430
    },
    {
      "epoch": 2.3583084751710826,
      "grad_norm": 7.753114223480225,
      "learning_rate": 1.070948119553138e-05,
      "loss": 1.6548,
      "step": 13440
    },
    {
      "epoch": 2.3600631689770135,
      "grad_norm": 10.95726203918457,
      "learning_rate": 1.0680236298765866e-05,
      "loss": 2.0978,
      "step": 13450
    },
    {
      "epoch": 2.3618178627829445,
      "grad_norm": 7.187926769256592,
      "learning_rate": 1.0650991402000352e-05,
      "loss": 1.4536,
      "step": 13460
    },
    {
      "epoch": 2.363572556588875,
      "grad_norm": 9.180319786071777,
      "learning_rate": 1.0621746505234836e-05,
      "loss": 1.5292,
      "step": 13470
    },
    {
      "epoch": 2.365327250394806,
      "grad_norm": 9.292977333068848,
      "learning_rate": 1.0592501608469323e-05,
      "loss": 1.6964,
      "step": 13480
    },
    {
      "epoch": 2.367081944200737,
      "grad_norm": 9.413402557373047,
      "learning_rate": 1.0563256711703807e-05,
      "loss": 1.891,
      "step": 13490
    },
    {
      "epoch": 2.368836638006668,
      "grad_norm": 8.960217475891113,
      "learning_rate": 1.0534011814938294e-05,
      "loss": 1.7694,
      "step": 13500
    },
    {
      "epoch": 2.370591331812599,
      "grad_norm": 10.04561710357666,
      "learning_rate": 1.0504766918172778e-05,
      "loss": 1.4429,
      "step": 13510
    },
    {
      "epoch": 2.37234602561853,
      "grad_norm": 9.929319381713867,
      "learning_rate": 1.0475522021407265e-05,
      "loss": 1.7005,
      "step": 13520
    },
    {
      "epoch": 2.3741007194244603,
      "grad_norm": 9.81406307220459,
      "learning_rate": 1.044627712464175e-05,
      "loss": 1.4579,
      "step": 13530
    },
    {
      "epoch": 2.3758554132303913,
      "grad_norm": 7.629086971282959,
      "learning_rate": 1.0417032227876237e-05,
      "loss": 1.4212,
      "step": 13540
    },
    {
      "epoch": 2.3776101070363223,
      "grad_norm": 12.680874824523926,
      "learning_rate": 1.038778733111072e-05,
      "loss": 1.7534,
      "step": 13550
    },
    {
      "epoch": 2.379364800842253,
      "grad_norm": 10.328368186950684,
      "learning_rate": 1.0358542434345208e-05,
      "loss": 1.9936,
      "step": 13560
    },
    {
      "epoch": 2.3811194946481837,
      "grad_norm": 11.458696365356445,
      "learning_rate": 1.0329297537579692e-05,
      "loss": 1.6579,
      "step": 13570
    },
    {
      "epoch": 2.3828741884541147,
      "grad_norm": 11.96529483795166,
      "learning_rate": 1.0300052640814179e-05,
      "loss": 1.7946,
      "step": 13580
    },
    {
      "epoch": 2.3846288822600457,
      "grad_norm": 8.25255298614502,
      "learning_rate": 1.0270807744048665e-05,
      "loss": 1.7329,
      "step": 13590
    },
    {
      "epoch": 2.3863835760659766,
      "grad_norm": 11.283097267150879,
      "learning_rate": 1.024156284728315e-05,
      "loss": 1.6037,
      "step": 13600
    },
    {
      "epoch": 2.3881382698719076,
      "grad_norm": 9.22374439239502,
      "learning_rate": 1.0212317950517636e-05,
      "loss": 1.8139,
      "step": 13610
    },
    {
      "epoch": 2.389892963677838,
      "grad_norm": 10.745966911315918,
      "learning_rate": 1.0183073053752122e-05,
      "loss": 1.522,
      "step": 13620
    },
    {
      "epoch": 2.391647657483769,
      "grad_norm": 10.848986625671387,
      "learning_rate": 1.0153828156986607e-05,
      "loss": 1.9591,
      "step": 13630
    },
    {
      "epoch": 2.3934023512897,
      "grad_norm": 10.120378494262695,
      "learning_rate": 1.0124583260221091e-05,
      "loss": 1.6117,
      "step": 13640
    },
    {
      "epoch": 2.395157045095631,
      "grad_norm": 10.55793285369873,
      "learning_rate": 1.0095338363455578e-05,
      "loss": 1.7997,
      "step": 13650
    },
    {
      "epoch": 2.3969117389015615,
      "grad_norm": 7.291249752044678,
      "learning_rate": 1.0066093466690062e-05,
      "loss": 1.7894,
      "step": 13660
    },
    {
      "epoch": 2.3986664327074925,
      "grad_norm": 8.930307388305664,
      "learning_rate": 1.003684856992455e-05,
      "loss": 1.6266,
      "step": 13670
    },
    {
      "epoch": 2.4004211265134234,
      "grad_norm": 9.010039329528809,
      "learning_rate": 1.0007603673159034e-05,
      "loss": 1.6828,
      "step": 13680
    },
    {
      "epoch": 2.4021758203193544,
      "grad_norm": 13.396020889282227,
      "learning_rate": 9.978358776393521e-06,
      "loss": 1.654,
      "step": 13690
    },
    {
      "epoch": 2.4039305141252854,
      "grad_norm": 9.065564155578613,
      "learning_rate": 9.949113879628005e-06,
      "loss": 1.7755,
      "step": 13700
    },
    {
      "epoch": 2.405685207931216,
      "grad_norm": 10.318144798278809,
      "learning_rate": 9.919868982862492e-06,
      "loss": 1.7832,
      "step": 13710
    },
    {
      "epoch": 2.407439901737147,
      "grad_norm": 8.064554214477539,
      "learning_rate": 9.890624086096976e-06,
      "loss": 1.8857,
      "step": 13720
    },
    {
      "epoch": 2.409194595543078,
      "grad_norm": 7.749662399291992,
      "learning_rate": 9.861379189331463e-06,
      "loss": 1.8143,
      "step": 13730
    },
    {
      "epoch": 2.4109492893490088,
      "grad_norm": 9.218371391296387,
      "learning_rate": 9.832134292565947e-06,
      "loss": 1.7025,
      "step": 13740
    },
    {
      "epoch": 2.4127039831549393,
      "grad_norm": 9.345975875854492,
      "learning_rate": 9.802889395800435e-06,
      "loss": 1.9164,
      "step": 13750
    },
    {
      "epoch": 2.4144586769608702,
      "grad_norm": 7.808262348175049,
      "learning_rate": 9.773644499034919e-06,
      "loss": 1.5219,
      "step": 13760
    },
    {
      "epoch": 2.416213370766801,
      "grad_norm": 7.67164421081543,
      "learning_rate": 9.744399602269406e-06,
      "loss": 1.5263,
      "step": 13770
    },
    {
      "epoch": 2.417968064572732,
      "grad_norm": 9.931161880493164,
      "learning_rate": 9.71515470550389e-06,
      "loss": 1.6807,
      "step": 13780
    },
    {
      "epoch": 2.419722758378663,
      "grad_norm": 12.258331298828125,
      "learning_rate": 9.685909808738375e-06,
      "loss": 1.7906,
      "step": 13790
    },
    {
      "epoch": 2.4214774521845936,
      "grad_norm": 8.477108001708984,
      "learning_rate": 9.656664911972861e-06,
      "loss": 1.7789,
      "step": 13800
    },
    {
      "epoch": 2.4232321459905246,
      "grad_norm": 10.017561912536621,
      "learning_rate": 9.627420015207347e-06,
      "loss": 1.5335,
      "step": 13810
    },
    {
      "epoch": 2.4249868397964556,
      "grad_norm": 10.653675079345703,
      "learning_rate": 9.598175118441832e-06,
      "loss": 1.7404,
      "step": 13820
    },
    {
      "epoch": 2.4267415336023865,
      "grad_norm": 8.191985130310059,
      "learning_rate": 9.568930221676318e-06,
      "loss": 1.5927,
      "step": 13830
    },
    {
      "epoch": 2.428496227408317,
      "grad_norm": 7.998400688171387,
      "learning_rate": 9.539685324910804e-06,
      "loss": 1.7773,
      "step": 13840
    },
    {
      "epoch": 2.430250921214248,
      "grad_norm": 11.356735229492188,
      "learning_rate": 9.51044042814529e-06,
      "loss": 1.9174,
      "step": 13850
    },
    {
      "epoch": 2.432005615020179,
      "grad_norm": 8.042566299438477,
      "learning_rate": 9.481195531379775e-06,
      "loss": 1.8009,
      "step": 13860
    },
    {
      "epoch": 2.43376030882611,
      "grad_norm": 11.54372787475586,
      "learning_rate": 9.45195063461426e-06,
      "loss": 1.8591,
      "step": 13870
    },
    {
      "epoch": 2.435515002632041,
      "grad_norm": 10.542534828186035,
      "learning_rate": 9.422705737848746e-06,
      "loss": 1.5971,
      "step": 13880
    },
    {
      "epoch": 2.4372696964379714,
      "grad_norm": 11.579117774963379,
      "learning_rate": 9.393460841083232e-06,
      "loss": 1.9718,
      "step": 13890
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 11.917464256286621,
      "learning_rate": 9.364215944317717e-06,
      "loss": 1.8402,
      "step": 13900
    },
    {
      "epoch": 2.4407790840498333,
      "grad_norm": 9.635371208190918,
      "learning_rate": 9.334971047552203e-06,
      "loss": 1.7411,
      "step": 13910
    },
    {
      "epoch": 2.4425337778557643,
      "grad_norm": 8.953424453735352,
      "learning_rate": 9.305726150786689e-06,
      "loss": 1.3209,
      "step": 13920
    },
    {
      "epoch": 2.444288471661695,
      "grad_norm": 9.75516128540039,
      "learning_rate": 9.276481254021174e-06,
      "loss": 1.7384,
      "step": 13930
    },
    {
      "epoch": 2.446043165467626,
      "grad_norm": 10.503266334533691,
      "learning_rate": 9.24723635725566e-06,
      "loss": 1.8538,
      "step": 13940
    },
    {
      "epoch": 2.4477978592735568,
      "grad_norm": 11.912735939025879,
      "learning_rate": 9.217991460490145e-06,
      "loss": 1.7643,
      "step": 13950
    },
    {
      "epoch": 2.4495525530794877,
      "grad_norm": 8.033175468444824,
      "learning_rate": 9.188746563724631e-06,
      "loss": 1.4942,
      "step": 13960
    },
    {
      "epoch": 2.4513072468854187,
      "grad_norm": 7.182776927947998,
      "learning_rate": 9.159501666959117e-06,
      "loss": 1.769,
      "step": 13970
    },
    {
      "epoch": 2.453061940691349,
      "grad_norm": 8.57338809967041,
      "learning_rate": 9.130256770193602e-06,
      "loss": 1.7797,
      "step": 13980
    },
    {
      "epoch": 2.45481663449728,
      "grad_norm": 8.054478645324707,
      "learning_rate": 9.101011873428088e-06,
      "loss": 1.9231,
      "step": 13990
    },
    {
      "epoch": 2.456571328303211,
      "grad_norm": 9.045747756958008,
      "learning_rate": 9.071766976662572e-06,
      "loss": 1.7815,
      "step": 14000
    },
    {
      "epoch": 2.458326022109142,
      "grad_norm": 7.350876331329346,
      "learning_rate": 9.042522079897059e-06,
      "loss": 1.9272,
      "step": 14010
    },
    {
      "epoch": 2.4600807159150726,
      "grad_norm": 8.225334167480469,
      "learning_rate": 9.013277183131543e-06,
      "loss": 1.6756,
      "step": 14020
    },
    {
      "epoch": 2.4618354097210036,
      "grad_norm": 7.895933628082275,
      "learning_rate": 8.98403228636603e-06,
      "loss": 1.8001,
      "step": 14030
    },
    {
      "epoch": 2.4635901035269345,
      "grad_norm": 8.563897132873535,
      "learning_rate": 8.954787389600514e-06,
      "loss": 1.3251,
      "step": 14040
    },
    {
      "epoch": 2.4653447973328655,
      "grad_norm": 9.129951477050781,
      "learning_rate": 8.925542492835002e-06,
      "loss": 1.698,
      "step": 14050
    },
    {
      "epoch": 2.4670994911387965,
      "grad_norm": 8.809282302856445,
      "learning_rate": 8.896297596069486e-06,
      "loss": 1.6217,
      "step": 14060
    },
    {
      "epoch": 2.468854184944727,
      "grad_norm": 10.999918937683105,
      "learning_rate": 8.867052699303973e-06,
      "loss": 1.644,
      "step": 14070
    },
    {
      "epoch": 2.470608878750658,
      "grad_norm": 6.634308338165283,
      "learning_rate": 8.837807802538457e-06,
      "loss": 1.8099,
      "step": 14080
    },
    {
      "epoch": 2.472363572556589,
      "grad_norm": 8.900280952453613,
      "learning_rate": 8.808562905772944e-06,
      "loss": 1.6496,
      "step": 14090
    },
    {
      "epoch": 2.47411826636252,
      "grad_norm": 10.7770357131958,
      "learning_rate": 8.779318009007428e-06,
      "loss": 1.6854,
      "step": 14100
    },
    {
      "epoch": 2.4758729601684504,
      "grad_norm": 9.073016166687012,
      "learning_rate": 8.750073112241915e-06,
      "loss": 1.7405,
      "step": 14110
    },
    {
      "epoch": 2.4776276539743813,
      "grad_norm": 5.504694938659668,
      "learning_rate": 8.7208282154764e-06,
      "loss": 1.5172,
      "step": 14120
    },
    {
      "epoch": 2.4793823477803123,
      "grad_norm": 7.739165306091309,
      "learning_rate": 8.691583318710887e-06,
      "loss": 1.7858,
      "step": 14130
    },
    {
      "epoch": 2.4811370415862433,
      "grad_norm": 6.816597938537598,
      "learning_rate": 8.66233842194537e-06,
      "loss": 1.6104,
      "step": 14140
    },
    {
      "epoch": 2.4828917353921742,
      "grad_norm": 10.813065528869629,
      "learning_rate": 8.633093525179858e-06,
      "loss": 1.6484,
      "step": 14150
    },
    {
      "epoch": 2.4846464291981047,
      "grad_norm": 7.765227317810059,
      "learning_rate": 8.603848628414342e-06,
      "loss": 1.7376,
      "step": 14160
    },
    {
      "epoch": 2.4864011230040357,
      "grad_norm": 7.9919915199279785,
      "learning_rate": 8.574603731648827e-06,
      "loss": 1.6868,
      "step": 14170
    },
    {
      "epoch": 2.4881558168099667,
      "grad_norm": 8.764063835144043,
      "learning_rate": 8.545358834883313e-06,
      "loss": 1.7113,
      "step": 14180
    },
    {
      "epoch": 2.4899105106158976,
      "grad_norm": 9.438891410827637,
      "learning_rate": 8.516113938117799e-06,
      "loss": 1.5295,
      "step": 14190
    },
    {
      "epoch": 2.491665204421828,
      "grad_norm": 9.567488670349121,
      "learning_rate": 8.486869041352284e-06,
      "loss": 1.7835,
      "step": 14200
    },
    {
      "epoch": 2.493419898227759,
      "grad_norm": 9.49712085723877,
      "learning_rate": 8.45762414458677e-06,
      "loss": 1.4923,
      "step": 14210
    },
    {
      "epoch": 2.49517459203369,
      "grad_norm": 10.245087623596191,
      "learning_rate": 8.428379247821255e-06,
      "loss": 1.5537,
      "step": 14220
    },
    {
      "epoch": 2.496929285839621,
      "grad_norm": 8.222081184387207,
      "learning_rate": 8.399134351055741e-06,
      "loss": 1.4949,
      "step": 14230
    },
    {
      "epoch": 2.498683979645552,
      "grad_norm": 8.98157024383545,
      "learning_rate": 8.369889454290227e-06,
      "loss": 1.4694,
      "step": 14240
    },
    {
      "epoch": 2.500438673451483,
      "grad_norm": 10.613973617553711,
      "learning_rate": 8.340644557524712e-06,
      "loss": 1.7893,
      "step": 14250
    },
    {
      "epoch": 2.5021933672574135,
      "grad_norm": 11.563247680664062,
      "learning_rate": 8.311399660759198e-06,
      "loss": 1.6179,
      "step": 14260
    },
    {
      "epoch": 2.5039480610633444,
      "grad_norm": 8.940529823303223,
      "learning_rate": 8.282154763993684e-06,
      "loss": 1.7312,
      "step": 14270
    },
    {
      "epoch": 2.5057027548692754,
      "grad_norm": 7.7222580909729,
      "learning_rate": 8.25290986722817e-06,
      "loss": 1.4978,
      "step": 14280
    },
    {
      "epoch": 2.507457448675206,
      "grad_norm": 8.646711349487305,
      "learning_rate": 8.223664970462655e-06,
      "loss": 1.5683,
      "step": 14290
    },
    {
      "epoch": 2.509212142481137,
      "grad_norm": 8.397575378417969,
      "learning_rate": 8.19442007369714e-06,
      "loss": 1.723,
      "step": 14300
    },
    {
      "epoch": 2.510966836287068,
      "grad_norm": 8.369034767150879,
      "learning_rate": 8.165175176931626e-06,
      "loss": 1.5282,
      "step": 14310
    },
    {
      "epoch": 2.512721530092999,
      "grad_norm": 11.033632278442383,
      "learning_rate": 8.135930280166112e-06,
      "loss": 1.7674,
      "step": 14320
    },
    {
      "epoch": 2.5144762238989298,
      "grad_norm": 9.339025497436523,
      "learning_rate": 8.106685383400597e-06,
      "loss": 1.6237,
      "step": 14330
    },
    {
      "epoch": 2.5162309177048607,
      "grad_norm": 14.642125129699707,
      "learning_rate": 8.077440486635083e-06,
      "loss": 1.5124,
      "step": 14340
    },
    {
      "epoch": 2.5179856115107913,
      "grad_norm": 7.983888149261475,
      "learning_rate": 8.048195589869569e-06,
      "loss": 1.4437,
      "step": 14350
    },
    {
      "epoch": 2.519740305316722,
      "grad_norm": 9.013731002807617,
      "learning_rate": 8.018950693104054e-06,
      "loss": 1.6534,
      "step": 14360
    },
    {
      "epoch": 2.521494999122653,
      "grad_norm": 9.050007820129395,
      "learning_rate": 7.98970579633854e-06,
      "loss": 1.6122,
      "step": 14370
    },
    {
      "epoch": 2.5232496929285837,
      "grad_norm": 8.406530380249023,
      "learning_rate": 7.960460899573024e-06,
      "loss": 1.9589,
      "step": 14380
    },
    {
      "epoch": 2.5250043867345147,
      "grad_norm": 7.773029327392578,
      "learning_rate": 7.931216002807511e-06,
      "loss": 1.4498,
      "step": 14390
    },
    {
      "epoch": 2.5267590805404456,
      "grad_norm": 6.66819429397583,
      "learning_rate": 7.901971106041995e-06,
      "loss": 1.3632,
      "step": 14400
    },
    {
      "epoch": 2.5285137743463766,
      "grad_norm": 8.840697288513184,
      "learning_rate": 7.872726209276482e-06,
      "loss": 1.391,
      "step": 14410
    },
    {
      "epoch": 2.5302684681523075,
      "grad_norm": 7.8222174644470215,
      "learning_rate": 7.843481312510966e-06,
      "loss": 1.6927,
      "step": 14420
    },
    {
      "epoch": 2.5320231619582385,
      "grad_norm": 7.943393230438232,
      "learning_rate": 7.814236415745454e-06,
      "loss": 1.636,
      "step": 14430
    },
    {
      "epoch": 2.533777855764169,
      "grad_norm": 9.93956470489502,
      "learning_rate": 7.784991518979937e-06,
      "loss": 1.9846,
      "step": 14440
    },
    {
      "epoch": 2.5355325495701,
      "grad_norm": 9.247699737548828,
      "learning_rate": 7.755746622214425e-06,
      "loss": 1.5954,
      "step": 14450
    },
    {
      "epoch": 2.537287243376031,
      "grad_norm": 10.40567398071289,
      "learning_rate": 7.726501725448909e-06,
      "loss": 1.6184,
      "step": 14460
    },
    {
      "epoch": 2.5390419371819615,
      "grad_norm": 7.688776969909668,
      "learning_rate": 7.697256828683396e-06,
      "loss": 1.7527,
      "step": 14470
    },
    {
      "epoch": 2.5407966309878924,
      "grad_norm": 9.311962127685547,
      "learning_rate": 7.66801193191788e-06,
      "loss": 1.5294,
      "step": 14480
    },
    {
      "epoch": 2.5425513247938234,
      "grad_norm": 7.817040920257568,
      "learning_rate": 7.638767035152367e-06,
      "loss": 1.6559,
      "step": 14490
    },
    {
      "epoch": 2.5443060185997544,
      "grad_norm": 9.249129295349121,
      "learning_rate": 7.609522138386851e-06,
      "loss": 1.4422,
      "step": 14500
    },
    {
      "epoch": 2.5460607124056853,
      "grad_norm": 8.826889991760254,
      "learning_rate": 7.580277241621338e-06,
      "loss": 1.6902,
      "step": 14510
    },
    {
      "epoch": 2.5478154062116163,
      "grad_norm": 10.240103721618652,
      "learning_rate": 7.551032344855822e-06,
      "loss": 1.7551,
      "step": 14520
    },
    {
      "epoch": 2.549570100017547,
      "grad_norm": 8.056459426879883,
      "learning_rate": 7.521787448090309e-06,
      "loss": 1.9054,
      "step": 14530
    },
    {
      "epoch": 2.5513247938234778,
      "grad_norm": 8.00229549407959,
      "learning_rate": 7.492542551324794e-06,
      "loss": 1.752,
      "step": 14540
    },
    {
      "epoch": 2.5530794876294087,
      "grad_norm": 10.195976257324219,
      "learning_rate": 7.46329765455928e-06,
      "loss": 1.7544,
      "step": 14550
    },
    {
      "epoch": 2.5548341814353397,
      "grad_norm": 9.786632537841797,
      "learning_rate": 7.434052757793765e-06,
      "loss": 1.698,
      "step": 14560
    },
    {
      "epoch": 2.55658887524127,
      "grad_norm": 8.085206031799316,
      "learning_rate": 7.404807861028251e-06,
      "loss": 1.6958,
      "step": 14570
    },
    {
      "epoch": 2.558343569047201,
      "grad_norm": 8.883294105529785,
      "learning_rate": 7.375562964262736e-06,
      "loss": 1.8814,
      "step": 14580
    },
    {
      "epoch": 2.560098262853132,
      "grad_norm": 14.245621681213379,
      "learning_rate": 7.346318067497223e-06,
      "loss": 1.8115,
      "step": 14590
    },
    {
      "epoch": 2.561852956659063,
      "grad_norm": 7.835910797119141,
      "learning_rate": 7.317073170731707e-06,
      "loss": 1.6539,
      "step": 14600
    },
    {
      "epoch": 2.563607650464994,
      "grad_norm": 11.050742149353027,
      "learning_rate": 7.287828273966194e-06,
      "loss": 1.7481,
      "step": 14610
    },
    {
      "epoch": 2.5653623442709246,
      "grad_norm": 9.182851791381836,
      "learning_rate": 7.258583377200679e-06,
      "loss": 1.7477,
      "step": 14620
    },
    {
      "epoch": 2.5671170380768555,
      "grad_norm": 7.702027797698975,
      "learning_rate": 7.229338480435164e-06,
      "loss": 1.7305,
      "step": 14630
    },
    {
      "epoch": 2.5688717318827865,
      "grad_norm": 12.886272430419922,
      "learning_rate": 7.20009358366965e-06,
      "loss": 1.7673,
      "step": 14640
    },
    {
      "epoch": 2.5706264256887175,
      "grad_norm": 11.518818855285645,
      "learning_rate": 7.1708486869041355e-06,
      "loss": 1.7264,
      "step": 14650
    },
    {
      "epoch": 2.572381119494648,
      "grad_norm": 7.432494640350342,
      "learning_rate": 7.141603790138621e-06,
      "loss": 1.6717,
      "step": 14660
    },
    {
      "epoch": 2.574135813300579,
      "grad_norm": 9.990527153015137,
      "learning_rate": 7.112358893373107e-06,
      "loss": 1.4694,
      "step": 14670
    },
    {
      "epoch": 2.57589050710651,
      "grad_norm": 9.886332511901855,
      "learning_rate": 7.0831139966075915e-06,
      "loss": 1.4661,
      "step": 14680
    },
    {
      "epoch": 2.577645200912441,
      "grad_norm": 10.129083633422852,
      "learning_rate": 7.053869099842078e-06,
      "loss": 1.3689,
      "step": 14690
    },
    {
      "epoch": 2.579399894718372,
      "grad_norm": 9.526561737060547,
      "learning_rate": 7.024624203076563e-06,
      "loss": 1.6725,
      "step": 14700
    },
    {
      "epoch": 2.5811545885243024,
      "grad_norm": 9.979799270629883,
      "learning_rate": 6.995379306311049e-06,
      "loss": 1.7053,
      "step": 14710
    },
    {
      "epoch": 2.5829092823302333,
      "grad_norm": 11.991808891296387,
      "learning_rate": 6.966134409545534e-06,
      "loss": 1.5889,
      "step": 14720
    },
    {
      "epoch": 2.5846639761361643,
      "grad_norm": 8.931793212890625,
      "learning_rate": 6.9368895127800205e-06,
      "loss": 1.6979,
      "step": 14730
    },
    {
      "epoch": 2.5864186699420952,
      "grad_norm": 9.103188514709473,
      "learning_rate": 6.907644616014505e-06,
      "loss": 1.7323,
      "step": 14740
    },
    {
      "epoch": 2.5881733637480258,
      "grad_norm": 8.408836364746094,
      "learning_rate": 6.878399719248992e-06,
      "loss": 1.7655,
      "step": 14750
    },
    {
      "epoch": 2.5899280575539567,
      "grad_norm": 9.00632095336914,
      "learning_rate": 6.8491548224834765e-06,
      "loss": 1.6556,
      "step": 14760
    },
    {
      "epoch": 2.5916827513598877,
      "grad_norm": 12.623860359191895,
      "learning_rate": 6.819909925717963e-06,
      "loss": 1.9449,
      "step": 14770
    },
    {
      "epoch": 2.5934374451658186,
      "grad_norm": 8.497760772705078,
      "learning_rate": 6.790665028952448e-06,
      "loss": 1.774,
      "step": 14780
    },
    {
      "epoch": 2.5951921389717496,
      "grad_norm": 9.051836967468262,
      "learning_rate": 6.761420132186934e-06,
      "loss": 1.7874,
      "step": 14790
    },
    {
      "epoch": 2.5969468327776806,
      "grad_norm": 8.196075439453125,
      "learning_rate": 6.732175235421419e-06,
      "loss": 1.7469,
      "step": 14800
    },
    {
      "epoch": 2.598701526583611,
      "grad_norm": 11.562450408935547,
      "learning_rate": 6.7029303386559054e-06,
      "loss": 1.723,
      "step": 14810
    },
    {
      "epoch": 2.600456220389542,
      "grad_norm": 8.533567428588867,
      "learning_rate": 6.67368544189039e-06,
      "loss": 2.0397,
      "step": 14820
    },
    {
      "epoch": 2.602210914195473,
      "grad_norm": 7.987309455871582,
      "learning_rate": 6.644440545124876e-06,
      "loss": 1.7347,
      "step": 14830
    },
    {
      "epoch": 2.6039656080014035,
      "grad_norm": 8.63297176361084,
      "learning_rate": 6.6151956483593614e-06,
      "loss": 1.5393,
      "step": 14840
    },
    {
      "epoch": 2.6057203018073345,
      "grad_norm": 9.275545120239258,
      "learning_rate": 6.585950751593847e-06,
      "loss": 1.7786,
      "step": 14850
    },
    {
      "epoch": 2.6074749956132655,
      "grad_norm": 11.452154159545898,
      "learning_rate": 6.556705854828332e-06,
      "loss": 2.003,
      "step": 14860
    },
    {
      "epoch": 2.6092296894191964,
      "grad_norm": 11.468515396118164,
      "learning_rate": 6.527460958062818e-06,
      "loss": 1.6188,
      "step": 14870
    },
    {
      "epoch": 2.6109843832251274,
      "grad_norm": 7.899378776550293,
      "learning_rate": 6.498216061297303e-06,
      "loss": 1.5153,
      "step": 14880
    },
    {
      "epoch": 2.6127390770310583,
      "grad_norm": 5.483935356140137,
      "learning_rate": 6.4689711645317895e-06,
      "loss": 1.6892,
      "step": 14890
    },
    {
      "epoch": 2.614493770836989,
      "grad_norm": 9.33360767364502,
      "learning_rate": 6.439726267766274e-06,
      "loss": 1.607,
      "step": 14900
    },
    {
      "epoch": 2.61624846464292,
      "grad_norm": 6.083929061889648,
      "learning_rate": 6.410481371000761e-06,
      "loss": 1.5405,
      "step": 14910
    },
    {
      "epoch": 2.618003158448851,
      "grad_norm": 8.296771049499512,
      "learning_rate": 6.3812364742352456e-06,
      "loss": 1.6861,
      "step": 14920
    },
    {
      "epoch": 2.6197578522547813,
      "grad_norm": 9.990641593933105,
      "learning_rate": 6.351991577469732e-06,
      "loss": 1.5082,
      "step": 14930
    },
    {
      "epoch": 2.6215125460607123,
      "grad_norm": 9.364606857299805,
      "learning_rate": 6.322746680704217e-06,
      "loss": 1.5279,
      "step": 14940
    },
    {
      "epoch": 2.6232672398666432,
      "grad_norm": 9.995523452758789,
      "learning_rate": 6.293501783938703e-06,
      "loss": 1.7808,
      "step": 14950
    },
    {
      "epoch": 2.625021933672574,
      "grad_norm": 10.782010078430176,
      "learning_rate": 6.264256887173188e-06,
      "loss": 1.7238,
      "step": 14960
    },
    {
      "epoch": 2.626776627478505,
      "grad_norm": 6.956750392913818,
      "learning_rate": 6.2350119904076745e-06,
      "loss": 1.5476,
      "step": 14970
    },
    {
      "epoch": 2.628531321284436,
      "grad_norm": 6.397398948669434,
      "learning_rate": 6.20576709364216e-06,
      "loss": 1.6135,
      "step": 14980
    },
    {
      "epoch": 2.6302860150903666,
      "grad_norm": 7.484159469604492,
      "learning_rate": 6.176522196876646e-06,
      "loss": 1.8966,
      "step": 14990
    },
    {
      "epoch": 2.6320407088962976,
      "grad_norm": 8.565834045410156,
      "learning_rate": 6.147277300111131e-06,
      "loss": 1.4966,
      "step": 15000
    },
    {
      "epoch": 2.6337954027022286,
      "grad_norm": 9.041723251342773,
      "learning_rate": 6.118032403345616e-06,
      "loss": 1.5529,
      "step": 15010
    },
    {
      "epoch": 2.635550096508159,
      "grad_norm": 9.702596664428711,
      "learning_rate": 6.088787506580102e-06,
      "loss": 1.6841,
      "step": 15020
    },
    {
      "epoch": 2.63730479031409,
      "grad_norm": 6.821257591247559,
      "learning_rate": 6.059542609814587e-06,
      "loss": 1.6717,
      "step": 15030
    },
    {
      "epoch": 2.639059484120021,
      "grad_norm": 9.756086349487305,
      "learning_rate": 6.030297713049073e-06,
      "loss": 1.732,
      "step": 15040
    },
    {
      "epoch": 2.640814177925952,
      "grad_norm": 9.687307357788086,
      "learning_rate": 6.001052816283559e-06,
      "loss": 1.6301,
      "step": 15050
    },
    {
      "epoch": 2.642568871731883,
      "grad_norm": 9.898833274841309,
      "learning_rate": 5.971807919518044e-06,
      "loss": 1.6584,
      "step": 15060
    },
    {
      "epoch": 2.644323565537814,
      "grad_norm": 7.930302143096924,
      "learning_rate": 5.94256302275253e-06,
      "loss": 1.4387,
      "step": 15070
    },
    {
      "epoch": 2.6460782593437444,
      "grad_norm": 9.639969825744629,
      "learning_rate": 5.9133181259870155e-06,
      "loss": 1.6661,
      "step": 15080
    },
    {
      "epoch": 2.6478329531496754,
      "grad_norm": 9.55455493927002,
      "learning_rate": 5.884073229221501e-06,
      "loss": 1.8266,
      "step": 15090
    },
    {
      "epoch": 2.6495876469556063,
      "grad_norm": 10.099885940551758,
      "learning_rate": 5.854828332455987e-06,
      "loss": 2.0347,
      "step": 15100
    },
    {
      "epoch": 2.651342340761537,
      "grad_norm": 11.158753395080566,
      "learning_rate": 5.825583435690472e-06,
      "loss": 1.4452,
      "step": 15110
    },
    {
      "epoch": 2.653097034567468,
      "grad_norm": 8.500933647155762,
      "learning_rate": 5.796338538924958e-06,
      "loss": 1.8888,
      "step": 15120
    },
    {
      "epoch": 2.654851728373399,
      "grad_norm": 9.349578857421875,
      "learning_rate": 5.767093642159444e-06,
      "loss": 1.6553,
      "step": 15130
    },
    {
      "epoch": 2.6566064221793297,
      "grad_norm": 11.398921012878418,
      "learning_rate": 5.737848745393929e-06,
      "loss": 1.5924,
      "step": 15140
    },
    {
      "epoch": 2.6583611159852607,
      "grad_norm": 5.9486870765686035,
      "learning_rate": 5.708603848628415e-06,
      "loss": 1.5988,
      "step": 15150
    },
    {
      "epoch": 2.6601158097911917,
      "grad_norm": 10.075296401977539,
      "learning_rate": 5.6793589518629005e-06,
      "loss": 1.592,
      "step": 15160
    },
    {
      "epoch": 2.661870503597122,
      "grad_norm": 8.704886436462402,
      "learning_rate": 5.650114055097386e-06,
      "loss": 1.4624,
      "step": 15170
    },
    {
      "epoch": 2.663625197403053,
      "grad_norm": 9.534256935119629,
      "learning_rate": 5.620869158331872e-06,
      "loss": 1.7105,
      "step": 15180
    },
    {
      "epoch": 2.665379891208984,
      "grad_norm": 11.845640182495117,
      "learning_rate": 5.591624261566357e-06,
      "loss": 1.9024,
      "step": 15190
    },
    {
      "epoch": 2.6671345850149146,
      "grad_norm": 9.668817520141602,
      "learning_rate": 5.562379364800842e-06,
      "loss": 1.863,
      "step": 15200
    },
    {
      "epoch": 2.6688892788208456,
      "grad_norm": 8.989290237426758,
      "learning_rate": 5.533134468035328e-06,
      "loss": 1.9991,
      "step": 15210
    },
    {
      "epoch": 2.6706439726267766,
      "grad_norm": 7.989533424377441,
      "learning_rate": 5.503889571269813e-06,
      "loss": 1.7716,
      "step": 15220
    },
    {
      "epoch": 2.6723986664327075,
      "grad_norm": 7.502648830413818,
      "learning_rate": 5.474644674504299e-06,
      "loss": 1.6504,
      "step": 15230
    },
    {
      "epoch": 2.6741533602386385,
      "grad_norm": 7.212196350097656,
      "learning_rate": 5.445399777738785e-06,
      "loss": 1.4462,
      "step": 15240
    },
    {
      "epoch": 2.6759080540445694,
      "grad_norm": 10.902347564697266,
      "learning_rate": 5.41615488097327e-06,
      "loss": 1.554,
      "step": 15250
    },
    {
      "epoch": 2.6776627478505,
      "grad_norm": 10.892104148864746,
      "learning_rate": 5.386909984207756e-06,
      "loss": 1.6093,
      "step": 15260
    },
    {
      "epoch": 2.679417441656431,
      "grad_norm": 8.169456481933594,
      "learning_rate": 5.3576650874422415e-06,
      "loss": 1.8171,
      "step": 15270
    },
    {
      "epoch": 2.681172135462362,
      "grad_norm": 10.372757911682129,
      "learning_rate": 5.328420190676727e-06,
      "loss": 1.8467,
      "step": 15280
    },
    {
      "epoch": 2.682926829268293,
      "grad_norm": 10.016525268554688,
      "learning_rate": 5.299175293911213e-06,
      "loss": 1.4128,
      "step": 15290
    },
    {
      "epoch": 2.6846815230742234,
      "grad_norm": 13.620837211608887,
      "learning_rate": 5.269930397145698e-06,
      "loss": 1.8654,
      "step": 15300
    },
    {
      "epoch": 2.6864362168801543,
      "grad_norm": 11.166543960571289,
      "learning_rate": 5.240685500380184e-06,
      "loss": 1.5427,
      "step": 15310
    },
    {
      "epoch": 2.6881909106860853,
      "grad_norm": 9.543486595153809,
      "learning_rate": 5.2114406036146696e-06,
      "loss": 1.4164,
      "step": 15320
    },
    {
      "epoch": 2.6899456044920163,
      "grad_norm": 9.628117561340332,
      "learning_rate": 5.182195706849155e-06,
      "loss": 1.4334,
      "step": 15330
    },
    {
      "epoch": 2.691700298297947,
      "grad_norm": 10.05155086517334,
      "learning_rate": 5.152950810083641e-06,
      "loss": 1.7745,
      "step": 15340
    },
    {
      "epoch": 2.6934549921038777,
      "grad_norm": 8.524969100952148,
      "learning_rate": 5.1237059133181264e-06,
      "loss": 1.926,
      "step": 15350
    },
    {
      "epoch": 2.6952096859098087,
      "grad_norm": 7.399255275726318,
      "learning_rate": 5.094461016552612e-06,
      "loss": 1.547,
      "step": 15360
    },
    {
      "epoch": 2.6969643797157397,
      "grad_norm": 6.165450572967529,
      "learning_rate": 5.068140609463649e-06,
      "loss": 1.4764,
      "step": 15370
    },
    {
      "epoch": 2.6987190735216706,
      "grad_norm": 11.48918342590332,
      "learning_rate": 5.0388957126981346e-06,
      "loss": 1.837,
      "step": 15380
    },
    {
      "epoch": 2.700473767327601,
      "grad_norm": 9.388453483581543,
      "learning_rate": 5.00965081593262e-06,
      "loss": 1.6402,
      "step": 15390
    },
    {
      "epoch": 2.702228461133532,
      "grad_norm": 9.589130401611328,
      "learning_rate": 4.980405919167106e-06,
      "loss": 1.6355,
      "step": 15400
    },
    {
      "epoch": 2.703983154939463,
      "grad_norm": 10.026840209960938,
      "learning_rate": 4.951161022401591e-06,
      "loss": 1.4784,
      "step": 15410
    },
    {
      "epoch": 2.705737848745394,
      "grad_norm": 9.703341484069824,
      "learning_rate": 4.921916125636076e-06,
      "loss": 1.7128,
      "step": 15420
    },
    {
      "epoch": 2.707492542551325,
      "grad_norm": 8.729011535644531,
      "learning_rate": 4.892671228870562e-06,
      "loss": 1.686,
      "step": 15430
    },
    {
      "epoch": 2.7092472363572555,
      "grad_norm": 7.271738052368164,
      "learning_rate": 4.8634263321050474e-06,
      "loss": 1.7222,
      "step": 15440
    },
    {
      "epoch": 2.7110019301631865,
      "grad_norm": 9.160360336303711,
      "learning_rate": 4.834181435339533e-06,
      "loss": 1.462,
      "step": 15450
    },
    {
      "epoch": 2.7127566239691174,
      "grad_norm": 7.66973876953125,
      "learning_rate": 4.804936538574019e-06,
      "loss": 1.5494,
      "step": 15460
    },
    {
      "epoch": 2.7145113177750484,
      "grad_norm": 9.656290054321289,
      "learning_rate": 4.775691641808504e-06,
      "loss": 1.6915,
      "step": 15470
    },
    {
      "epoch": 2.716266011580979,
      "grad_norm": 9.164076805114746,
      "learning_rate": 4.74644674504299e-06,
      "loss": 1.6638,
      "step": 15480
    },
    {
      "epoch": 2.71802070538691,
      "grad_norm": 9.569046974182129,
      "learning_rate": 4.7172018482774756e-06,
      "loss": 1.5826,
      "step": 15490
    },
    {
      "epoch": 2.719775399192841,
      "grad_norm": 7.865939617156982,
      "learning_rate": 4.687956951511961e-06,
      "loss": 1.4964,
      "step": 15500
    },
    {
      "epoch": 2.721530092998772,
      "grad_norm": 8.718551635742188,
      "learning_rate": 4.658712054746447e-06,
      "loss": 1.638,
      "step": 15510
    },
    {
      "epoch": 2.7232847868047028,
      "grad_norm": 9.029409408569336,
      "learning_rate": 4.629467157980932e-06,
      "loss": 1.4222,
      "step": 15520
    },
    {
      "epoch": 2.7250394806106333,
      "grad_norm": 8.725171089172363,
      "learning_rate": 4.600222261215418e-06,
      "loss": 1.9838,
      "step": 15530
    },
    {
      "epoch": 2.7267941744165642,
      "grad_norm": 6.139978408813477,
      "learning_rate": 4.570977364449904e-06,
      "loss": 1.5613,
      "step": 15540
    },
    {
      "epoch": 2.728548868222495,
      "grad_norm": 10.893752098083496,
      "learning_rate": 4.541732467684389e-06,
      "loss": 1.7734,
      "step": 15550
    },
    {
      "epoch": 2.730303562028426,
      "grad_norm": 9.918615341186523,
      "learning_rate": 4.512487570918875e-06,
      "loss": 1.8066,
      "step": 15560
    },
    {
      "epoch": 2.7320582558343567,
      "grad_norm": 8.984365463256836,
      "learning_rate": 4.4832426741533605e-06,
      "loss": 1.6936,
      "step": 15570
    },
    {
      "epoch": 2.7338129496402876,
      "grad_norm": 11.422110557556152,
      "learning_rate": 4.453997777387846e-06,
      "loss": 1.5511,
      "step": 15580
    },
    {
      "epoch": 2.7355676434462186,
      "grad_norm": 10.053499221801758,
      "learning_rate": 4.424752880622332e-06,
      "loss": 1.6543,
      "step": 15590
    },
    {
      "epoch": 2.7373223372521496,
      "grad_norm": 6.35817289352417,
      "learning_rate": 4.3955079838568165e-06,
      "loss": 1.4667,
      "step": 15600
    },
    {
      "epoch": 2.7390770310580805,
      "grad_norm": 10.458972930908203,
      "learning_rate": 4.366263087091302e-06,
      "loss": 1.5689,
      "step": 15610
    },
    {
      "epoch": 2.7408317248640115,
      "grad_norm": 7.79014778137207,
      "learning_rate": 4.337018190325788e-06,
      "loss": 1.912,
      "step": 15620
    },
    {
      "epoch": 2.742586418669942,
      "grad_norm": 8.951939582824707,
      "learning_rate": 4.307773293560273e-06,
      "loss": 1.6752,
      "step": 15630
    },
    {
      "epoch": 2.744341112475873,
      "grad_norm": 10.781221389770508,
      "learning_rate": 4.278528396794759e-06,
      "loss": 1.5779,
      "step": 15640
    },
    {
      "epoch": 2.746095806281804,
      "grad_norm": 6.285202503204346,
      "learning_rate": 4.249283500029245e-06,
      "loss": 1.6846,
      "step": 15650
    },
    {
      "epoch": 2.7478505000877345,
      "grad_norm": 7.577511310577393,
      "learning_rate": 4.22003860326373e-06,
      "loss": 1.4791,
      "step": 15660
    },
    {
      "epoch": 2.7496051938936654,
      "grad_norm": 8.12559986114502,
      "learning_rate": 4.190793706498216e-06,
      "loss": 1.5575,
      "step": 15670
    },
    {
      "epoch": 2.7513598876995964,
      "grad_norm": 8.328699111938477,
      "learning_rate": 4.1615488097327015e-06,
      "loss": 1.6766,
      "step": 15680
    },
    {
      "epoch": 2.7531145815055273,
      "grad_norm": 8.706053733825684,
      "learning_rate": 4.132303912967187e-06,
      "loss": 1.5462,
      "step": 15690
    },
    {
      "epoch": 2.7548692753114583,
      "grad_norm": 9.522966384887695,
      "learning_rate": 4.103059016201673e-06,
      "loss": 1.6635,
      "step": 15700
    },
    {
      "epoch": 2.7566239691173893,
      "grad_norm": 9.372928619384766,
      "learning_rate": 4.073814119436158e-06,
      "loss": 1.7436,
      "step": 15710
    },
    {
      "epoch": 2.75837866292332,
      "grad_norm": 9.178277015686035,
      "learning_rate": 4.044569222670644e-06,
      "loss": 1.69,
      "step": 15720
    },
    {
      "epoch": 2.7601333567292508,
      "grad_norm": 9.425773620605469,
      "learning_rate": 4.01532432590513e-06,
      "loss": 1.876,
      "step": 15730
    },
    {
      "epoch": 2.7618880505351817,
      "grad_norm": 7.931166648864746,
      "learning_rate": 3.986079429139615e-06,
      "loss": 1.7877,
      "step": 15740
    },
    {
      "epoch": 2.7636427443411122,
      "grad_norm": 8.770408630371094,
      "learning_rate": 3.956834532374101e-06,
      "loss": 1.7318,
      "step": 15750
    },
    {
      "epoch": 2.765397438147043,
      "grad_norm": 7.548095226287842,
      "learning_rate": 3.9275896356085865e-06,
      "loss": 1.5626,
      "step": 15760
    },
    {
      "epoch": 2.767152131952974,
      "grad_norm": 6.591577053070068,
      "learning_rate": 3.898344738843072e-06,
      "loss": 1.4995,
      "step": 15770
    },
    {
      "epoch": 2.768906825758905,
      "grad_norm": 9.39083194732666,
      "learning_rate": 3.869099842077558e-06,
      "loss": 1.8488,
      "step": 15780
    },
    {
      "epoch": 2.770661519564836,
      "grad_norm": 11.350668907165527,
      "learning_rate": 3.8398549453120425e-06,
      "loss": 1.5942,
      "step": 15790
    },
    {
      "epoch": 2.772416213370767,
      "grad_norm": 9.935489654541016,
      "learning_rate": 3.8106100485465285e-06,
      "loss": 1.5029,
      "step": 15800
    },
    {
      "epoch": 2.7741709071766976,
      "grad_norm": 8.826499938964844,
      "learning_rate": 3.781365151781014e-06,
      "loss": 1.7744,
      "step": 15810
    },
    {
      "epoch": 2.7759256009826285,
      "grad_norm": 7.917942523956299,
      "learning_rate": 3.7521202550154998e-06,
      "loss": 1.634,
      "step": 15820
    },
    {
      "epoch": 2.7776802947885595,
      "grad_norm": 9.976666450500488,
      "learning_rate": 3.722875358249986e-06,
      "loss": 1.6829,
      "step": 15830
    },
    {
      "epoch": 2.77943498859449,
      "grad_norm": 7.6246418952941895,
      "learning_rate": 3.6936304614844715e-06,
      "loss": 1.7571,
      "step": 15840
    },
    {
      "epoch": 2.781189682400421,
      "grad_norm": 10.928044319152832,
      "learning_rate": 3.664385564718957e-06,
      "loss": 1.8193,
      "step": 15850
    },
    {
      "epoch": 2.782944376206352,
      "grad_norm": 10.197668075561523,
      "learning_rate": 3.6351406679534427e-06,
      "loss": 1.6291,
      "step": 15860
    },
    {
      "epoch": 2.784699070012283,
      "grad_norm": 10.772892951965332,
      "learning_rate": 3.6058957711879283e-06,
      "loss": 1.517,
      "step": 15870
    },
    {
      "epoch": 2.786453763818214,
      "grad_norm": 9.08673095703125,
      "learning_rate": 3.576650874422414e-06,
      "loss": 1.5576,
      "step": 15880
    },
    {
      "epoch": 2.788208457624145,
      "grad_norm": 10.192018508911133,
      "learning_rate": 3.5474059776568996e-06,
      "loss": 1.59,
      "step": 15890
    },
    {
      "epoch": 2.7899631514300753,
      "grad_norm": 10.217608451843262,
      "learning_rate": 3.5181610808913848e-06,
      "loss": 1.8297,
      "step": 15900
    },
    {
      "epoch": 2.7917178452360063,
      "grad_norm": 9.4536714553833,
      "learning_rate": 3.4889161841258704e-06,
      "loss": 1.639,
      "step": 15910
    },
    {
      "epoch": 2.7934725390419373,
      "grad_norm": 9.206808090209961,
      "learning_rate": 3.459671287360356e-06,
      "loss": 1.641,
      "step": 15920
    },
    {
      "epoch": 2.795227232847868,
      "grad_norm": 9.878952026367188,
      "learning_rate": 3.4304263905948416e-06,
      "loss": 2.0318,
      "step": 15930
    },
    {
      "epoch": 2.7969819266537987,
      "grad_norm": 9.702049255371094,
      "learning_rate": 3.4011814938293272e-06,
      "loss": 1.6668,
      "step": 15940
    },
    {
      "epoch": 2.7987366204597297,
      "grad_norm": 8.002836227416992,
      "learning_rate": 3.371936597063813e-06,
      "loss": 1.5499,
      "step": 15950
    },
    {
      "epoch": 2.8004913142656607,
      "grad_norm": 11.81038761138916,
      "learning_rate": 3.3426917002982985e-06,
      "loss": 1.575,
      "step": 15960
    },
    {
      "epoch": 2.8022460080715916,
      "grad_norm": 10.094677925109863,
      "learning_rate": 3.313446803532784e-06,
      "loss": 1.7473,
      "step": 15970
    },
    {
      "epoch": 2.8040007018775226,
      "grad_norm": 9.763067245483398,
      "learning_rate": 3.2842019067672697e-06,
      "loss": 1.6354,
      "step": 15980
    },
    {
      "epoch": 2.805755395683453,
      "grad_norm": 8.111393928527832,
      "learning_rate": 3.254957010001755e-06,
      "loss": 1.5874,
      "step": 15990
    },
    {
      "epoch": 2.807510089489384,
      "grad_norm": 10.001049995422363,
      "learning_rate": 3.2257121132362405e-06,
      "loss": 1.8663,
      "step": 16000
    },
    {
      "epoch": 2.809264783295315,
      "grad_norm": 9.437524795532227,
      "learning_rate": 3.196467216470726e-06,
      "loss": 1.771,
      "step": 16010
    },
    {
      "epoch": 2.8110194771012456,
      "grad_norm": 7.47150182723999,
      "learning_rate": 3.1672223197052118e-06,
      "loss": 1.6778,
      "step": 16020
    },
    {
      "epoch": 2.8127741709071765,
      "grad_norm": 9.624898910522461,
      "learning_rate": 3.1379774229396974e-06,
      "loss": 1.9172,
      "step": 16030
    },
    {
      "epoch": 2.8145288647131075,
      "grad_norm": 12.628206253051758,
      "learning_rate": 3.1087325261741826e-06,
      "loss": 1.6243,
      "step": 16040
    },
    {
      "epoch": 2.8162835585190384,
      "grad_norm": 8.434456825256348,
      "learning_rate": 3.0794876294086682e-06,
      "loss": 1.5856,
      "step": 16050
    },
    {
      "epoch": 2.8180382523249694,
      "grad_norm": 8.119586944580078,
      "learning_rate": 3.050242732643154e-06,
      "loss": 1.7963,
      "step": 16060
    },
    {
      "epoch": 2.8197929461309004,
      "grad_norm": 6.105545997619629,
      "learning_rate": 3.0209978358776395e-06,
      "loss": 1.4319,
      "step": 16070
    },
    {
      "epoch": 2.821547639936831,
      "grad_norm": 11.82246208190918,
      "learning_rate": 2.991752939112125e-06,
      "loss": 1.6105,
      "step": 16080
    },
    {
      "epoch": 2.823302333742762,
      "grad_norm": 7.456233978271484,
      "learning_rate": 2.9625080423466107e-06,
      "loss": 1.5937,
      "step": 16090
    },
    {
      "epoch": 2.825057027548693,
      "grad_norm": 6.409269332885742,
      "learning_rate": 2.933263145581096e-06,
      "loss": 1.489,
      "step": 16100
    },
    {
      "epoch": 2.8268117213546238,
      "grad_norm": 8.53232192993164,
      "learning_rate": 2.9040182488155815e-06,
      "loss": 1.5467,
      "step": 16110
    },
    {
      "epoch": 2.8285664151605543,
      "grad_norm": 9.642433166503906,
      "learning_rate": 2.874773352050067e-06,
      "loss": 1.4579,
      "step": 16120
    },
    {
      "epoch": 2.8303211089664853,
      "grad_norm": 6.633491039276123,
      "learning_rate": 2.8455284552845528e-06,
      "loss": 1.7142,
      "step": 16130
    },
    {
      "epoch": 2.832075802772416,
      "grad_norm": 8.658233642578125,
      "learning_rate": 2.8162835585190384e-06,
      "loss": 1.8942,
      "step": 16140
    },
    {
      "epoch": 2.833830496578347,
      "grad_norm": 8.644275665283203,
      "learning_rate": 2.787038661753524e-06,
      "loss": 1.5108,
      "step": 16150
    },
    {
      "epoch": 2.835585190384278,
      "grad_norm": 9.34287166595459,
      "learning_rate": 2.7577937649880096e-06,
      "loss": 1.555,
      "step": 16160
    },
    {
      "epoch": 2.8373398841902087,
      "grad_norm": 8.563217163085938,
      "learning_rate": 2.7285488682224953e-06,
      "loss": 1.6188,
      "step": 16170
    },
    {
      "epoch": 2.8390945779961396,
      "grad_norm": 10.401700973510742,
      "learning_rate": 2.699303971456981e-06,
      "loss": 1.6998,
      "step": 16180
    },
    {
      "epoch": 2.8408492718020706,
      "grad_norm": 9.831092834472656,
      "learning_rate": 2.6700590746914665e-06,
      "loss": 1.6327,
      "step": 16190
    },
    {
      "epoch": 2.8426039656080015,
      "grad_norm": 9.051868438720703,
      "learning_rate": 2.6408141779259517e-06,
      "loss": 1.6407,
      "step": 16200
    },
    {
      "epoch": 2.844358659413932,
      "grad_norm": 8.495462417602539,
      "learning_rate": 2.6115692811604373e-06,
      "loss": 1.67,
      "step": 16210
    },
    {
      "epoch": 2.846113353219863,
      "grad_norm": 7.669014930725098,
      "learning_rate": 2.582324384394923e-06,
      "loss": 1.3931,
      "step": 16220
    },
    {
      "epoch": 2.847868047025794,
      "grad_norm": 10.231613159179688,
      "learning_rate": 2.553079487629409e-06,
      "loss": 1.6208,
      "step": 16230
    },
    {
      "epoch": 2.849622740831725,
      "grad_norm": 10.074528694152832,
      "learning_rate": 2.5238345908638946e-06,
      "loss": 1.6691,
      "step": 16240
    },
    {
      "epoch": 2.851377434637656,
      "grad_norm": 5.901974678039551,
      "learning_rate": 2.4945896940983802e-06,
      "loss": 1.8555,
      "step": 16250
    },
    {
      "epoch": 2.8531321284435864,
      "grad_norm": 9.209985733032227,
      "learning_rate": 2.465344797332866e-06,
      "loss": 1.9213,
      "step": 16260
    },
    {
      "epoch": 2.8548868222495174,
      "grad_norm": 7.385744094848633,
      "learning_rate": 2.4360999005673515e-06,
      "loss": 1.7254,
      "step": 16270
    },
    {
      "epoch": 2.8566415160554484,
      "grad_norm": 9.289555549621582,
      "learning_rate": 2.4068550038018367e-06,
      "loss": 1.5184,
      "step": 16280
    },
    {
      "epoch": 2.8583962098613793,
      "grad_norm": 10.650801658630371,
      "learning_rate": 2.3776101070363223e-06,
      "loss": 1.7128,
      "step": 16290
    },
    {
      "epoch": 2.86015090366731,
      "grad_norm": 10.751947402954102,
      "learning_rate": 2.348365210270808e-06,
      "loss": 1.85,
      "step": 16300
    },
    {
      "epoch": 2.861905597473241,
      "grad_norm": 7.236440181732178,
      "learning_rate": 2.322044803181845e-06,
      "loss": 1.7142,
      "step": 16310
    },
    {
      "epoch": 2.8636602912791718,
      "grad_norm": 8.120468139648438,
      "learning_rate": 2.2927999064163304e-06,
      "loss": 1.5924,
      "step": 16320
    },
    {
      "epoch": 2.8654149850851027,
      "grad_norm": 9.277167320251465,
      "learning_rate": 2.263555009650816e-06,
      "loss": 1.7535,
      "step": 16330
    },
    {
      "epoch": 2.8671696788910337,
      "grad_norm": 11.536969184875488,
      "learning_rate": 2.2343101128853017e-06,
      "loss": 1.6969,
      "step": 16340
    },
    {
      "epoch": 2.8689243726969647,
      "grad_norm": 8.755691528320312,
      "learning_rate": 2.2050652161197873e-06,
      "loss": 1.6373,
      "step": 16350
    },
    {
      "epoch": 2.870679066502895,
      "grad_norm": 7.548205852508545,
      "learning_rate": 2.175820319354273e-06,
      "loss": 1.67,
      "step": 16360
    },
    {
      "epoch": 2.872433760308826,
      "grad_norm": 11.150089263916016,
      "learning_rate": 2.1465754225887585e-06,
      "loss": 1.805,
      "step": 16370
    },
    {
      "epoch": 2.874188454114757,
      "grad_norm": 8.624950408935547,
      "learning_rate": 2.117330525823244e-06,
      "loss": 1.5372,
      "step": 16380
    },
    {
      "epoch": 2.8759431479206876,
      "grad_norm": 8.165184020996094,
      "learning_rate": 2.0880856290577294e-06,
      "loss": 1.7438,
      "step": 16390
    },
    {
      "epoch": 2.8776978417266186,
      "grad_norm": 7.212044715881348,
      "learning_rate": 2.058840732292215e-06,
      "loss": 1.578,
      "step": 16400
    },
    {
      "epoch": 2.8794525355325495,
      "grad_norm": 8.83226203918457,
      "learning_rate": 2.0295958355267006e-06,
      "loss": 1.8115,
      "step": 16410
    },
    {
      "epoch": 2.8812072293384805,
      "grad_norm": 9.217389106750488,
      "learning_rate": 2.0003509387611862e-06,
      "loss": 1.5574,
      "step": 16420
    },
    {
      "epoch": 2.8829619231444115,
      "grad_norm": 7.542665481567383,
      "learning_rate": 1.971106041995672e-06,
      "loss": 1.6561,
      "step": 16430
    },
    {
      "epoch": 2.8847166169503424,
      "grad_norm": 7.748632431030273,
      "learning_rate": 1.9418611452301575e-06,
      "loss": 1.4371,
      "step": 16440
    },
    {
      "epoch": 2.886471310756273,
      "grad_norm": 8.266504287719727,
      "learning_rate": 1.912616248464643e-06,
      "loss": 1.577,
      "step": 16450
    },
    {
      "epoch": 2.888226004562204,
      "grad_norm": 10.179500579833984,
      "learning_rate": 1.8833713516991287e-06,
      "loss": 1.6039,
      "step": 16460
    },
    {
      "epoch": 2.889980698368135,
      "grad_norm": 9.199576377868652,
      "learning_rate": 1.8541264549336141e-06,
      "loss": 1.9246,
      "step": 16470
    },
    {
      "epoch": 2.8917353921740654,
      "grad_norm": 8.180495262145996,
      "learning_rate": 1.8248815581680997e-06,
      "loss": 1.496,
      "step": 16480
    },
    {
      "epoch": 2.8934900859799964,
      "grad_norm": 9.50771427154541,
      "learning_rate": 1.7956366614025854e-06,
      "loss": 1.9049,
      "step": 16490
    },
    {
      "epoch": 2.8952447797859273,
      "grad_norm": 8.445624351501465,
      "learning_rate": 1.766391764637071e-06,
      "loss": 1.6662,
      "step": 16500
    },
    {
      "epoch": 2.8969994735918583,
      "grad_norm": 7.404898643493652,
      "learning_rate": 1.7371468678715564e-06,
      "loss": 1.7207,
      "step": 16510
    },
    {
      "epoch": 2.8987541673977892,
      "grad_norm": 8.446850776672363,
      "learning_rate": 1.707901971106042e-06,
      "loss": 1.8396,
      "step": 16520
    },
    {
      "epoch": 2.90050886120372,
      "grad_norm": 9.189311981201172,
      "learning_rate": 1.6786570743405276e-06,
      "loss": 1.3986,
      "step": 16530
    },
    {
      "epoch": 2.9022635550096507,
      "grad_norm": 10.797232627868652,
      "learning_rate": 1.6494121775750132e-06,
      "loss": 1.7994,
      "step": 16540
    },
    {
      "epoch": 2.9040182488155817,
      "grad_norm": 8.855878829956055,
      "learning_rate": 1.6201672808094989e-06,
      "loss": 1.6317,
      "step": 16550
    },
    {
      "epoch": 2.9057729426215126,
      "grad_norm": 7.847501754760742,
      "learning_rate": 1.5909223840439843e-06,
      "loss": 1.7544,
      "step": 16560
    },
    {
      "epoch": 2.907527636427443,
      "grad_norm": 11.339461326599121,
      "learning_rate": 1.56167748727847e-06,
      "loss": 1.6748,
      "step": 16570
    },
    {
      "epoch": 2.909282330233374,
      "grad_norm": 9.446579933166504,
      "learning_rate": 1.5324325905129555e-06,
      "loss": 1.5185,
      "step": 16580
    },
    {
      "epoch": 2.911037024039305,
      "grad_norm": 6.881307125091553,
      "learning_rate": 1.5031876937474411e-06,
      "loss": 1.612,
      "step": 16590
    },
    {
      "epoch": 2.912791717845236,
      "grad_norm": 8.829447746276855,
      "learning_rate": 1.4739427969819268e-06,
      "loss": 1.8611,
      "step": 16600
    },
    {
      "epoch": 2.914546411651167,
      "grad_norm": 8.555230140686035,
      "learning_rate": 1.4446979002164122e-06,
      "loss": 1.5906,
      "step": 16610
    },
    {
      "epoch": 2.916301105457098,
      "grad_norm": 9.823177337646484,
      "learning_rate": 1.4154530034508978e-06,
      "loss": 1.5405,
      "step": 16620
    },
    {
      "epoch": 2.9180557992630285,
      "grad_norm": 10.046297073364258,
      "learning_rate": 1.3862081066853834e-06,
      "loss": 1.5781,
      "step": 16630
    },
    {
      "epoch": 2.9198104930689595,
      "grad_norm": 9.877511978149414,
      "learning_rate": 1.356963209919869e-06,
      "loss": 1.6965,
      "step": 16640
    },
    {
      "epoch": 2.9215651868748904,
      "grad_norm": 8.361727714538574,
      "learning_rate": 1.3277183131543544e-06,
      "loss": 1.8652,
      "step": 16650
    },
    {
      "epoch": 2.923319880680821,
      "grad_norm": 8.068330764770508,
      "learning_rate": 1.29847341638884e-06,
      "loss": 1.6056,
      "step": 16660
    },
    {
      "epoch": 2.925074574486752,
      "grad_norm": 8.749267578125,
      "learning_rate": 1.269228519623326e-06,
      "loss": 1.5087,
      "step": 16670
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 7.444699287414551,
      "learning_rate": 1.2399836228578115e-06,
      "loss": 1.8732,
      "step": 16680
    },
    {
      "epoch": 2.928583962098614,
      "grad_norm": 12.103963851928711,
      "learning_rate": 1.210738726092297e-06,
      "loss": 1.6992,
      "step": 16690
    },
    {
      "epoch": 2.930338655904545,
      "grad_norm": 11.347219467163086,
      "learning_rate": 1.1814938293267826e-06,
      "loss": 1.9467,
      "step": 16700
    },
    {
      "epoch": 2.9320933497104757,
      "grad_norm": 8.131087303161621,
      "learning_rate": 1.1522489325612682e-06,
      "loss": 1.5232,
      "step": 16710
    },
    {
      "epoch": 2.9338480435164063,
      "grad_norm": 9.113883972167969,
      "learning_rate": 1.1230040357957538e-06,
      "loss": 2.002,
      "step": 16720
    },
    {
      "epoch": 2.9356027373223372,
      "grad_norm": 13.104113578796387,
      "learning_rate": 1.0937591390302394e-06,
      "loss": 1.7324,
      "step": 16730
    },
    {
      "epoch": 2.937357431128268,
      "grad_norm": 12.095128059387207,
      "learning_rate": 1.0645142422647248e-06,
      "loss": 1.447,
      "step": 16740
    },
    {
      "epoch": 2.9391121249341987,
      "grad_norm": 9.14134693145752,
      "learning_rate": 1.0352693454992104e-06,
      "loss": 1.7949,
      "step": 16750
    },
    {
      "epoch": 2.9408668187401297,
      "grad_norm": 9.772993087768555,
      "learning_rate": 1.006024448733696e-06,
      "loss": 1.7791,
      "step": 16760
    },
    {
      "epoch": 2.9426215125460606,
      "grad_norm": 9.084077835083008,
      "learning_rate": 9.767795519681817e-07,
      "loss": 1.5993,
      "step": 16770
    },
    {
      "epoch": 2.9443762063519916,
      "grad_norm": 7.6606764793396,
      "learning_rate": 9.475346552026672e-07,
      "loss": 1.6206,
      "step": 16780
    },
    {
      "epoch": 2.9461309001579226,
      "grad_norm": 8.837339401245117,
      "learning_rate": 9.182897584371528e-07,
      "loss": 1.5697,
      "step": 16790
    },
    {
      "epoch": 2.9478855939638535,
      "grad_norm": 9.262345314025879,
      "learning_rate": 8.890448616716383e-07,
      "loss": 1.5551,
      "step": 16800
    },
    {
      "epoch": 2.949640287769784,
      "grad_norm": 10.026158332824707,
      "learning_rate": 8.59799964906124e-07,
      "loss": 1.5104,
      "step": 16810
    },
    {
      "epoch": 2.951394981575715,
      "grad_norm": 12.036643028259277,
      "learning_rate": 8.305550681406095e-07,
      "loss": 1.723,
      "step": 16820
    },
    {
      "epoch": 2.953149675381646,
      "grad_norm": 9.28067398071289,
      "learning_rate": 8.013101713750951e-07,
      "loss": 1.7703,
      "step": 16830
    },
    {
      "epoch": 2.954904369187577,
      "grad_norm": 11.86589241027832,
      "learning_rate": 7.720652746095807e-07,
      "loss": 1.8022,
      "step": 16840
    },
    {
      "epoch": 2.9566590629935074,
      "grad_norm": 9.24710750579834,
      "learning_rate": 7.428203778440662e-07,
      "loss": 1.6499,
      "step": 16850
    },
    {
      "epoch": 2.9584137567994384,
      "grad_norm": 10.675002098083496,
      "learning_rate": 7.135754810785519e-07,
      "loss": 1.7111,
      "step": 16860
    },
    {
      "epoch": 2.9601684506053694,
      "grad_norm": 13.351837158203125,
      "learning_rate": 6.843305843130374e-07,
      "loss": 1.7695,
      "step": 16870
    },
    {
      "epoch": 2.9619231444113003,
      "grad_norm": 9.916999816894531,
      "learning_rate": 6.55085687547523e-07,
      "loss": 1.7569,
      "step": 16880
    },
    {
      "epoch": 2.9636778382172313,
      "grad_norm": 7.8928937911987305,
      "learning_rate": 6.258407907820085e-07,
      "loss": 1.7727,
      "step": 16890
    },
    {
      "epoch": 2.965432532023162,
      "grad_norm": 9.25905704498291,
      "learning_rate": 5.965958940164941e-07,
      "loss": 1.5979,
      "step": 16900
    },
    {
      "epoch": 2.967187225829093,
      "grad_norm": 8.133151054382324,
      "learning_rate": 5.673509972509798e-07,
      "loss": 1.5886,
      "step": 16910
    },
    {
      "epoch": 2.9689419196350237,
      "grad_norm": 10.689207077026367,
      "learning_rate": 5.381061004854653e-07,
      "loss": 1.5701,
      "step": 16920
    },
    {
      "epoch": 2.9706966134409547,
      "grad_norm": 9.656947135925293,
      "learning_rate": 5.088612037199509e-07,
      "loss": 1.8599,
      "step": 16930
    },
    {
      "epoch": 2.972451307246885,
      "grad_norm": 7.813300132751465,
      "learning_rate": 4.796163069544364e-07,
      "loss": 1.634,
      "step": 16940
    },
    {
      "epoch": 2.974206001052816,
      "grad_norm": 8.904184341430664,
      "learning_rate": 4.50371410188922e-07,
      "loss": 1.6401,
      "step": 16950
    },
    {
      "epoch": 2.975960694858747,
      "grad_norm": 11.313161849975586,
      "learning_rate": 4.211265134234076e-07,
      "loss": 1.8649,
      "step": 16960
    },
    {
      "epoch": 2.977715388664678,
      "grad_norm": 7.799441814422607,
      "learning_rate": 3.9188161665789327e-07,
      "loss": 1.5549,
      "step": 16970
    },
    {
      "epoch": 2.979470082470609,
      "grad_norm": 8.641485214233398,
      "learning_rate": 3.626367198923788e-07,
      "loss": 1.5995,
      "step": 16980
    },
    {
      "epoch": 2.9812247762765396,
      "grad_norm": 9.165802955627441,
      "learning_rate": 3.3339182312686435e-07,
      "loss": 1.533,
      "step": 16990
    },
    {
      "epoch": 2.9829794700824706,
      "grad_norm": 6.774945259094238,
      "learning_rate": 3.041469263613499e-07,
      "loss": 1.6207,
      "step": 17000
    },
    {
      "epoch": 2.9847341638884015,
      "grad_norm": 7.3221001625061035,
      "learning_rate": 2.7490202959583554e-07,
      "loss": 1.5908,
      "step": 17010
    },
    {
      "epoch": 2.9864888576943325,
      "grad_norm": 9.271488189697266,
      "learning_rate": 2.4565713283032116e-07,
      "loss": 1.4304,
      "step": 17020
    },
    {
      "epoch": 2.988243551500263,
      "grad_norm": 7.857954025268555,
      "learning_rate": 2.164122360648067e-07,
      "loss": 1.659,
      "step": 17030
    },
    {
      "epoch": 2.989998245306194,
      "grad_norm": 10.560916900634766,
      "learning_rate": 1.871673392992923e-07,
      "loss": 1.7763,
      "step": 17040
    },
    {
      "epoch": 2.991752939112125,
      "grad_norm": 10.121676445007324,
      "learning_rate": 1.5792244253377787e-07,
      "loss": 1.7612,
      "step": 17050
    },
    {
      "epoch": 2.993507632918056,
      "grad_norm": 10.353948593139648,
      "learning_rate": 1.2867754576826344e-07,
      "loss": 1.5918,
      "step": 17060
    },
    {
      "epoch": 2.995262326723987,
      "grad_norm": 6.958789348602295,
      "learning_rate": 9.943264900274902e-08,
      "loss": 1.4506,
      "step": 17070
    },
    {
      "epoch": 2.9970170205299174,
      "grad_norm": 9.107274055480957,
      "learning_rate": 7.018775223723461e-08,
      "loss": 1.5984,
      "step": 17080
    },
    {
      "epoch": 2.9987717143358483,
      "grad_norm": 8.380897521972656,
      "learning_rate": 4.094285547172019e-08,
      "loss": 1.4757,
      "step": 17090
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.4531016934280951,
      "eval_f1_C01": 0.43179587831207067,
      "eval_f1_C02": 0.19444444444444445,
      "eval_f1_C03": 0.5401459854014599,
      "eval_f1_C04": 0.5829006576670128,
      "eval_f1_C05": 0.3592085235920852,
      "eval_f1_C06": 0.48522550544323484,
      "eval_f1_C07": 0.31016042780748665,
      "eval_f1_C08": 0.44190140845070425,
      "eval_f1_C09": 0.3460207612456747,
      "eval_f1_C10": 0.47364996746909566,
      "eval_f1_C11": 0.5023923444976076,
      "eval_f1_C12": 0.5369244135534318,
      "eval_f1_C13": 0.4739454094292804,
      "eval_f1_C14": 0.6292217327459618,
      "eval_f1_C15": 0.2513089005235602,
      "eval_f1_C16": 0.3114754098360656,
      "eval_f1_C17": 0.38580246913580246,
      "eval_f1_C18": 0.49934296977660975,
      "eval_f1_C19": 0.2704626334519573,
      "eval_f1_C20": 0.48434237995824636,
      "eval_f1_C21": 0.5379803395889187,
      "eval_f1_C22": 0.0,
      "eval_f1_C23": 0.21727917472598324,
      "eval_f1_macro": 0.40286659726333457,
      "eval_loss": 1.6587235927581787,
      "eval_precision_C01": 0.43052837573385516,
      "eval_precision_C02": 0.2777777777777778,
      "eval_precision_C03": 0.7115384615384616,
      "eval_precision_C04": 0.51879235982748,
      "eval_precision_C05": 0.367601246105919,
      "eval_precision_C06": 0.45348837209302323,
      "eval_precision_C07": 0.35365853658536583,
      "eval_precision_C08": 0.40614886731391586,
      "eval_precision_C09": 0.3424657534246575,
      "eval_precision_C10": 0.4745762711864407,
      "eval_precision_C11": 0.481651376146789,
      "eval_precision_C12": 0.4775888717156105,
      "eval_precision_C13": 0.3970893970893971,
      "eval_precision_C14": 0.569813829787234,
      "eval_precision_C15": 0.3779527559055118,
      "eval_precision_C16": 0.3825503355704698,
      "eval_precision_C17": 0.38461538461538464,
      "eval_precision_C18": 0.5039787798408488,
      "eval_precision_C19": 0.35185185185185186,
      "eval_precision_C20": 0.4275184275184275,
      "eval_precision_C21": 0.5657894736842105,
      "eval_precision_C22": 0.0,
      "eval_precision_C23": 0.28559322033898304,
      "eval_precision_global": 0.4148943358978963,
      "eval_recall_C01": 0.4330708661417323,
      "eval_recall_C02": 0.14957264957264957,
      "eval_recall_C03": 0.43529411764705883,
      "eval_recall_C04": 0.665086887835703,
      "eval_recall_C05": 0.35119047619047616,
      "eval_recall_C06": 0.5217391304347826,
      "eval_recall_C07": 0.2761904761904762,
      "eval_recall_C08": 0.48455598455598453,
      "eval_recall_C09": 0.34965034965034963,
      "eval_recall_C10": 0.4727272727272727,
      "eval_recall_C11": 0.525,
      "eval_recall_C12": 0.6130952380952381,
      "eval_recall_C13": 0.5876923076923077,
      "eval_recall_C14": 0.7024590163934427,
      "eval_recall_C15": 0.18823529411764706,
      "eval_recall_C16": 0.2626728110599078,
      "eval_recall_C17": 0.38699690402476783,
      "eval_recall_C18": 0.4947916666666667,
      "eval_recall_C19": 0.21965317919075145,
      "eval_recall_C20": 0.5585874799357945,
      "eval_recall_C21": 0.5127768313458262,
      "eval_recall_C22": 0.0,
      "eval_recall_C23": 0.1753381893860562,
      "eval_recall_global": 0.4072337882110823,
      "eval_runtime": 51.5524,
      "eval_samples_per_second": 221.076,
      "eval_steps_per_second": 27.642,
      "step": 17097
    }
  ],
  "logging_steps": 10,
  "max_steps": 17097,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.8440665371961344e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
