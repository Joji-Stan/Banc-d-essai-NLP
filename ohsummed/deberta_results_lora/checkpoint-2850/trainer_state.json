{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2850,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007017543859649123,
      "grad_norm": 1.2604175806045532,
      "learning_rate": 3.994385964912281e-05,
      "loss": 3.1428,
      "step": 10
    },
    {
      "epoch": 0.014035087719298246,
      "grad_norm": 1.3774422407150269,
      "learning_rate": 3.988771929824561e-05,
      "loss": 3.1305,
      "step": 20
    },
    {
      "epoch": 0.021052631578947368,
      "grad_norm": 1.4562627077102661,
      "learning_rate": 3.9831578947368425e-05,
      "loss": 3.1109,
      "step": 30
    },
    {
      "epoch": 0.028070175438596492,
      "grad_norm": 1.4487625360488892,
      "learning_rate": 3.977543859649123e-05,
      "loss": 3.0808,
      "step": 40
    },
    {
      "epoch": 0.03508771929824561,
      "grad_norm": 1.5011128187179565,
      "learning_rate": 3.9719298245614034e-05,
      "loss": 3.0743,
      "step": 50
    },
    {
      "epoch": 0.042105263157894736,
      "grad_norm": 1.277342438697815,
      "learning_rate": 3.9663157894736846e-05,
      "loss": 3.0305,
      "step": 60
    },
    {
      "epoch": 0.04912280701754386,
      "grad_norm": 1.5399539470672607,
      "learning_rate": 3.960701754385965e-05,
      "loss": 2.9921,
      "step": 70
    },
    {
      "epoch": 0.056140350877192984,
      "grad_norm": 1.383981466293335,
      "learning_rate": 3.955087719298246e-05,
      "loss": 2.9621,
      "step": 80
    },
    {
      "epoch": 0.06315789473684211,
      "grad_norm": 1.611066460609436,
      "learning_rate": 3.949473684210527e-05,
      "loss": 2.913,
      "step": 90
    },
    {
      "epoch": 0.07017543859649122,
      "grad_norm": 1.7793769836425781,
      "learning_rate": 3.943859649122807e-05,
      "loss": 2.8376,
      "step": 100
    },
    {
      "epoch": 0.07719298245614035,
      "grad_norm": 1.577679991722107,
      "learning_rate": 3.9382456140350883e-05,
      "loss": 2.8608,
      "step": 110
    },
    {
      "epoch": 0.08421052631578947,
      "grad_norm": 2.0095579624176025,
      "learning_rate": 3.932631578947369e-05,
      "loss": 2.8454,
      "step": 120
    },
    {
      "epoch": 0.0912280701754386,
      "grad_norm": 1.786095142364502,
      "learning_rate": 3.927017543859649e-05,
      "loss": 2.8373,
      "step": 130
    },
    {
      "epoch": 0.09824561403508772,
      "grad_norm": 1.9585458040237427,
      "learning_rate": 3.92140350877193e-05,
      "loss": 2.8047,
      "step": 140
    },
    {
      "epoch": 0.10526315789473684,
      "grad_norm": 1.7891299724578857,
      "learning_rate": 3.915789473684211e-05,
      "loss": 2.8204,
      "step": 150
    },
    {
      "epoch": 0.11228070175438597,
      "grad_norm": 2.3338568210601807,
      "learning_rate": 3.9101754385964914e-05,
      "loss": 2.7982,
      "step": 160
    },
    {
      "epoch": 0.11929824561403508,
      "grad_norm": 1.973358392715454,
      "learning_rate": 3.904561403508772e-05,
      "loss": 2.8458,
      "step": 170
    },
    {
      "epoch": 0.12631578947368421,
      "grad_norm": 2.152691125869751,
      "learning_rate": 3.898947368421053e-05,
      "loss": 2.8232,
      "step": 180
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 2.107297658920288,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 2.7975,
      "step": 190
    },
    {
      "epoch": 0.14035087719298245,
      "grad_norm": 2.069648265838623,
      "learning_rate": 3.887719298245615e-05,
      "loss": 2.8476,
      "step": 200
    },
    {
      "epoch": 0.14736842105263157,
      "grad_norm": 1.7535184621810913,
      "learning_rate": 3.882105263157895e-05,
      "loss": 2.8507,
      "step": 210
    },
    {
      "epoch": 0.1543859649122807,
      "grad_norm": 1.8440556526184082,
      "learning_rate": 3.876491228070176e-05,
      "loss": 2.845,
      "step": 220
    },
    {
      "epoch": 0.16140350877192983,
      "grad_norm": 2.739072322845459,
      "learning_rate": 3.870877192982457e-05,
      "loss": 2.7908,
      "step": 230
    },
    {
      "epoch": 0.16842105263157894,
      "grad_norm": 2.2442898750305176,
      "learning_rate": 3.865263157894737e-05,
      "loss": 2.8799,
      "step": 240
    },
    {
      "epoch": 0.17543859649122806,
      "grad_norm": 1.558708906173706,
      "learning_rate": 3.859649122807018e-05,
      "loss": 2.8199,
      "step": 250
    },
    {
      "epoch": 0.1824561403508772,
      "grad_norm": 2.080963611602783,
      "learning_rate": 3.854035087719298e-05,
      "loss": 2.8463,
      "step": 260
    },
    {
      "epoch": 0.18947368421052632,
      "grad_norm": 1.878458023071289,
      "learning_rate": 3.848421052631579e-05,
      "loss": 2.7909,
      "step": 270
    },
    {
      "epoch": 0.19649122807017544,
      "grad_norm": 2.703425884246826,
      "learning_rate": 3.84280701754386e-05,
      "loss": 2.8115,
      "step": 280
    },
    {
      "epoch": 0.20350877192982456,
      "grad_norm": 2.2161977291107178,
      "learning_rate": 3.8371929824561404e-05,
      "loss": 2.8731,
      "step": 290
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 1.74897038936615,
      "learning_rate": 3.8315789473684216e-05,
      "loss": 2.8434,
      "step": 300
    },
    {
      "epoch": 0.21754385964912282,
      "grad_norm": 2.2848715782165527,
      "learning_rate": 3.825964912280702e-05,
      "loss": 2.8003,
      "step": 310
    },
    {
      "epoch": 0.22456140350877193,
      "grad_norm": 1.5549567937850952,
      "learning_rate": 3.820350877192983e-05,
      "loss": 2.7947,
      "step": 320
    },
    {
      "epoch": 0.23157894736842105,
      "grad_norm": 2.6425788402557373,
      "learning_rate": 3.814736842105264e-05,
      "loss": 2.8205,
      "step": 330
    },
    {
      "epoch": 0.23859649122807017,
      "grad_norm": 1.802551507949829,
      "learning_rate": 3.809122807017544e-05,
      "loss": 2.7145,
      "step": 340
    },
    {
      "epoch": 0.24561403508771928,
      "grad_norm": 2.1852502822875977,
      "learning_rate": 3.8035087719298247e-05,
      "loss": 2.8111,
      "step": 350
    },
    {
      "epoch": 0.25263157894736843,
      "grad_norm": 2.368276357650757,
      "learning_rate": 3.797894736842106e-05,
      "loss": 2.8463,
      "step": 360
    },
    {
      "epoch": 0.2596491228070175,
      "grad_norm": 2.226445198059082,
      "learning_rate": 3.792280701754386e-05,
      "loss": 2.9111,
      "step": 370
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.7844003438949585,
      "learning_rate": 3.786666666666667e-05,
      "loss": 2.7675,
      "step": 380
    },
    {
      "epoch": 0.2736842105263158,
      "grad_norm": 2.330906629562378,
      "learning_rate": 3.781052631578947e-05,
      "loss": 2.8501,
      "step": 390
    },
    {
      "epoch": 0.2807017543859649,
      "grad_norm": 2.0081257820129395,
      "learning_rate": 3.7754385964912284e-05,
      "loss": 2.832,
      "step": 400
    },
    {
      "epoch": 0.28771929824561404,
      "grad_norm": 2.00434947013855,
      "learning_rate": 3.769824561403509e-05,
      "loss": 2.8165,
      "step": 410
    },
    {
      "epoch": 0.29473684210526313,
      "grad_norm": 1.8228042125701904,
      "learning_rate": 3.76421052631579e-05,
      "loss": 2.8218,
      "step": 420
    },
    {
      "epoch": 0.3017543859649123,
      "grad_norm": 2.1645519733428955,
      "learning_rate": 3.7585964912280705e-05,
      "loss": 2.7658,
      "step": 430
    },
    {
      "epoch": 0.3087719298245614,
      "grad_norm": 2.1925501823425293,
      "learning_rate": 3.752982456140352e-05,
      "loss": 2.7958,
      "step": 440
    },
    {
      "epoch": 0.3157894736842105,
      "grad_norm": 2.2661092281341553,
      "learning_rate": 3.747368421052632e-05,
      "loss": 2.7793,
      "step": 450
    },
    {
      "epoch": 0.32280701754385965,
      "grad_norm": 2.250624895095825,
      "learning_rate": 3.741754385964913e-05,
      "loss": 2.8316,
      "step": 460
    },
    {
      "epoch": 0.3298245614035088,
      "grad_norm": 2.080410957336426,
      "learning_rate": 3.736140350877193e-05,
      "loss": 2.7856,
      "step": 470
    },
    {
      "epoch": 0.3368421052631579,
      "grad_norm": 1.7985458374023438,
      "learning_rate": 3.7305263157894736e-05,
      "loss": 2.7606,
      "step": 480
    },
    {
      "epoch": 0.34385964912280703,
      "grad_norm": 2.2048001289367676,
      "learning_rate": 3.724912280701755e-05,
      "loss": 2.7462,
      "step": 490
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 2.1360793113708496,
      "learning_rate": 3.719298245614035e-05,
      "loss": 2.784,
      "step": 500
    },
    {
      "epoch": 0.35789473684210527,
      "grad_norm": 2.2884340286254883,
      "learning_rate": 3.713684210526316e-05,
      "loss": 2.732,
      "step": 510
    },
    {
      "epoch": 0.3649122807017544,
      "grad_norm": 2.053619384765625,
      "learning_rate": 3.708070175438597e-05,
      "loss": 2.7212,
      "step": 520
    },
    {
      "epoch": 0.3719298245614035,
      "grad_norm": 2.4182064533233643,
      "learning_rate": 3.7024561403508774e-05,
      "loss": 2.7451,
      "step": 530
    },
    {
      "epoch": 0.37894736842105264,
      "grad_norm": 3.0081398487091064,
      "learning_rate": 3.6968421052631586e-05,
      "loss": 2.6418,
      "step": 540
    },
    {
      "epoch": 0.38596491228070173,
      "grad_norm": 2.160547971725464,
      "learning_rate": 3.691228070175439e-05,
      "loss": 2.7656,
      "step": 550
    },
    {
      "epoch": 0.3929824561403509,
      "grad_norm": 1.9496092796325684,
      "learning_rate": 3.6856140350877195e-05,
      "loss": 2.7134,
      "step": 560
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.012284278869629,
      "learning_rate": 3.680000000000001e-05,
      "loss": 2.7416,
      "step": 570
    },
    {
      "epoch": 0.4070175438596491,
      "grad_norm": 2.2347826957702637,
      "learning_rate": 3.674385964912281e-05,
      "loss": 2.7713,
      "step": 580
    },
    {
      "epoch": 0.41403508771929826,
      "grad_norm": 2.124694585800171,
      "learning_rate": 3.6687719298245616e-05,
      "loss": 2.736,
      "step": 590
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 2.7977211475372314,
      "learning_rate": 3.663157894736842e-05,
      "loss": 2.7214,
      "step": 600
    },
    {
      "epoch": 0.4280701754385965,
      "grad_norm": 2.994447946548462,
      "learning_rate": 3.657543859649123e-05,
      "loss": 2.7129,
      "step": 610
    },
    {
      "epoch": 0.43508771929824563,
      "grad_norm": 1.8589369058609009,
      "learning_rate": 3.651929824561404e-05,
      "loss": 2.6292,
      "step": 620
    },
    {
      "epoch": 0.4421052631578947,
      "grad_norm": 2.6086878776550293,
      "learning_rate": 3.646315789473684e-05,
      "loss": 2.711,
      "step": 630
    },
    {
      "epoch": 0.44912280701754387,
      "grad_norm": 2.4385807514190674,
      "learning_rate": 3.640701754385965e-05,
      "loss": 2.7657,
      "step": 640
    },
    {
      "epoch": 0.45614035087719296,
      "grad_norm": 1.9509507417678833,
      "learning_rate": 3.635087719298246e-05,
      "loss": 2.6432,
      "step": 650
    },
    {
      "epoch": 0.4631578947368421,
      "grad_norm": 2.312866449356079,
      "learning_rate": 3.6294736842105264e-05,
      "loss": 2.5241,
      "step": 660
    },
    {
      "epoch": 0.47017543859649125,
      "grad_norm": 2.4003210067749023,
      "learning_rate": 3.6238596491228075e-05,
      "loss": 2.7032,
      "step": 670
    },
    {
      "epoch": 0.47719298245614034,
      "grad_norm": 2.3473784923553467,
      "learning_rate": 3.618245614035088e-05,
      "loss": 2.6075,
      "step": 680
    },
    {
      "epoch": 0.4842105263157895,
      "grad_norm": 2.4621615409851074,
      "learning_rate": 3.612631578947369e-05,
      "loss": 2.7459,
      "step": 690
    },
    {
      "epoch": 0.49122807017543857,
      "grad_norm": 2.4075381755828857,
      "learning_rate": 3.6070175438596497e-05,
      "loss": 2.5369,
      "step": 700
    },
    {
      "epoch": 0.4982456140350877,
      "grad_norm": 1.8976678848266602,
      "learning_rate": 3.60140350877193e-05,
      "loss": 2.653,
      "step": 710
    },
    {
      "epoch": 0.5052631578947369,
      "grad_norm": 2.1373486518859863,
      "learning_rate": 3.5957894736842106e-05,
      "loss": 2.6255,
      "step": 720
    },
    {
      "epoch": 0.512280701754386,
      "grad_norm": 2.804265260696411,
      "learning_rate": 3.590175438596491e-05,
      "loss": 2.6311,
      "step": 730
    },
    {
      "epoch": 0.519298245614035,
      "grad_norm": 3.347088575363159,
      "learning_rate": 3.584561403508772e-05,
      "loss": 2.5796,
      "step": 740
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 3.2459871768951416,
      "learning_rate": 3.578947368421053e-05,
      "loss": 2.612,
      "step": 750
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.4887938499450684,
      "learning_rate": 3.573333333333333e-05,
      "loss": 2.6504,
      "step": 760
    },
    {
      "epoch": 0.5403508771929825,
      "grad_norm": 3.0579938888549805,
      "learning_rate": 3.5677192982456144e-05,
      "loss": 2.6607,
      "step": 770
    },
    {
      "epoch": 0.5473684210526316,
      "grad_norm": 2.490828037261963,
      "learning_rate": 3.562105263157895e-05,
      "loss": 2.7033,
      "step": 780
    },
    {
      "epoch": 0.5543859649122806,
      "grad_norm": 2.008807420730591,
      "learning_rate": 3.556491228070176e-05,
      "loss": 2.7719,
      "step": 790
    },
    {
      "epoch": 0.5614035087719298,
      "grad_norm": 2.113839626312256,
      "learning_rate": 3.5508771929824565e-05,
      "loss": 2.6976,
      "step": 800
    },
    {
      "epoch": 0.5684210526315789,
      "grad_norm": 1.725063443183899,
      "learning_rate": 3.545263157894737e-05,
      "loss": 2.6096,
      "step": 810
    },
    {
      "epoch": 0.5754385964912281,
      "grad_norm": 2.627526044845581,
      "learning_rate": 3.539649122807018e-05,
      "loss": 2.644,
      "step": 820
    },
    {
      "epoch": 0.5824561403508772,
      "grad_norm": 2.834496259689331,
      "learning_rate": 3.5340350877192986e-05,
      "loss": 2.6244,
      "step": 830
    },
    {
      "epoch": 0.5894736842105263,
      "grad_norm": 2.663980007171631,
      "learning_rate": 3.528421052631579e-05,
      "loss": 2.6561,
      "step": 840
    },
    {
      "epoch": 0.5964912280701754,
      "grad_norm": 2.16851806640625,
      "learning_rate": 3.5228070175438596e-05,
      "loss": 2.7719,
      "step": 850
    },
    {
      "epoch": 0.6035087719298246,
      "grad_norm": 2.2657828330993652,
      "learning_rate": 3.517192982456141e-05,
      "loss": 2.6184,
      "step": 860
    },
    {
      "epoch": 0.6105263157894737,
      "grad_norm": 3.8279430866241455,
      "learning_rate": 3.511578947368421e-05,
      "loss": 2.5488,
      "step": 870
    },
    {
      "epoch": 0.6175438596491228,
      "grad_norm": 2.2188053131103516,
      "learning_rate": 3.505964912280702e-05,
      "loss": 2.7421,
      "step": 880
    },
    {
      "epoch": 0.624561403508772,
      "grad_norm": 3.396851062774658,
      "learning_rate": 3.500350877192983e-05,
      "loss": 2.6973,
      "step": 890
    },
    {
      "epoch": 0.631578947368421,
      "grad_norm": 2.3508105278015137,
      "learning_rate": 3.4947368421052634e-05,
      "loss": 2.6578,
      "step": 900
    },
    {
      "epoch": 0.6385964912280702,
      "grad_norm": 2.473639488220215,
      "learning_rate": 3.4891228070175445e-05,
      "loss": 2.7415,
      "step": 910
    },
    {
      "epoch": 0.6456140350877193,
      "grad_norm": 2.55133318901062,
      "learning_rate": 3.483508771929825e-05,
      "loss": 2.6536,
      "step": 920
    },
    {
      "epoch": 0.6526315789473685,
      "grad_norm": 2.5952935218811035,
      "learning_rate": 3.4778947368421055e-05,
      "loss": 2.6127,
      "step": 930
    },
    {
      "epoch": 0.6596491228070176,
      "grad_norm": 2.2811100482940674,
      "learning_rate": 3.472280701754386e-05,
      "loss": 2.5652,
      "step": 940
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 2.6754233837127686,
      "learning_rate": 3.466666666666667e-05,
      "loss": 2.6682,
      "step": 950
    },
    {
      "epoch": 0.6736842105263158,
      "grad_norm": 2.2592999935150146,
      "learning_rate": 3.4610526315789476e-05,
      "loss": 2.5958,
      "step": 960
    },
    {
      "epoch": 0.6807017543859649,
      "grad_norm": 2.110684871673584,
      "learning_rate": 3.455438596491228e-05,
      "loss": 2.6584,
      "step": 970
    },
    {
      "epoch": 0.6877192982456141,
      "grad_norm": 3.2575292587280273,
      "learning_rate": 3.4498245614035086e-05,
      "loss": 2.5861,
      "step": 980
    },
    {
      "epoch": 0.6947368421052632,
      "grad_norm": 2.5890519618988037,
      "learning_rate": 3.44421052631579e-05,
      "loss": 2.6398,
      "step": 990
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 2.7118802070617676,
      "learning_rate": 3.43859649122807e-05,
      "loss": 2.6392,
      "step": 1000
    },
    {
      "epoch": 0.7087719298245614,
      "grad_norm": 2.4427032470703125,
      "learning_rate": 3.4329824561403514e-05,
      "loss": 2.6997,
      "step": 1010
    },
    {
      "epoch": 0.7157894736842105,
      "grad_norm": 5.150656700134277,
      "learning_rate": 3.427368421052632e-05,
      "loss": 2.5541,
      "step": 1020
    },
    {
      "epoch": 0.7228070175438597,
      "grad_norm": 2.1586952209472656,
      "learning_rate": 3.421754385964913e-05,
      "loss": 2.5476,
      "step": 1030
    },
    {
      "epoch": 0.7298245614035088,
      "grad_norm": 2.615097999572754,
      "learning_rate": 3.4161403508771935e-05,
      "loss": 2.5683,
      "step": 1040
    },
    {
      "epoch": 0.7368421052631579,
      "grad_norm": 2.702409505844116,
      "learning_rate": 3.410526315789474e-05,
      "loss": 2.6038,
      "step": 1050
    },
    {
      "epoch": 0.743859649122807,
      "grad_norm": 4.925928592681885,
      "learning_rate": 3.4049122807017545e-05,
      "loss": 2.6016,
      "step": 1060
    },
    {
      "epoch": 0.7508771929824561,
      "grad_norm": 3.0463132858276367,
      "learning_rate": 3.3992982456140356e-05,
      "loss": 2.5698,
      "step": 1070
    },
    {
      "epoch": 0.7578947368421053,
      "grad_norm": 3.2607173919677734,
      "learning_rate": 3.393684210526316e-05,
      "loss": 2.5488,
      "step": 1080
    },
    {
      "epoch": 0.7649122807017544,
      "grad_norm": 4.5317792892456055,
      "learning_rate": 3.3880701754385966e-05,
      "loss": 2.5297,
      "step": 1090
    },
    {
      "epoch": 0.7719298245614035,
      "grad_norm": 3.968909502029419,
      "learning_rate": 3.382456140350877e-05,
      "loss": 2.6739,
      "step": 1100
    },
    {
      "epoch": 0.7789473684210526,
      "grad_norm": 2.962228536605835,
      "learning_rate": 3.376842105263158e-05,
      "loss": 2.5119,
      "step": 1110
    },
    {
      "epoch": 0.7859649122807018,
      "grad_norm": 2.5637850761413574,
      "learning_rate": 3.371228070175439e-05,
      "loss": 2.6663,
      "step": 1120
    },
    {
      "epoch": 0.7929824561403509,
      "grad_norm": 3.1981775760650635,
      "learning_rate": 3.36561403508772e-05,
      "loss": 2.5304,
      "step": 1130
    },
    {
      "epoch": 0.8,
      "grad_norm": 4.480305194854736,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 2.5741,
      "step": 1140
    },
    {
      "epoch": 0.8070175438596491,
      "grad_norm": 2.4116127490997314,
      "learning_rate": 3.3543859649122815e-05,
      "loss": 2.4903,
      "step": 1150
    },
    {
      "epoch": 0.8140350877192982,
      "grad_norm": 3.944413900375366,
      "learning_rate": 3.348771929824562e-05,
      "loss": 2.5365,
      "step": 1160
    },
    {
      "epoch": 0.8210526315789474,
      "grad_norm": 4.275918960571289,
      "learning_rate": 3.3431578947368425e-05,
      "loss": 2.5737,
      "step": 1170
    },
    {
      "epoch": 0.8280701754385965,
      "grad_norm": 3.6713762283325195,
      "learning_rate": 3.337543859649123e-05,
      "loss": 2.5388,
      "step": 1180
    },
    {
      "epoch": 0.8350877192982457,
      "grad_norm": 2.3156754970550537,
      "learning_rate": 3.3319298245614034e-05,
      "loss": 2.4443,
      "step": 1190
    },
    {
      "epoch": 0.8421052631578947,
      "grad_norm": 3.9870316982269287,
      "learning_rate": 3.3263157894736846e-05,
      "loss": 2.6256,
      "step": 1200
    },
    {
      "epoch": 0.8491228070175438,
      "grad_norm": 3.9808921813964844,
      "learning_rate": 3.320701754385965e-05,
      "loss": 2.6013,
      "step": 1210
    },
    {
      "epoch": 0.856140350877193,
      "grad_norm": 3.728199005126953,
      "learning_rate": 3.3150877192982456e-05,
      "loss": 2.5283,
      "step": 1220
    },
    {
      "epoch": 0.8631578947368421,
      "grad_norm": 4.356791019439697,
      "learning_rate": 3.309473684210527e-05,
      "loss": 2.5084,
      "step": 1230
    },
    {
      "epoch": 0.8701754385964913,
      "grad_norm": 3.26163387298584,
      "learning_rate": 3.303859649122807e-05,
      "loss": 2.5304,
      "step": 1240
    },
    {
      "epoch": 0.8771929824561403,
      "grad_norm": 4.752171993255615,
      "learning_rate": 3.298245614035088e-05,
      "loss": 2.68,
      "step": 1250
    },
    {
      "epoch": 0.8842105263157894,
      "grad_norm": 3.594406843185425,
      "learning_rate": 3.292631578947369e-05,
      "loss": 2.5157,
      "step": 1260
    },
    {
      "epoch": 0.8912280701754386,
      "grad_norm": 4.665381908416748,
      "learning_rate": 3.287017543859649e-05,
      "loss": 2.5215,
      "step": 1270
    },
    {
      "epoch": 0.8982456140350877,
      "grad_norm": 3.9278173446655273,
      "learning_rate": 3.2814035087719305e-05,
      "loss": 2.5225,
      "step": 1280
    },
    {
      "epoch": 0.9052631578947369,
      "grad_norm": 4.352803707122803,
      "learning_rate": 3.275789473684211e-05,
      "loss": 2.5333,
      "step": 1290
    },
    {
      "epoch": 0.9122807017543859,
      "grad_norm": 3.2656311988830566,
      "learning_rate": 3.2701754385964915e-05,
      "loss": 2.4274,
      "step": 1300
    },
    {
      "epoch": 0.9192982456140351,
      "grad_norm": 4.750765800476074,
      "learning_rate": 3.264561403508772e-05,
      "loss": 2.5407,
      "step": 1310
    },
    {
      "epoch": 0.9263157894736842,
      "grad_norm": 4.201137065887451,
      "learning_rate": 3.258947368421053e-05,
      "loss": 2.4999,
      "step": 1320
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 4.61777400970459,
      "learning_rate": 3.2533333333333336e-05,
      "loss": 2.4864,
      "step": 1330
    },
    {
      "epoch": 0.9403508771929825,
      "grad_norm": 5.378518581390381,
      "learning_rate": 3.247719298245614e-05,
      "loss": 2.5005,
      "step": 1340
    },
    {
      "epoch": 0.9473684210526315,
      "grad_norm": 3.8440842628479004,
      "learning_rate": 3.2421052631578945e-05,
      "loss": 2.475,
      "step": 1350
    },
    {
      "epoch": 0.9543859649122807,
      "grad_norm": 2.9044268131256104,
      "learning_rate": 3.236491228070176e-05,
      "loss": 2.4692,
      "step": 1360
    },
    {
      "epoch": 0.9614035087719298,
      "grad_norm": 5.908802032470703,
      "learning_rate": 3.230877192982456e-05,
      "loss": 2.4603,
      "step": 1370
    },
    {
      "epoch": 0.968421052631579,
      "grad_norm": 8.3141508102417,
      "learning_rate": 3.2252631578947373e-05,
      "loss": 2.3701,
      "step": 1380
    },
    {
      "epoch": 0.9754385964912281,
      "grad_norm": 5.039294242858887,
      "learning_rate": 3.219649122807018e-05,
      "loss": 2.5616,
      "step": 1390
    },
    {
      "epoch": 0.9824561403508771,
      "grad_norm": 3.7861204147338867,
      "learning_rate": 3.214035087719299e-05,
      "loss": 2.5694,
      "step": 1400
    },
    {
      "epoch": 0.9894736842105263,
      "grad_norm": 3.851377487182617,
      "learning_rate": 3.2084210526315795e-05,
      "loss": 2.4802,
      "step": 1410
    },
    {
      "epoch": 0.9964912280701754,
      "grad_norm": 4.255686283111572,
      "learning_rate": 3.20280701754386e-05,
      "loss": 2.3554,
      "step": 1420
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.26252522593665,
      "eval_f1_C01": 0.0,
      "eval_f1_C02": 0.0,
      "eval_f1_C03": 0.0,
      "eval_f1_C04": 0.3417624521072797,
      "eval_f1_C05": 0.0,
      "eval_f1_C06": 0.0,
      "eval_f1_C07": 0.0,
      "eval_f1_C08": 0.0,
      "eval_f1_C09": 0.0,
      "eval_f1_C10": 0.0025575447570332483,
      "eval_f1_C11": 0.0,
      "eval_f1_C12": 0.0,
      "eval_f1_C13": 0.0,
      "eval_f1_C14": 0.5859689476710753,
      "eval_f1_C15": 0.0,
      "eval_f1_C16": 0.0,
      "eval_f1_C17": 0.0,
      "eval_f1_C18": 0.1437908496732026,
      "eval_f1_C19": 0.0,
      "eval_f1_C20": 0.0,
      "eval_f1_C21": 0.0,
      "eval_f1_C22": 0.0,
      "eval_f1_C23": 0.28856592540710907,
      "eval_f1_macro": 0.05924546607024782,
      "eval_loss": 2.452338933944702,
      "eval_precision_C01": 1.0,
      "eval_precision_C02": 1.0,
      "eval_precision_C03": 1.0,
      "eval_precision_C04": 0.2120174938201179,
      "eval_precision_C05": 1.0,
      "eval_precision_C06": 0.0,
      "eval_precision_C07": 1.0,
      "eval_precision_C08": 1.0,
      "eval_precision_C09": 1.0,
      "eval_precision_C10": 0.08333333333333333,
      "eval_precision_C11": 1.0,
      "eval_precision_C12": 1.0,
      "eval_precision_C13": 1.0,
      "eval_precision_C14": 0.4512843224092117,
      "eval_precision_C15": 1.0,
      "eval_precision_C16": 1.0,
      "eval_precision_C17": 1.0,
      "eval_precision_C18": 0.44,
      "eval_precision_C19": 1.0,
      "eval_precision_C20": 0.0,
      "eval_precision_C21": 1.0,
      "eval_precision_C22": 1.0,
      "eval_precision_C23": 0.2174716283979942,
      "eval_precision_global": 0.7567002946939416,
      "eval_recall_C01": 0.0,
      "eval_recall_C02": 0.0,
      "eval_recall_C03": 0.0,
      "eval_recall_C04": 0.8807266982622433,
      "eval_recall_C05": 0.0,
      "eval_recall_C06": 0.0,
      "eval_recall_C07": 0.0,
      "eval_recall_C08": 0.0,
      "eval_recall_C09": 0.0,
      "eval_recall_C10": 0.0012987012987012987,
      "eval_recall_C11": 0.0,
      "eval_recall_C12": 0.0,
      "eval_recall_C13": 0.0,
      "eval_recall_C14": 0.8352459016393443,
      "eval_recall_C15": 0.0,
      "eval_recall_C16": 0.0,
      "eval_recall_C17": 0.0,
      "eval_recall_C18": 0.0859375,
      "eval_recall_C19": 0.0,
      "eval_recall_C20": 0.0,
      "eval_recall_C21": 0.0,
      "eval_recall_C22": 0.0,
      "eval_recall_C23": 0.4287200832466181,
      "eval_recall_global": 0.09704038628030032,
      "eval_runtime": 153.8668,
      "eval_samples_per_second": 74.071,
      "eval_steps_per_second": 2.32,
      "step": 1425
    },
    {
      "epoch": 1.0035087719298246,
      "grad_norm": 3.352334976196289,
      "learning_rate": 3.1971929824561404e-05,
      "loss": 2.413,
      "step": 1430
    },
    {
      "epoch": 1.0105263157894737,
      "grad_norm": 3.5179443359375,
      "learning_rate": 3.191578947368421e-05,
      "loss": 2.43,
      "step": 1440
    },
    {
      "epoch": 1.0175438596491229,
      "grad_norm": 5.062627792358398,
      "learning_rate": 3.185964912280702e-05,
      "loss": 2.3661,
      "step": 1450
    },
    {
      "epoch": 1.024561403508772,
      "grad_norm": 5.112011909484863,
      "learning_rate": 3.1803508771929826e-05,
      "loss": 2.437,
      "step": 1460
    },
    {
      "epoch": 1.0315789473684212,
      "grad_norm": 4.127882480621338,
      "learning_rate": 3.174736842105263e-05,
      "loss": 2.3331,
      "step": 1470
    },
    {
      "epoch": 1.03859649122807,
      "grad_norm": 3.2103652954101562,
      "learning_rate": 3.169122807017544e-05,
      "loss": 2.4082,
      "step": 1480
    },
    {
      "epoch": 1.0456140350877192,
      "grad_norm": 4.083942413330078,
      "learning_rate": 3.163508771929825e-05,
      "loss": 2.3657,
      "step": 1490
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 5.568539142608643,
      "learning_rate": 3.157894736842106e-05,
      "loss": 2.4369,
      "step": 1500
    },
    {
      "epoch": 1.0596491228070175,
      "grad_norm": 4.869932174682617,
      "learning_rate": 3.152280701754386e-05,
      "loss": 2.3785,
      "step": 1510
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 4.727568626403809,
      "learning_rate": 3.146666666666667e-05,
      "loss": 2.4251,
      "step": 1520
    },
    {
      "epoch": 1.0736842105263158,
      "grad_norm": 4.278483867645264,
      "learning_rate": 3.141052631578948e-05,
      "loss": 2.3825,
      "step": 1530
    },
    {
      "epoch": 1.080701754385965,
      "grad_norm": 4.590001583099365,
      "learning_rate": 3.1354385964912284e-05,
      "loss": 2.397,
      "step": 1540
    },
    {
      "epoch": 1.087719298245614,
      "grad_norm": 4.267868518829346,
      "learning_rate": 3.129824561403509e-05,
      "loss": 2.4603,
      "step": 1550
    },
    {
      "epoch": 1.0947368421052632,
      "grad_norm": 4.847787857055664,
      "learning_rate": 3.1242105263157894e-05,
      "loss": 2.4594,
      "step": 1560
    },
    {
      "epoch": 1.1017543859649124,
      "grad_norm": 7.898068904876709,
      "learning_rate": 3.1185964912280706e-05,
      "loss": 2.4067,
      "step": 1570
    },
    {
      "epoch": 1.1087719298245613,
      "grad_norm": 6.920315742492676,
      "learning_rate": 3.112982456140351e-05,
      "loss": 2.318,
      "step": 1580
    },
    {
      "epoch": 1.1157894736842104,
      "grad_norm": 5.103217601776123,
      "learning_rate": 3.1073684210526315e-05,
      "loss": 2.4409,
      "step": 1590
    },
    {
      "epoch": 1.1228070175438596,
      "grad_norm": 4.604344844818115,
      "learning_rate": 3.101754385964913e-05,
      "loss": 2.4666,
      "step": 1600
    },
    {
      "epoch": 1.1298245614035087,
      "grad_norm": 4.570755481719971,
      "learning_rate": 3.096140350877193e-05,
      "loss": 2.5074,
      "step": 1610
    },
    {
      "epoch": 1.1368421052631579,
      "grad_norm": 4.245136260986328,
      "learning_rate": 3.090526315789474e-05,
      "loss": 2.4041,
      "step": 1620
    },
    {
      "epoch": 1.143859649122807,
      "grad_norm": 6.885979652404785,
      "learning_rate": 3.084912280701755e-05,
      "loss": 2.273,
      "step": 1630
    },
    {
      "epoch": 1.1508771929824562,
      "grad_norm": 4.872701644897461,
      "learning_rate": 3.079298245614035e-05,
      "loss": 2.288,
      "step": 1640
    },
    {
      "epoch": 1.1578947368421053,
      "grad_norm": 5.850711822509766,
      "learning_rate": 3.073684210526316e-05,
      "loss": 2.3211,
      "step": 1650
    },
    {
      "epoch": 1.1649122807017545,
      "grad_norm": 6.322419166564941,
      "learning_rate": 3.068070175438597e-05,
      "loss": 2.4346,
      "step": 1660
    },
    {
      "epoch": 1.1719298245614036,
      "grad_norm": 4.245922565460205,
      "learning_rate": 3.0624561403508774e-05,
      "loss": 2.4625,
      "step": 1670
    },
    {
      "epoch": 1.1789473684210527,
      "grad_norm": 6.492407321929932,
      "learning_rate": 3.056842105263158e-05,
      "loss": 2.3055,
      "step": 1680
    },
    {
      "epoch": 1.1859649122807017,
      "grad_norm": 3.70711088180542,
      "learning_rate": 3.0512280701754387e-05,
      "loss": 2.3194,
      "step": 1690
    },
    {
      "epoch": 1.1929824561403508,
      "grad_norm": 4.266017436981201,
      "learning_rate": 3.0456140350877195e-05,
      "loss": 2.3328,
      "step": 1700
    },
    {
      "epoch": 1.2,
      "grad_norm": 5.330011367797852,
      "learning_rate": 3.0400000000000004e-05,
      "loss": 2.2194,
      "step": 1710
    },
    {
      "epoch": 1.207017543859649,
      "grad_norm": 4.36406135559082,
      "learning_rate": 3.034385964912281e-05,
      "loss": 2.3422,
      "step": 1720
    },
    {
      "epoch": 1.2140350877192982,
      "grad_norm": 4.72511100769043,
      "learning_rate": 3.0287719298245613e-05,
      "loss": 2.3306,
      "step": 1730
    },
    {
      "epoch": 1.2210526315789474,
      "grad_norm": 6.6082444190979,
      "learning_rate": 3.0231578947368425e-05,
      "loss": 2.3352,
      "step": 1740
    },
    {
      "epoch": 1.2280701754385965,
      "grad_norm": 5.410521984100342,
      "learning_rate": 3.017543859649123e-05,
      "loss": 2.333,
      "step": 1750
    },
    {
      "epoch": 1.2350877192982457,
      "grad_norm": 5.625265121459961,
      "learning_rate": 3.0119298245614038e-05,
      "loss": 2.2418,
      "step": 1760
    },
    {
      "epoch": 1.2421052631578948,
      "grad_norm": 6.6500349044799805,
      "learning_rate": 3.0063157894736843e-05,
      "loss": 2.3265,
      "step": 1770
    },
    {
      "epoch": 1.2491228070175437,
      "grad_norm": 5.18841552734375,
      "learning_rate": 3.0007017543859654e-05,
      "loss": 2.3631,
      "step": 1780
    },
    {
      "epoch": 1.256140350877193,
      "grad_norm": 6.000190258026123,
      "learning_rate": 2.995087719298246e-05,
      "loss": 2.4296,
      "step": 1790
    },
    {
      "epoch": 1.263157894736842,
      "grad_norm": 5.714511871337891,
      "learning_rate": 2.9894736842105264e-05,
      "loss": 2.3687,
      "step": 1800
    },
    {
      "epoch": 1.2701754385964912,
      "grad_norm": 5.697600841522217,
      "learning_rate": 2.9838596491228072e-05,
      "loss": 2.296,
      "step": 1810
    },
    {
      "epoch": 1.2771929824561403,
      "grad_norm": 4.978451251983643,
      "learning_rate": 2.978245614035088e-05,
      "loss": 2.2945,
      "step": 1820
    },
    {
      "epoch": 1.2842105263157895,
      "grad_norm": 7.050954341888428,
      "learning_rate": 2.972631578947369e-05,
      "loss": 2.3049,
      "step": 1830
    },
    {
      "epoch": 1.2912280701754386,
      "grad_norm": 6.645350933074951,
      "learning_rate": 2.9670175438596493e-05,
      "loss": 2.2493,
      "step": 1840
    },
    {
      "epoch": 1.2982456140350878,
      "grad_norm": 5.105212688446045,
      "learning_rate": 2.9614035087719298e-05,
      "loss": 2.2929,
      "step": 1850
    },
    {
      "epoch": 1.305263157894737,
      "grad_norm": 4.831392765045166,
      "learning_rate": 2.955789473684211e-05,
      "loss": 2.3432,
      "step": 1860
    },
    {
      "epoch": 1.312280701754386,
      "grad_norm": 6.868584632873535,
      "learning_rate": 2.9501754385964915e-05,
      "loss": 2.3975,
      "step": 1870
    },
    {
      "epoch": 1.3192982456140352,
      "grad_norm": 6.108323097229004,
      "learning_rate": 2.9445614035087723e-05,
      "loss": 2.2867,
      "step": 1880
    },
    {
      "epoch": 1.3263157894736843,
      "grad_norm": 6.238369464874268,
      "learning_rate": 2.9389473684210528e-05,
      "loss": 2.4191,
      "step": 1890
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 5.028003215789795,
      "learning_rate": 2.9333333333333333e-05,
      "loss": 2.3321,
      "step": 1900
    },
    {
      "epoch": 1.3403508771929824,
      "grad_norm": 4.82411003112793,
      "learning_rate": 2.9277192982456144e-05,
      "loss": 2.3233,
      "step": 1910
    },
    {
      "epoch": 1.3473684210526315,
      "grad_norm": 6.009828090667725,
      "learning_rate": 2.922105263157895e-05,
      "loss": 2.2195,
      "step": 1920
    },
    {
      "epoch": 1.3543859649122807,
      "grad_norm": 4.929795265197754,
      "learning_rate": 2.9164912280701757e-05,
      "loss": 2.2792,
      "step": 1930
    },
    {
      "epoch": 1.3614035087719298,
      "grad_norm": 4.8765549659729,
      "learning_rate": 2.9108771929824562e-05,
      "loss": 2.3024,
      "step": 1940
    },
    {
      "epoch": 1.368421052631579,
      "grad_norm": 5.817769527435303,
      "learning_rate": 2.9052631578947374e-05,
      "loss": 2.273,
      "step": 1950
    },
    {
      "epoch": 1.3754385964912281,
      "grad_norm": 5.3321380615234375,
      "learning_rate": 2.899649122807018e-05,
      "loss": 2.1755,
      "step": 1960
    },
    {
      "epoch": 1.3824561403508773,
      "grad_norm": 6.554256439208984,
      "learning_rate": 2.8940350877192983e-05,
      "loss": 2.4053,
      "step": 1970
    },
    {
      "epoch": 1.3894736842105262,
      "grad_norm": 7.97580099105835,
      "learning_rate": 2.888421052631579e-05,
      "loss": 2.3809,
      "step": 1980
    },
    {
      "epoch": 1.3964912280701753,
      "grad_norm": 7.029486179351807,
      "learning_rate": 2.88280701754386e-05,
      "loss": 2.3751,
      "step": 1990
    },
    {
      "epoch": 1.4035087719298245,
      "grad_norm": 5.708216667175293,
      "learning_rate": 2.8771929824561408e-05,
      "loss": 2.3141,
      "step": 2000
    },
    {
      "epoch": 1.4105263157894736,
      "grad_norm": 6.331243991851807,
      "learning_rate": 2.8715789473684213e-05,
      "loss": 2.293,
      "step": 2010
    },
    {
      "epoch": 1.4175438596491228,
      "grad_norm": 6.089460372924805,
      "learning_rate": 2.8659649122807018e-05,
      "loss": 2.3051,
      "step": 2020
    },
    {
      "epoch": 1.424561403508772,
      "grad_norm": 5.31583309173584,
      "learning_rate": 2.860350877192983e-05,
      "loss": 2.343,
      "step": 2030
    },
    {
      "epoch": 1.431578947368421,
      "grad_norm": 6.687865257263184,
      "learning_rate": 2.8547368421052634e-05,
      "loss": 2.3286,
      "step": 2040
    },
    {
      "epoch": 1.4385964912280702,
      "grad_norm": 6.770966529846191,
      "learning_rate": 2.8491228070175442e-05,
      "loss": 2.263,
      "step": 2050
    },
    {
      "epoch": 1.4456140350877194,
      "grad_norm": 5.422611713409424,
      "learning_rate": 2.8435087719298247e-05,
      "loss": 2.2283,
      "step": 2060
    },
    {
      "epoch": 1.4526315789473685,
      "grad_norm": 7.409144401550293,
      "learning_rate": 2.837894736842106e-05,
      "loss": 2.218,
      "step": 2070
    },
    {
      "epoch": 1.4596491228070176,
      "grad_norm": 7.448114395141602,
      "learning_rate": 2.8322807017543863e-05,
      "loss": 2.3449,
      "step": 2080
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 5.215080738067627,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 2.2612,
      "step": 2090
    },
    {
      "epoch": 1.4736842105263157,
      "grad_norm": 6.8806257247924805,
      "learning_rate": 2.8210526315789476e-05,
      "loss": 2.271,
      "step": 2100
    },
    {
      "epoch": 1.4807017543859649,
      "grad_norm": 5.618961334228516,
      "learning_rate": 2.815438596491228e-05,
      "loss": 2.2147,
      "step": 2110
    },
    {
      "epoch": 1.487719298245614,
      "grad_norm": 5.533901214599609,
      "learning_rate": 2.8098245614035093e-05,
      "loss": 2.4186,
      "step": 2120
    },
    {
      "epoch": 1.4947368421052631,
      "grad_norm": 6.76760196685791,
      "learning_rate": 2.8042105263157898e-05,
      "loss": 2.3097,
      "step": 2130
    },
    {
      "epoch": 1.5017543859649123,
      "grad_norm": 7.384055137634277,
      "learning_rate": 2.7985964912280702e-05,
      "loss": 2.1522,
      "step": 2140
    },
    {
      "epoch": 1.5087719298245614,
      "grad_norm": 6.516120433807373,
      "learning_rate": 2.792982456140351e-05,
      "loss": 2.2627,
      "step": 2150
    },
    {
      "epoch": 1.5157894736842106,
      "grad_norm": 7.412403583526611,
      "learning_rate": 2.787368421052632e-05,
      "loss": 2.201,
      "step": 2160
    },
    {
      "epoch": 1.5228070175438595,
      "grad_norm": 7.785566329956055,
      "learning_rate": 2.7817543859649124e-05,
      "loss": 2.2248,
      "step": 2170
    },
    {
      "epoch": 1.5298245614035086,
      "grad_norm": 6.905685901641846,
      "learning_rate": 2.7761403508771932e-05,
      "loss": 2.2137,
      "step": 2180
    },
    {
      "epoch": 1.5368421052631578,
      "grad_norm": 5.93710470199585,
      "learning_rate": 2.7705263157894737e-05,
      "loss": 2.3342,
      "step": 2190
    },
    {
      "epoch": 1.543859649122807,
      "grad_norm": 7.470151901245117,
      "learning_rate": 2.764912280701755e-05,
      "loss": 2.2466,
      "step": 2200
    },
    {
      "epoch": 1.550877192982456,
      "grad_norm": 5.307878494262695,
      "learning_rate": 2.7592982456140353e-05,
      "loss": 2.2459,
      "step": 2210
    },
    {
      "epoch": 1.5578947368421052,
      "grad_norm": 9.417170524597168,
      "learning_rate": 2.7536842105263158e-05,
      "loss": 2.2396,
      "step": 2220
    },
    {
      "epoch": 1.5649122807017544,
      "grad_norm": 5.283676624298096,
      "learning_rate": 2.7480701754385966e-05,
      "loss": 2.1899,
      "step": 2230
    },
    {
      "epoch": 1.5719298245614035,
      "grad_norm": 7.939864158630371,
      "learning_rate": 2.7424561403508774e-05,
      "loss": 2.1592,
      "step": 2240
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 8.377013206481934,
      "learning_rate": 2.7368421052631583e-05,
      "loss": 2.2353,
      "step": 2250
    },
    {
      "epoch": 1.5859649122807018,
      "grad_norm": 4.5263214111328125,
      "learning_rate": 2.7312280701754387e-05,
      "loss": 2.1759,
      "step": 2260
    },
    {
      "epoch": 1.592982456140351,
      "grad_norm": 8.07304859161377,
      "learning_rate": 2.7256140350877192e-05,
      "loss": 2.1598,
      "step": 2270
    },
    {
      "epoch": 1.6,
      "grad_norm": 5.810146808624268,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 2.1041,
      "step": 2280
    },
    {
      "epoch": 1.6070175438596492,
      "grad_norm": 7.78114652633667,
      "learning_rate": 2.714385964912281e-05,
      "loss": 2.274,
      "step": 2290
    },
    {
      "epoch": 1.6140350877192984,
      "grad_norm": 9.419349670410156,
      "learning_rate": 2.7087719298245617e-05,
      "loss": 2.2715,
      "step": 2300
    },
    {
      "epoch": 1.6210526315789475,
      "grad_norm": 7.486435413360596,
      "learning_rate": 2.7031578947368422e-05,
      "loss": 2.2216,
      "step": 2310
    },
    {
      "epoch": 1.6280701754385964,
      "grad_norm": 6.941974639892578,
      "learning_rate": 2.6975438596491233e-05,
      "loss": 2.127,
      "step": 2320
    },
    {
      "epoch": 1.6350877192982456,
      "grad_norm": 7.547976016998291,
      "learning_rate": 2.6919298245614038e-05,
      "loss": 2.1629,
      "step": 2330
    },
    {
      "epoch": 1.6421052631578947,
      "grad_norm": 6.792576789855957,
      "learning_rate": 2.6863157894736843e-05,
      "loss": 2.1126,
      "step": 2340
    },
    {
      "epoch": 1.6491228070175439,
      "grad_norm": 7.17717981338501,
      "learning_rate": 2.680701754385965e-05,
      "loss": 2.1915,
      "step": 2350
    },
    {
      "epoch": 1.656140350877193,
      "grad_norm": 9.284516334533691,
      "learning_rate": 2.6750877192982456e-05,
      "loss": 2.1265,
      "step": 2360
    },
    {
      "epoch": 1.663157894736842,
      "grad_norm": 6.731569290161133,
      "learning_rate": 2.6694736842105268e-05,
      "loss": 2.1695,
      "step": 2370
    },
    {
      "epoch": 1.670175438596491,
      "grad_norm": 7.223387718200684,
      "learning_rate": 2.6638596491228072e-05,
      "loss": 2.1087,
      "step": 2380
    },
    {
      "epoch": 1.6771929824561402,
      "grad_norm": 10.516717910766602,
      "learning_rate": 2.6582456140350877e-05,
      "loss": 2.2434,
      "step": 2390
    },
    {
      "epoch": 1.6842105263157894,
      "grad_norm": 8.88536262512207,
      "learning_rate": 2.6526315789473685e-05,
      "loss": 2.1783,
      "step": 2400
    },
    {
      "epoch": 1.6912280701754385,
      "grad_norm": 5.981401443481445,
      "learning_rate": 2.6470175438596494e-05,
      "loss": 2.303,
      "step": 2410
    },
    {
      "epoch": 1.6982456140350877,
      "grad_norm": 7.468846321105957,
      "learning_rate": 2.6414035087719302e-05,
      "loss": 2.3237,
      "step": 2420
    },
    {
      "epoch": 1.7052631578947368,
      "grad_norm": 7.450871467590332,
      "learning_rate": 2.6357894736842107e-05,
      "loss": 2.2302,
      "step": 2430
    },
    {
      "epoch": 1.712280701754386,
      "grad_norm": 7.649069786071777,
      "learning_rate": 2.630175438596491e-05,
      "loss": 2.1434,
      "step": 2440
    },
    {
      "epoch": 1.719298245614035,
      "grad_norm": 5.361613750457764,
      "learning_rate": 2.6245614035087723e-05,
      "loss": 2.2304,
      "step": 2450
    },
    {
      "epoch": 1.7263157894736842,
      "grad_norm": 9.959081649780273,
      "learning_rate": 2.6189473684210528e-05,
      "loss": 2.259,
      "step": 2460
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 6.624756813049316,
      "learning_rate": 2.6133333333333336e-05,
      "loss": 2.1732,
      "step": 2470
    },
    {
      "epoch": 1.7403508771929825,
      "grad_norm": 6.469422817230225,
      "learning_rate": 2.607719298245614e-05,
      "loss": 2.2444,
      "step": 2480
    },
    {
      "epoch": 1.7473684210526317,
      "grad_norm": 5.8155622482299805,
      "learning_rate": 2.6021052631578953e-05,
      "loss": 2.3413,
      "step": 2490
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 6.275063514709473,
      "learning_rate": 2.5964912280701757e-05,
      "loss": 2.2765,
      "step": 2500
    },
    {
      "epoch": 1.76140350877193,
      "grad_norm": 5.989220142364502,
      "learning_rate": 2.5908771929824562e-05,
      "loss": 2.0881,
      "step": 2510
    },
    {
      "epoch": 1.768421052631579,
      "grad_norm": 10.014102935791016,
      "learning_rate": 2.585263157894737e-05,
      "loss": 2.1615,
      "step": 2520
    },
    {
      "epoch": 1.775438596491228,
      "grad_norm": 8.324996948242188,
      "learning_rate": 2.579649122807018e-05,
      "loss": 2.1137,
      "step": 2530
    },
    {
      "epoch": 1.7824561403508772,
      "grad_norm": 6.744744777679443,
      "learning_rate": 2.5740350877192987e-05,
      "loss": 2.2451,
      "step": 2540
    },
    {
      "epoch": 1.7894736842105263,
      "grad_norm": 8.71381950378418,
      "learning_rate": 2.568421052631579e-05,
      "loss": 2.1092,
      "step": 2550
    },
    {
      "epoch": 1.7964912280701755,
      "grad_norm": 8.320711135864258,
      "learning_rate": 2.5628070175438596e-05,
      "loss": 2.1396,
      "step": 2560
    },
    {
      "epoch": 1.8035087719298246,
      "grad_norm": 8.399272918701172,
      "learning_rate": 2.5571929824561405e-05,
      "loss": 2.3282,
      "step": 2570
    },
    {
      "epoch": 1.8105263157894735,
      "grad_norm": 6.804418563842773,
      "learning_rate": 2.5515789473684213e-05,
      "loss": 2.2749,
      "step": 2580
    },
    {
      "epoch": 1.8175438596491227,
      "grad_norm": 7.321113109588623,
      "learning_rate": 2.545964912280702e-05,
      "loss": 2.1532,
      "step": 2590
    },
    {
      "epoch": 1.8245614035087718,
      "grad_norm": 7.992355823516846,
      "learning_rate": 2.5403508771929826e-05,
      "loss": 2.1099,
      "step": 2600
    },
    {
      "epoch": 1.831578947368421,
      "grad_norm": 9.716031074523926,
      "learning_rate": 2.534736842105263e-05,
      "loss": 2.1025,
      "step": 2610
    },
    {
      "epoch": 1.8385964912280701,
      "grad_norm": 8.386366844177246,
      "learning_rate": 2.5291228070175442e-05,
      "loss": 2.1148,
      "step": 2620
    },
    {
      "epoch": 1.8456140350877193,
      "grad_norm": 5.373336315155029,
      "learning_rate": 2.5235087719298247e-05,
      "loss": 2.2584,
      "step": 2630
    },
    {
      "epoch": 1.8526315789473684,
      "grad_norm": 5.363504409790039,
      "learning_rate": 2.5178947368421055e-05,
      "loss": 2.1805,
      "step": 2640
    },
    {
      "epoch": 1.8596491228070176,
      "grad_norm": 6.231789588928223,
      "learning_rate": 2.512280701754386e-05,
      "loss": 2.1948,
      "step": 2650
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 7.278851509094238,
      "learning_rate": 2.5066666666666672e-05,
      "loss": 2.2637,
      "step": 2660
    },
    {
      "epoch": 1.8736842105263158,
      "grad_norm": 6.644517421722412,
      "learning_rate": 2.5010526315789477e-05,
      "loss": 2.1575,
      "step": 2670
    },
    {
      "epoch": 1.880701754385965,
      "grad_norm": 7.254465579986572,
      "learning_rate": 2.495438596491228e-05,
      "loss": 2.1597,
      "step": 2680
    },
    {
      "epoch": 1.8877192982456141,
      "grad_norm": 6.8525071144104,
      "learning_rate": 2.489824561403509e-05,
      "loss": 2.3623,
      "step": 2690
    },
    {
      "epoch": 1.8947368421052633,
      "grad_norm": 7.920596599578857,
      "learning_rate": 2.4842105263157898e-05,
      "loss": 2.1031,
      "step": 2700
    },
    {
      "epoch": 1.9017543859649124,
      "grad_norm": 5.9179534912109375,
      "learning_rate": 2.4785964912280706e-05,
      "loss": 2.107,
      "step": 2710
    },
    {
      "epoch": 1.9087719298245616,
      "grad_norm": 6.949718952178955,
      "learning_rate": 2.472982456140351e-05,
      "loss": 2.0985,
      "step": 2720
    },
    {
      "epoch": 1.9157894736842105,
      "grad_norm": 11.526226997375488,
      "learning_rate": 2.4673684210526316e-05,
      "loss": 2.1367,
      "step": 2730
    },
    {
      "epoch": 1.9228070175438596,
      "grad_norm": 6.466208457946777,
      "learning_rate": 2.4617543859649127e-05,
      "loss": 2.2185,
      "step": 2740
    },
    {
      "epoch": 1.9298245614035088,
      "grad_norm": 6.723085403442383,
      "learning_rate": 2.4561403508771932e-05,
      "loss": 2.0791,
      "step": 2750
    },
    {
      "epoch": 1.936842105263158,
      "grad_norm": 7.635095119476318,
      "learning_rate": 2.450526315789474e-05,
      "loss": 2.1277,
      "step": 2760
    },
    {
      "epoch": 1.943859649122807,
      "grad_norm": 10.347837448120117,
      "learning_rate": 2.4449122807017545e-05,
      "loss": 2.1581,
      "step": 2770
    },
    {
      "epoch": 1.950877192982456,
      "grad_norm": 7.853734493255615,
      "learning_rate": 2.4392982456140353e-05,
      "loss": 2.0903,
      "step": 2780
    },
    {
      "epoch": 1.9578947368421051,
      "grad_norm": 7.0681023597717285,
      "learning_rate": 2.433684210526316e-05,
      "loss": 2.0885,
      "step": 2790
    },
    {
      "epoch": 1.9649122807017543,
      "grad_norm": 7.3175129890441895,
      "learning_rate": 2.4280701754385966e-05,
      "loss": 1.9894,
      "step": 2800
    },
    {
      "epoch": 1.9719298245614034,
      "grad_norm": 10.827235221862793,
      "learning_rate": 2.422456140350877e-05,
      "loss": 2.1543,
      "step": 2810
    },
    {
      "epoch": 1.9789473684210526,
      "grad_norm": 7.789448261260986,
      "learning_rate": 2.416842105263158e-05,
      "loss": 2.1105,
      "step": 2820
    },
    {
      "epoch": 1.9859649122807017,
      "grad_norm": 10.489773750305176,
      "learning_rate": 2.4112280701754388e-05,
      "loss": 1.994,
      "step": 2830
    },
    {
      "epoch": 1.9929824561403509,
      "grad_norm": 6.103429317474365,
      "learning_rate": 2.4056140350877196e-05,
      "loss": 2.1759,
      "step": 2840
    },
    {
      "epoch": 2.0,
      "grad_norm": 11.882257461547852,
      "learning_rate": 2.4e-05,
      "loss": 1.9987,
      "step": 2850
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.36676318329384927,
      "eval_f1_C01": 0.18114143920595532,
      "eval_f1_C02": 0.0,
      "eval_f1_C03": 0.0,
      "eval_f1_C04": 0.5784424379232506,
      "eval_f1_C05": 0.055401662049861494,
      "eval_f1_C06": 0.15782664941785252,
      "eval_f1_C07": 0.0,
      "eval_f1_C08": 0.3,
      "eval_f1_C09": 0.0,
      "eval_f1_C10": 0.4507042253521127,
      "eval_f1_C11": 0.029556650246305417,
      "eval_f1_C12": 0.39433962264150946,
      "eval_f1_C13": 0.03954802259887006,
      "eval_f1_C14": 0.6390134529147982,
      "eval_f1_C15": 0.0,
      "eval_f1_C16": 0.0,
      "eval_f1_C17": 0.19172932330827067,
      "eval_f1_C18": 0.4607594936708861,
      "eval_f1_C19": 0.011299435028248588,
      "eval_f1_C20": 0.3453815261044177,
      "eval_f1_C21": 0.4392041267501842,
      "eval_f1_C22": 0.0,
      "eval_f1_C23": 0.24013157894736842,
      "eval_f1_macro": 0.19628172374608224,
      "eval_loss": 2.0952224731445312,
      "eval_precision_C01": 0.24496644295302014,
      "eval_precision_C02": 1.0,
      "eval_precision_C03": 1.0,
      "eval_precision_C04": 0.4499561018437226,
      "eval_precision_C05": 0.4,
      "eval_precision_C06": 0.3485714285714286,
      "eval_precision_C07": 1.0,
      "eval_precision_C08": 0.40728476821192056,
      "eval_precision_C09": 1.0,
      "eval_precision_C10": 0.41113490364025695,
      "eval_precision_C11": 1.0,
      "eval_precision_C12": 0.37589928057553956,
      "eval_precision_C13": 0.2413793103448276,
      "eval_precision_C14": 0.5872252747252747,
      "eval_precision_C15": 1.0,
      "eval_precision_C16": 1.0,
      "eval_precision_C17": 0.24401913875598086,
      "eval_precision_C18": 0.4482758620689655,
      "eval_precision_C19": 0.25,
      "eval_precision_C20": 0.2391841779975278,
      "eval_precision_C21": 0.38701298701298703,
      "eval_precision_C22": 1.0,
      "eval_precision_C23": 0.21893744644387317,
      "eval_precision_global": 0.5762542227454489,
      "eval_recall_C01": 0.1437007874015748,
      "eval_recall_C02": 0.0,
      "eval_recall_C03": 0.0,
      "eval_recall_C04": 0.8096366508688784,
      "eval_recall_C05": 0.02976190476190476,
      "eval_recall_C06": 0.1020066889632107,
      "eval_recall_C07": 0.0,
      "eval_recall_C08": 0.23745173745173745,
      "eval_recall_C09": 0.0,
      "eval_recall_C10": 0.4987012987012987,
      "eval_recall_C11": 0.015,
      "eval_recall_C12": 0.4146825396825397,
      "eval_recall_C13": 0.021538461538461538,
      "eval_recall_C14": 0.7008196721311475,
      "eval_recall_C15": 0.0,
      "eval_recall_C16": 0.0,
      "eval_recall_C17": 0.15789473684210525,
      "eval_recall_C18": 0.4739583333333333,
      "eval_recall_C19": 0.005780346820809248,
      "eval_recall_C20": 0.6211878009630819,
      "eval_recall_C21": 0.5076660988074957,
      "eval_recall_C22": 0.0,
      "eval_recall_C23": 0.26586888657648283,
      "eval_recall_global": 0.21763721499322006,
      "eval_runtime": 154.0138,
      "eval_samples_per_second": 74.0,
      "eval_steps_per_second": 2.318,
      "step": 2850
    }
  ],
  "logging_steps": 10,
  "max_steps": 7125,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.437394023697203e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
