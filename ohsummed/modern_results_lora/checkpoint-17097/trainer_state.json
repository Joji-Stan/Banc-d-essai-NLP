{
  "best_global_step": 17097,
  "best_metric": 0.09232683224125567,
  "best_model_checkpoint": "./results_lora/checkpoint-17097",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 17097,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001754693805930865,
      "grad_norm": 6.772942543029785,
      "learning_rate": 4.997075510323449e-05,
      "loss": 3.091,
      "step": 10
    },
    {
      "epoch": 0.00350938761186173,
      "grad_norm": 6.493109226226807,
      "learning_rate": 4.9941510206468974e-05,
      "loss": 3.027,
      "step": 20
    },
    {
      "epoch": 0.005264081417792595,
      "grad_norm": 5.638149261474609,
      "learning_rate": 4.991226530970346e-05,
      "loss": 2.9459,
      "step": 30
    },
    {
      "epoch": 0.00701877522372346,
      "grad_norm": 6.605985641479492,
      "learning_rate": 4.9883020412937945e-05,
      "loss": 3.0007,
      "step": 40
    },
    {
      "epoch": 0.008773469029654325,
      "grad_norm": 6.261245250701904,
      "learning_rate": 4.985377551617243e-05,
      "loss": 2.8702,
      "step": 50
    },
    {
      "epoch": 0.01052816283558519,
      "grad_norm": 6.250457286834717,
      "learning_rate": 4.9824530619406916e-05,
      "loss": 2.7865,
      "step": 60
    },
    {
      "epoch": 0.012282856641516056,
      "grad_norm": 6.431131839752197,
      "learning_rate": 4.97952857226414e-05,
      "loss": 2.8823,
      "step": 70
    },
    {
      "epoch": 0.01403755044744692,
      "grad_norm": 7.317582607269287,
      "learning_rate": 4.976604082587589e-05,
      "loss": 2.9024,
      "step": 80
    },
    {
      "epoch": 0.015792244253377784,
      "grad_norm": 6.35418701171875,
      "learning_rate": 4.973679592911037e-05,
      "loss": 2.9939,
      "step": 90
    },
    {
      "epoch": 0.01754693805930865,
      "grad_norm": 5.19711446762085,
      "learning_rate": 4.970755103234486e-05,
      "loss": 2.8444,
      "step": 100
    },
    {
      "epoch": 0.019301631865239515,
      "grad_norm": 6.6686015129089355,
      "learning_rate": 4.9678306135579344e-05,
      "loss": 2.936,
      "step": 110
    },
    {
      "epoch": 0.02105632567117038,
      "grad_norm": 6.21428108215332,
      "learning_rate": 4.964906123881383e-05,
      "loss": 2.7704,
      "step": 120
    },
    {
      "epoch": 0.022811019477101246,
      "grad_norm": 5.760121822357178,
      "learning_rate": 4.9619816342048315e-05,
      "loss": 2.9119,
      "step": 130
    },
    {
      "epoch": 0.02456571328303211,
      "grad_norm": 6.889328479766846,
      "learning_rate": 4.95905714452828e-05,
      "loss": 3.0647,
      "step": 140
    },
    {
      "epoch": 0.026320407088962976,
      "grad_norm": 6.6439104080200195,
      "learning_rate": 4.956132654851729e-05,
      "loss": 2.9029,
      "step": 150
    },
    {
      "epoch": 0.02807510089489384,
      "grad_norm": 6.643672466278076,
      "learning_rate": 4.953208165175177e-05,
      "loss": 2.7354,
      "step": 160
    },
    {
      "epoch": 0.029829794700824707,
      "grad_norm": 5.422804355621338,
      "learning_rate": 4.950283675498626e-05,
      "loss": 2.8333,
      "step": 170
    },
    {
      "epoch": 0.03158448850675557,
      "grad_norm": 6.292634963989258,
      "learning_rate": 4.9473591858220744e-05,
      "loss": 2.9505,
      "step": 180
    },
    {
      "epoch": 0.033339182312686434,
      "grad_norm": 5.5913166999816895,
      "learning_rate": 4.944434696145523e-05,
      "loss": 2.9193,
      "step": 190
    },
    {
      "epoch": 0.0350938761186173,
      "grad_norm": 6.149282455444336,
      "learning_rate": 4.9415102064689715e-05,
      "loss": 2.9567,
      "step": 200
    },
    {
      "epoch": 0.036848569924548165,
      "grad_norm": Infinity,
      "learning_rate": 4.938878165760075e-05,
      "loss": 2.798,
      "step": 210
    },
    {
      "epoch": 0.03860326373047903,
      "grad_norm": 6.5546979904174805,
      "learning_rate": 4.9359536760835235e-05,
      "loss": 2.8107,
      "step": 220
    },
    {
      "epoch": 0.040357957536409896,
      "grad_norm": 5.857735633850098,
      "learning_rate": 4.933029186406972e-05,
      "loss": 2.9486,
      "step": 230
    },
    {
      "epoch": 0.04211265134234076,
      "grad_norm": 6.321648120880127,
      "learning_rate": 4.930104696730421e-05,
      "loss": 2.9887,
      "step": 240
    },
    {
      "epoch": 0.043867345148271626,
      "grad_norm": 6.716445446014404,
      "learning_rate": 4.927180207053869e-05,
      "loss": 2.8555,
      "step": 250
    },
    {
      "epoch": 0.04562203895420249,
      "grad_norm": 5.1349382400512695,
      "learning_rate": 4.924255717377318e-05,
      "loss": 2.6818,
      "step": 260
    },
    {
      "epoch": 0.04737673276013336,
      "grad_norm": 6.327927589416504,
      "learning_rate": 4.921331227700767e-05,
      "loss": 2.7471,
      "step": 270
    },
    {
      "epoch": 0.04913142656606422,
      "grad_norm": 7.434772968292236,
      "learning_rate": 4.918406738024215e-05,
      "loss": 2.7421,
      "step": 280
    },
    {
      "epoch": 0.05088612037199509,
      "grad_norm": 8.278626441955566,
      "learning_rate": 4.9154822483476634e-05,
      "loss": 2.7997,
      "step": 290
    },
    {
      "epoch": 0.05264081417792595,
      "grad_norm": 6.504897594451904,
      "learning_rate": 4.912557758671112e-05,
      "loss": 2.8048,
      "step": 300
    },
    {
      "epoch": 0.05439550798385682,
      "grad_norm": 6.249392032623291,
      "learning_rate": 4.9096332689945605e-05,
      "loss": 2.992,
      "step": 310
    },
    {
      "epoch": 0.05615020178978768,
      "grad_norm": 6.253667831420898,
      "learning_rate": 4.906708779318009e-05,
      "loss": 2.8836,
      "step": 320
    },
    {
      "epoch": 0.05790489559571855,
      "grad_norm": 6.304611682891846,
      "learning_rate": 4.903784289641458e-05,
      "loss": 3.0236,
      "step": 330
    },
    {
      "epoch": 0.059659589401649414,
      "grad_norm": 6.617813587188721,
      "learning_rate": 4.900859799964906e-05,
      "loss": 2.8963,
      "step": 340
    },
    {
      "epoch": 0.06141428320758028,
      "grad_norm": 6.473119258880615,
      "learning_rate": 4.8979353102883555e-05,
      "loss": 2.9502,
      "step": 350
    },
    {
      "epoch": 0.06316897701351114,
      "grad_norm": 6.088491916656494,
      "learning_rate": 4.8950108206118033e-05,
      "loss": 2.792,
      "step": 360
    },
    {
      "epoch": 0.064923670819442,
      "grad_norm": 6.332667350769043,
      "learning_rate": 4.892086330935252e-05,
      "loss": 2.8468,
      "step": 370
    },
    {
      "epoch": 0.06667836462537287,
      "grad_norm": 5.767425537109375,
      "learning_rate": 4.8891618412587005e-05,
      "loss": 2.7601,
      "step": 380
    },
    {
      "epoch": 0.06843305843130373,
      "grad_norm": 5.643768310546875,
      "learning_rate": 4.886237351582149e-05,
      "loss": 2.7879,
      "step": 390
    },
    {
      "epoch": 0.0701877522372346,
      "grad_norm": 7.653735160827637,
      "learning_rate": 4.8833128619055976e-05,
      "loss": 2.8331,
      "step": 400
    },
    {
      "epoch": 0.07194244604316546,
      "grad_norm": 5.744213581085205,
      "learning_rate": 4.880388372229046e-05,
      "loss": 2.7953,
      "step": 410
    },
    {
      "epoch": 0.07369713984909633,
      "grad_norm": 6.411333084106445,
      "learning_rate": 4.877463882552495e-05,
      "loss": 2.8467,
      "step": 420
    },
    {
      "epoch": 0.0754518336550272,
      "grad_norm": 6.1392412185668945,
      "learning_rate": 4.874539392875944e-05,
      "loss": 2.8023,
      "step": 430
    },
    {
      "epoch": 0.07720652746095806,
      "grad_norm": 7.01930046081543,
      "learning_rate": 4.871614903199392e-05,
      "loss": 2.9049,
      "step": 440
    },
    {
      "epoch": 0.07896122126688893,
      "grad_norm": 7.491669178009033,
      "learning_rate": 4.8686904135228404e-05,
      "loss": 2.6892,
      "step": 450
    },
    {
      "epoch": 0.08071591507281979,
      "grad_norm": 6.7289509773254395,
      "learning_rate": 4.865765923846289e-05,
      "loss": 2.972,
      "step": 460
    },
    {
      "epoch": 0.08247060887875066,
      "grad_norm": 7.910562515258789,
      "learning_rate": 4.8628414341697375e-05,
      "loss": 3.0977,
      "step": 470
    },
    {
      "epoch": 0.08422530268468152,
      "grad_norm": 6.3587541580200195,
      "learning_rate": 4.859916944493186e-05,
      "loss": 2.8187,
      "step": 480
    },
    {
      "epoch": 0.08597999649061239,
      "grad_norm": 7.671270370483398,
      "learning_rate": 4.8569924548166347e-05,
      "loss": 2.8092,
      "step": 490
    },
    {
      "epoch": 0.08773469029654325,
      "grad_norm": 5.704912185668945,
      "learning_rate": 4.854067965140083e-05,
      "loss": 2.7904,
      "step": 500
    },
    {
      "epoch": 0.08948938410247412,
      "grad_norm": 6.223719596862793,
      "learning_rate": 4.851143475463532e-05,
      "loss": 2.8761,
      "step": 510
    },
    {
      "epoch": 0.09124407790840498,
      "grad_norm": 6.52095890045166,
      "learning_rate": 4.8482189857869803e-05,
      "loss": 2.831,
      "step": 520
    },
    {
      "epoch": 0.09299877171433585,
      "grad_norm": 5.10875940322876,
      "learning_rate": 4.845294496110429e-05,
      "loss": 2.9211,
      "step": 530
    },
    {
      "epoch": 0.09475346552026671,
      "grad_norm": 5.185791015625,
      "learning_rate": 4.8423700064338775e-05,
      "loss": 2.9468,
      "step": 540
    },
    {
      "epoch": 0.09650815932619758,
      "grad_norm": 6.279374122619629,
      "learning_rate": 4.839445516757326e-05,
      "loss": 2.7901,
      "step": 550
    },
    {
      "epoch": 0.09826285313212844,
      "grad_norm": 5.796026706695557,
      "learning_rate": 4.8365210270807746e-05,
      "loss": 2.8061,
      "step": 560
    },
    {
      "epoch": 0.10001754693805931,
      "grad_norm": 6.016565799713135,
      "learning_rate": 4.833596537404223e-05,
      "loss": 2.8509,
      "step": 570
    },
    {
      "epoch": 0.10177224074399017,
      "grad_norm": 5.669590950012207,
      "learning_rate": 4.830672047727672e-05,
      "loss": 2.8414,
      "step": 580
    },
    {
      "epoch": 0.10352693454992104,
      "grad_norm": 5.270317554473877,
      "learning_rate": 4.82774755805112e-05,
      "loss": 2.8888,
      "step": 590
    },
    {
      "epoch": 0.1052816283558519,
      "grad_norm": 6.443649768829346,
      "learning_rate": 4.824823068374569e-05,
      "loss": 2.7934,
      "step": 600
    },
    {
      "epoch": 0.10703632216178277,
      "grad_norm": 6.244164943695068,
      "learning_rate": 4.8218985786980174e-05,
      "loss": 2.9464,
      "step": 610
    },
    {
      "epoch": 0.10879101596771364,
      "grad_norm": 8.416333198547363,
      "learning_rate": 4.818974089021466e-05,
      "loss": 2.8934,
      "step": 620
    },
    {
      "epoch": 0.1105457097736445,
      "grad_norm": 5.498358249664307,
      "learning_rate": 4.8160495993449145e-05,
      "loss": 3.0318,
      "step": 630
    },
    {
      "epoch": 0.11230040357957537,
      "grad_norm": 5.489185333251953,
      "learning_rate": 4.813125109668363e-05,
      "loss": 2.7412,
      "step": 640
    },
    {
      "epoch": 0.11405509738550623,
      "grad_norm": 5.604339122772217,
      "learning_rate": 4.8102006199918117e-05,
      "loss": 2.8906,
      "step": 650
    },
    {
      "epoch": 0.1158097911914371,
      "grad_norm": 6.819319725036621,
      "learning_rate": 4.80727613031526e-05,
      "loss": 2.7285,
      "step": 660
    },
    {
      "epoch": 0.11756448499736796,
      "grad_norm": 6.5298566818237305,
      "learning_rate": 4.804351640638709e-05,
      "loss": 2.7713,
      "step": 670
    },
    {
      "epoch": 0.11931917880329883,
      "grad_norm": 5.530370235443115,
      "learning_rate": 4.801427150962157e-05,
      "loss": 2.8369,
      "step": 680
    },
    {
      "epoch": 0.1210738726092297,
      "grad_norm": 6.010349750518799,
      "learning_rate": 4.798502661285606e-05,
      "loss": 2.8049,
      "step": 690
    },
    {
      "epoch": 0.12282856641516056,
      "grad_norm": 6.068904876708984,
      "learning_rate": 4.7955781716090545e-05,
      "loss": 3.0319,
      "step": 700
    },
    {
      "epoch": 0.12458326022109142,
      "grad_norm": 5.182581424713135,
      "learning_rate": 4.792653681932503e-05,
      "loss": 2.7973,
      "step": 710
    },
    {
      "epoch": 0.12633795402702228,
      "grad_norm": 5.849311351776123,
      "learning_rate": 4.7897291922559516e-05,
      "loss": 2.7667,
      "step": 720
    },
    {
      "epoch": 0.12809264783295315,
      "grad_norm": 5.463101863861084,
      "learning_rate": 4.7868047025794e-05,
      "loss": 2.8331,
      "step": 730
    },
    {
      "epoch": 0.129847341638884,
      "grad_norm": 7.034355163574219,
      "learning_rate": 4.783880212902849e-05,
      "loss": 2.7647,
      "step": 740
    },
    {
      "epoch": 0.13160203544481489,
      "grad_norm": 6.779574394226074,
      "learning_rate": 4.780955723226297e-05,
      "loss": 2.781,
      "step": 750
    },
    {
      "epoch": 0.13335672925074574,
      "grad_norm": 5.62937068939209,
      "learning_rate": 4.778031233549746e-05,
      "loss": 2.7786,
      "step": 760
    },
    {
      "epoch": 0.13511142305667662,
      "grad_norm": 6.353093147277832,
      "learning_rate": 4.7751067438731944e-05,
      "loss": 2.8615,
      "step": 770
    },
    {
      "epoch": 0.13686611686260747,
      "grad_norm": 6.0209879875183105,
      "learning_rate": 4.772182254196643e-05,
      "loss": 2.9797,
      "step": 780
    },
    {
      "epoch": 0.13862081066853835,
      "grad_norm": 6.068411350250244,
      "learning_rate": 4.7692577645200915e-05,
      "loss": 2.8385,
      "step": 790
    },
    {
      "epoch": 0.1403755044744692,
      "grad_norm": 5.05585241317749,
      "learning_rate": 4.76633327484354e-05,
      "loss": 2.8475,
      "step": 800
    },
    {
      "epoch": 0.14213019828040008,
      "grad_norm": 6.654353141784668,
      "learning_rate": 4.7634087851669886e-05,
      "loss": 2.9012,
      "step": 810
    },
    {
      "epoch": 0.14388489208633093,
      "grad_norm": 4.914608001708984,
      "learning_rate": 4.760484295490437e-05,
      "loss": 2.7266,
      "step": 820
    },
    {
      "epoch": 0.1456395858922618,
      "grad_norm": 7.056940078735352,
      "learning_rate": 4.757559805813886e-05,
      "loss": 2.7759,
      "step": 830
    },
    {
      "epoch": 0.14739427969819266,
      "grad_norm": 5.350067615509033,
      "learning_rate": 4.754635316137334e-05,
      "loss": 2.7657,
      "step": 840
    },
    {
      "epoch": 0.14914897350412354,
      "grad_norm": 5.688477993011475,
      "learning_rate": 4.751710826460783e-05,
      "loss": 2.8334,
      "step": 850
    },
    {
      "epoch": 0.1509036673100544,
      "grad_norm": 6.799375057220459,
      "learning_rate": 4.7487863367842315e-05,
      "loss": 2.7546,
      "step": 860
    },
    {
      "epoch": 0.15265836111598527,
      "grad_norm": 5.405704021453857,
      "learning_rate": 4.74586184710768e-05,
      "loss": 2.7841,
      "step": 870
    },
    {
      "epoch": 0.15441305492191612,
      "grad_norm": 6.582126617431641,
      "learning_rate": 4.7429373574311286e-05,
      "loss": 2.7106,
      "step": 880
    },
    {
      "epoch": 0.156167748727847,
      "grad_norm": 6.675332546234131,
      "learning_rate": 4.740012867754577e-05,
      "loss": 2.7651,
      "step": 890
    },
    {
      "epoch": 0.15792244253377785,
      "grad_norm": 6.794093608856201,
      "learning_rate": 4.737088378078026e-05,
      "loss": 2.921,
      "step": 900
    },
    {
      "epoch": 0.15967713633970873,
      "grad_norm": 6.358955383300781,
      "learning_rate": 4.734163888401474e-05,
      "loss": 2.7279,
      "step": 910
    },
    {
      "epoch": 0.16143183014563958,
      "grad_norm": 8.141843795776367,
      "learning_rate": 4.731239398724923e-05,
      "loss": 2.8572,
      "step": 920
    },
    {
      "epoch": 0.16318652395157046,
      "grad_norm": 6.3868231773376465,
      "learning_rate": 4.7283149090483714e-05,
      "loss": 2.7767,
      "step": 930
    },
    {
      "epoch": 0.1649412177575013,
      "grad_norm": 6.008510112762451,
      "learning_rate": 4.725390419371819e-05,
      "loss": 2.552,
      "step": 940
    },
    {
      "epoch": 0.1666959115634322,
      "grad_norm": 6.665671348571777,
      "learning_rate": 4.7224659296952685e-05,
      "loss": 2.8953,
      "step": 950
    },
    {
      "epoch": 0.16845060536936304,
      "grad_norm": 7.894848346710205,
      "learning_rate": 4.719541440018717e-05,
      "loss": 2.8191,
      "step": 960
    },
    {
      "epoch": 0.17020529917529392,
      "grad_norm": 6.661086559295654,
      "learning_rate": 4.7166169503421656e-05,
      "loss": 2.7687,
      "step": 970
    },
    {
      "epoch": 0.17195999298122477,
      "grad_norm": 5.674933910369873,
      "learning_rate": 4.713692460665614e-05,
      "loss": 2.8234,
      "step": 980
    },
    {
      "epoch": 0.17371468678715565,
      "grad_norm": 5.966406345367432,
      "learning_rate": 4.710767970989063e-05,
      "loss": 2.8106,
      "step": 990
    },
    {
      "epoch": 0.1754693805930865,
      "grad_norm": 5.639625549316406,
      "learning_rate": 4.707843481312511e-05,
      "loss": 2.9163,
      "step": 1000
    },
    {
      "epoch": 0.17722407439901738,
      "grad_norm": 5.718420028686523,
      "learning_rate": 4.70491899163596e-05,
      "loss": 2.8391,
      "step": 1010
    },
    {
      "epoch": 0.17897876820494824,
      "grad_norm": 5.8105878829956055,
      "learning_rate": 4.701994501959408e-05,
      "loss": 2.9142,
      "step": 1020
    },
    {
      "epoch": 0.18073346201087911,
      "grad_norm": 5.006986618041992,
      "learning_rate": 4.699070012282857e-05,
      "loss": 2.8249,
      "step": 1030
    },
    {
      "epoch": 0.18248815581680997,
      "grad_norm": 5.145247459411621,
      "learning_rate": 4.6961455226063056e-05,
      "loss": 2.849,
      "step": 1040
    },
    {
      "epoch": 0.18424284962274085,
      "grad_norm": 4.7626051902771,
      "learning_rate": 4.6932210329297535e-05,
      "loss": 2.6709,
      "step": 1050
    },
    {
      "epoch": 0.1859975434286717,
      "grad_norm": 7.6557159423828125,
      "learning_rate": 4.690296543253203e-05,
      "loss": 2.938,
      "step": 1060
    },
    {
      "epoch": 0.18775223723460255,
      "grad_norm": 5.958755016326904,
      "learning_rate": 4.687372053576651e-05,
      "loss": 2.7168,
      "step": 1070
    },
    {
      "epoch": 0.18950693104053343,
      "grad_norm": 6.738500118255615,
      "learning_rate": 4.6844475639001e-05,
      "loss": 2.9086,
      "step": 1080
    },
    {
      "epoch": 0.19126162484646428,
      "grad_norm": 6.806204795837402,
      "learning_rate": 4.6815230742235484e-05,
      "loss": 2.9228,
      "step": 1090
    },
    {
      "epoch": 0.19301631865239516,
      "grad_norm": 5.824126720428467,
      "learning_rate": 4.678598584546996e-05,
      "loss": 2.8427,
      "step": 1100
    },
    {
      "epoch": 0.194771012458326,
      "grad_norm": 7.037979602813721,
      "learning_rate": 4.6756740948704455e-05,
      "loss": 2.8635,
      "step": 1110
    },
    {
      "epoch": 0.1965257062642569,
      "grad_norm": 5.722196102142334,
      "learning_rate": 4.672749605193894e-05,
      "loss": 2.6799,
      "step": 1120
    },
    {
      "epoch": 0.19828040007018774,
      "grad_norm": 6.434422016143799,
      "learning_rate": 4.669825115517342e-05,
      "loss": 2.6996,
      "step": 1130
    },
    {
      "epoch": 0.20003509387611862,
      "grad_norm": 5.321297645568848,
      "learning_rate": 4.666900625840791e-05,
      "loss": 2.8864,
      "step": 1140
    },
    {
      "epoch": 0.20178978768204947,
      "grad_norm": 6.737574577331543,
      "learning_rate": 4.66397613616424e-05,
      "loss": 2.8865,
      "step": 1150
    },
    {
      "epoch": 0.20354448148798035,
      "grad_norm": 7.311441421508789,
      "learning_rate": 4.661051646487688e-05,
      "loss": 2.8558,
      "step": 1160
    },
    {
      "epoch": 0.2052991752939112,
      "grad_norm": 6.411780834197998,
      "learning_rate": 4.658127156811137e-05,
      "loss": 2.908,
      "step": 1170
    },
    {
      "epoch": 0.20705386909984208,
      "grad_norm": 5.073333263397217,
      "learning_rate": 4.655202667134585e-05,
      "loss": 2.7198,
      "step": 1180
    },
    {
      "epoch": 0.20880856290577293,
      "grad_norm": 6.948720455169678,
      "learning_rate": 4.652278177458034e-05,
      "loss": 2.9292,
      "step": 1190
    },
    {
      "epoch": 0.2105632567117038,
      "grad_norm": 6.057514667510986,
      "learning_rate": 4.6493536877814826e-05,
      "loss": 2.7298,
      "step": 1200
    },
    {
      "epoch": 0.21231795051763466,
      "grad_norm": 5.735996723175049,
      "learning_rate": 4.6464291981049305e-05,
      "loss": 2.8833,
      "step": 1210
    },
    {
      "epoch": 0.21407264432356554,
      "grad_norm": 5.766659736633301,
      "learning_rate": 4.64350470842838e-05,
      "loss": 2.718,
      "step": 1220
    },
    {
      "epoch": 0.2158273381294964,
      "grad_norm": 5.133947849273682,
      "learning_rate": 4.640580218751828e-05,
      "loss": 2.8714,
      "step": 1230
    },
    {
      "epoch": 0.21758203193542727,
      "grad_norm": 5.943274021148682,
      "learning_rate": 4.637655729075276e-05,
      "loss": 2.7766,
      "step": 1240
    },
    {
      "epoch": 0.21933672574135812,
      "grad_norm": 6.634350299835205,
      "learning_rate": 4.6347312393987254e-05,
      "loss": 2.7077,
      "step": 1250
    },
    {
      "epoch": 0.221091419547289,
      "grad_norm": 6.156688213348389,
      "learning_rate": 4.631806749722173e-05,
      "loss": 2.8602,
      "step": 1260
    },
    {
      "epoch": 0.22284611335321985,
      "grad_norm": 6.521077632904053,
      "learning_rate": 4.6288822600456225e-05,
      "loss": 2.8188,
      "step": 1270
    },
    {
      "epoch": 0.22460080715915073,
      "grad_norm": 6.942357063293457,
      "learning_rate": 4.625957770369071e-05,
      "loss": 2.7524,
      "step": 1280
    },
    {
      "epoch": 0.22635550096508159,
      "grad_norm": 6.511401176452637,
      "learning_rate": 4.623033280692519e-05,
      "loss": 2.9115,
      "step": 1290
    },
    {
      "epoch": 0.22811019477101246,
      "grad_norm": 5.418570518493652,
      "learning_rate": 4.620108791015968e-05,
      "loss": 2.9321,
      "step": 1300
    },
    {
      "epoch": 0.22986488857694332,
      "grad_norm": 5.403509616851807,
      "learning_rate": 4.617184301339417e-05,
      "loss": 2.8931,
      "step": 1310
    },
    {
      "epoch": 0.2316195823828742,
      "grad_norm": 5.885687351226807,
      "learning_rate": 4.6142598116628646e-05,
      "loss": 2.8342,
      "step": 1320
    },
    {
      "epoch": 0.23337427618880505,
      "grad_norm": 5.317553997039795,
      "learning_rate": 4.611335321986314e-05,
      "loss": 2.848,
      "step": 1330
    },
    {
      "epoch": 0.23512896999473593,
      "grad_norm": 4.9089813232421875,
      "learning_rate": 4.608410832309762e-05,
      "loss": 2.9244,
      "step": 1340
    },
    {
      "epoch": 0.23688366380066678,
      "grad_norm": 6.142111778259277,
      "learning_rate": 4.60548634263321e-05,
      "loss": 2.7428,
      "step": 1350
    },
    {
      "epoch": 0.23863835760659766,
      "grad_norm": 6.069694995880127,
      "learning_rate": 4.6025618529566596e-05,
      "loss": 2.7836,
      "step": 1360
    },
    {
      "epoch": 0.2403930514125285,
      "grad_norm": 6.323974609375,
      "learning_rate": 4.5996373632801074e-05,
      "loss": 2.8089,
      "step": 1370
    },
    {
      "epoch": 0.2421477452184594,
      "grad_norm": 5.238837242126465,
      "learning_rate": 4.596712873603557e-05,
      "loss": 2.796,
      "step": 1380
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 5.533031463623047,
      "learning_rate": 4.593788383927005e-05,
      "loss": 2.765,
      "step": 1390
    },
    {
      "epoch": 0.24565713283032112,
      "grad_norm": 6.1066765785217285,
      "learning_rate": 4.590863894250453e-05,
      "loss": 2.8935,
      "step": 1400
    },
    {
      "epoch": 0.24741182663625197,
      "grad_norm": 6.256274223327637,
      "learning_rate": 4.5879394045739024e-05,
      "loss": 2.7339,
      "step": 1410
    },
    {
      "epoch": 0.24916652044218285,
      "grad_norm": 5.8457560539245605,
      "learning_rate": 4.58501491489735e-05,
      "loss": 2.9535,
      "step": 1420
    },
    {
      "epoch": 0.2509212142481137,
      "grad_norm": 6.134856224060059,
      "learning_rate": 4.582090425220799e-05,
      "loss": 2.9685,
      "step": 1430
    },
    {
      "epoch": 0.25267590805404455,
      "grad_norm": 6.336967945098877,
      "learning_rate": 4.579165935544248e-05,
      "loss": 2.8943,
      "step": 1440
    },
    {
      "epoch": 0.25443060185997546,
      "grad_norm": 5.3570380210876465,
      "learning_rate": 4.576241445867696e-05,
      "loss": 2.8481,
      "step": 1450
    },
    {
      "epoch": 0.2561852956659063,
      "grad_norm": 6.523495674133301,
      "learning_rate": 4.573316956191145e-05,
      "loss": 2.9252,
      "step": 1460
    },
    {
      "epoch": 0.25793998947183716,
      "grad_norm": 6.0146942138671875,
      "learning_rate": 4.570392466514594e-05,
      "loss": 2.8101,
      "step": 1470
    },
    {
      "epoch": 0.259694683277768,
      "grad_norm": 6.417367935180664,
      "learning_rate": 4.5674679768380416e-05,
      "loss": 2.8279,
      "step": 1480
    },
    {
      "epoch": 0.2614493770836989,
      "grad_norm": 4.924767017364502,
      "learning_rate": 4.564543487161491e-05,
      "loss": 2.8059,
      "step": 1490
    },
    {
      "epoch": 0.26320407088962977,
      "grad_norm": 5.75739860534668,
      "learning_rate": 4.5616189974849394e-05,
      "loss": 2.7749,
      "step": 1500
    },
    {
      "epoch": 0.2649587646955606,
      "grad_norm": 6.315576076507568,
      "learning_rate": 4.558694507808387e-05,
      "loss": 2.84,
      "step": 1510
    },
    {
      "epoch": 0.2667134585014915,
      "grad_norm": 6.1826066970825195,
      "learning_rate": 4.5557700181318366e-05,
      "loss": 2.7272,
      "step": 1520
    },
    {
      "epoch": 0.2684681523074224,
      "grad_norm": 6.495425224304199,
      "learning_rate": 4.5528455284552844e-05,
      "loss": 2.7012,
      "step": 1530
    },
    {
      "epoch": 0.27022284611335323,
      "grad_norm": 5.307542324066162,
      "learning_rate": 4.549921038778733e-05,
      "loss": 2.7831,
      "step": 1540
    },
    {
      "epoch": 0.2719775399192841,
      "grad_norm": 5.517131328582764,
      "learning_rate": 4.546996549102182e-05,
      "loss": 2.7097,
      "step": 1550
    },
    {
      "epoch": 0.27373223372521494,
      "grad_norm": 6.080484867095947,
      "learning_rate": 4.54407205942563e-05,
      "loss": 2.8136,
      "step": 1560
    },
    {
      "epoch": 0.27548692753114584,
      "grad_norm": 7.132598876953125,
      "learning_rate": 4.5411475697490794e-05,
      "loss": 2.8149,
      "step": 1570
    },
    {
      "epoch": 0.2772416213370767,
      "grad_norm": 4.955841541290283,
      "learning_rate": 4.538223080072528e-05,
      "loss": 2.8915,
      "step": 1580
    },
    {
      "epoch": 0.27899631514300754,
      "grad_norm": 6.456575870513916,
      "learning_rate": 4.535298590395976e-05,
      "loss": 2.9046,
      "step": 1590
    },
    {
      "epoch": 0.2807510089489384,
      "grad_norm": 5.613997936248779,
      "learning_rate": 4.532374100719425e-05,
      "loss": 2.8786,
      "step": 1600
    },
    {
      "epoch": 0.28250570275486925,
      "grad_norm": 7.220841884613037,
      "learning_rate": 4.529449611042873e-05,
      "loss": 2.7954,
      "step": 1610
    },
    {
      "epoch": 0.28426039656080015,
      "grad_norm": 5.392770767211914,
      "learning_rate": 4.5265251213663215e-05,
      "loss": 2.8637,
      "step": 1620
    },
    {
      "epoch": 0.286015090366731,
      "grad_norm": 5.928794860839844,
      "learning_rate": 4.523600631689771e-05,
      "loss": 2.855,
      "step": 1630
    },
    {
      "epoch": 0.28776978417266186,
      "grad_norm": 5.437707424163818,
      "learning_rate": 4.5206761420132186e-05,
      "loss": 2.8406,
      "step": 1640
    },
    {
      "epoch": 0.2895244779785927,
      "grad_norm": 5.672708988189697,
      "learning_rate": 4.517751652336668e-05,
      "loss": 2.774,
      "step": 1650
    },
    {
      "epoch": 0.2912791717845236,
      "grad_norm": 5.858765602111816,
      "learning_rate": 4.5148271626601164e-05,
      "loss": 2.6532,
      "step": 1660
    },
    {
      "epoch": 0.29303386559045447,
      "grad_norm": 5.574642181396484,
      "learning_rate": 4.511902672983564e-05,
      "loss": 2.9134,
      "step": 1670
    },
    {
      "epoch": 0.2947885593963853,
      "grad_norm": 5.012214660644531,
      "learning_rate": 4.5089781833070136e-05,
      "loss": 2.8111,
      "step": 1680
    },
    {
      "epoch": 0.29654325320231617,
      "grad_norm": 5.559623718261719,
      "learning_rate": 4.5060536936304614e-05,
      "loss": 2.8998,
      "step": 1690
    },
    {
      "epoch": 0.2982979470082471,
      "grad_norm": 5.823104381561279,
      "learning_rate": 4.50312920395391e-05,
      "loss": 2.8068,
      "step": 1700
    },
    {
      "epoch": 0.30005264081417793,
      "grad_norm": 5.694516181945801,
      "learning_rate": 4.500204714277359e-05,
      "loss": 2.7955,
      "step": 1710
    },
    {
      "epoch": 0.3018073346201088,
      "grad_norm": 6.033149242401123,
      "learning_rate": 4.497280224600807e-05,
      "loss": 2.8186,
      "step": 1720
    },
    {
      "epoch": 0.30356202842603963,
      "grad_norm": 6.465758800506592,
      "learning_rate": 4.494355734924256e-05,
      "loss": 2.8349,
      "step": 1730
    },
    {
      "epoch": 0.30531672223197054,
      "grad_norm": 5.578110218048096,
      "learning_rate": 4.491431245247705e-05,
      "loss": 2.7865,
      "step": 1740
    },
    {
      "epoch": 0.3070714160379014,
      "grad_norm": 6.285759925842285,
      "learning_rate": 4.488506755571153e-05,
      "loss": 2.8963,
      "step": 1750
    },
    {
      "epoch": 0.30882610984383224,
      "grad_norm": 5.878334999084473,
      "learning_rate": 4.485582265894602e-05,
      "loss": 2.9104,
      "step": 1760
    },
    {
      "epoch": 0.3105808036497631,
      "grad_norm": 5.382410526275635,
      "learning_rate": 4.48265777621805e-05,
      "loss": 2.8287,
      "step": 1770
    },
    {
      "epoch": 0.312335497455694,
      "grad_norm": 5.257350444793701,
      "learning_rate": 4.4797332865414985e-05,
      "loss": 2.877,
      "step": 1780
    },
    {
      "epoch": 0.31409019126162485,
      "grad_norm": 5.192446231842041,
      "learning_rate": 4.476808796864948e-05,
      "loss": 2.8822,
      "step": 1790
    },
    {
      "epoch": 0.3158448850675557,
      "grad_norm": 5.047576427459717,
      "learning_rate": 4.4738843071883956e-05,
      "loss": 2.8228,
      "step": 1800
    },
    {
      "epoch": 0.31759957887348655,
      "grad_norm": 5.748327732086182,
      "learning_rate": 4.470959817511844e-05,
      "loss": 2.8617,
      "step": 1810
    },
    {
      "epoch": 0.31935427267941746,
      "grad_norm": 5.593839645385742,
      "learning_rate": 4.4680353278352934e-05,
      "loss": 2.7886,
      "step": 1820
    },
    {
      "epoch": 0.3211089664853483,
      "grad_norm": 5.490955829620361,
      "learning_rate": 4.465110838158741e-05,
      "loss": 2.7433,
      "step": 1830
    },
    {
      "epoch": 0.32286366029127916,
      "grad_norm": 6.028989791870117,
      "learning_rate": 4.46218634848219e-05,
      "loss": 2.861,
      "step": 1840
    },
    {
      "epoch": 0.32461835409721,
      "grad_norm": 5.263760089874268,
      "learning_rate": 4.4592618588056384e-05,
      "loss": 2.9026,
      "step": 1850
    },
    {
      "epoch": 0.3263730479031409,
      "grad_norm": 5.813746929168701,
      "learning_rate": 4.456337369129087e-05,
      "loss": 2.9313,
      "step": 1860
    },
    {
      "epoch": 0.3281277417090718,
      "grad_norm": 6.8072357177734375,
      "learning_rate": 4.453412879452536e-05,
      "loss": 2.8375,
      "step": 1870
    },
    {
      "epoch": 0.3298824355150026,
      "grad_norm": 5.220357894897461,
      "learning_rate": 4.450488389775984e-05,
      "loss": 2.7431,
      "step": 1880
    },
    {
      "epoch": 0.3316371293209335,
      "grad_norm": 6.386417388916016,
      "learning_rate": 4.447563900099433e-05,
      "loss": 2.6655,
      "step": 1890
    },
    {
      "epoch": 0.3333918231268644,
      "grad_norm": 5.529221057891846,
      "learning_rate": 4.444639410422882e-05,
      "loss": 2.6737,
      "step": 1900
    },
    {
      "epoch": 0.33514651693279524,
      "grad_norm": 5.971353054046631,
      "learning_rate": 4.44171492074633e-05,
      "loss": 2.6326,
      "step": 1910
    },
    {
      "epoch": 0.3369012107387261,
      "grad_norm": 5.225106239318848,
      "learning_rate": 4.4387904310697784e-05,
      "loss": 2.9431,
      "step": 1920
    },
    {
      "epoch": 0.33865590454465694,
      "grad_norm": 6.1725921630859375,
      "learning_rate": 4.435865941393227e-05,
      "loss": 2.7446,
      "step": 1930
    },
    {
      "epoch": 0.34041059835058785,
      "grad_norm": 6.56099271774292,
      "learning_rate": 4.4329414517166755e-05,
      "loss": 2.7911,
      "step": 1940
    },
    {
      "epoch": 0.3421652921565187,
      "grad_norm": 5.228060245513916,
      "learning_rate": 4.430016962040125e-05,
      "loss": 2.8595,
      "step": 1950
    },
    {
      "epoch": 0.34391998596244955,
      "grad_norm": 5.620738506317139,
      "learning_rate": 4.4270924723635726e-05,
      "loss": 3.001,
      "step": 1960
    },
    {
      "epoch": 0.3456746797683804,
      "grad_norm": 6.490886688232422,
      "learning_rate": 4.424167982687021e-05,
      "loss": 2.7162,
      "step": 1970
    },
    {
      "epoch": 0.3474293735743113,
      "grad_norm": 5.96988582611084,
      "learning_rate": 4.4212434930104704e-05,
      "loss": 2.7411,
      "step": 1980
    },
    {
      "epoch": 0.34918406738024216,
      "grad_norm": 5.9100494384765625,
      "learning_rate": 4.418319003333918e-05,
      "loss": 2.882,
      "step": 1990
    },
    {
      "epoch": 0.350938761186173,
      "grad_norm": 5.709218978881836,
      "learning_rate": 4.415394513657367e-05,
      "loss": 2.7397,
      "step": 2000
    },
    {
      "epoch": 0.35269345499210386,
      "grad_norm": 5.139623641967773,
      "learning_rate": 4.4124700239808154e-05,
      "loss": 2.68,
      "step": 2010
    },
    {
      "epoch": 0.35444814879803477,
      "grad_norm": 5.546078205108643,
      "learning_rate": 4.409545534304264e-05,
      "loss": 2.9057,
      "step": 2020
    },
    {
      "epoch": 0.3562028426039656,
      "grad_norm": 5.276262283325195,
      "learning_rate": 4.4066210446277125e-05,
      "loss": 2.7854,
      "step": 2030
    },
    {
      "epoch": 0.35795753640989647,
      "grad_norm": 5.79536771774292,
      "learning_rate": 4.403696554951161e-05,
      "loss": 2.6812,
      "step": 2040
    },
    {
      "epoch": 0.3597122302158273,
      "grad_norm": 6.039234161376953,
      "learning_rate": 4.40077206527461e-05,
      "loss": 2.8404,
      "step": 2050
    },
    {
      "epoch": 0.36146692402175823,
      "grad_norm": 6.2127203941345215,
      "learning_rate": 4.397847575598059e-05,
      "loss": 2.81,
      "step": 2060
    },
    {
      "epoch": 0.3632216178276891,
      "grad_norm": 5.507876873016357,
      "learning_rate": 4.394923085921507e-05,
      "loss": 2.7935,
      "step": 2070
    },
    {
      "epoch": 0.36497631163361993,
      "grad_norm": 5.170819282531738,
      "learning_rate": 4.3919985962449554e-05,
      "loss": 2.7726,
      "step": 2080
    },
    {
      "epoch": 0.3667310054395508,
      "grad_norm": 6.283104419708252,
      "learning_rate": 4.389074106568404e-05,
      "loss": 2.9609,
      "step": 2090
    },
    {
      "epoch": 0.3684856992454817,
      "grad_norm": 5.586540699005127,
      "learning_rate": 4.3861496168918525e-05,
      "loss": 2.6962,
      "step": 2100
    },
    {
      "epoch": 0.37024039305141254,
      "grad_norm": 5.756563663482666,
      "learning_rate": 4.383225127215301e-05,
      "loss": 2.8557,
      "step": 2110
    },
    {
      "epoch": 0.3719950868573434,
      "grad_norm": 5.682807922363281,
      "learning_rate": 4.3803006375387496e-05,
      "loss": 2.7841,
      "step": 2120
    },
    {
      "epoch": 0.37374978066327424,
      "grad_norm": 5.749135971069336,
      "learning_rate": 4.377376147862198e-05,
      "loss": 2.7112,
      "step": 2130
    },
    {
      "epoch": 0.3755044744692051,
      "grad_norm": 6.777560710906982,
      "learning_rate": 4.374451658185647e-05,
      "loss": 2.7995,
      "step": 2140
    },
    {
      "epoch": 0.377259168275136,
      "grad_norm": 6.828304767608643,
      "learning_rate": 4.371527168509095e-05,
      "loss": 3.0084,
      "step": 2150
    },
    {
      "epoch": 0.37901386208106685,
      "grad_norm": 6.982832908630371,
      "learning_rate": 4.368602678832544e-05,
      "loss": 2.7994,
      "step": 2160
    },
    {
      "epoch": 0.3807685558869977,
      "grad_norm": 7.106773853302002,
      "learning_rate": 4.3656781891559924e-05,
      "loss": 2.7299,
      "step": 2170
    },
    {
      "epoch": 0.38252324969292856,
      "grad_norm": 5.849787712097168,
      "learning_rate": 4.362753699479441e-05,
      "loss": 2.7665,
      "step": 2180
    },
    {
      "epoch": 0.38427794349885946,
      "grad_norm": 6.877793312072754,
      "learning_rate": 4.3598292098028895e-05,
      "loss": 2.8174,
      "step": 2190
    },
    {
      "epoch": 0.3860326373047903,
      "grad_norm": 6.9962029457092285,
      "learning_rate": 4.356904720126338e-05,
      "loss": 2.6943,
      "step": 2200
    },
    {
      "epoch": 0.38778733111072117,
      "grad_norm": 6.063364505767822,
      "learning_rate": 4.353980230449787e-05,
      "loss": 2.7551,
      "step": 2210
    },
    {
      "epoch": 0.389542024916652,
      "grad_norm": 5.369275093078613,
      "learning_rate": 4.351055740773235e-05,
      "loss": 2.9761,
      "step": 2220
    },
    {
      "epoch": 0.3912967187225829,
      "grad_norm": 5.913261413574219,
      "learning_rate": 4.348131251096684e-05,
      "loss": 2.8204,
      "step": 2230
    },
    {
      "epoch": 0.3930514125285138,
      "grad_norm": 5.858199119567871,
      "learning_rate": 4.3452067614201324e-05,
      "loss": 2.7733,
      "step": 2240
    },
    {
      "epoch": 0.39480610633444463,
      "grad_norm": 6.11492919921875,
      "learning_rate": 4.342282271743581e-05,
      "loss": 2.8153,
      "step": 2250
    },
    {
      "epoch": 0.3965608001403755,
      "grad_norm": 6.649184703826904,
      "learning_rate": 4.3393577820670295e-05,
      "loss": 2.7128,
      "step": 2260
    },
    {
      "epoch": 0.3983154939463064,
      "grad_norm": 6.553282737731934,
      "learning_rate": 4.336433292390478e-05,
      "loss": 2.8869,
      "step": 2270
    },
    {
      "epoch": 0.40007018775223724,
      "grad_norm": 6.116930961608887,
      "learning_rate": 4.3335088027139266e-05,
      "loss": 2.8524,
      "step": 2280
    },
    {
      "epoch": 0.4018248815581681,
      "grad_norm": 6.643488883972168,
      "learning_rate": 4.330584313037375e-05,
      "loss": 2.8599,
      "step": 2290
    },
    {
      "epoch": 0.40357957536409894,
      "grad_norm": 6.311044216156006,
      "learning_rate": 4.327659823360824e-05,
      "loss": 2.8084,
      "step": 2300
    },
    {
      "epoch": 0.40533426917002985,
      "grad_norm": 6.8668646812438965,
      "learning_rate": 4.324735333684272e-05,
      "loss": 2.7422,
      "step": 2310
    },
    {
      "epoch": 0.4070889629759607,
      "grad_norm": 6.293309211730957,
      "learning_rate": 4.321810844007721e-05,
      "loss": 2.7848,
      "step": 2320
    },
    {
      "epoch": 0.40884365678189155,
      "grad_norm": 5.89540433883667,
      "learning_rate": 4.3188863543311694e-05,
      "loss": 2.6998,
      "step": 2330
    },
    {
      "epoch": 0.4105983505878224,
      "grad_norm": 6.2611308097839355,
      "learning_rate": 4.315961864654618e-05,
      "loss": 2.9222,
      "step": 2340
    },
    {
      "epoch": 0.4123530443937533,
      "grad_norm": 5.699769020080566,
      "learning_rate": 4.3130373749780665e-05,
      "loss": 2.6991,
      "step": 2350
    },
    {
      "epoch": 0.41410773819968416,
      "grad_norm": 6.62695837020874,
      "learning_rate": 4.310112885301515e-05,
      "loss": 2.8151,
      "step": 2360
    },
    {
      "epoch": 0.415862432005615,
      "grad_norm": 6.021080017089844,
      "learning_rate": 4.307480844592619e-05,
      "loss": 2.6907,
      "step": 2370
    },
    {
      "epoch": 0.41761712581154586,
      "grad_norm": 5.9306488037109375,
      "learning_rate": 4.304556354916067e-05,
      "loss": 2.7262,
      "step": 2380
    },
    {
      "epoch": 0.41937181961747677,
      "grad_norm": 6.207338333129883,
      "learning_rate": 4.301631865239516e-05,
      "loss": 2.8649,
      "step": 2390
    },
    {
      "epoch": 0.4211265134234076,
      "grad_norm": 5.0818257331848145,
      "learning_rate": 4.298707375562964e-05,
      "loss": 2.8996,
      "step": 2400
    },
    {
      "epoch": 0.4228812072293385,
      "grad_norm": 4.957367897033691,
      "learning_rate": 4.295782885886413e-05,
      "loss": 2.7287,
      "step": 2410
    },
    {
      "epoch": 0.4246359010352693,
      "grad_norm": 5.277100563049316,
      "learning_rate": 4.292858396209862e-05,
      "loss": 2.8134,
      "step": 2420
    },
    {
      "epoch": 0.42639059484120023,
      "grad_norm": 5.192728519439697,
      "learning_rate": 4.28993390653331e-05,
      "loss": 2.7702,
      "step": 2430
    },
    {
      "epoch": 0.4281452886471311,
      "grad_norm": 5.9327921867370605,
      "learning_rate": 4.2870094168567585e-05,
      "loss": 2.5652,
      "step": 2440
    },
    {
      "epoch": 0.42989998245306194,
      "grad_norm": 6.824719429016113,
      "learning_rate": 4.284084927180208e-05,
      "loss": 2.7971,
      "step": 2450
    },
    {
      "epoch": 0.4316546762589928,
      "grad_norm": 7.3048930168151855,
      "learning_rate": 4.2811604375036556e-05,
      "loss": 2.7546,
      "step": 2460
    },
    {
      "epoch": 0.4334093700649237,
      "grad_norm": 5.929998397827148,
      "learning_rate": 4.278235947827104e-05,
      "loss": 2.7004,
      "step": 2470
    },
    {
      "epoch": 0.43516406387085454,
      "grad_norm": 6.946867942810059,
      "learning_rate": 4.275311458150553e-05,
      "loss": 2.6305,
      "step": 2480
    },
    {
      "epoch": 0.4369187576767854,
      "grad_norm": 6.064707279205322,
      "learning_rate": 4.272386968474001e-05,
      "loss": 2.9401,
      "step": 2490
    },
    {
      "epoch": 0.43867345148271625,
      "grad_norm": 5.538208961486816,
      "learning_rate": 4.2694624787974505e-05,
      "loss": 2.8005,
      "step": 2500
    },
    {
      "epoch": 0.44042814528864715,
      "grad_norm": 6.473751068115234,
      "learning_rate": 4.2665379891208984e-05,
      "loss": 2.9042,
      "step": 2510
    },
    {
      "epoch": 0.442182839094578,
      "grad_norm": 6.04946231842041,
      "learning_rate": 4.263613499444347e-05,
      "loss": 2.6708,
      "step": 2520
    },
    {
      "epoch": 0.44393753290050886,
      "grad_norm": 6.2683186531066895,
      "learning_rate": 4.260689009767796e-05,
      "loss": 2.6307,
      "step": 2530
    },
    {
      "epoch": 0.4456922267064397,
      "grad_norm": 6.933496475219727,
      "learning_rate": 4.257764520091244e-05,
      "loss": 3.0612,
      "step": 2540
    },
    {
      "epoch": 0.4474469205123706,
      "grad_norm": 5.435051441192627,
      "learning_rate": 4.2548400304146927e-05,
      "loss": 2.5903,
      "step": 2550
    },
    {
      "epoch": 0.44920161431830147,
      "grad_norm": 6.674374580383301,
      "learning_rate": 4.251915540738141e-05,
      "loss": 2.8858,
      "step": 2560
    },
    {
      "epoch": 0.4509563081242323,
      "grad_norm": 7.609883785247803,
      "learning_rate": 4.24899105106159e-05,
      "loss": 2.6864,
      "step": 2570
    },
    {
      "epoch": 0.45271100193016317,
      "grad_norm": 5.378966331481934,
      "learning_rate": 4.246066561385038e-05,
      "loss": 2.489,
      "step": 2580
    },
    {
      "epoch": 0.4544656957360941,
      "grad_norm": 5.846722602844238,
      "learning_rate": 4.243142071708487e-05,
      "loss": 2.7326,
      "step": 2590
    },
    {
      "epoch": 0.45622038954202493,
      "grad_norm": 6.611082077026367,
      "learning_rate": 4.2402175820319355e-05,
      "loss": 2.6927,
      "step": 2600
    },
    {
      "epoch": 0.4579750833479558,
      "grad_norm": 6.581131458282471,
      "learning_rate": 4.237293092355385e-05,
      "loss": 2.782,
      "step": 2610
    },
    {
      "epoch": 0.45972977715388663,
      "grad_norm": 5.717341899871826,
      "learning_rate": 4.2343686026788326e-05,
      "loss": 2.6432,
      "step": 2620
    },
    {
      "epoch": 0.46148447095981754,
      "grad_norm": 7.074581623077393,
      "learning_rate": 4.231444113002281e-05,
      "loss": 2.5335,
      "step": 2630
    },
    {
      "epoch": 0.4632391647657484,
      "grad_norm": 7.010384559631348,
      "learning_rate": 4.22851962332573e-05,
      "loss": 2.6377,
      "step": 2640
    },
    {
      "epoch": 0.46499385857167924,
      "grad_norm": 5.935247898101807,
      "learning_rate": 4.225595133649178e-05,
      "loss": 2.9137,
      "step": 2650
    },
    {
      "epoch": 0.4667485523776101,
      "grad_norm": 6.929603576660156,
      "learning_rate": 4.222670643972627e-05,
      "loss": 2.6743,
      "step": 2660
    },
    {
      "epoch": 0.46850324618354094,
      "grad_norm": 6.273268699645996,
      "learning_rate": 4.2197461542960754e-05,
      "loss": 2.9319,
      "step": 2670
    },
    {
      "epoch": 0.47025793998947185,
      "grad_norm": 7.634737968444824,
      "learning_rate": 4.216821664619524e-05,
      "loss": 2.7412,
      "step": 2680
    },
    {
      "epoch": 0.4720126337954027,
      "grad_norm": 6.504032135009766,
      "learning_rate": 4.213897174942973e-05,
      "loss": 2.6732,
      "step": 2690
    },
    {
      "epoch": 0.47376732760133355,
      "grad_norm": 5.952571392059326,
      "learning_rate": 4.210972685266421e-05,
      "loss": 2.6426,
      "step": 2700
    },
    {
      "epoch": 0.4755220214072644,
      "grad_norm": 5.845285892486572,
      "learning_rate": 4.2080481955898696e-05,
      "loss": 2.7073,
      "step": 2710
    },
    {
      "epoch": 0.4772767152131953,
      "grad_norm": 5.274291515350342,
      "learning_rate": 4.205123705913318e-05,
      "loss": 2.7926,
      "step": 2720
    },
    {
      "epoch": 0.47903140901912616,
      "grad_norm": 6.269782066345215,
      "learning_rate": 4.202199216236767e-05,
      "loss": 2.8473,
      "step": 2730
    },
    {
      "epoch": 0.480786102825057,
      "grad_norm": 6.2173075675964355,
      "learning_rate": 4.199274726560215e-05,
      "loss": 2.774,
      "step": 2740
    },
    {
      "epoch": 0.48254079663098787,
      "grad_norm": 5.829827308654785,
      "learning_rate": 4.196350236883664e-05,
      "loss": 2.6455,
      "step": 2750
    },
    {
      "epoch": 0.4842954904369188,
      "grad_norm": 6.569485664367676,
      "learning_rate": 4.1934257472071125e-05,
      "loss": 2.516,
      "step": 2760
    },
    {
      "epoch": 0.4860501842428496,
      "grad_norm": 7.7006988525390625,
      "learning_rate": 4.190501257530561e-05,
      "loss": 2.6398,
      "step": 2770
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 5.2117204666137695,
      "learning_rate": 4.1875767678540096e-05,
      "loss": 2.7151,
      "step": 2780
    },
    {
      "epoch": 0.48955957185471133,
      "grad_norm": 8.750080108642578,
      "learning_rate": 4.184652278177458e-05,
      "loss": 2.7144,
      "step": 2790
    },
    {
      "epoch": 0.49131426566064224,
      "grad_norm": 7.126880645751953,
      "learning_rate": 4.181727788500907e-05,
      "loss": 2.6372,
      "step": 2800
    },
    {
      "epoch": 0.4930689594665731,
      "grad_norm": 5.69364595413208,
      "learning_rate": 4.178803298824355e-05,
      "loss": 2.4901,
      "step": 2810
    },
    {
      "epoch": 0.49482365327250394,
      "grad_norm": 5.37415075302124,
      "learning_rate": 4.175878809147804e-05,
      "loss": 2.5413,
      "step": 2820
    },
    {
      "epoch": 0.4965783470784348,
      "grad_norm": 7.441705703735352,
      "learning_rate": 4.1729543194712524e-05,
      "loss": 2.8368,
      "step": 2830
    },
    {
      "epoch": 0.4983330408843657,
      "grad_norm": 5.812159538269043,
      "learning_rate": 4.170029829794701e-05,
      "loss": 2.6284,
      "step": 2840
    },
    {
      "epoch": 0.5000877346902965,
      "grad_norm": 8.291953086853027,
      "learning_rate": 4.1671053401181495e-05,
      "loss": 2.6683,
      "step": 2850
    },
    {
      "epoch": 0.5018424284962274,
      "grad_norm": 5.471755027770996,
      "learning_rate": 4.164180850441598e-05,
      "loss": 2.6135,
      "step": 2860
    },
    {
      "epoch": 0.5035971223021583,
      "grad_norm": 7.570827007293701,
      "learning_rate": 4.1612563607650466e-05,
      "loss": 2.8093,
      "step": 2870
    },
    {
      "epoch": 0.5053518161080891,
      "grad_norm": 6.1980977058410645,
      "learning_rate": 4.158331871088495e-05,
      "loss": 2.7721,
      "step": 2880
    },
    {
      "epoch": 0.50710650991402,
      "grad_norm": 6.496702194213867,
      "learning_rate": 4.155407381411944e-05,
      "loss": 2.7791,
      "step": 2890
    },
    {
      "epoch": 0.5088612037199509,
      "grad_norm": 5.755092144012451,
      "learning_rate": 4.152482891735392e-05,
      "loss": 2.5563,
      "step": 2900
    },
    {
      "epoch": 0.5106158975258818,
      "grad_norm": 5.53985595703125,
      "learning_rate": 4.149558402058841e-05,
      "loss": 2.8867,
      "step": 2910
    },
    {
      "epoch": 0.5123705913318126,
      "grad_norm": 7.056730270385742,
      "learning_rate": 4.1466339123822894e-05,
      "loss": 2.6425,
      "step": 2920
    },
    {
      "epoch": 0.5141252851377435,
      "grad_norm": 6.84183931350708,
      "learning_rate": 4.143709422705738e-05,
      "loss": 2.6972,
      "step": 2930
    },
    {
      "epoch": 0.5158799789436743,
      "grad_norm": 5.827425479888916,
      "learning_rate": 4.1407849330291866e-05,
      "loss": 2.8526,
      "step": 2940
    },
    {
      "epoch": 0.5176346727496052,
      "grad_norm": 5.104124069213867,
      "learning_rate": 4.137860443352635e-05,
      "loss": 2.7152,
      "step": 2950
    },
    {
      "epoch": 0.519389366555536,
      "grad_norm": 5.733360290527344,
      "learning_rate": 4.134935953676084e-05,
      "loss": 2.7704,
      "step": 2960
    },
    {
      "epoch": 0.5211440603614669,
      "grad_norm": 6.032989501953125,
      "learning_rate": 4.132011463999532e-05,
      "loss": 2.794,
      "step": 2970
    },
    {
      "epoch": 0.5228987541673978,
      "grad_norm": 8.53226089477539,
      "learning_rate": 4.129086974322981e-05,
      "loss": 2.8737,
      "step": 2980
    },
    {
      "epoch": 0.5246534479733287,
      "grad_norm": 5.656920433044434,
      "learning_rate": 4.1261624846464294e-05,
      "loss": 2.671,
      "step": 2990
    },
    {
      "epoch": 0.5264081417792595,
      "grad_norm": 8.354413986206055,
      "learning_rate": 4.123237994969878e-05,
      "loss": 2.8174,
      "step": 3000
    },
    {
      "epoch": 0.5281628355851904,
      "grad_norm": 5.193705081939697,
      "learning_rate": 4.1203135052933265e-05,
      "loss": 2.8083,
      "step": 3010
    },
    {
      "epoch": 0.5299175293911212,
      "grad_norm": 7.4976487159729,
      "learning_rate": 4.117389015616775e-05,
      "loss": 2.8991,
      "step": 3020
    },
    {
      "epoch": 0.5316722231970521,
      "grad_norm": 6.994223117828369,
      "learning_rate": 4.1144645259402236e-05,
      "loss": 2.7688,
      "step": 3030
    },
    {
      "epoch": 0.533426917002983,
      "grad_norm": 6.845330715179443,
      "learning_rate": 4.111540036263672e-05,
      "loss": 2.6027,
      "step": 3040
    },
    {
      "epoch": 0.5351816108089138,
      "grad_norm": 6.524192810058594,
      "learning_rate": 4.108615546587121e-05,
      "loss": 2.7632,
      "step": 3050
    },
    {
      "epoch": 0.5369363046148448,
      "grad_norm": 5.937326431274414,
      "learning_rate": 4.105691056910569e-05,
      "loss": 2.5763,
      "step": 3060
    },
    {
      "epoch": 0.5386909984207756,
      "grad_norm": 5.611225605010986,
      "learning_rate": 4.102766567234018e-05,
      "loss": 2.7161,
      "step": 3070
    },
    {
      "epoch": 0.5404456922267065,
      "grad_norm": 6.987038612365723,
      "learning_rate": 4.0998420775574664e-05,
      "loss": 2.8914,
      "step": 3080
    },
    {
      "epoch": 0.5422003860326373,
      "grad_norm": 5.609161853790283,
      "learning_rate": 4.096917587880915e-05,
      "loss": 2.6606,
      "step": 3090
    },
    {
      "epoch": 0.5439550798385682,
      "grad_norm": 8.091532707214355,
      "learning_rate": 4.0939930982043636e-05,
      "loss": 2.572,
      "step": 3100
    },
    {
      "epoch": 0.545709773644499,
      "grad_norm": 6.725534439086914,
      "learning_rate": 4.091068608527812e-05,
      "loss": 2.5904,
      "step": 3110
    },
    {
      "epoch": 0.5474644674504299,
      "grad_norm": 5.651915073394775,
      "learning_rate": 4.088144118851261e-05,
      "loss": 2.6825,
      "step": 3120
    },
    {
      "epoch": 0.5492191612563607,
      "grad_norm": 11.457286834716797,
      "learning_rate": 4.085219629174709e-05,
      "loss": 2.545,
      "step": 3130
    },
    {
      "epoch": 0.5509738550622917,
      "grad_norm": 6.447844982147217,
      "learning_rate": 4.082295139498158e-05,
      "loss": 2.6676,
      "step": 3140
    },
    {
      "epoch": 0.5527285488682225,
      "grad_norm": 5.578318119049072,
      "learning_rate": 4.0793706498216064e-05,
      "loss": 2.6917,
      "step": 3150
    },
    {
      "epoch": 0.5544832426741534,
      "grad_norm": 7.0226874351501465,
      "learning_rate": 4.076446160145055e-05,
      "loss": 2.7441,
      "step": 3160
    },
    {
      "epoch": 0.5562379364800842,
      "grad_norm": 9.508084297180176,
      "learning_rate": 4.0735216704685035e-05,
      "loss": 2.4661,
      "step": 3170
    },
    {
      "epoch": 0.5579926302860151,
      "grad_norm": 6.92988395690918,
      "learning_rate": 4.070597180791952e-05,
      "loss": 2.7846,
      "step": 3180
    },
    {
      "epoch": 0.5597473240919459,
      "grad_norm": 5.9877471923828125,
      "learning_rate": 4.0676726911154006e-05,
      "loss": 2.7101,
      "step": 3190
    },
    {
      "epoch": 0.5615020178978768,
      "grad_norm": 4.968700885772705,
      "learning_rate": 4.064748201438849e-05,
      "loss": 2.7863,
      "step": 3200
    },
    {
      "epoch": 0.5632567117038076,
      "grad_norm": 5.862491130828857,
      "learning_rate": 4.061823711762298e-05,
      "loss": 2.7777,
      "step": 3210
    },
    {
      "epoch": 0.5650114055097385,
      "grad_norm": 5.484644889831543,
      "learning_rate": 4.058899222085746e-05,
      "loss": 2.7463,
      "step": 3220
    },
    {
      "epoch": 0.5667660993156695,
      "grad_norm": 6.259829998016357,
      "learning_rate": 4.055974732409195e-05,
      "loss": 2.5476,
      "step": 3230
    },
    {
      "epoch": 0.5685207931216003,
      "grad_norm": 9.217742919921875,
      "learning_rate": 4.0530502427326434e-05,
      "loss": 2.9201,
      "step": 3240
    },
    {
      "epoch": 0.5702754869275312,
      "grad_norm": 6.148740768432617,
      "learning_rate": 4.050125753056092e-05,
      "loss": 2.7227,
      "step": 3250
    },
    {
      "epoch": 0.572030180733462,
      "grad_norm": 6.771231651306152,
      "learning_rate": 4.0472012633795406e-05,
      "loss": 2.7463,
      "step": 3260
    },
    {
      "epoch": 0.5737848745393929,
      "grad_norm": 5.38832950592041,
      "learning_rate": 4.044276773702989e-05,
      "loss": 2.6047,
      "step": 3270
    },
    {
      "epoch": 0.5755395683453237,
      "grad_norm": 6.827566623687744,
      "learning_rate": 4.041352284026438e-05,
      "loss": 2.8911,
      "step": 3280
    },
    {
      "epoch": 0.5772942621512546,
      "grad_norm": 6.599300861358643,
      "learning_rate": 4.038427794349886e-05,
      "loss": 2.588,
      "step": 3290
    },
    {
      "epoch": 0.5790489559571854,
      "grad_norm": 6.757265567779541,
      "learning_rate": 4.035503304673335e-05,
      "loss": 2.8813,
      "step": 3300
    },
    {
      "epoch": 0.5808036497631164,
      "grad_norm": 5.444268703460693,
      "learning_rate": 4.032578814996783e-05,
      "loss": 2.7879,
      "step": 3310
    },
    {
      "epoch": 0.5825583435690472,
      "grad_norm": 8.2647066116333,
      "learning_rate": 4.029654325320232e-05,
      "loss": 2.7602,
      "step": 3320
    },
    {
      "epoch": 0.5843130373749781,
      "grad_norm": 6.348752975463867,
      "learning_rate": 4.0267298356436805e-05,
      "loss": 2.5839,
      "step": 3330
    },
    {
      "epoch": 0.5860677311809089,
      "grad_norm": 7.504943370819092,
      "learning_rate": 4.023805345967129e-05,
      "loss": 2.6253,
      "step": 3340
    },
    {
      "epoch": 0.5878224249868398,
      "grad_norm": 7.76424503326416,
      "learning_rate": 4.0208808562905776e-05,
      "loss": 2.6255,
      "step": 3350
    },
    {
      "epoch": 0.5895771187927706,
      "grad_norm": 6.305349826812744,
      "learning_rate": 4.017956366614026e-05,
      "loss": 2.7535,
      "step": 3360
    },
    {
      "epoch": 0.5913318125987015,
      "grad_norm": 7.033076763153076,
      "learning_rate": 4.015031876937475e-05,
      "loss": 2.7804,
      "step": 3370
    },
    {
      "epoch": 0.5930865064046323,
      "grad_norm": 6.900142192840576,
      "learning_rate": 4.012107387260923e-05,
      "loss": 2.4577,
      "step": 3380
    },
    {
      "epoch": 0.5948412002105633,
      "grad_norm": 5.61320161819458,
      "learning_rate": 4.009182897584371e-05,
      "loss": 2.7819,
      "step": 3390
    },
    {
      "epoch": 0.5965958940164942,
      "grad_norm": 7.372130870819092,
      "learning_rate": 4.0062584079078204e-05,
      "loss": 2.8015,
      "step": 3400
    },
    {
      "epoch": 0.598350587822425,
      "grad_norm": 5.700352668762207,
      "learning_rate": 4.003333918231269e-05,
      "loss": 2.7309,
      "step": 3410
    },
    {
      "epoch": 0.6001052816283559,
      "grad_norm": 5.911738395690918,
      "learning_rate": 4.0004094285547176e-05,
      "loss": 2.7325,
      "step": 3420
    },
    {
      "epoch": 0.6018599754342867,
      "grad_norm": 5.834084987640381,
      "learning_rate": 3.997484938878166e-05,
      "loss": 2.8294,
      "step": 3430
    },
    {
      "epoch": 0.6036146692402176,
      "grad_norm": 6.553126335144043,
      "learning_rate": 3.994560449201615e-05,
      "loss": 2.6806,
      "step": 3440
    },
    {
      "epoch": 0.6053693630461484,
      "grad_norm": 6.601226806640625,
      "learning_rate": 3.991635959525063e-05,
      "loss": 2.7225,
      "step": 3450
    },
    {
      "epoch": 0.6071240568520793,
      "grad_norm": 5.074618339538574,
      "learning_rate": 3.988711469848512e-05,
      "loss": 2.6201,
      "step": 3460
    },
    {
      "epoch": 0.6088787506580102,
      "grad_norm": 5.821758270263672,
      "learning_rate": 3.98578698017196e-05,
      "loss": 2.7435,
      "step": 3470
    },
    {
      "epoch": 0.6106334444639411,
      "grad_norm": 6.899997711181641,
      "learning_rate": 3.982862490495409e-05,
      "loss": 2.6186,
      "step": 3480
    },
    {
      "epoch": 0.6123881382698719,
      "grad_norm": 5.577863693237305,
      "learning_rate": 3.9799380008188575e-05,
      "loss": 2.85,
      "step": 3490
    },
    {
      "epoch": 0.6141428320758028,
      "grad_norm": 7.4698896408081055,
      "learning_rate": 3.9770135111423054e-05,
      "loss": 2.6488,
      "step": 3500
    },
    {
      "epoch": 0.6158975258817336,
      "grad_norm": 6.2457194328308105,
      "learning_rate": 3.9740890214657546e-05,
      "loss": 2.7557,
      "step": 3510
    },
    {
      "epoch": 0.6176522196876645,
      "grad_norm": 6.210348129272461,
      "learning_rate": 3.971164531789203e-05,
      "loss": 2.7573,
      "step": 3520
    },
    {
      "epoch": 0.6194069134935953,
      "grad_norm": 6.533004283905029,
      "learning_rate": 3.968240042112652e-05,
      "loss": 2.8129,
      "step": 3530
    },
    {
      "epoch": 0.6211616072995262,
      "grad_norm": 6.112951278686523,
      "learning_rate": 3.9653155524361e-05,
      "loss": 2.821,
      "step": 3540
    },
    {
      "epoch": 0.6229163011054571,
      "grad_norm": 6.6125664710998535,
      "learning_rate": 3.962391062759548e-05,
      "loss": 2.8926,
      "step": 3550
    },
    {
      "epoch": 0.624670994911388,
      "grad_norm": 6.460550308227539,
      "learning_rate": 3.9594665730829974e-05,
      "loss": 2.7364,
      "step": 3560
    },
    {
      "epoch": 0.6264256887173189,
      "grad_norm": 5.346030235290527,
      "learning_rate": 3.956542083406446e-05,
      "loss": 2.6613,
      "step": 3570
    },
    {
      "epoch": 0.6281803825232497,
      "grad_norm": 5.743297576904297,
      "learning_rate": 3.953617593729894e-05,
      "loss": 2.8457,
      "step": 3580
    },
    {
      "epoch": 0.6299350763291806,
      "grad_norm": 6.046746730804443,
      "learning_rate": 3.950693104053343e-05,
      "loss": 2.597,
      "step": 3590
    },
    {
      "epoch": 0.6316897701351114,
      "grad_norm": 7.0634284019470215,
      "learning_rate": 3.947768614376792e-05,
      "loss": 2.7342,
      "step": 3600
    },
    {
      "epoch": 0.6334444639410423,
      "grad_norm": 5.288614749908447,
      "learning_rate": 3.9448441247002396e-05,
      "loss": 2.7227,
      "step": 3610
    },
    {
      "epoch": 0.6351991577469731,
      "grad_norm": 5.639553070068359,
      "learning_rate": 3.941919635023689e-05,
      "loss": 2.8739,
      "step": 3620
    },
    {
      "epoch": 0.6369538515529041,
      "grad_norm": 5.855603218078613,
      "learning_rate": 3.938995145347137e-05,
      "loss": 2.5043,
      "step": 3630
    },
    {
      "epoch": 0.6387085453588349,
      "grad_norm": 6.574014186859131,
      "learning_rate": 3.936070655670586e-05,
      "loss": 2.57,
      "step": 3640
    },
    {
      "epoch": 0.6404632391647658,
      "grad_norm": 7.241365909576416,
      "learning_rate": 3.9331461659940345e-05,
      "loss": 2.6563,
      "step": 3650
    },
    {
      "epoch": 0.6422179329706966,
      "grad_norm": 6.350118637084961,
      "learning_rate": 3.9302216763174824e-05,
      "loss": 2.7651,
      "step": 3660
    },
    {
      "epoch": 0.6439726267766275,
      "grad_norm": 9.0690279006958,
      "learning_rate": 3.9272971866409316e-05,
      "loss": 2.712,
      "step": 3670
    },
    {
      "epoch": 0.6457273205825583,
      "grad_norm": 6.9500651359558105,
      "learning_rate": 3.92437269696438e-05,
      "loss": 2.618,
      "step": 3680
    },
    {
      "epoch": 0.6474820143884892,
      "grad_norm": 6.188437461853027,
      "learning_rate": 3.921448207287828e-05,
      "loss": 2.6859,
      "step": 3690
    },
    {
      "epoch": 0.64923670819442,
      "grad_norm": 7.2810492515563965,
      "learning_rate": 3.918523717611277e-05,
      "loss": 2.4215,
      "step": 3700
    },
    {
      "epoch": 0.650991402000351,
      "grad_norm": 5.286552906036377,
      "learning_rate": 3.915599227934725e-05,
      "loss": 2.8107,
      "step": 3710
    },
    {
      "epoch": 0.6527460958062818,
      "grad_norm": 5.8797607421875,
      "learning_rate": 3.9126747382581744e-05,
      "loss": 2.6934,
      "step": 3720
    },
    {
      "epoch": 0.6545007896122127,
      "grad_norm": 10.222784996032715,
      "learning_rate": 3.909750248581623e-05,
      "loss": 2.5773,
      "step": 3730
    },
    {
      "epoch": 0.6562554834181435,
      "grad_norm": 6.72861385345459,
      "learning_rate": 3.906825758905071e-05,
      "loss": 2.7165,
      "step": 3740
    },
    {
      "epoch": 0.6580101772240744,
      "grad_norm": 8.348873138427734,
      "learning_rate": 3.90390126922852e-05,
      "loss": 2.6284,
      "step": 3750
    },
    {
      "epoch": 0.6597648710300053,
      "grad_norm": 7.336050510406494,
      "learning_rate": 3.900976779551969e-05,
      "loss": 2.6368,
      "step": 3760
    },
    {
      "epoch": 0.6615195648359361,
      "grad_norm": 5.896008014678955,
      "learning_rate": 3.8980522898754166e-05,
      "loss": 2.7529,
      "step": 3770
    },
    {
      "epoch": 0.663274258641867,
      "grad_norm": 6.430300235748291,
      "learning_rate": 3.895127800198866e-05,
      "loss": 2.7661,
      "step": 3780
    },
    {
      "epoch": 0.6650289524477978,
      "grad_norm": 6.9744720458984375,
      "learning_rate": 3.892203310522314e-05,
      "loss": 2.5161,
      "step": 3790
    },
    {
      "epoch": 0.6667836462537288,
      "grad_norm": 7.457771301269531,
      "learning_rate": 3.889278820845762e-05,
      "loss": 2.7453,
      "step": 3800
    },
    {
      "epoch": 0.6685383400596596,
      "grad_norm": 7.2476277351379395,
      "learning_rate": 3.8863543311692115e-05,
      "loss": 2.8006,
      "step": 3810
    },
    {
      "epoch": 0.6702930338655905,
      "grad_norm": 7.162204265594482,
      "learning_rate": 3.8834298414926594e-05,
      "loss": 2.882,
      "step": 3820
    },
    {
      "epoch": 0.6720477276715213,
      "grad_norm": 5.6807861328125,
      "learning_rate": 3.8805053518161086e-05,
      "loss": 2.7271,
      "step": 3830
    },
    {
      "epoch": 0.6738024214774522,
      "grad_norm": 5.3137946128845215,
      "learning_rate": 3.877580862139557e-05,
      "loss": 2.6614,
      "step": 3840
    },
    {
      "epoch": 0.675557115283383,
      "grad_norm": 7.929653644561768,
      "learning_rate": 3.874656372463005e-05,
      "loss": 2.9398,
      "step": 3850
    },
    {
      "epoch": 0.6773118090893139,
      "grad_norm": 6.1686272621154785,
      "learning_rate": 3.871731882786454e-05,
      "loss": 2.7615,
      "step": 3860
    },
    {
      "epoch": 0.6790665028952447,
      "grad_norm": 6.027119159698486,
      "learning_rate": 3.868807393109902e-05,
      "loss": 2.8362,
      "step": 3870
    },
    {
      "epoch": 0.6808211967011757,
      "grad_norm": 11.434728622436523,
      "learning_rate": 3.865882903433351e-05,
      "loss": 2.5808,
      "step": 3880
    },
    {
      "epoch": 0.6825758905071065,
      "grad_norm": 6.039453029632568,
      "learning_rate": 3.8629584137568e-05,
      "loss": 2.8367,
      "step": 3890
    },
    {
      "epoch": 0.6843305843130374,
      "grad_norm": 6.94126033782959,
      "learning_rate": 3.860033924080248e-05,
      "loss": 2.6921,
      "step": 3900
    },
    {
      "epoch": 0.6860852781189682,
      "grad_norm": 6.616915225982666,
      "learning_rate": 3.857109434403697e-05,
      "loss": 2.6715,
      "step": 3910
    },
    {
      "epoch": 0.6878399719248991,
      "grad_norm": 6.718637943267822,
      "learning_rate": 3.854184944727146e-05,
      "loss": 2.6195,
      "step": 3920
    },
    {
      "epoch": 0.68959466573083,
      "grad_norm": 6.909640789031982,
      "learning_rate": 3.8512604550505935e-05,
      "loss": 2.711,
      "step": 3930
    },
    {
      "epoch": 0.6913493595367608,
      "grad_norm": 7.769559383392334,
      "learning_rate": 3.848335965374043e-05,
      "loss": 2.5123,
      "step": 3940
    },
    {
      "epoch": 0.6931040533426917,
      "grad_norm": 6.227227210998535,
      "learning_rate": 3.845411475697491e-05,
      "loss": 2.7512,
      "step": 3950
    },
    {
      "epoch": 0.6948587471486226,
      "grad_norm": 6.984363079071045,
      "learning_rate": 3.842486986020939e-05,
      "loss": 2.6669,
      "step": 3960
    },
    {
      "epoch": 0.6966134409545535,
      "grad_norm": 7.788311958312988,
      "learning_rate": 3.8395624963443885e-05,
      "loss": 2.5699,
      "step": 3970
    },
    {
      "epoch": 0.6983681347604843,
      "grad_norm": 7.757559299468994,
      "learning_rate": 3.8366380066678364e-05,
      "loss": 2.6105,
      "step": 3980
    },
    {
      "epoch": 0.7001228285664152,
      "grad_norm": 5.2965803146362305,
      "learning_rate": 3.833713516991285e-05,
      "loss": 2.6078,
      "step": 3990
    },
    {
      "epoch": 0.701877522372346,
      "grad_norm": 7.693796634674072,
      "learning_rate": 3.830789027314734e-05,
      "loss": 2.6366,
      "step": 4000
    },
    {
      "epoch": 0.7036322161782769,
      "grad_norm": 7.445805072784424,
      "learning_rate": 3.827864537638182e-05,
      "loss": 2.7447,
      "step": 4010
    },
    {
      "epoch": 0.7053869099842077,
      "grad_norm": 5.995882034301758,
      "learning_rate": 3.824940047961631e-05,
      "loss": 2.7356,
      "step": 4020
    },
    {
      "epoch": 0.7071416037901386,
      "grad_norm": 6.639626979827881,
      "learning_rate": 3.822015558285079e-05,
      "loss": 2.5016,
      "step": 4030
    },
    {
      "epoch": 0.7088962975960695,
      "grad_norm": 7.871476173400879,
      "learning_rate": 3.819091068608528e-05,
      "loss": 2.805,
      "step": 4040
    },
    {
      "epoch": 0.7106509914020004,
      "grad_norm": 6.189019680023193,
      "learning_rate": 3.816166578931977e-05,
      "loss": 2.5948,
      "step": 4050
    },
    {
      "epoch": 0.7124056852079312,
      "grad_norm": 5.989374160766602,
      "learning_rate": 3.813242089255425e-05,
      "loss": 2.8395,
      "step": 4060
    },
    {
      "epoch": 0.7141603790138621,
      "grad_norm": 6.01303243637085,
      "learning_rate": 3.8103175995788734e-05,
      "loss": 2.6874,
      "step": 4070
    },
    {
      "epoch": 0.7159150728197929,
      "grad_norm": 13.793198585510254,
      "learning_rate": 3.8073931099023227e-05,
      "loss": 2.6052,
      "step": 4080
    },
    {
      "epoch": 0.7176697666257238,
      "grad_norm": 7.549479961395264,
      "learning_rate": 3.8044686202257705e-05,
      "loss": 2.6076,
      "step": 4090
    },
    {
      "epoch": 0.7194244604316546,
      "grad_norm": 10.684185028076172,
      "learning_rate": 3.801544130549219e-05,
      "loss": 2.7383,
      "step": 4100
    },
    {
      "epoch": 0.7211791542375855,
      "grad_norm": 7.262774467468262,
      "learning_rate": 3.798619640872668e-05,
      "loss": 2.714,
      "step": 4110
    },
    {
      "epoch": 0.7229338480435165,
      "grad_norm": 5.711001873016357,
      "learning_rate": 3.795695151196116e-05,
      "loss": 2.4364,
      "step": 4120
    },
    {
      "epoch": 0.7246885418494473,
      "grad_norm": 7.579013347625732,
      "learning_rate": 3.7927706615195655e-05,
      "loss": 2.6968,
      "step": 4130
    },
    {
      "epoch": 0.7264432356553782,
      "grad_norm": 6.789124965667725,
      "learning_rate": 3.7898461718430134e-05,
      "loss": 2.7585,
      "step": 4140
    },
    {
      "epoch": 0.728197929461309,
      "grad_norm": 6.222605228424072,
      "learning_rate": 3.786921682166462e-05,
      "loss": 2.8411,
      "step": 4150
    },
    {
      "epoch": 0.7299526232672399,
      "grad_norm": 5.7399773597717285,
      "learning_rate": 3.783997192489911e-05,
      "loss": 2.6467,
      "step": 4160
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 7.79063606262207,
      "learning_rate": 3.781072702813359e-05,
      "loss": 2.8843,
      "step": 4170
    },
    {
      "epoch": 0.7334620108791016,
      "grad_norm": 5.553554534912109,
      "learning_rate": 3.7781482131368076e-05,
      "loss": 2.7747,
      "step": 4180
    },
    {
      "epoch": 0.7352167046850324,
      "grad_norm": 5.166622638702393,
      "learning_rate": 3.775223723460256e-05,
      "loss": 2.6201,
      "step": 4190
    },
    {
      "epoch": 0.7369713984909634,
      "grad_norm": 7.39240026473999,
      "learning_rate": 3.772299233783705e-05,
      "loss": 2.6991,
      "step": 4200
    },
    {
      "epoch": 0.7387260922968942,
      "grad_norm": 5.937695503234863,
      "learning_rate": 3.769374744107154e-05,
      "loss": 2.513,
      "step": 4210
    },
    {
      "epoch": 0.7404807861028251,
      "grad_norm": 7.485251426696777,
      "learning_rate": 3.766450254430602e-05,
      "loss": 2.8381,
      "step": 4220
    },
    {
      "epoch": 0.7422354799087559,
      "grad_norm": 7.244956970214844,
      "learning_rate": 3.7635257647540504e-05,
      "loss": 2.7835,
      "step": 4230
    },
    {
      "epoch": 0.7439901737146868,
      "grad_norm": 6.049688816070557,
      "learning_rate": 3.7606012750774997e-05,
      "loss": 2.685,
      "step": 4240
    },
    {
      "epoch": 0.7457448675206176,
      "grad_norm": 6.036685943603516,
      "learning_rate": 3.7576767854009475e-05,
      "loss": 2.7647,
      "step": 4250
    },
    {
      "epoch": 0.7474995613265485,
      "grad_norm": 6.335845470428467,
      "learning_rate": 3.754752295724396e-05,
      "loss": 2.7399,
      "step": 4260
    },
    {
      "epoch": 0.7492542551324793,
      "grad_norm": 8.014859199523926,
      "learning_rate": 3.7518278060478447e-05,
      "loss": 2.6235,
      "step": 4270
    },
    {
      "epoch": 0.7510089489384102,
      "grad_norm": 7.276232719421387,
      "learning_rate": 3.748903316371293e-05,
      "loss": 2.4666,
      "step": 4280
    },
    {
      "epoch": 0.7527636427443412,
      "grad_norm": 7.235744476318359,
      "learning_rate": 3.745978826694742e-05,
      "loss": 2.8333,
      "step": 4290
    },
    {
      "epoch": 0.754518336550272,
      "grad_norm": 6.067704200744629,
      "learning_rate": 3.7430543370181903e-05,
      "loss": 2.6818,
      "step": 4300
    },
    {
      "epoch": 0.7562730303562029,
      "grad_norm": 7.4713592529296875,
      "learning_rate": 3.740129847341639e-05,
      "loss": 2.571,
      "step": 4310
    },
    {
      "epoch": 0.7580277241621337,
      "grad_norm": 7.6107354164123535,
      "learning_rate": 3.737205357665088e-05,
      "loss": 2.4749,
      "step": 4320
    },
    {
      "epoch": 0.7597824179680646,
      "grad_norm": 5.730342388153076,
      "learning_rate": 3.734280867988536e-05,
      "loss": 2.783,
      "step": 4330
    },
    {
      "epoch": 0.7615371117739954,
      "grad_norm": 7.3192949295043945,
      "learning_rate": 3.7313563783119846e-05,
      "loss": 2.6888,
      "step": 4340
    },
    {
      "epoch": 0.7632918055799263,
      "grad_norm": 7.086453437805176,
      "learning_rate": 3.728431888635433e-05,
      "loss": 2.8674,
      "step": 4350
    },
    {
      "epoch": 0.7650464993858571,
      "grad_norm": 5.259917259216309,
      "learning_rate": 3.725507398958882e-05,
      "loss": 2.7382,
      "step": 4360
    },
    {
      "epoch": 0.7668011931917881,
      "grad_norm": 5.457756996154785,
      "learning_rate": 3.72258290928233e-05,
      "loss": 2.4582,
      "step": 4370
    },
    {
      "epoch": 0.7685558869977189,
      "grad_norm": 6.964674949645996,
      "learning_rate": 3.719658419605779e-05,
      "loss": 2.555,
      "step": 4380
    },
    {
      "epoch": 0.7703105808036498,
      "grad_norm": 5.456439971923828,
      "learning_rate": 3.7167339299292274e-05,
      "loss": 2.5488,
      "step": 4390
    },
    {
      "epoch": 0.7720652746095806,
      "grad_norm": 6.084720134735107,
      "learning_rate": 3.713809440252676e-05,
      "loss": 2.801,
      "step": 4400
    },
    {
      "epoch": 0.7738199684155115,
      "grad_norm": 6.050457954406738,
      "learning_rate": 3.7108849505761245e-05,
      "loss": 2.6893,
      "step": 4410
    },
    {
      "epoch": 0.7755746622214423,
      "grad_norm": 5.8292951583862305,
      "learning_rate": 3.707960460899573e-05,
      "loss": 2.5361,
      "step": 4420
    },
    {
      "epoch": 0.7773293560273732,
      "grad_norm": 7.324831008911133,
      "learning_rate": 3.7050359712230217e-05,
      "loss": 2.7091,
      "step": 4430
    },
    {
      "epoch": 0.779084049833304,
      "grad_norm": 6.846329212188721,
      "learning_rate": 3.70211148154647e-05,
      "loss": 2.8142,
      "step": 4440
    },
    {
      "epoch": 0.780838743639235,
      "grad_norm": 6.456788539886475,
      "learning_rate": 3.699186991869919e-05,
      "loss": 2.5385,
      "step": 4450
    },
    {
      "epoch": 0.7825934374451659,
      "grad_norm": 6.843525409698486,
      "learning_rate": 3.6962625021933673e-05,
      "loss": 2.7256,
      "step": 4460
    },
    {
      "epoch": 0.7843481312510967,
      "grad_norm": 6.248363494873047,
      "learning_rate": 3.693338012516816e-05,
      "loss": 2.6471,
      "step": 4470
    },
    {
      "epoch": 0.7861028250570276,
      "grad_norm": 10.748063087463379,
      "learning_rate": 3.6904135228402645e-05,
      "loss": 2.6673,
      "step": 4480
    },
    {
      "epoch": 0.7878575188629584,
      "grad_norm": 6.439659118652344,
      "learning_rate": 3.687489033163713e-05,
      "loss": 2.6874,
      "step": 4490
    },
    {
      "epoch": 0.7896122126688893,
      "grad_norm": 6.558787822723389,
      "learning_rate": 3.684856992454817e-05,
      "loss": 2.7386,
      "step": 4500
    },
    {
      "epoch": 0.7913669064748201,
      "grad_norm": 5.791163921356201,
      "learning_rate": 3.681932502778265e-05,
      "loss": 2.9323,
      "step": 4510
    },
    {
      "epoch": 0.793121600280751,
      "grad_norm": 6.517026424407959,
      "learning_rate": 3.679008013101714e-05,
      "loss": 2.6998,
      "step": 4520
    },
    {
      "epoch": 0.7948762940866819,
      "grad_norm": 5.915233612060547,
      "learning_rate": 3.676083523425162e-05,
      "loss": 2.4777,
      "step": 4530
    },
    {
      "epoch": 0.7966309878926128,
      "grad_norm": 5.6498260498046875,
      "learning_rate": 3.673159033748611e-05,
      "loss": 2.7267,
      "step": 4540
    },
    {
      "epoch": 0.7983856816985436,
      "grad_norm": 6.972443580627441,
      "learning_rate": 3.67023454407206e-05,
      "loss": 2.6238,
      "step": 4550
    },
    {
      "epoch": 0.8001403755044745,
      "grad_norm": 5.594479560852051,
      "learning_rate": 3.667310054395508e-05,
      "loss": 2.5387,
      "step": 4560
    },
    {
      "epoch": 0.8018950693104053,
      "grad_norm": 8.37380599975586,
      "learning_rate": 3.664385564718957e-05,
      "loss": 2.4469,
      "step": 4570
    },
    {
      "epoch": 0.8036497631163362,
      "grad_norm": 10.708383560180664,
      "learning_rate": 3.6614610750424056e-05,
      "loss": 2.817,
      "step": 4580
    },
    {
      "epoch": 0.805404456922267,
      "grad_norm": 7.389172554016113,
      "learning_rate": 3.6585365853658535e-05,
      "loss": 2.6379,
      "step": 4590
    },
    {
      "epoch": 0.8071591507281979,
      "grad_norm": 7.337276458740234,
      "learning_rate": 3.655612095689303e-05,
      "loss": 2.6985,
      "step": 4600
    },
    {
      "epoch": 0.8089138445341288,
      "grad_norm": 5.694016456604004,
      "learning_rate": 3.6526876060127506e-05,
      "loss": 2.8509,
      "step": 4610
    },
    {
      "epoch": 0.8106685383400597,
      "grad_norm": 7.573892116546631,
      "learning_rate": 3.649763116336199e-05,
      "loss": 2.7925,
      "step": 4620
    },
    {
      "epoch": 0.8124232321459905,
      "grad_norm": 5.032703399658203,
      "learning_rate": 3.6468386266596484e-05,
      "loss": 2.5821,
      "step": 4630
    },
    {
      "epoch": 0.8141779259519214,
      "grad_norm": 5.956598281860352,
      "learning_rate": 3.643914136983096e-05,
      "loss": 2.6308,
      "step": 4640
    },
    {
      "epoch": 0.8159326197578523,
      "grad_norm": 5.732999801635742,
      "learning_rate": 3.6409896473065456e-05,
      "loss": 2.5656,
      "step": 4650
    },
    {
      "epoch": 0.8176873135637831,
      "grad_norm": 6.672980785369873,
      "learning_rate": 3.638065157629994e-05,
      "loss": 2.5595,
      "step": 4660
    },
    {
      "epoch": 0.819442007369714,
      "grad_norm": 7.300610065460205,
      "learning_rate": 3.635140667953442e-05,
      "loss": 2.5268,
      "step": 4670
    },
    {
      "epoch": 0.8211967011756448,
      "grad_norm": 6.577507019042969,
      "learning_rate": 3.632216178276891e-05,
      "loss": 2.6993,
      "step": 4680
    },
    {
      "epoch": 0.8229513949815758,
      "grad_norm": 5.6707634925842285,
      "learning_rate": 3.629291688600339e-05,
      "loss": 2.631,
      "step": 4690
    },
    {
      "epoch": 0.8247060887875066,
      "grad_norm": 7.947882175445557,
      "learning_rate": 3.626367198923788e-05,
      "loss": 2.6353,
      "step": 4700
    },
    {
      "epoch": 0.8264607825934375,
      "grad_norm": 6.358967304229736,
      "learning_rate": 3.623442709247237e-05,
      "loss": 2.6594,
      "step": 4710
    },
    {
      "epoch": 0.8282154763993683,
      "grad_norm": 6.43321418762207,
      "learning_rate": 3.620518219570685e-05,
      "loss": 2.7947,
      "step": 4720
    },
    {
      "epoch": 0.8299701702052992,
      "grad_norm": 6.337188243865967,
      "learning_rate": 3.6175937298941334e-05,
      "loss": 2.8194,
      "step": 4730
    },
    {
      "epoch": 0.83172486401123,
      "grad_norm": 6.520514011383057,
      "learning_rate": 3.6146692402175826e-05,
      "loss": 2.756,
      "step": 4740
    },
    {
      "epoch": 0.8334795578171609,
      "grad_norm": 7.164801597595215,
      "learning_rate": 3.6117447505410305e-05,
      "loss": 2.6053,
      "step": 4750
    },
    {
      "epoch": 0.8352342516230917,
      "grad_norm": 7.015561580657959,
      "learning_rate": 3.60882026086448e-05,
      "loss": 2.7317,
      "step": 4760
    },
    {
      "epoch": 0.8369889454290227,
      "grad_norm": 5.939584732055664,
      "learning_rate": 3.6058957711879276e-05,
      "loss": 2.7957,
      "step": 4770
    },
    {
      "epoch": 0.8387436392349535,
      "grad_norm": 7.142246723175049,
      "learning_rate": 3.602971281511376e-05,
      "loss": 2.5559,
      "step": 4780
    },
    {
      "epoch": 0.8404983330408844,
      "grad_norm": 6.612362384796143,
      "learning_rate": 3.6000467918348254e-05,
      "loss": 2.6448,
      "step": 4790
    },
    {
      "epoch": 0.8422530268468152,
      "grad_norm": 5.499001502990723,
      "learning_rate": 3.597122302158273e-05,
      "loss": 2.7767,
      "step": 4800
    },
    {
      "epoch": 0.8440077206527461,
      "grad_norm": 6.713254928588867,
      "learning_rate": 3.594197812481722e-05,
      "loss": 2.6149,
      "step": 4810
    },
    {
      "epoch": 0.845762414458677,
      "grad_norm": 5.875740051269531,
      "learning_rate": 3.591273322805171e-05,
      "loss": 2.5712,
      "step": 4820
    },
    {
      "epoch": 0.8475171082646078,
      "grad_norm": 7.396605491638184,
      "learning_rate": 3.588348833128619e-05,
      "loss": 2.617,
      "step": 4830
    },
    {
      "epoch": 0.8492718020705387,
      "grad_norm": 7.700014591217041,
      "learning_rate": 3.5854243434520676e-05,
      "loss": 2.6557,
      "step": 4840
    },
    {
      "epoch": 0.8510264958764695,
      "grad_norm": 7.1234049797058105,
      "learning_rate": 3.582499853775516e-05,
      "loss": 2.5581,
      "step": 4850
    },
    {
      "epoch": 0.8527811896824005,
      "grad_norm": 6.133018970489502,
      "learning_rate": 3.579575364098965e-05,
      "loss": 2.6127,
      "step": 4860
    },
    {
      "epoch": 0.8545358834883313,
      "grad_norm": 6.263479709625244,
      "learning_rate": 3.576650874422414e-05,
      "loss": 2.6001,
      "step": 4870
    },
    {
      "epoch": 0.8562905772942622,
      "grad_norm": 7.01969051361084,
      "learning_rate": 3.573726384745862e-05,
      "loss": 2.714,
      "step": 4880
    },
    {
      "epoch": 0.858045271100193,
      "grad_norm": 10.702774047851562,
      "learning_rate": 3.5708018950693104e-05,
      "loss": 2.5216,
      "step": 4890
    },
    {
      "epoch": 0.8597999649061239,
      "grad_norm": 6.852715969085693,
      "learning_rate": 3.5678774053927596e-05,
      "loss": 2.7039,
      "step": 4900
    },
    {
      "epoch": 0.8615546587120547,
      "grad_norm": 6.840342998504639,
      "learning_rate": 3.5649529157162075e-05,
      "loss": 2.609,
      "step": 4910
    },
    {
      "epoch": 0.8633093525179856,
      "grad_norm": 5.382511615753174,
      "learning_rate": 3.562028426039656e-05,
      "loss": 2.3525,
      "step": 4920
    },
    {
      "epoch": 0.8650640463239164,
      "grad_norm": 10.717127799987793,
      "learning_rate": 3.5591039363631046e-05,
      "loss": 2.7251,
      "step": 4930
    },
    {
      "epoch": 0.8668187401298474,
      "grad_norm": 6.865050792694092,
      "learning_rate": 3.556179446686553e-05,
      "loss": 2.636,
      "step": 4940
    },
    {
      "epoch": 0.8685734339357782,
      "grad_norm": 5.587786674499512,
      "learning_rate": 3.5532549570100024e-05,
      "loss": 2.7109,
      "step": 4950
    },
    {
      "epoch": 0.8703281277417091,
      "grad_norm": 6.708760738372803,
      "learning_rate": 3.55033046733345e-05,
      "loss": 2.5923,
      "step": 4960
    },
    {
      "epoch": 0.8720828215476399,
      "grad_norm": 10.305667877197266,
      "learning_rate": 3.547405977656899e-05,
      "loss": 2.8693,
      "step": 4970
    },
    {
      "epoch": 0.8738375153535708,
      "grad_norm": 6.35567569732666,
      "learning_rate": 3.544481487980348e-05,
      "loss": 2.7928,
      "step": 4980
    },
    {
      "epoch": 0.8755922091595016,
      "grad_norm": 6.4913458824157715,
      "learning_rate": 3.541556998303796e-05,
      "loss": 2.7002,
      "step": 4990
    },
    {
      "epoch": 0.8773469029654325,
      "grad_norm": 7.500043869018555,
      "learning_rate": 3.5386325086272446e-05,
      "loss": 2.7331,
      "step": 5000
    },
    {
      "epoch": 0.8791015967713633,
      "grad_norm": 5.215460777282715,
      "learning_rate": 3.535708018950693e-05,
      "loss": 2.5603,
      "step": 5010
    },
    {
      "epoch": 0.8808562905772943,
      "grad_norm": 7.61776876449585,
      "learning_rate": 3.532783529274142e-05,
      "loss": 2.9009,
      "step": 5020
    },
    {
      "epoch": 0.8826109843832252,
      "grad_norm": 5.8777852058410645,
      "learning_rate": 3.52985903959759e-05,
      "loss": 2.639,
      "step": 5030
    },
    {
      "epoch": 0.884365678189156,
      "grad_norm": 7.281164646148682,
      "learning_rate": 3.526934549921039e-05,
      "loss": 2.7105,
      "step": 5040
    },
    {
      "epoch": 0.8861203719950869,
      "grad_norm": 6.997308731079102,
      "learning_rate": 3.5240100602444874e-05,
      "loss": 2.7861,
      "step": 5050
    },
    {
      "epoch": 0.8878750658010177,
      "grad_norm": 8.840312004089355,
      "learning_rate": 3.5210855705679366e-05,
      "loss": 2.4813,
      "step": 5060
    },
    {
      "epoch": 0.8896297596069486,
      "grad_norm": 6.228816986083984,
      "learning_rate": 3.5181610808913845e-05,
      "loss": 2.7323,
      "step": 5070
    },
    {
      "epoch": 0.8913844534128794,
      "grad_norm": 5.518157005310059,
      "learning_rate": 3.515236591214833e-05,
      "loss": 2.4692,
      "step": 5080
    },
    {
      "epoch": 0.8931391472188103,
      "grad_norm": 6.103092670440674,
      "learning_rate": 3.5123121015382816e-05,
      "loss": 2.8394,
      "step": 5090
    },
    {
      "epoch": 0.8948938410247412,
      "grad_norm": 6.7193779945373535,
      "learning_rate": 3.50938761186173e-05,
      "loss": 2.4459,
      "step": 5100
    },
    {
      "epoch": 0.8966485348306721,
      "grad_norm": 6.605923652648926,
      "learning_rate": 3.506463122185179e-05,
      "loss": 2.7717,
      "step": 5110
    },
    {
      "epoch": 0.8984032286366029,
      "grad_norm": 8.357544898986816,
      "learning_rate": 3.503538632508627e-05,
      "loss": 2.6167,
      "step": 5120
    },
    {
      "epoch": 0.9001579224425338,
      "grad_norm": 7.778968334197998,
      "learning_rate": 3.500614142832076e-05,
      "loss": 2.728,
      "step": 5130
    },
    {
      "epoch": 0.9019126162484646,
      "grad_norm": 6.7574849128723145,
      "learning_rate": 3.4976896531555244e-05,
      "loss": 2.6403,
      "step": 5140
    },
    {
      "epoch": 0.9036673100543955,
      "grad_norm": 8.466309547424316,
      "learning_rate": 3.494765163478973e-05,
      "loss": 2.6151,
      "step": 5150
    },
    {
      "epoch": 0.9054220038603263,
      "grad_norm": 5.95130729675293,
      "learning_rate": 3.4918406738024216e-05,
      "loss": 2.6943,
      "step": 5160
    },
    {
      "epoch": 0.9071766976662572,
      "grad_norm": 7.043776512145996,
      "learning_rate": 3.48891618412587e-05,
      "loss": 2.5252,
      "step": 5170
    },
    {
      "epoch": 0.9089313914721882,
      "grad_norm": 7.319398880004883,
      "learning_rate": 3.485991694449319e-05,
      "loss": 2.5757,
      "step": 5180
    },
    {
      "epoch": 0.910686085278119,
      "grad_norm": 6.818327903747559,
      "learning_rate": 3.483067204772767e-05,
      "loss": 2.5293,
      "step": 5190
    },
    {
      "epoch": 0.9124407790840499,
      "grad_norm": 7.138484477996826,
      "learning_rate": 3.480142715096216e-05,
      "loss": 2.7028,
      "step": 5200
    },
    {
      "epoch": 0.9141954728899807,
      "grad_norm": 6.306097030639648,
      "learning_rate": 3.4772182254196644e-05,
      "loss": 2.7363,
      "step": 5210
    },
    {
      "epoch": 0.9159501666959116,
      "grad_norm": 8.70468807220459,
      "learning_rate": 3.474293735743113e-05,
      "loss": 2.3146,
      "step": 5220
    },
    {
      "epoch": 0.9177048605018424,
      "grad_norm": 7.26501989364624,
      "learning_rate": 3.4713692460665615e-05,
      "loss": 2.6491,
      "step": 5230
    },
    {
      "epoch": 0.9194595543077733,
      "grad_norm": 8.660721778869629,
      "learning_rate": 3.46844475639001e-05,
      "loss": 2.8679,
      "step": 5240
    },
    {
      "epoch": 0.9212142481137041,
      "grad_norm": 8.421683311462402,
      "learning_rate": 3.4655202667134586e-05,
      "loss": 2.6949,
      "step": 5250
    },
    {
      "epoch": 0.9229689419196351,
      "grad_norm": 8.263585090637207,
      "learning_rate": 3.462595777036907e-05,
      "loss": 2.5691,
      "step": 5260
    },
    {
      "epoch": 0.9247236357255659,
      "grad_norm": 8.66184139251709,
      "learning_rate": 3.459671287360356e-05,
      "loss": 2.7329,
      "step": 5270
    },
    {
      "epoch": 0.9264783295314968,
      "grad_norm": 6.613345146179199,
      "learning_rate": 3.456746797683804e-05,
      "loss": 2.5699,
      "step": 5280
    },
    {
      "epoch": 0.9282330233374276,
      "grad_norm": 6.001324653625488,
      "learning_rate": 3.453822308007253e-05,
      "loss": 2.8323,
      "step": 5290
    },
    {
      "epoch": 0.9299877171433585,
      "grad_norm": 6.665243625640869,
      "learning_rate": 3.4508978183307014e-05,
      "loss": 2.6328,
      "step": 5300
    },
    {
      "epoch": 0.9317424109492893,
      "grad_norm": 7.661280632019043,
      "learning_rate": 3.44797332865415e-05,
      "loss": 2.7788,
      "step": 5310
    },
    {
      "epoch": 0.9334971047552202,
      "grad_norm": 6.4841628074646,
      "learning_rate": 3.4450488389775986e-05,
      "loss": 2.5483,
      "step": 5320
    },
    {
      "epoch": 0.935251798561151,
      "grad_norm": 7.01010799407959,
      "learning_rate": 3.442124349301047e-05,
      "loss": 2.5212,
      "step": 5330
    },
    {
      "epoch": 0.9370064923670819,
      "grad_norm": 7.488576889038086,
      "learning_rate": 3.439199859624496e-05,
      "loss": 2.6335,
      "step": 5340
    },
    {
      "epoch": 0.9387611861730129,
      "grad_norm": 6.177280426025391,
      "learning_rate": 3.436275369947944e-05,
      "loss": 2.6975,
      "step": 5350
    },
    {
      "epoch": 0.9405158799789437,
      "grad_norm": 8.43828010559082,
      "learning_rate": 3.433350880271393e-05,
      "loss": 2.6903,
      "step": 5360
    },
    {
      "epoch": 0.9422705737848746,
      "grad_norm": 9.574827194213867,
      "learning_rate": 3.4304263905948414e-05,
      "loss": 2.8101,
      "step": 5370
    },
    {
      "epoch": 0.9440252675908054,
      "grad_norm": 6.46991491317749,
      "learning_rate": 3.42750190091829e-05,
      "loss": 2.3811,
      "step": 5380
    },
    {
      "epoch": 0.9457799613967363,
      "grad_norm": 8.494192123413086,
      "learning_rate": 3.4245774112417385e-05,
      "loss": 2.5061,
      "step": 5390
    },
    {
      "epoch": 0.9475346552026671,
      "grad_norm": 6.545167922973633,
      "learning_rate": 3.421652921565187e-05,
      "loss": 2.7223,
      "step": 5400
    },
    {
      "epoch": 0.949289349008598,
      "grad_norm": 6.49595308303833,
      "learning_rate": 3.4187284318886356e-05,
      "loss": 2.6896,
      "step": 5410
    },
    {
      "epoch": 0.9510440428145288,
      "grad_norm": 6.522353172302246,
      "learning_rate": 3.415803942212084e-05,
      "loss": 2.536,
      "step": 5420
    },
    {
      "epoch": 0.9527987366204598,
      "grad_norm": 9.006741523742676,
      "learning_rate": 3.412879452535533e-05,
      "loss": 2.5333,
      "step": 5430
    },
    {
      "epoch": 0.9545534304263906,
      "grad_norm": 7.539669513702393,
      "learning_rate": 3.409954962858981e-05,
      "loss": 2.7381,
      "step": 5440
    },
    {
      "epoch": 0.9563081242323215,
      "grad_norm": 7.80057430267334,
      "learning_rate": 3.40703047318243e-05,
      "loss": 2.6716,
      "step": 5450
    },
    {
      "epoch": 0.9580628180382523,
      "grad_norm": 5.5246500968933105,
      "learning_rate": 3.4041059835058784e-05,
      "loss": 2.712,
      "step": 5460
    },
    {
      "epoch": 0.9598175118441832,
      "grad_norm": 8.641154289245605,
      "learning_rate": 3.401181493829327e-05,
      "loss": 2.7767,
      "step": 5470
    },
    {
      "epoch": 0.961572205650114,
      "grad_norm": 6.379006862640381,
      "learning_rate": 3.3982570041527755e-05,
      "loss": 2.6793,
      "step": 5480
    },
    {
      "epoch": 0.9633268994560449,
      "grad_norm": 5.735892295837402,
      "learning_rate": 3.395332514476224e-05,
      "loss": 2.5145,
      "step": 5490
    },
    {
      "epoch": 0.9650815932619757,
      "grad_norm": 7.647717475891113,
      "learning_rate": 3.392408024799673e-05,
      "loss": 2.7506,
      "step": 5500
    },
    {
      "epoch": 0.9668362870679067,
      "grad_norm": 10.197486877441406,
      "learning_rate": 3.389483535123121e-05,
      "loss": 2.5173,
      "step": 5510
    },
    {
      "epoch": 0.9685909808738375,
      "grad_norm": 6.783901691436768,
      "learning_rate": 3.38655904544657e-05,
      "loss": 2.8186,
      "step": 5520
    },
    {
      "epoch": 0.9703456746797684,
      "grad_norm": 6.287844181060791,
      "learning_rate": 3.3836345557700184e-05,
      "loss": 2.7297,
      "step": 5530
    },
    {
      "epoch": 0.9721003684856993,
      "grad_norm": 6.762240886688232,
      "learning_rate": 3.380710066093467e-05,
      "loss": 2.6659,
      "step": 5540
    },
    {
      "epoch": 0.9738550622916301,
      "grad_norm": 7.31514835357666,
      "learning_rate": 3.3777855764169155e-05,
      "loss": 2.6157,
      "step": 5550
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 6.600619792938232,
      "learning_rate": 3.374861086740364e-05,
      "loss": 2.6394,
      "step": 5560
    },
    {
      "epoch": 0.9773644499034918,
      "grad_norm": 7.857025623321533,
      "learning_rate": 3.371936597063812e-05,
      "loss": 2.5937,
      "step": 5570
    },
    {
      "epoch": 0.9791191437094227,
      "grad_norm": 6.5462775230407715,
      "learning_rate": 3.369012107387261e-05,
      "loss": 2.5482,
      "step": 5580
    },
    {
      "epoch": 0.9808738375153536,
      "grad_norm": 9.50663948059082,
      "learning_rate": 3.36608761771071e-05,
      "loss": 2.8499,
      "step": 5590
    },
    {
      "epoch": 0.9826285313212845,
      "grad_norm": 7.4960126876831055,
      "learning_rate": 3.363163128034158e-05,
      "loss": 2.6055,
      "step": 5600
    },
    {
      "epoch": 0.9843832251272153,
      "grad_norm": 5.9395270347595215,
      "learning_rate": 3.360238638357607e-05,
      "loss": 2.7012,
      "step": 5610
    },
    {
      "epoch": 0.9861379189331462,
      "grad_norm": 6.638782501220703,
      "learning_rate": 3.3573141486810554e-05,
      "loss": 2.3939,
      "step": 5620
    },
    {
      "epoch": 0.987892612739077,
      "grad_norm": 8.605127334594727,
      "learning_rate": 3.354389659004504e-05,
      "loss": 2.9063,
      "step": 5630
    },
    {
      "epoch": 0.9896473065450079,
      "grad_norm": 7.272956371307373,
      "learning_rate": 3.3514651693279525e-05,
      "loss": 2.4226,
      "step": 5640
    },
    {
      "epoch": 0.9914020003509387,
      "grad_norm": 10.674217224121094,
      "learning_rate": 3.3485406796514004e-05,
      "loss": 2.564,
      "step": 5650
    },
    {
      "epoch": 0.9931566941568696,
      "grad_norm": 9.227364540100098,
      "learning_rate": 3.34561618997485e-05,
      "loss": 2.5779,
      "step": 5660
    },
    {
      "epoch": 0.9949113879628005,
      "grad_norm": 7.240214824676514,
      "learning_rate": 3.342691700298298e-05,
      "loss": 2.5684,
      "step": 5670
    },
    {
      "epoch": 0.9966660817687314,
      "grad_norm": 8.824708938598633,
      "learning_rate": 3.339767210621747e-05,
      "loss": 2.379,
      "step": 5680
    },
    {
      "epoch": 0.9984207755746622,
      "grad_norm": 6.851132869720459,
      "learning_rate": 3.3368427209451954e-05,
      "loss": 2.6133,
      "step": 5690
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.25717294024743353,
      "eval_f1_C01": 0.0,
      "eval_f1_C02": 0.0,
      "eval_f1_C03": 0.0,
      "eval_f1_C04": 0.4162729658792651,
      "eval_f1_C05": 0.0,
      "eval_f1_C06": 0.0,
      "eval_f1_C07": 0.0,
      "eval_f1_C08": 0.0,
      "eval_f1_C09": 0.0,
      "eval_f1_C10": 0.0,
      "eval_f1_C11": 0.0,
      "eval_f1_C12": 0.0,
      "eval_f1_C13": 0.0,
      "eval_f1_C14": 0.5370577281191806,
      "eval_f1_C15": 0.0,
      "eval_f1_C16": 0.0,
      "eval_f1_C17": 0.0,
      "eval_f1_C18": 0.0,
      "eval_f1_C19": 0.0,
      "eval_f1_C20": 0.26989984185556143,
      "eval_f1_C21": 0.0,
      "eval_f1_C22": 0.0,
      "eval_f1_C23": 0.2889497262319562,
      "eval_f1_macro": 0.06574696791678102,
      "eval_loss": 2.59120512008667,
      "eval_precision_C01": 1.0,
      "eval_precision_C02": 1.0,
      "eval_precision_C03": 1.0,
      "eval_precision_C04": 0.3117138364779874,
      "eval_precision_C05": 1.0,
      "eval_precision_C06": 1.0,
      "eval_precision_C07": 1.0,
      "eval_precision_C08": 1.0,
      "eval_precision_C09": 1.0,
      "eval_precision_C10": 1.0,
      "eval_precision_C11": 1.0,
      "eval_precision_C12": 1.0,
      "eval_precision_C13": 1.0,
      "eval_precision_C14": 0.49215017064846417,
      "eval_precision_C15": 1.0,
      "eval_precision_C16": 1.0,
      "eval_precision_C17": 1.0,
      "eval_precision_C18": 1.0,
      "eval_precision_C19": 1.0,
      "eval_precision_C20": 0.20094191522762953,
      "eval_precision_C21": 1.0,
      "eval_precision_C22": 1.0,
      "eval_precision_C23": 0.18989205103042198,
      "eval_precision_global": 0.8780303466688915,
      "eval_recall_C01": 0.0,
      "eval_recall_C02": 0.0,
      "eval_recall_C03": 0.0,
      "eval_recall_C04": 0.6263823064770933,
      "eval_recall_C05": 0.0,
      "eval_recall_C06": 0.0,
      "eval_recall_C07": 0.0,
      "eval_recall_C08": 0.0,
      "eval_recall_C09": 0.0,
      "eval_recall_C10": 0.0,
      "eval_recall_C11": 0.0,
      "eval_recall_C12": 0.0,
      "eval_recall_C13": 0.0,
      "eval_recall_C14": 0.590983606557377,
      "eval_recall_C15": 0.0,
      "eval_recall_C16": 0.0,
      "eval_recall_C17": 0.0,
      "eval_recall_C18": 0.0,
      "eval_recall_C19": 0.0,
      "eval_recall_C20": 0.41091492776886035,
      "eval_recall_C21": 0.0,
      "eval_recall_C22": 0.0,
      "eval_recall_C23": 0.6040582726326743,
      "eval_recall_global": 0.09705822232330456,
      "eval_runtime": 51.1123,
      "eval_samples_per_second": 222.98,
      "eval_steps_per_second": 27.88,
      "step": 5699
    },
    {
      "epoch": 1.000175469380593,
      "grad_norm": 7.19379186630249,
      "learning_rate": 3.333918231268644e-05,
      "loss": 2.6007,
      "step": 5700
    },
    {
      "epoch": 1.001930163186524,
      "grad_norm": 8.341737747192383,
      "learning_rate": 3.3309937415920925e-05,
      "loss": 2.7683,
      "step": 5710
    },
    {
      "epoch": 1.0036848569924548,
      "grad_norm": 6.191802501678467,
      "learning_rate": 3.328069251915541e-05,
      "loss": 2.5618,
      "step": 5720
    },
    {
      "epoch": 1.0054395507983858,
      "grad_norm": 7.071993827819824,
      "learning_rate": 3.325144762238989e-05,
      "loss": 2.5889,
      "step": 5730
    },
    {
      "epoch": 1.0071942446043165,
      "grad_norm": 6.677352428436279,
      "learning_rate": 3.322220272562438e-05,
      "loss": 2.558,
      "step": 5740
    },
    {
      "epoch": 1.0089489384102475,
      "grad_norm": 6.485890865325928,
      "learning_rate": 3.319295782885887e-05,
      "loss": 2.8032,
      "step": 5750
    },
    {
      "epoch": 1.0107036322161782,
      "grad_norm": 10.894336700439453,
      "learning_rate": 3.3163712932093346e-05,
      "loss": 2.5611,
      "step": 5760
    },
    {
      "epoch": 1.0124583260221092,
      "grad_norm": 6.681111812591553,
      "learning_rate": 3.313446803532784e-05,
      "loss": 2.5352,
      "step": 5770
    },
    {
      "epoch": 1.01421301982804,
      "grad_norm": 8.374484062194824,
      "learning_rate": 3.3105223138562324e-05,
      "loss": 2.8326,
      "step": 5780
    },
    {
      "epoch": 1.0159677136339709,
      "grad_norm": 6.022005081176758,
      "learning_rate": 3.307597824179681e-05,
      "loss": 2.7252,
      "step": 5790
    },
    {
      "epoch": 1.0177224074399018,
      "grad_norm": 5.6978230476379395,
      "learning_rate": 3.3046733345031295e-05,
      "loss": 2.671,
      "step": 5800
    },
    {
      "epoch": 1.0194771012458326,
      "grad_norm": 8.529339790344238,
      "learning_rate": 3.3017488448265774e-05,
      "loss": 2.5806,
      "step": 5810
    },
    {
      "epoch": 1.0212317950517635,
      "grad_norm": 6.3191657066345215,
      "learning_rate": 3.298824355150027e-05,
      "loss": 2.4727,
      "step": 5820
    },
    {
      "epoch": 1.0229864888576943,
      "grad_norm": 7.655612945556641,
      "learning_rate": 3.295899865473475e-05,
      "loss": 2.7952,
      "step": 5830
    },
    {
      "epoch": 1.0247411826636252,
      "grad_norm": 7.240119934082031,
      "learning_rate": 3.292975375796923e-05,
      "loss": 2.4524,
      "step": 5840
    },
    {
      "epoch": 1.026495876469556,
      "grad_norm": 7.937342643737793,
      "learning_rate": 3.2900508861203723e-05,
      "loss": 2.7693,
      "step": 5850
    },
    {
      "epoch": 1.028250570275487,
      "grad_norm": 9.267410278320312,
      "learning_rate": 3.287126396443821e-05,
      "loss": 2.7154,
      "step": 5860
    },
    {
      "epoch": 1.0300052640814177,
      "grad_norm": 8.225443840026855,
      "learning_rate": 3.284201906767269e-05,
      "loss": 2.7585,
      "step": 5870
    },
    {
      "epoch": 1.0317599578873486,
      "grad_norm": 6.200613975524902,
      "learning_rate": 3.281277417090718e-05,
      "loss": 2.8316,
      "step": 5880
    },
    {
      "epoch": 1.0335146516932796,
      "grad_norm": 9.79472827911377,
      "learning_rate": 3.278352927414166e-05,
      "loss": 2.5969,
      "step": 5890
    },
    {
      "epoch": 1.0352693454992103,
      "grad_norm": 9.139256477355957,
      "learning_rate": 3.275428437737615e-05,
      "loss": 2.5956,
      "step": 5900
    },
    {
      "epoch": 1.0370240393051413,
      "grad_norm": 7.577220916748047,
      "learning_rate": 3.272503948061064e-05,
      "loss": 2.3997,
      "step": 5910
    },
    {
      "epoch": 1.038778733111072,
      "grad_norm": 6.002050399780273,
      "learning_rate": 3.2695794583845116e-05,
      "loss": 2.7892,
      "step": 5920
    },
    {
      "epoch": 1.040533426917003,
      "grad_norm": 8.233250617980957,
      "learning_rate": 3.266654968707961e-05,
      "loss": 2.3606,
      "step": 5930
    },
    {
      "epoch": 1.0422881207229338,
      "grad_norm": 7.028896331787109,
      "learning_rate": 3.2637304790314094e-05,
      "loss": 2.5206,
      "step": 5940
    },
    {
      "epoch": 1.0440428145288647,
      "grad_norm": 7.905579090118408,
      "learning_rate": 3.260805989354857e-05,
      "loss": 2.4142,
      "step": 5950
    },
    {
      "epoch": 1.0457975083347957,
      "grad_norm": 7.364858150482178,
      "learning_rate": 3.2578814996783065e-05,
      "loss": 2.4371,
      "step": 5960
    },
    {
      "epoch": 1.0475522021407264,
      "grad_norm": 7.1374101638793945,
      "learning_rate": 3.254957010001755e-05,
      "loss": 2.6062,
      "step": 5970
    },
    {
      "epoch": 1.0493068959466574,
      "grad_norm": 7.911693572998047,
      "learning_rate": 3.2520325203252037e-05,
      "loss": 2.6909,
      "step": 5980
    },
    {
      "epoch": 1.0510615897525881,
      "grad_norm": 7.774408340454102,
      "learning_rate": 3.249108030648652e-05,
      "loss": 2.6686,
      "step": 5990
    },
    {
      "epoch": 1.052816283558519,
      "grad_norm": 7.225823402404785,
      "learning_rate": 3.2461835409721e-05,
      "loss": 2.6167,
      "step": 6000
    },
    {
      "epoch": 1.0545709773644498,
      "grad_norm": 5.579904079437256,
      "learning_rate": 3.2432590512955493e-05,
      "loss": 2.492,
      "step": 6010
    },
    {
      "epoch": 1.0563256711703808,
      "grad_norm": 6.831628322601318,
      "learning_rate": 3.240334561618998e-05,
      "loss": 2.6537,
      "step": 6020
    },
    {
      "epoch": 1.0580803649763115,
      "grad_norm": 8.93891429901123,
      "learning_rate": 3.237410071942446e-05,
      "loss": 2.7001,
      "step": 6030
    },
    {
      "epoch": 1.0598350587822425,
      "grad_norm": 5.811459541320801,
      "learning_rate": 3.234485582265895e-05,
      "loss": 2.6311,
      "step": 6040
    },
    {
      "epoch": 1.0615897525881735,
      "grad_norm": 6.178510665893555,
      "learning_rate": 3.2315610925893436e-05,
      "loss": 2.728,
      "step": 6050
    },
    {
      "epoch": 1.0633444463941042,
      "grad_norm": 7.5517191886901855,
      "learning_rate": 3.2286366029127915e-05,
      "loss": 2.6273,
      "step": 6060
    },
    {
      "epoch": 1.0650991402000352,
      "grad_norm": 6.247074604034424,
      "learning_rate": 3.225712113236241e-05,
      "loss": 2.6918,
      "step": 6070
    },
    {
      "epoch": 1.066853834005966,
      "grad_norm": 6.949503421783447,
      "learning_rate": 3.2227876235596886e-05,
      "loss": 2.4854,
      "step": 6080
    },
    {
      "epoch": 1.0686085278118969,
      "grad_norm": 7.082756519317627,
      "learning_rate": 3.219863133883138e-05,
      "loss": 2.6069,
      "step": 6090
    },
    {
      "epoch": 1.0703632216178276,
      "grad_norm": 6.480466842651367,
      "learning_rate": 3.2169386442065864e-05,
      "loss": 2.4725,
      "step": 6100
    },
    {
      "epoch": 1.0721179154237586,
      "grad_norm": 8.836786270141602,
      "learning_rate": 3.214014154530034e-05,
      "loss": 2.7903,
      "step": 6110
    },
    {
      "epoch": 1.0738726092296895,
      "grad_norm": 6.099123001098633,
      "learning_rate": 3.2110896648534835e-05,
      "loss": 2.6386,
      "step": 6120
    },
    {
      "epoch": 1.0756273030356203,
      "grad_norm": 7.138528347015381,
      "learning_rate": 3.208165175176932e-05,
      "loss": 2.5985,
      "step": 6130
    },
    {
      "epoch": 1.0773819968415512,
      "grad_norm": 10.45720386505127,
      "learning_rate": 3.20524068550038e-05,
      "loss": 2.5783,
      "step": 6140
    },
    {
      "epoch": 1.079136690647482,
      "grad_norm": 7.413000106811523,
      "learning_rate": 3.202316195823829e-05,
      "loss": 2.5655,
      "step": 6150
    },
    {
      "epoch": 1.080891384453413,
      "grad_norm": 5.859747886657715,
      "learning_rate": 3.199391706147277e-05,
      "loss": 2.6069,
      "step": 6160
    },
    {
      "epoch": 1.0826460782593437,
      "grad_norm": 11.136911392211914,
      "learning_rate": 3.196467216470726e-05,
      "loss": 2.4623,
      "step": 6170
    },
    {
      "epoch": 1.0844007720652746,
      "grad_norm": 4.939743995666504,
      "learning_rate": 3.193542726794175e-05,
      "loss": 2.4923,
      "step": 6180
    },
    {
      "epoch": 1.0861554658712054,
      "grad_norm": 6.8476643562316895,
      "learning_rate": 3.190618237117623e-05,
      "loss": 2.6705,
      "step": 6190
    },
    {
      "epoch": 1.0879101596771363,
      "grad_norm": 7.275580883026123,
      "learning_rate": 3.187693747441072e-05,
      "loss": 2.3671,
      "step": 6200
    },
    {
      "epoch": 1.0896648534830673,
      "grad_norm": 7.491101264953613,
      "learning_rate": 3.1847692577645206e-05,
      "loss": 2.642,
      "step": 6210
    },
    {
      "epoch": 1.091419547288998,
      "grad_norm": 6.865249156951904,
      "learning_rate": 3.1818447680879685e-05,
      "loss": 2.5363,
      "step": 6220
    },
    {
      "epoch": 1.093174241094929,
      "grad_norm": 9.786487579345703,
      "learning_rate": 3.178920278411418e-05,
      "loss": 2.4837,
      "step": 6230
    },
    {
      "epoch": 1.0949289349008597,
      "grad_norm": 9.36767292022705,
      "learning_rate": 3.1759957887348656e-05,
      "loss": 2.838,
      "step": 6240
    },
    {
      "epoch": 1.0966836287067907,
      "grad_norm": 6.449735164642334,
      "learning_rate": 3.173071299058314e-05,
      "loss": 2.5446,
      "step": 6250
    },
    {
      "epoch": 1.0984383225127214,
      "grad_norm": 6.02560567855835,
      "learning_rate": 3.1701468093817634e-05,
      "loss": 2.434,
      "step": 6260
    },
    {
      "epoch": 1.1001930163186524,
      "grad_norm": 8.025788307189941,
      "learning_rate": 3.167222319705211e-05,
      "loss": 2.624,
      "step": 6270
    },
    {
      "epoch": 1.1019477101245831,
      "grad_norm": 6.879753112792969,
      "learning_rate": 3.1642978300286605e-05,
      "loss": 2.5248,
      "step": 6280
    },
    {
      "epoch": 1.103702403930514,
      "grad_norm": 8.463440895080566,
      "learning_rate": 3.161373340352109e-05,
      "loss": 2.5133,
      "step": 6290
    },
    {
      "epoch": 1.105457097736445,
      "grad_norm": 7.685964107513428,
      "learning_rate": 3.158448850675557e-05,
      "loss": 2.5252,
      "step": 6300
    },
    {
      "epoch": 1.1072117915423758,
      "grad_norm": 5.959208011627197,
      "learning_rate": 3.155524360999006e-05,
      "loss": 2.7887,
      "step": 6310
    },
    {
      "epoch": 1.1089664853483068,
      "grad_norm": 7.703197479248047,
      "learning_rate": 3.152599871322454e-05,
      "loss": 2.4877,
      "step": 6320
    },
    {
      "epoch": 1.1107211791542375,
      "grad_norm": 10.262956619262695,
      "learning_rate": 3.1496753816459027e-05,
      "loss": 2.4841,
      "step": 6330
    },
    {
      "epoch": 1.1124758729601685,
      "grad_norm": 7.785036563873291,
      "learning_rate": 3.146750891969352e-05,
      "loss": 2.7381,
      "step": 6340
    },
    {
      "epoch": 1.1142305667660992,
      "grad_norm": 8.491252899169922,
      "learning_rate": 3.1438264022928e-05,
      "loss": 2.6043,
      "step": 6350
    },
    {
      "epoch": 1.1159852605720302,
      "grad_norm": 10.004986763000488,
      "learning_rate": 3.1409019126162483e-05,
      "loss": 2.6379,
      "step": 6360
    },
    {
      "epoch": 1.1177399543779611,
      "grad_norm": 7.900562286376953,
      "learning_rate": 3.1379774229396976e-05,
      "loss": 2.5961,
      "step": 6370
    },
    {
      "epoch": 1.1194946481838919,
      "grad_norm": 7.843745231628418,
      "learning_rate": 3.1350529332631455e-05,
      "loss": 2.7957,
      "step": 6380
    },
    {
      "epoch": 1.1212493419898228,
      "grad_norm": 9.447955131530762,
      "learning_rate": 3.132128443586595e-05,
      "loss": 2.494,
      "step": 6390
    },
    {
      "epoch": 1.1230040357957536,
      "grad_norm": 6.303800106048584,
      "learning_rate": 3.1292039539100426e-05,
      "loss": 2.6568,
      "step": 6400
    },
    {
      "epoch": 1.1247587296016845,
      "grad_norm": 8.54776382446289,
      "learning_rate": 3.126279464233491e-05,
      "loss": 2.547,
      "step": 6410
    },
    {
      "epoch": 1.1265134234076153,
      "grad_norm": 9.025435447692871,
      "learning_rate": 3.1233549745569404e-05,
      "loss": 2.6422,
      "step": 6420
    },
    {
      "epoch": 1.1282681172135463,
      "grad_norm": 6.1790971755981445,
      "learning_rate": 3.120430484880388e-05,
      "loss": 2.4851,
      "step": 6430
    },
    {
      "epoch": 1.1300228110194772,
      "grad_norm": 6.996207237243652,
      "learning_rate": 3.117505995203837e-05,
      "loss": 2.5091,
      "step": 6440
    },
    {
      "epoch": 1.131777504825408,
      "grad_norm": 11.503508567810059,
      "learning_rate": 3.114581505527286e-05,
      "loss": 2.6612,
      "step": 6450
    },
    {
      "epoch": 1.133532198631339,
      "grad_norm": 8.59346866607666,
      "learning_rate": 3.111657015850734e-05,
      "loss": 2.3507,
      "step": 6460
    },
    {
      "epoch": 1.1352868924372697,
      "grad_norm": 9.37136173248291,
      "learning_rate": 3.108732526174183e-05,
      "loss": 2.7405,
      "step": 6470
    },
    {
      "epoch": 1.1370415862432006,
      "grad_norm": 7.014244556427002,
      "learning_rate": 3.105808036497631e-05,
      "loss": 2.428,
      "step": 6480
    },
    {
      "epoch": 1.1387962800491314,
      "grad_norm": 6.549679756164551,
      "learning_rate": 3.1028835468210796e-05,
      "loss": 2.7414,
      "step": 6490
    },
    {
      "epoch": 1.1405509738550623,
      "grad_norm": 7.035908222198486,
      "learning_rate": 3.099959057144529e-05,
      "loss": 2.7412,
      "step": 6500
    },
    {
      "epoch": 1.142305667660993,
      "grad_norm": 6.67206335067749,
      "learning_rate": 3.097034567467977e-05,
      "loss": 2.6334,
      "step": 6510
    },
    {
      "epoch": 1.144060361466924,
      "grad_norm": 8.21670913696289,
      "learning_rate": 3.094110077791425e-05,
      "loss": 2.6265,
      "step": 6520
    },
    {
      "epoch": 1.145815055272855,
      "grad_norm": 6.8239898681640625,
      "learning_rate": 3.0911855881148746e-05,
      "loss": 2.6309,
      "step": 6530
    },
    {
      "epoch": 1.1475697490787857,
      "grad_norm": 11.369582176208496,
      "learning_rate": 3.0882610984383225e-05,
      "loss": 2.5347,
      "step": 6540
    },
    {
      "epoch": 1.1493244428847167,
      "grad_norm": 7.922140598297119,
      "learning_rate": 3.085336608761771e-05,
      "loss": 2.5357,
      "step": 6550
    },
    {
      "epoch": 1.1510791366906474,
      "grad_norm": 9.255209922790527,
      "learning_rate": 3.0824121190852196e-05,
      "loss": 2.7001,
      "step": 6560
    },
    {
      "epoch": 1.1528338304965784,
      "grad_norm": 7.985259532928467,
      "learning_rate": 3.079487629408668e-05,
      "loss": 2.5374,
      "step": 6570
    },
    {
      "epoch": 1.1545885243025091,
      "grad_norm": 7.038367748260498,
      "learning_rate": 3.0765631397321174e-05,
      "loss": 2.5303,
      "step": 6580
    },
    {
      "epoch": 1.15634321810844,
      "grad_norm": 7.516530990600586,
      "learning_rate": 3.073638650055565e-05,
      "loss": 2.6424,
      "step": 6590
    },
    {
      "epoch": 1.1580979119143708,
      "grad_norm": 7.897937297821045,
      "learning_rate": 3.070714160379014e-05,
      "loss": 2.6245,
      "step": 6600
    },
    {
      "epoch": 1.1598526057203018,
      "grad_norm": 8.484212875366211,
      "learning_rate": 3.067789670702463e-05,
      "loss": 2.5915,
      "step": 6610
    },
    {
      "epoch": 1.1616072995262328,
      "grad_norm": 7.852838516235352,
      "learning_rate": 3.064865181025911e-05,
      "loss": 2.5104,
      "step": 6620
    },
    {
      "epoch": 1.1633619933321635,
      "grad_norm": 6.956040382385254,
      "learning_rate": 3.0619406913493595e-05,
      "loss": 2.6439,
      "step": 6630
    },
    {
      "epoch": 1.1651166871380945,
      "grad_norm": 8.169350624084473,
      "learning_rate": 3.059016201672808e-05,
      "loss": 2.6326,
      "step": 6640
    },
    {
      "epoch": 1.1668713809440252,
      "grad_norm": 6.9605607986450195,
      "learning_rate": 3.0560917119962566e-05,
      "loss": 2.4726,
      "step": 6650
    },
    {
      "epoch": 1.1686260747499562,
      "grad_norm": 7.406200408935547,
      "learning_rate": 3.053167222319705e-05,
      "loss": 2.6945,
      "step": 6660
    },
    {
      "epoch": 1.170380768555887,
      "grad_norm": 9.741435050964355,
      "learning_rate": 3.050535181610809e-05,
      "loss": 2.424,
      "step": 6670
    },
    {
      "epoch": 1.1721354623618179,
      "grad_norm": 7.3329057693481445,
      "learning_rate": 3.047610691934258e-05,
      "loss": 2.4586,
      "step": 6680
    },
    {
      "epoch": 1.1738901561677486,
      "grad_norm": 7.076883316040039,
      "learning_rate": 3.044686202257706e-05,
      "loss": 2.5689,
      "step": 6690
    },
    {
      "epoch": 1.1756448499736796,
      "grad_norm": 7.425924301147461,
      "learning_rate": 3.0417617125811547e-05,
      "loss": 2.5682,
      "step": 6700
    },
    {
      "epoch": 1.1773995437796105,
      "grad_norm": 11.752230644226074,
      "learning_rate": 3.038837222904603e-05,
      "loss": 2.556,
      "step": 6710
    },
    {
      "epoch": 1.1791542375855413,
      "grad_norm": 8.988065719604492,
      "learning_rate": 3.0359127332280518e-05,
      "loss": 2.6565,
      "step": 6720
    },
    {
      "epoch": 1.1809089313914722,
      "grad_norm": 7.568305969238281,
      "learning_rate": 3.0329882435515007e-05,
      "loss": 2.7344,
      "step": 6730
    },
    {
      "epoch": 1.182663625197403,
      "grad_norm": 7.604425430297852,
      "learning_rate": 3.030063753874949e-05,
      "loss": 2.6259,
      "step": 6740
    },
    {
      "epoch": 1.184418319003334,
      "grad_norm": 6.596051216125488,
      "learning_rate": 3.0271392641983975e-05,
      "loss": 2.7027,
      "step": 6750
    },
    {
      "epoch": 1.186173012809265,
      "grad_norm": 8.312219619750977,
      "learning_rate": 3.0242147745218464e-05,
      "loss": 2.5949,
      "step": 6760
    },
    {
      "epoch": 1.1879277066151956,
      "grad_norm": 7.302507400512695,
      "learning_rate": 3.0212902848452946e-05,
      "loss": 2.4954,
      "step": 6770
    },
    {
      "epoch": 1.1896824004211266,
      "grad_norm": 7.932557106018066,
      "learning_rate": 3.018365795168743e-05,
      "loss": 2.6739,
      "step": 6780
    },
    {
      "epoch": 1.1914370942270573,
      "grad_norm": 6.919439315795898,
      "learning_rate": 3.0154413054921914e-05,
      "loss": 2.9016,
      "step": 6790
    },
    {
      "epoch": 1.1931917880329883,
      "grad_norm": 6.23912239074707,
      "learning_rate": 3.0125168158156403e-05,
      "loss": 2.7264,
      "step": 6800
    },
    {
      "epoch": 1.194946481838919,
      "grad_norm": 6.879748344421387,
      "learning_rate": 3.0095923261390892e-05,
      "loss": 2.4986,
      "step": 6810
    },
    {
      "epoch": 1.19670117564485,
      "grad_norm": 8.823283195495605,
      "learning_rate": 3.0066678364625374e-05,
      "loss": 2.7736,
      "step": 6820
    },
    {
      "epoch": 1.1984558694507808,
      "grad_norm": 6.479238986968994,
      "learning_rate": 3.003743346785986e-05,
      "loss": 2.709,
      "step": 6830
    },
    {
      "epoch": 1.2002105632567117,
      "grad_norm": 7.247750282287598,
      "learning_rate": 3.000818857109435e-05,
      "loss": 2.7557,
      "step": 6840
    },
    {
      "epoch": 1.2019652570626427,
      "grad_norm": 10.896687507629395,
      "learning_rate": 2.997894367432883e-05,
      "loss": 2.6037,
      "step": 6850
    },
    {
      "epoch": 1.2037199508685734,
      "grad_norm": 7.371707916259766,
      "learning_rate": 2.9949698777563317e-05,
      "loss": 2.4398,
      "step": 6860
    },
    {
      "epoch": 1.2054746446745044,
      "grad_norm": 7.1012725830078125,
      "learning_rate": 2.99204538807978e-05,
      "loss": 2.7469,
      "step": 6870
    },
    {
      "epoch": 1.2072293384804351,
      "grad_norm": 6.8232316970825195,
      "learning_rate": 2.9891208984032288e-05,
      "loss": 2.7375,
      "step": 6880
    },
    {
      "epoch": 1.208984032286366,
      "grad_norm": 7.817514896392822,
      "learning_rate": 2.9861964087266773e-05,
      "loss": 2.7624,
      "step": 6890
    },
    {
      "epoch": 1.2107387260922968,
      "grad_norm": 6.436650276184082,
      "learning_rate": 2.9832719190501256e-05,
      "loss": 2.6658,
      "step": 6900
    },
    {
      "epoch": 1.2124934198982278,
      "grad_norm": 7.3881659507751465,
      "learning_rate": 2.9803474293735745e-05,
      "loss": 2.8021,
      "step": 6910
    },
    {
      "epoch": 1.2142481137041585,
      "grad_norm": 8.798844337463379,
      "learning_rate": 2.9774229396970234e-05,
      "loss": 2.8071,
      "step": 6920
    },
    {
      "epoch": 1.2160028075100895,
      "grad_norm": 6.865242004394531,
      "learning_rate": 2.9744984500204716e-05,
      "loss": 2.7148,
      "step": 6930
    },
    {
      "epoch": 1.2177575013160205,
      "grad_norm": 6.732700824737549,
      "learning_rate": 2.97157396034392e-05,
      "loss": 2.6898,
      "step": 6940
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 7.394552230834961,
      "learning_rate": 2.9686494706673684e-05,
      "loss": 2.6754,
      "step": 6950
    },
    {
      "epoch": 1.2212668889278822,
      "grad_norm": 8.938925743103027,
      "learning_rate": 2.9657249809908173e-05,
      "loss": 2.593,
      "step": 6960
    },
    {
      "epoch": 1.223021582733813,
      "grad_norm": 6.594483375549316,
      "learning_rate": 2.962800491314266e-05,
      "loss": 2.7291,
      "step": 6970
    },
    {
      "epoch": 1.2247762765397439,
      "grad_norm": 11.093573570251465,
      "learning_rate": 2.959876001637714e-05,
      "loss": 2.4998,
      "step": 6980
    },
    {
      "epoch": 1.2265309703456746,
      "grad_norm": 5.420322895050049,
      "learning_rate": 2.956951511961163e-05,
      "loss": 2.6868,
      "step": 6990
    },
    {
      "epoch": 1.2282856641516056,
      "grad_norm": 8.576526641845703,
      "learning_rate": 2.954027022284612e-05,
      "loss": 2.4839,
      "step": 7000
    },
    {
      "epoch": 1.2300403579575363,
      "grad_norm": 6.819916725158691,
      "learning_rate": 2.9511025326080598e-05,
      "loss": 2.5906,
      "step": 7010
    },
    {
      "epoch": 1.2317950517634673,
      "grad_norm": 6.998815059661865,
      "learning_rate": 2.9481780429315087e-05,
      "loss": 2.5816,
      "step": 7020
    },
    {
      "epoch": 1.2335497455693982,
      "grad_norm": 6.3592305183410645,
      "learning_rate": 2.945253553254957e-05,
      "loss": 2.5755,
      "step": 7030
    },
    {
      "epoch": 1.235304439375329,
      "grad_norm": 6.637050628662109,
      "learning_rate": 2.9423290635784058e-05,
      "loss": 2.6976,
      "step": 7040
    },
    {
      "epoch": 1.23705913318126,
      "grad_norm": 10.050164222717285,
      "learning_rate": 2.9394045739018543e-05,
      "loss": 2.5192,
      "step": 7050
    },
    {
      "epoch": 1.2388138269871907,
      "grad_norm": 10.817394256591797,
      "learning_rate": 2.9364800842253026e-05,
      "loss": 2.4023,
      "step": 7060
    },
    {
      "epoch": 1.2405685207931216,
      "grad_norm": 9.119894981384277,
      "learning_rate": 2.9335555945487515e-05,
      "loss": 2.4698,
      "step": 7070
    },
    {
      "epoch": 1.2423232145990524,
      "grad_norm": 7.0750837326049805,
      "learning_rate": 2.9306311048722e-05,
      "loss": 2.3988,
      "step": 7080
    },
    {
      "epoch": 1.2440779084049833,
      "grad_norm": 8.144158363342285,
      "learning_rate": 2.9277066151956482e-05,
      "loss": 2.5596,
      "step": 7090
    },
    {
      "epoch": 1.245832602210914,
      "grad_norm": 8.476472854614258,
      "learning_rate": 2.924782125519097e-05,
      "loss": 2.5198,
      "step": 7100
    },
    {
      "epoch": 1.247587296016845,
      "grad_norm": 10.587729454040527,
      "learning_rate": 2.9218576358425454e-05,
      "loss": 2.6467,
      "step": 7110
    },
    {
      "epoch": 1.249341989822776,
      "grad_norm": 7.2170634269714355,
      "learning_rate": 2.9189331461659943e-05,
      "loss": 2.8147,
      "step": 7120
    },
    {
      "epoch": 1.2510966836287067,
      "grad_norm": 7.3130292892456055,
      "learning_rate": 2.916008656489443e-05,
      "loss": 2.5564,
      "step": 7130
    },
    {
      "epoch": 1.2528513774346377,
      "grad_norm": 8.703418731689453,
      "learning_rate": 2.913084166812891e-05,
      "loss": 2.5442,
      "step": 7140
    },
    {
      "epoch": 1.2546060712405684,
      "grad_norm": 6.491286754608154,
      "learning_rate": 2.91015967713634e-05,
      "loss": 2.521,
      "step": 7150
    },
    {
      "epoch": 1.2563607650464994,
      "grad_norm": 10.904858589172363,
      "learning_rate": 2.9072351874597885e-05,
      "loss": 2.6877,
      "step": 7160
    },
    {
      "epoch": 1.2581154588524304,
      "grad_norm": 10.29307746887207,
      "learning_rate": 2.9043106977832367e-05,
      "loss": 2.5628,
      "step": 7170
    },
    {
      "epoch": 1.259870152658361,
      "grad_norm": 7.937627792358398,
      "learning_rate": 2.9013862081066856e-05,
      "loss": 2.76,
      "step": 7180
    },
    {
      "epoch": 1.2616248464642918,
      "grad_norm": 9.398367881774902,
      "learning_rate": 2.898461718430134e-05,
      "loss": 2.5069,
      "step": 7190
    },
    {
      "epoch": 1.2633795402702228,
      "grad_norm": 6.133733749389648,
      "learning_rate": 2.8955372287535824e-05,
      "loss": 2.5132,
      "step": 7200
    },
    {
      "epoch": 1.2651342340761538,
      "grad_norm": 7.139654159545898,
      "learning_rate": 2.8926127390770313e-05,
      "loss": 2.6159,
      "step": 7210
    },
    {
      "epoch": 1.2668889278820845,
      "grad_norm": 5.896698951721191,
      "learning_rate": 2.8896882494004796e-05,
      "loss": 2.5075,
      "step": 7220
    },
    {
      "epoch": 1.2686436216880155,
      "grad_norm": 8.485532760620117,
      "learning_rate": 2.8867637597239285e-05,
      "loss": 2.35,
      "step": 7230
    },
    {
      "epoch": 1.2703983154939462,
      "grad_norm": 7.459963798522949,
      "learning_rate": 2.883839270047377e-05,
      "loss": 2.4537,
      "step": 7240
    },
    {
      "epoch": 1.2721530092998772,
      "grad_norm": 6.758697509765625,
      "learning_rate": 2.8809147803708252e-05,
      "loss": 2.6329,
      "step": 7250
    },
    {
      "epoch": 1.2739077031058081,
      "grad_norm": 7.917322158813477,
      "learning_rate": 2.877990290694274e-05,
      "loss": 2.4136,
      "step": 7260
    },
    {
      "epoch": 1.2756623969117389,
      "grad_norm": 9.158447265625,
      "learning_rate": 2.8750658010177224e-05,
      "loss": 2.6527,
      "step": 7270
    },
    {
      "epoch": 1.2774170907176698,
      "grad_norm": 7.89088773727417,
      "learning_rate": 2.872141311341171e-05,
      "loss": 2.4328,
      "step": 7280
    },
    {
      "epoch": 1.2791717845236006,
      "grad_norm": 8.227778434753418,
      "learning_rate": 2.8692168216646198e-05,
      "loss": 2.5413,
      "step": 7290
    },
    {
      "epoch": 1.2809264783295315,
      "grad_norm": 11.159090042114258,
      "learning_rate": 2.866292331988068e-05,
      "loss": 2.4606,
      "step": 7300
    },
    {
      "epoch": 1.2826811721354623,
      "grad_norm": 12.332362174987793,
      "learning_rate": 2.8633678423115166e-05,
      "loss": 2.725,
      "step": 7310
    },
    {
      "epoch": 1.2844358659413933,
      "grad_norm": 7.988089561462402,
      "learning_rate": 2.8604433526349655e-05,
      "loss": 2.5961,
      "step": 7320
    },
    {
      "epoch": 1.286190559747324,
      "grad_norm": 6.451818943023682,
      "learning_rate": 2.8575188629584137e-05,
      "loss": 2.3843,
      "step": 7330
    },
    {
      "epoch": 1.287945253553255,
      "grad_norm": 8.297503471374512,
      "learning_rate": 2.8545943732818626e-05,
      "loss": 2.5488,
      "step": 7340
    },
    {
      "epoch": 1.289699947359186,
      "grad_norm": 8.46206283569336,
      "learning_rate": 2.851669883605311e-05,
      "loss": 2.5654,
      "step": 7350
    },
    {
      "epoch": 1.2914546411651167,
      "grad_norm": 8.057948112487793,
      "learning_rate": 2.8487453939287594e-05,
      "loss": 2.807,
      "step": 7360
    },
    {
      "epoch": 1.2932093349710476,
      "grad_norm": 6.796939373016357,
      "learning_rate": 2.8458209042522083e-05,
      "loss": 2.4982,
      "step": 7370
    },
    {
      "epoch": 1.2949640287769784,
      "grad_norm": 10.305560111999512,
      "learning_rate": 2.8428964145756565e-05,
      "loss": 2.3951,
      "step": 7380
    },
    {
      "epoch": 1.2967187225829093,
      "grad_norm": 7.181297779083252,
      "learning_rate": 2.839971924899105e-05,
      "loss": 2.7059,
      "step": 7390
    },
    {
      "epoch": 1.2984734163888403,
      "grad_norm": 10.033380508422852,
      "learning_rate": 2.837047435222554e-05,
      "loss": 2.5347,
      "step": 7400
    },
    {
      "epoch": 1.300228110194771,
      "grad_norm": 8.194957733154297,
      "learning_rate": 2.8341229455460022e-05,
      "loss": 2.4715,
      "step": 7410
    },
    {
      "epoch": 1.3019828040007018,
      "grad_norm": 7.715719223022461,
      "learning_rate": 2.831198455869451e-05,
      "loss": 2.6662,
      "step": 7420
    },
    {
      "epoch": 1.3037374978066327,
      "grad_norm": 5.320291519165039,
      "learning_rate": 2.8282739661928997e-05,
      "loss": 2.5924,
      "step": 7430
    },
    {
      "epoch": 1.3054921916125637,
      "grad_norm": 6.0370917320251465,
      "learning_rate": 2.825349476516348e-05,
      "loss": 2.3677,
      "step": 7440
    },
    {
      "epoch": 1.3072468854184944,
      "grad_norm": 7.080707550048828,
      "learning_rate": 2.8224249868397968e-05,
      "loss": 2.4747,
      "step": 7450
    },
    {
      "epoch": 1.3090015792244254,
      "grad_norm": 5.780218124389648,
      "learning_rate": 2.819500497163245e-05,
      "loss": 2.5796,
      "step": 7460
    },
    {
      "epoch": 1.3107562730303561,
      "grad_norm": 5.904273986816406,
      "learning_rate": 2.8165760074866936e-05,
      "loss": 2.7945,
      "step": 7470
    },
    {
      "epoch": 1.312510966836287,
      "grad_norm": 7.834219932556152,
      "learning_rate": 2.8136515178101425e-05,
      "loss": 2.5657,
      "step": 7480
    },
    {
      "epoch": 1.314265660642218,
      "grad_norm": 7.412631034851074,
      "learning_rate": 2.8107270281335907e-05,
      "loss": 2.2352,
      "step": 7490
    },
    {
      "epoch": 1.3160203544481488,
      "grad_norm": 6.77131462097168,
      "learning_rate": 2.8078025384570393e-05,
      "loss": 2.655,
      "step": 7500
    },
    {
      "epoch": 1.3177750482540795,
      "grad_norm": 6.985350131988525,
      "learning_rate": 2.8048780487804882e-05,
      "loss": 2.6228,
      "step": 7510
    },
    {
      "epoch": 1.3195297420600105,
      "grad_norm": 8.582695007324219,
      "learning_rate": 2.8019535591039364e-05,
      "loss": 2.6517,
      "step": 7520
    },
    {
      "epoch": 1.3212844358659415,
      "grad_norm": 6.723232269287109,
      "learning_rate": 2.7990290694273853e-05,
      "loss": 2.4357,
      "step": 7530
    },
    {
      "epoch": 1.3230391296718722,
      "grad_norm": 6.506564617156982,
      "learning_rate": 2.7961045797508335e-05,
      "loss": 2.611,
      "step": 7540
    },
    {
      "epoch": 1.3247938234778032,
      "grad_norm": 6.097020626068115,
      "learning_rate": 2.793180090074282e-05,
      "loss": 2.6499,
      "step": 7550
    },
    {
      "epoch": 1.326548517283734,
      "grad_norm": 7.380405426025391,
      "learning_rate": 2.790255600397731e-05,
      "loss": 2.5242,
      "step": 7560
    },
    {
      "epoch": 1.3283032110896649,
      "grad_norm": 9.392109870910645,
      "learning_rate": 2.7873311107211792e-05,
      "loss": 2.4864,
      "step": 7570
    },
    {
      "epoch": 1.3300579048955958,
      "grad_norm": 8.75788402557373,
      "learning_rate": 2.7844066210446278e-05,
      "loss": 2.4404,
      "step": 7580
    },
    {
      "epoch": 1.3318125987015266,
      "grad_norm": 5.321434020996094,
      "learning_rate": 2.7814821313680767e-05,
      "loss": 2.2981,
      "step": 7590
    },
    {
      "epoch": 1.3335672925074573,
      "grad_norm": 6.157309532165527,
      "learning_rate": 2.778557641691525e-05,
      "loss": 2.4988,
      "step": 7600
    },
    {
      "epoch": 1.3353219863133883,
      "grad_norm": 9.383076667785645,
      "learning_rate": 2.7756331520149738e-05,
      "loss": 2.8307,
      "step": 7610
    },
    {
      "epoch": 1.3370766801193192,
      "grad_norm": 8.826440811157227,
      "learning_rate": 2.7727086623384217e-05,
      "loss": 2.7284,
      "step": 7620
    },
    {
      "epoch": 1.33883137392525,
      "grad_norm": 8.147628784179688,
      "learning_rate": 2.7697841726618706e-05,
      "loss": 2.642,
      "step": 7630
    },
    {
      "epoch": 1.340586067731181,
      "grad_norm": 9.24235725402832,
      "learning_rate": 2.7668596829853195e-05,
      "loss": 2.4803,
      "step": 7640
    },
    {
      "epoch": 1.3423407615371117,
      "grad_norm": 8.453271865844727,
      "learning_rate": 2.7639351933087677e-05,
      "loss": 2.573,
      "step": 7650
    },
    {
      "epoch": 1.3440954553430426,
      "grad_norm": 9.14659595489502,
      "learning_rate": 2.7610107036322163e-05,
      "loss": 2.4065,
      "step": 7660
    },
    {
      "epoch": 1.3458501491489736,
      "grad_norm": 9.763973236083984,
      "learning_rate": 2.7580862139556652e-05,
      "loss": 2.429,
      "step": 7670
    },
    {
      "epoch": 1.3476048429549043,
      "grad_norm": 7.388269901275635,
      "learning_rate": 2.7551617242791134e-05,
      "loss": 2.6632,
      "step": 7680
    },
    {
      "epoch": 1.3493595367608353,
      "grad_norm": 6.144397258758545,
      "learning_rate": 2.752237234602562e-05,
      "loss": 2.4663,
      "step": 7690
    },
    {
      "epoch": 1.351114230566766,
      "grad_norm": 7.6530232429504395,
      "learning_rate": 2.7493127449260102e-05,
      "loss": 2.7561,
      "step": 7700
    },
    {
      "epoch": 1.352868924372697,
      "grad_norm": 10.762320518493652,
      "learning_rate": 2.746388255249459e-05,
      "loss": 2.5712,
      "step": 7710
    },
    {
      "epoch": 1.3546236181786278,
      "grad_norm": 6.7159647941589355,
      "learning_rate": 2.743463765572908e-05,
      "loss": 2.2566,
      "step": 7720
    },
    {
      "epoch": 1.3563783119845587,
      "grad_norm": 6.784246921539307,
      "learning_rate": 2.7405392758963562e-05,
      "loss": 2.5471,
      "step": 7730
    },
    {
      "epoch": 1.3581330057904895,
      "grad_norm": 8.811464309692383,
      "learning_rate": 2.7376147862198048e-05,
      "loss": 2.6408,
      "step": 7740
    },
    {
      "epoch": 1.3598876995964204,
      "grad_norm": 7.084649085998535,
      "learning_rate": 2.7346902965432537e-05,
      "loss": 2.5254,
      "step": 7750
    },
    {
      "epoch": 1.3616423934023514,
      "grad_norm": 7.637373924255371,
      "learning_rate": 2.731765806866702e-05,
      "loss": 2.4144,
      "step": 7760
    },
    {
      "epoch": 1.3633970872082821,
      "grad_norm": 8.775015830993652,
      "learning_rate": 2.7288413171901505e-05,
      "loss": 2.6267,
      "step": 7770
    },
    {
      "epoch": 1.365151781014213,
      "grad_norm": 7.469054222106934,
      "learning_rate": 2.7259168275135987e-05,
      "loss": 2.7057,
      "step": 7780
    },
    {
      "epoch": 1.3669064748201438,
      "grad_norm": 9.865140914916992,
      "learning_rate": 2.7229923378370476e-05,
      "loss": 2.6192,
      "step": 7790
    },
    {
      "epoch": 1.3686611686260748,
      "grad_norm": 6.580178260803223,
      "learning_rate": 2.720067848160496e-05,
      "loss": 2.642,
      "step": 7800
    },
    {
      "epoch": 1.3704158624320057,
      "grad_norm": 8.0690336227417,
      "learning_rate": 2.7171433584839444e-05,
      "loss": 2.0875,
      "step": 7810
    },
    {
      "epoch": 1.3721705562379365,
      "grad_norm": 8.734535217285156,
      "learning_rate": 2.7142188688073933e-05,
      "loss": 2.5036,
      "step": 7820
    },
    {
      "epoch": 1.3739252500438672,
      "grad_norm": 7.878689289093018,
      "learning_rate": 2.7112943791308422e-05,
      "loss": 2.4848,
      "step": 7830
    },
    {
      "epoch": 1.3756799438497982,
      "grad_norm": 8.365216255187988,
      "learning_rate": 2.7083698894542904e-05,
      "loss": 2.4061,
      "step": 7840
    },
    {
      "epoch": 1.3774346376557292,
      "grad_norm": 9.654009819030762,
      "learning_rate": 2.705445399777739e-05,
      "loss": 2.4931,
      "step": 7850
    },
    {
      "epoch": 1.37918933146166,
      "grad_norm": 9.925758361816406,
      "learning_rate": 2.7025209101011872e-05,
      "loss": 2.6718,
      "step": 7860
    },
    {
      "epoch": 1.3809440252675909,
      "grad_norm": 6.667597770690918,
      "learning_rate": 2.699596420424636e-05,
      "loss": 2.7204,
      "step": 7870
    },
    {
      "epoch": 1.3826987190735216,
      "grad_norm": 9.797100067138672,
      "learning_rate": 2.6966719307480847e-05,
      "loss": 2.636,
      "step": 7880
    },
    {
      "epoch": 1.3844534128794526,
      "grad_norm": 7.063807964324951,
      "learning_rate": 2.693747441071533e-05,
      "loss": 2.5399,
      "step": 7890
    },
    {
      "epoch": 1.3862081066853835,
      "grad_norm": 6.776360988616943,
      "learning_rate": 2.6908229513949818e-05,
      "loss": 2.3736,
      "step": 7900
    },
    {
      "epoch": 1.3879628004913143,
      "grad_norm": 6.595865726470947,
      "learning_rate": 2.6878984617184307e-05,
      "loss": 2.4728,
      "step": 7910
    },
    {
      "epoch": 1.389717494297245,
      "grad_norm": 9.204446792602539,
      "learning_rate": 2.6849739720418786e-05,
      "loss": 2.6401,
      "step": 7920
    },
    {
      "epoch": 1.391472188103176,
      "grad_norm": 7.545524597167969,
      "learning_rate": 2.6820494823653275e-05,
      "loss": 2.6108,
      "step": 7930
    },
    {
      "epoch": 1.393226881909107,
      "grad_norm": 7.157631874084473,
      "learning_rate": 2.6791249926887757e-05,
      "loss": 2.4232,
      "step": 7940
    },
    {
      "epoch": 1.3949815757150377,
      "grad_norm": 8.702146530151367,
      "learning_rate": 2.6762005030122246e-05,
      "loss": 2.8095,
      "step": 7950
    },
    {
      "epoch": 1.3967362695209686,
      "grad_norm": 6.015751838684082,
      "learning_rate": 2.673276013335673e-05,
      "loss": 2.4506,
      "step": 7960
    },
    {
      "epoch": 1.3984909633268994,
      "grad_norm": 8.630348205566406,
      "learning_rate": 2.6703515236591214e-05,
      "loss": 2.4954,
      "step": 7970
    },
    {
      "epoch": 1.4002456571328303,
      "grad_norm": 8.270917892456055,
      "learning_rate": 2.6674270339825703e-05,
      "loss": 2.5618,
      "step": 7980
    },
    {
      "epoch": 1.4020003509387613,
      "grad_norm": 10.097850799560547,
      "learning_rate": 2.664502544306019e-05,
      "loss": 2.551,
      "step": 7990
    },
    {
      "epoch": 1.403755044744692,
      "grad_norm": 5.44428825378418,
      "learning_rate": 2.661578054629467e-05,
      "loss": 2.578,
      "step": 8000
    },
    {
      "epoch": 1.4055097385506228,
      "grad_norm": 7.563560962677002,
      "learning_rate": 2.658653564952916e-05,
      "loss": 2.3831,
      "step": 8010
    },
    {
      "epoch": 1.4072644323565537,
      "grad_norm": 7.976158142089844,
      "learning_rate": 2.6557290752763642e-05,
      "loss": 2.6313,
      "step": 8020
    },
    {
      "epoch": 1.4090191261624847,
      "grad_norm": 6.653617858886719,
      "learning_rate": 2.652804585599813e-05,
      "loss": 2.6744,
      "step": 8030
    },
    {
      "epoch": 1.4107738199684154,
      "grad_norm": 10.405373573303223,
      "learning_rate": 2.6498800959232616e-05,
      "loss": 2.5163,
      "step": 8040
    },
    {
      "epoch": 1.4125285137743464,
      "grad_norm": 5.839235782623291,
      "learning_rate": 2.64695560624671e-05,
      "loss": 2.627,
      "step": 8050
    },
    {
      "epoch": 1.4142832075802771,
      "grad_norm": 6.911857604980469,
      "learning_rate": 2.6440311165701588e-05,
      "loss": 2.3108,
      "step": 8060
    },
    {
      "epoch": 1.416037901386208,
      "grad_norm": 10.219745635986328,
      "learning_rate": 2.6411066268936073e-05,
      "loss": 2.3834,
      "step": 8070
    },
    {
      "epoch": 1.417792595192139,
      "grad_norm": 7.62451171875,
      "learning_rate": 2.6381821372170556e-05,
      "loss": 2.6812,
      "step": 8080
    },
    {
      "epoch": 1.4195472889980698,
      "grad_norm": 7.281433582305908,
      "learning_rate": 2.6352576475405045e-05,
      "loss": 2.6871,
      "step": 8090
    },
    {
      "epoch": 1.4213019828040008,
      "grad_norm": 7.160400390625,
      "learning_rate": 2.6323331578639527e-05,
      "loss": 2.512,
      "step": 8100
    },
    {
      "epoch": 1.4230566766099315,
      "grad_norm": 7.3748698234558105,
      "learning_rate": 2.6294086681874012e-05,
      "loss": 2.7682,
      "step": 8110
    },
    {
      "epoch": 1.4248113704158625,
      "grad_norm": 9.699748039245605,
      "learning_rate": 2.62648417851085e-05,
      "loss": 2.8035,
      "step": 8120
    },
    {
      "epoch": 1.4265660642217932,
      "grad_norm": 7.85721492767334,
      "learning_rate": 2.6235596888342984e-05,
      "loss": 2.502,
      "step": 8130
    },
    {
      "epoch": 1.4283207580277242,
      "grad_norm": 7.0325164794921875,
      "learning_rate": 2.6206351991577473e-05,
      "loss": 2.7012,
      "step": 8140
    },
    {
      "epoch": 1.430075451833655,
      "grad_norm": 8.453728675842285,
      "learning_rate": 2.617710709481196e-05,
      "loss": 2.4604,
      "step": 8150
    },
    {
      "epoch": 1.4318301456395859,
      "grad_norm": 6.018878936767578,
      "learning_rate": 2.614786219804644e-05,
      "loss": 2.5715,
      "step": 8160
    },
    {
      "epoch": 1.4335848394455168,
      "grad_norm": 7.940160751342773,
      "learning_rate": 2.611861730128093e-05,
      "loss": 2.6346,
      "step": 8170
    },
    {
      "epoch": 1.4353395332514476,
      "grad_norm": 7.17517614364624,
      "learning_rate": 2.6089372404515412e-05,
      "loss": 2.5645,
      "step": 8180
    },
    {
      "epoch": 1.4370942270573785,
      "grad_norm": 8.634786605834961,
      "learning_rate": 2.6060127507749897e-05,
      "loss": 2.7707,
      "step": 8190
    },
    {
      "epoch": 1.4388489208633093,
      "grad_norm": 7.052221775054932,
      "learning_rate": 2.6030882610984386e-05,
      "loss": 2.4712,
      "step": 8200
    },
    {
      "epoch": 1.4406036146692403,
      "grad_norm": 6.7384819984436035,
      "learning_rate": 2.600163771421887e-05,
      "loss": 2.6307,
      "step": 8210
    },
    {
      "epoch": 1.4423583084751712,
      "grad_norm": 6.232757568359375,
      "learning_rate": 2.5972392817453354e-05,
      "loss": 2.5849,
      "step": 8220
    },
    {
      "epoch": 1.444113002281102,
      "grad_norm": 8.405084609985352,
      "learning_rate": 2.5943147920687843e-05,
      "loss": 2.727,
      "step": 8230
    },
    {
      "epoch": 1.4458676960870327,
      "grad_norm": 6.958456516265869,
      "learning_rate": 2.5913903023922326e-05,
      "loss": 2.5824,
      "step": 8240
    },
    {
      "epoch": 1.4476223898929637,
      "grad_norm": 7.7234673500061035,
      "learning_rate": 2.5884658127156815e-05,
      "loss": 2.5278,
      "step": 8250
    },
    {
      "epoch": 1.4493770836988946,
      "grad_norm": 8.556300163269043,
      "learning_rate": 2.5855413230391297e-05,
      "loss": 2.6537,
      "step": 8260
    },
    {
      "epoch": 1.4511317775048254,
      "grad_norm": 10.800909996032715,
      "learning_rate": 2.5826168333625782e-05,
      "loss": 2.6094,
      "step": 8270
    },
    {
      "epoch": 1.4528864713107563,
      "grad_norm": 10.384915351867676,
      "learning_rate": 2.579692343686027e-05,
      "loss": 2.5407,
      "step": 8280
    },
    {
      "epoch": 1.454641165116687,
      "grad_norm": 7.303014755249023,
      "learning_rate": 2.5767678540094754e-05,
      "loss": 2.47,
      "step": 8290
    },
    {
      "epoch": 1.456395858922618,
      "grad_norm": 7.676727771759033,
      "learning_rate": 2.573843364332924e-05,
      "loss": 2.5491,
      "step": 8300
    },
    {
      "epoch": 1.458150552728549,
      "grad_norm": 6.8016157150268555,
      "learning_rate": 2.5709188746563728e-05,
      "loss": 2.5483,
      "step": 8310
    },
    {
      "epoch": 1.4599052465344797,
      "grad_norm": 8.490464210510254,
      "learning_rate": 2.567994384979821e-05,
      "loss": 2.643,
      "step": 8320
    },
    {
      "epoch": 1.4616599403404105,
      "grad_norm": 6.655008316040039,
      "learning_rate": 2.56506989530327e-05,
      "loss": 2.4659,
      "step": 8330
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 7.714569568634033,
      "learning_rate": 2.5621454056267182e-05,
      "loss": 2.5483,
      "step": 8340
    },
    {
      "epoch": 1.4651693279522724,
      "grad_norm": 7.164185047149658,
      "learning_rate": 2.5592209159501667e-05,
      "loss": 2.7316,
      "step": 8350
    },
    {
      "epoch": 1.4669240217582031,
      "grad_norm": 11.619872093200684,
      "learning_rate": 2.5562964262736156e-05,
      "loss": 2.4374,
      "step": 8360
    },
    {
      "epoch": 1.468678715564134,
      "grad_norm": 10.003293991088867,
      "learning_rate": 2.553371936597064e-05,
      "loss": 2.5003,
      "step": 8370
    },
    {
      "epoch": 1.4704334093700648,
      "grad_norm": 9.074236869812012,
      "learning_rate": 2.5504474469205124e-05,
      "loss": 2.7285,
      "step": 8380
    },
    {
      "epoch": 1.4721881031759958,
      "grad_norm": 6.4930853843688965,
      "learning_rate": 2.5475229572439613e-05,
      "loss": 2.4756,
      "step": 8390
    },
    {
      "epoch": 1.4739427969819268,
      "grad_norm": 8.881885528564453,
      "learning_rate": 2.5445984675674095e-05,
      "loss": 2.3927,
      "step": 8400
    },
    {
      "epoch": 1.4756974907878575,
      "grad_norm": 7.721909523010254,
      "learning_rate": 2.541673977890858e-05,
      "loss": 2.6106,
      "step": 8410
    },
    {
      "epoch": 1.4774521845937885,
      "grad_norm": 5.53702974319458,
      "learning_rate": 2.5387494882143063e-05,
      "loss": 2.5064,
      "step": 8420
    },
    {
      "epoch": 1.4792068783997192,
      "grad_norm": 7.088876724243164,
      "learning_rate": 2.5358249985377552e-05,
      "loss": 2.4975,
      "step": 8430
    },
    {
      "epoch": 1.4809615722056502,
      "grad_norm": 8.19938850402832,
      "learning_rate": 2.532900508861204e-05,
      "loss": 2.6989,
      "step": 8440
    },
    {
      "epoch": 1.482716266011581,
      "grad_norm": 11.538765907287598,
      "learning_rate": 2.5299760191846524e-05,
      "loss": 2.6248,
      "step": 8450
    },
    {
      "epoch": 1.4844709598175119,
      "grad_norm": 9.044817924499512,
      "learning_rate": 2.527051529508101e-05,
      "loss": 2.5623,
      "step": 8460
    },
    {
      "epoch": 1.4862256536234426,
      "grad_norm": 7.013259410858154,
      "learning_rate": 2.5241270398315498e-05,
      "loss": 2.4515,
      "step": 8470
    },
    {
      "epoch": 1.4879803474293736,
      "grad_norm": 7.751087665557861,
      "learning_rate": 2.521202550154998e-05,
      "loss": 2.5546,
      "step": 8480
    },
    {
      "epoch": 1.4897350412353045,
      "grad_norm": 8.060661315917969,
      "learning_rate": 2.5182780604784466e-05,
      "loss": 2.5019,
      "step": 8490
    },
    {
      "epoch": 1.4914897350412353,
      "grad_norm": 7.392489910125732,
      "learning_rate": 2.5153535708018948e-05,
      "loss": 2.4146,
      "step": 8500
    },
    {
      "epoch": 1.4932444288471662,
      "grad_norm": 10.322375297546387,
      "learning_rate": 2.5124290811253437e-05,
      "loss": 2.7449,
      "step": 8510
    },
    {
      "epoch": 1.494999122653097,
      "grad_norm": 6.7820048332214355,
      "learning_rate": 2.5095045914487926e-05,
      "loss": 2.4531,
      "step": 8520
    },
    {
      "epoch": 1.496753816459028,
      "grad_norm": 8.623927116394043,
      "learning_rate": 2.5065801017722405e-05,
      "loss": 2.5317,
      "step": 8530
    },
    {
      "epoch": 1.4985085102649587,
      "grad_norm": 13.381221771240234,
      "learning_rate": 2.5036556120956894e-05,
      "loss": 2.4631,
      "step": 8540
    },
    {
      "epoch": 1.5002632040708896,
      "grad_norm": 9.59679889678955,
      "learning_rate": 2.5007311224191383e-05,
      "loss": 2.5196,
      "step": 8550
    },
    {
      "epoch": 1.5020178978768204,
      "grad_norm": 7.059725284576416,
      "learning_rate": 2.4978066327425865e-05,
      "loss": 2.3927,
      "step": 8560
    },
    {
      "epoch": 1.5037725916827513,
      "grad_norm": 8.28464126586914,
      "learning_rate": 2.494882143066035e-05,
      "loss": 2.5472,
      "step": 8570
    },
    {
      "epoch": 1.5055272854886823,
      "grad_norm": 7.074644088745117,
      "learning_rate": 2.4919576533894837e-05,
      "loss": 2.3475,
      "step": 8580
    },
    {
      "epoch": 1.507281979294613,
      "grad_norm": 8.025415420532227,
      "learning_rate": 2.4890331637129322e-05,
      "loss": 2.5289,
      "step": 8590
    },
    {
      "epoch": 1.5090366731005438,
      "grad_norm": 8.584242820739746,
      "learning_rate": 2.4861086740363808e-05,
      "loss": 2.5716,
      "step": 8600
    },
    {
      "epoch": 1.5107913669064748,
      "grad_norm": 7.780894756317139,
      "learning_rate": 2.4831841843598294e-05,
      "loss": 2.3511,
      "step": 8610
    },
    {
      "epoch": 1.5125460607124057,
      "grad_norm": 7.0965256690979,
      "learning_rate": 2.480259694683278e-05,
      "loss": 2.6187,
      "step": 8620
    },
    {
      "epoch": 1.5143007545183367,
      "grad_norm": 8.484733581542969,
      "learning_rate": 2.4773352050067265e-05,
      "loss": 2.4306,
      "step": 8630
    },
    {
      "epoch": 1.5160554483242674,
      "grad_norm": 8.593045234680176,
      "learning_rate": 2.474410715330175e-05,
      "loss": 2.6564,
      "step": 8640
    },
    {
      "epoch": 1.5178101421301982,
      "grad_norm": 8.991607666015625,
      "learning_rate": 2.4714862256536236e-05,
      "loss": 2.351,
      "step": 8650
    },
    {
      "epoch": 1.5195648359361291,
      "grad_norm": 7.364739894866943,
      "learning_rate": 2.468561735977072e-05,
      "loss": 2.3759,
      "step": 8660
    },
    {
      "epoch": 1.52131952974206,
      "grad_norm": 7.557343006134033,
      "learning_rate": 2.4656372463005207e-05,
      "loss": 2.7892,
      "step": 8670
    },
    {
      "epoch": 1.5230742235479908,
      "grad_norm": 7.574860095977783,
      "learning_rate": 2.462712756623969e-05,
      "loss": 2.6851,
      "step": 8680
    },
    {
      "epoch": 1.5248289173539218,
      "grad_norm": 9.286500930786133,
      "learning_rate": 2.459788266947418e-05,
      "loss": 2.5849,
      "step": 8690
    },
    {
      "epoch": 1.5265836111598525,
      "grad_norm": 7.412055969238281,
      "learning_rate": 2.4568637772708664e-05,
      "loss": 2.6105,
      "step": 8700
    },
    {
      "epoch": 1.5283383049657835,
      "grad_norm": 9.37665843963623,
      "learning_rate": 2.453939287594315e-05,
      "loss": 2.4765,
      "step": 8710
    },
    {
      "epoch": 1.5300929987717145,
      "grad_norm": 7.479885101318359,
      "learning_rate": 2.4510147979177632e-05,
      "loss": 2.6591,
      "step": 8720
    },
    {
      "epoch": 1.5318476925776452,
      "grad_norm": 6.840064525604248,
      "learning_rate": 2.448090308241212e-05,
      "loss": 2.4364,
      "step": 8730
    },
    {
      "epoch": 1.533602386383576,
      "grad_norm": 6.424694061279297,
      "learning_rate": 2.4451658185646607e-05,
      "loss": 2.2945,
      "step": 8740
    },
    {
      "epoch": 1.535357080189507,
      "grad_norm": 6.509273529052734,
      "learning_rate": 2.442533777855764e-05,
      "loss": 2.5923,
      "step": 8750
    },
    {
      "epoch": 1.5371117739954379,
      "grad_norm": 8.528326034545898,
      "learning_rate": 2.439609288179213e-05,
      "loss": 2.5959,
      "step": 8760
    },
    {
      "epoch": 1.5388664678013688,
      "grad_norm": 12.139910697937012,
      "learning_rate": 2.4366847985026616e-05,
      "loss": 2.5909,
      "step": 8770
    },
    {
      "epoch": 1.5406211616072996,
      "grad_norm": 7.794528961181641,
      "learning_rate": 2.4337603088261098e-05,
      "loss": 2.4859,
      "step": 8780
    },
    {
      "epoch": 1.5423758554132303,
      "grad_norm": 8.52800464630127,
      "learning_rate": 2.4308358191495583e-05,
      "loss": 2.5794,
      "step": 8790
    },
    {
      "epoch": 1.5441305492191613,
      "grad_norm": 8.064706802368164,
      "learning_rate": 2.4279113294730072e-05,
      "loss": 2.7576,
      "step": 8800
    },
    {
      "epoch": 1.5458852430250922,
      "grad_norm": 6.681904315948486,
      "learning_rate": 2.4249868397964555e-05,
      "loss": 2.3369,
      "step": 8810
    },
    {
      "epoch": 1.547639936831023,
      "grad_norm": 10.715847969055176,
      "learning_rate": 2.422062350119904e-05,
      "loss": 2.4816,
      "step": 8820
    },
    {
      "epoch": 1.5493946306369537,
      "grad_norm": 7.941382884979248,
      "learning_rate": 2.419137860443353e-05,
      "loss": 2.3911,
      "step": 8830
    },
    {
      "epoch": 1.5511493244428847,
      "grad_norm": 9.662430763244629,
      "learning_rate": 2.4162133707668015e-05,
      "loss": 2.6579,
      "step": 8840
    },
    {
      "epoch": 1.5529040182488156,
      "grad_norm": 7.839733123779297,
      "learning_rate": 2.4132888810902497e-05,
      "loss": 2.6575,
      "step": 8850
    },
    {
      "epoch": 1.5546587120547466,
      "grad_norm": 6.879380226135254,
      "learning_rate": 2.4103643914136983e-05,
      "loss": 2.7205,
      "step": 8860
    },
    {
      "epoch": 1.5564134058606773,
      "grad_norm": 9.267351150512695,
      "learning_rate": 2.4074399017371472e-05,
      "loss": 2.5821,
      "step": 8870
    },
    {
      "epoch": 1.558168099666608,
      "grad_norm": 8.794708251953125,
      "learning_rate": 2.4045154120605957e-05,
      "loss": 2.4327,
      "step": 8880
    },
    {
      "epoch": 1.559922793472539,
      "grad_norm": 9.503324508666992,
      "learning_rate": 2.401590922384044e-05,
      "loss": 2.4712,
      "step": 8890
    },
    {
      "epoch": 1.56167748727847,
      "grad_norm": 6.681737422943115,
      "learning_rate": 2.3986664327074925e-05,
      "loss": 2.6373,
      "step": 8900
    },
    {
      "epoch": 1.5634321810844007,
      "grad_norm": 10.58415412902832,
      "learning_rate": 2.3957419430309414e-05,
      "loss": 2.6415,
      "step": 8910
    },
    {
      "epoch": 1.5651868748903315,
      "grad_norm": 9.74353313446045,
      "learning_rate": 2.39281745335439e-05,
      "loss": 2.6402,
      "step": 8920
    },
    {
      "epoch": 1.5669415686962624,
      "grad_norm": 8.404519081115723,
      "learning_rate": 2.3898929636778382e-05,
      "loss": 2.7463,
      "step": 8930
    },
    {
      "epoch": 1.5686962625021934,
      "grad_norm": 8.024301528930664,
      "learning_rate": 2.3869684740012868e-05,
      "loss": 2.4011,
      "step": 8940
    },
    {
      "epoch": 1.5704509563081244,
      "grad_norm": 8.099504470825195,
      "learning_rate": 2.3840439843247357e-05,
      "loss": 2.8166,
      "step": 8950
    },
    {
      "epoch": 1.572205650114055,
      "grad_norm": 8.157312393188477,
      "learning_rate": 2.381119494648184e-05,
      "loss": 2.5258,
      "step": 8960
    },
    {
      "epoch": 1.5739603439199858,
      "grad_norm": 8.05667781829834,
      "learning_rate": 2.3781950049716325e-05,
      "loss": 2.7729,
      "step": 8970
    },
    {
      "epoch": 1.5757150377259168,
      "grad_norm": 6.739985466003418,
      "learning_rate": 2.375270515295081e-05,
      "loss": 2.3485,
      "step": 8980
    },
    {
      "epoch": 1.5774697315318478,
      "grad_norm": 8.988300323486328,
      "learning_rate": 2.37234602561853e-05,
      "loss": 2.3907,
      "step": 8990
    },
    {
      "epoch": 1.5792244253377785,
      "grad_norm": 6.654885292053223,
      "learning_rate": 2.369421535941978e-05,
      "loss": 2.4212,
      "step": 9000
    },
    {
      "epoch": 1.5809791191437095,
      "grad_norm": 7.533867359161377,
      "learning_rate": 2.3664970462654267e-05,
      "loss": 2.4566,
      "step": 9010
    },
    {
      "epoch": 1.5827338129496402,
      "grad_norm": 10.559584617614746,
      "learning_rate": 2.3635725565888753e-05,
      "loss": 2.5513,
      "step": 9020
    },
    {
      "epoch": 1.5844885067555712,
      "grad_norm": 7.984595775604248,
      "learning_rate": 2.3606480669123242e-05,
      "loss": 2.2819,
      "step": 9030
    },
    {
      "epoch": 1.5862432005615021,
      "grad_norm": 7.25766134262085,
      "learning_rate": 2.3577235772357724e-05,
      "loss": 2.644,
      "step": 9040
    },
    {
      "epoch": 1.5879978943674329,
      "grad_norm": 8.017683029174805,
      "learning_rate": 2.354799087559221e-05,
      "loss": 2.6835,
      "step": 9050
    },
    {
      "epoch": 1.5897525881733636,
      "grad_norm": 6.510797500610352,
      "learning_rate": 2.3518745978826695e-05,
      "loss": 2.3396,
      "step": 9060
    },
    {
      "epoch": 1.5915072819792946,
      "grad_norm": 11.276437759399414,
      "learning_rate": 2.3489501082061184e-05,
      "loss": 2.5085,
      "step": 9070
    },
    {
      "epoch": 1.5932619757852255,
      "grad_norm": 7.579488277435303,
      "learning_rate": 2.3460256185295666e-05,
      "loss": 2.5628,
      "step": 9080
    },
    {
      "epoch": 1.5950166695911563,
      "grad_norm": 8.54929256439209,
      "learning_rate": 2.3431011288530152e-05,
      "loss": 2.3762,
      "step": 9090
    },
    {
      "epoch": 1.5967713633970873,
      "grad_norm": 9.025181770324707,
      "learning_rate": 2.3401766391764638e-05,
      "loss": 2.5544,
      "step": 9100
    },
    {
      "epoch": 1.598526057203018,
      "grad_norm": 12.222859382629395,
      "learning_rate": 2.3372521494999127e-05,
      "loss": 2.5968,
      "step": 9110
    },
    {
      "epoch": 1.600280751008949,
      "grad_norm": 7.562757968902588,
      "learning_rate": 2.334327659823361e-05,
      "loss": 2.4624,
      "step": 9120
    },
    {
      "epoch": 1.60203544481488,
      "grad_norm": 7.753794193267822,
      "learning_rate": 2.3314031701468095e-05,
      "loss": 2.4096,
      "step": 9130
    },
    {
      "epoch": 1.6037901386208107,
      "grad_norm": 7.717143535614014,
      "learning_rate": 2.328478680470258e-05,
      "loss": 2.4724,
      "step": 9140
    },
    {
      "epoch": 1.6055448324267414,
      "grad_norm": 7.134866714477539,
      "learning_rate": 2.3255541907937066e-05,
      "loss": 2.4254,
      "step": 9150
    },
    {
      "epoch": 1.6072995262326724,
      "grad_norm": 8.456048011779785,
      "learning_rate": 2.322629701117155e-05,
      "loss": 2.2672,
      "step": 9160
    },
    {
      "epoch": 1.6090542200386033,
      "grad_norm": 8.332234382629395,
      "learning_rate": 2.3197052114406037e-05,
      "loss": 2.6449,
      "step": 9170
    },
    {
      "epoch": 1.6108089138445343,
      "grad_norm": 8.427701950073242,
      "learning_rate": 2.3167807217640523e-05,
      "loss": 2.5166,
      "step": 9180
    },
    {
      "epoch": 1.612563607650465,
      "grad_norm": 7.050484657287598,
      "learning_rate": 2.3138562320875008e-05,
      "loss": 2.1885,
      "step": 9190
    },
    {
      "epoch": 1.6143183014563958,
      "grad_norm": 7.513171672821045,
      "learning_rate": 2.3109317424109494e-05,
      "loss": 2.7194,
      "step": 9200
    },
    {
      "epoch": 1.6160729952623267,
      "grad_norm": 6.565136909484863,
      "learning_rate": 2.308007252734398e-05,
      "loss": 2.5914,
      "step": 9210
    },
    {
      "epoch": 1.6178276890682577,
      "grad_norm": 8.623591423034668,
      "learning_rate": 2.3050827630578465e-05,
      "loss": 2.5726,
      "step": 9220
    },
    {
      "epoch": 1.6195823828741884,
      "grad_norm": 9.112482070922852,
      "learning_rate": 2.302158273381295e-05,
      "loss": 2.5053,
      "step": 9230
    },
    {
      "epoch": 1.6213370766801192,
      "grad_norm": 9.411352157592773,
      "learning_rate": 2.2992337837047436e-05,
      "loss": 2.7829,
      "step": 9240
    },
    {
      "epoch": 1.6230917704860501,
      "grad_norm": 15.266650199890137,
      "learning_rate": 2.2963092940281922e-05,
      "loss": 2.52,
      "step": 9250
    },
    {
      "epoch": 1.624846464291981,
      "grad_norm": 6.608583450317383,
      "learning_rate": 2.2933848043516408e-05,
      "loss": 2.5864,
      "step": 9260
    },
    {
      "epoch": 1.626601158097912,
      "grad_norm": 6.943233013153076,
      "learning_rate": 2.2904603146750893e-05,
      "loss": 2.4788,
      "step": 9270
    },
    {
      "epoch": 1.6283558519038428,
      "grad_norm": 7.746438026428223,
      "learning_rate": 2.287535824998538e-05,
      "loss": 2.5492,
      "step": 9280
    },
    {
      "epoch": 1.6301105457097735,
      "grad_norm": 10.71163558959961,
      "learning_rate": 2.2846113353219864e-05,
      "loss": 2.6836,
      "step": 9290
    },
    {
      "epoch": 1.6318652395157045,
      "grad_norm": 8.912590980529785,
      "learning_rate": 2.281686845645435e-05,
      "loss": 2.499,
      "step": 9300
    },
    {
      "epoch": 1.6336199333216355,
      "grad_norm": 7.632821083068848,
      "learning_rate": 2.2787623559688836e-05,
      "loss": 2.1981,
      "step": 9310
    },
    {
      "epoch": 1.6353746271275662,
      "grad_norm": 8.847405433654785,
      "learning_rate": 2.275837866292332e-05,
      "loss": 2.4584,
      "step": 9320
    },
    {
      "epoch": 1.637129320933497,
      "grad_norm": 6.878582954406738,
      "learning_rate": 2.2729133766157807e-05,
      "loss": 2.4507,
      "step": 9330
    },
    {
      "epoch": 1.638884014739428,
      "grad_norm": 8.263361930847168,
      "learning_rate": 2.2699888869392293e-05,
      "loss": 2.6886,
      "step": 9340
    },
    {
      "epoch": 1.6406387085453589,
      "grad_norm": 11.018692970275879,
      "learning_rate": 2.2670643972626778e-05,
      "loss": 2.8534,
      "step": 9350
    },
    {
      "epoch": 1.6423934023512898,
      "grad_norm": 8.778369903564453,
      "learning_rate": 2.2641399075861264e-05,
      "loss": 2.5376,
      "step": 9360
    },
    {
      "epoch": 1.6441480961572206,
      "grad_norm": 7.481838226318359,
      "learning_rate": 2.261215417909575e-05,
      "loss": 2.4569,
      "step": 9370
    },
    {
      "epoch": 1.6459027899631513,
      "grad_norm": 7.614025115966797,
      "learning_rate": 2.2582909282330235e-05,
      "loss": 2.4408,
      "step": 9380
    },
    {
      "epoch": 1.6476574837690823,
      "grad_norm": 9.5171480178833,
      "learning_rate": 2.255366438556472e-05,
      "loss": 2.718,
      "step": 9390
    },
    {
      "epoch": 1.6494121775750132,
      "grad_norm": 7.7909932136535645,
      "learning_rate": 2.2524419488799206e-05,
      "loss": 2.3504,
      "step": 9400
    },
    {
      "epoch": 1.651166871380944,
      "grad_norm": 8.76315689086914,
      "learning_rate": 2.2495174592033692e-05,
      "loss": 2.6558,
      "step": 9410
    },
    {
      "epoch": 1.652921565186875,
      "grad_norm": 8.03049373626709,
      "learning_rate": 2.2465929695268174e-05,
      "loss": 2.4887,
      "step": 9420
    },
    {
      "epoch": 1.6546762589928057,
      "grad_norm": 9.10473346710205,
      "learning_rate": 2.2436684798502663e-05,
      "loss": 2.4735,
      "step": 9430
    },
    {
      "epoch": 1.6564309527987366,
      "grad_norm": 9.130681991577148,
      "learning_rate": 2.240743990173715e-05,
      "loss": 2.3383,
      "step": 9440
    },
    {
      "epoch": 1.6581856466046676,
      "grad_norm": 9.541460990905762,
      "learning_rate": 2.2378195004971634e-05,
      "loss": 2.4212,
      "step": 9450
    },
    {
      "epoch": 1.6599403404105983,
      "grad_norm": 10.15073013305664,
      "learning_rate": 2.2348950108206117e-05,
      "loss": 2.6255,
      "step": 9460
    },
    {
      "epoch": 1.661695034216529,
      "grad_norm": 9.85584831237793,
      "learning_rate": 2.2319705211440606e-05,
      "loss": 2.6252,
      "step": 9470
    },
    {
      "epoch": 1.66344972802246,
      "grad_norm": 8.123339653015137,
      "learning_rate": 2.229046031467509e-05,
      "loss": 2.7239,
      "step": 9480
    },
    {
      "epoch": 1.665204421828391,
      "grad_norm": 9.606754302978516,
      "learning_rate": 2.2261215417909577e-05,
      "loss": 2.6228,
      "step": 9490
    },
    {
      "epoch": 1.666959115634322,
      "grad_norm": 7.863241672515869,
      "learning_rate": 2.223197052114406e-05,
      "loss": 2.356,
      "step": 9500
    },
    {
      "epoch": 1.6687138094402527,
      "grad_norm": 7.986182689666748,
      "learning_rate": 2.2202725624378548e-05,
      "loss": 2.5781,
      "step": 9510
    },
    {
      "epoch": 1.6704685032461835,
      "grad_norm": 7.7805023193359375,
      "learning_rate": 2.2173480727613034e-05,
      "loss": 2.5638,
      "step": 9520
    },
    {
      "epoch": 1.6722231970521144,
      "grad_norm": 11.734624862670898,
      "learning_rate": 2.214423583084752e-05,
      "loss": 2.5003,
      "step": 9530
    },
    {
      "epoch": 1.6739778908580454,
      "grad_norm": 9.262550354003906,
      "learning_rate": 2.2114990934082e-05,
      "loss": 2.3088,
      "step": 9540
    },
    {
      "epoch": 1.6757325846639761,
      "grad_norm": 11.716107368469238,
      "learning_rate": 2.208574603731649e-05,
      "loss": 2.5935,
      "step": 9550
    },
    {
      "epoch": 1.6774872784699069,
      "grad_norm": 9.838496208190918,
      "learning_rate": 2.2056501140550976e-05,
      "loss": 2.4683,
      "step": 9560
    },
    {
      "epoch": 1.6792419722758378,
      "grad_norm": 6.988750457763672,
      "learning_rate": 2.202725624378546e-05,
      "loss": 2.4296,
      "step": 9570
    },
    {
      "epoch": 1.6809966660817688,
      "grad_norm": 8.251781463623047,
      "learning_rate": 2.1998011347019944e-05,
      "loss": 2.5831,
      "step": 9580
    },
    {
      "epoch": 1.6827513598876997,
      "grad_norm": 9.932452201843262,
      "learning_rate": 2.1968766450254433e-05,
      "loss": 2.478,
      "step": 9590
    },
    {
      "epoch": 1.6845060536936305,
      "grad_norm": 7.261973857879639,
      "learning_rate": 2.193952155348892e-05,
      "loss": 2.5887,
      "step": 9600
    },
    {
      "epoch": 1.6862607474995612,
      "grad_norm": 9.051039695739746,
      "learning_rate": 2.19102766567234e-05,
      "loss": 2.5281,
      "step": 9610
    },
    {
      "epoch": 1.6880154413054922,
      "grad_norm": 7.640315055847168,
      "learning_rate": 2.1881031759957887e-05,
      "loss": 2.2788,
      "step": 9620
    },
    {
      "epoch": 1.6897701351114232,
      "grad_norm": 8.081206321716309,
      "learning_rate": 2.1851786863192376e-05,
      "loss": 2.5666,
      "step": 9630
    },
    {
      "epoch": 1.691524828917354,
      "grad_norm": 8.368937492370605,
      "learning_rate": 2.182254196642686e-05,
      "loss": 2.5183,
      "step": 9640
    },
    {
      "epoch": 1.6932795227232846,
      "grad_norm": 7.306811332702637,
      "learning_rate": 2.1793297069661343e-05,
      "loss": 2.5114,
      "step": 9650
    },
    {
      "epoch": 1.6950342165292156,
      "grad_norm": 7.15047550201416,
      "learning_rate": 2.176405217289583e-05,
      "loss": 2.4545,
      "step": 9660
    },
    {
      "epoch": 1.6967889103351466,
      "grad_norm": 7.526221752166748,
      "learning_rate": 2.1734807276130318e-05,
      "loss": 2.5414,
      "step": 9670
    },
    {
      "epoch": 1.6985436041410775,
      "grad_norm": 6.546568393707275,
      "learning_rate": 2.1705562379364804e-05,
      "loss": 2.4936,
      "step": 9680
    },
    {
      "epoch": 1.7002982979470083,
      "grad_norm": 7.850709915161133,
      "learning_rate": 2.1676317482599286e-05,
      "loss": 2.6003,
      "step": 9690
    },
    {
      "epoch": 1.702052991752939,
      "grad_norm": 7.522069454193115,
      "learning_rate": 2.164707258583377e-05,
      "loss": 2.4722,
      "step": 9700
    },
    {
      "epoch": 1.70380768555887,
      "grad_norm": 7.306347846984863,
      "learning_rate": 2.161782768906826e-05,
      "loss": 2.415,
      "step": 9710
    },
    {
      "epoch": 1.705562379364801,
      "grad_norm": 7.003805637359619,
      "learning_rate": 2.1588582792302743e-05,
      "loss": 2.42,
      "step": 9720
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 9.570571899414062,
      "learning_rate": 2.155933789553723e-05,
      "loss": 2.7376,
      "step": 9730
    },
    {
      "epoch": 1.7090717669766624,
      "grad_norm": 6.621268272399902,
      "learning_rate": 2.1530092998771714e-05,
      "loss": 2.6781,
      "step": 9740
    },
    {
      "epoch": 1.7108264607825934,
      "grad_norm": 6.764776706695557,
      "learning_rate": 2.1500848102006203e-05,
      "loss": 2.4465,
      "step": 9750
    },
    {
      "epoch": 1.7125811545885243,
      "grad_norm": 7.495988368988037,
      "learning_rate": 2.1471603205240685e-05,
      "loss": 2.4206,
      "step": 9760
    },
    {
      "epoch": 1.7143358483944553,
      "grad_norm": 7.387270450592041,
      "learning_rate": 2.144235830847517e-05,
      "loss": 2.6145,
      "step": 9770
    },
    {
      "epoch": 1.716090542200386,
      "grad_norm": 8.697684288024902,
      "learning_rate": 2.1413113411709657e-05,
      "loss": 2.4392,
      "step": 9780
    },
    {
      "epoch": 1.7178452360063168,
      "grad_norm": 6.157602310180664,
      "learning_rate": 2.1383868514944146e-05,
      "loss": 2.5615,
      "step": 9790
    },
    {
      "epoch": 1.7195999298122477,
      "grad_norm": 8.48629093170166,
      "learning_rate": 2.1354623618178628e-05,
      "loss": 2.4112,
      "step": 9800
    },
    {
      "epoch": 1.7213546236181787,
      "grad_norm": 7.3818464279174805,
      "learning_rate": 2.1325378721413113e-05,
      "loss": 2.5875,
      "step": 9810
    },
    {
      "epoch": 1.7231093174241094,
      "grad_norm": 8.8477201461792,
      "learning_rate": 2.12961338246476e-05,
      "loss": 2.416,
      "step": 9820
    },
    {
      "epoch": 1.7248640112300404,
      "grad_norm": 9.309297561645508,
      "learning_rate": 2.1266888927882088e-05,
      "loss": 2.0556,
      "step": 9830
    },
    {
      "epoch": 1.7266187050359711,
      "grad_norm": 7.227634429931641,
      "learning_rate": 2.123764403111657e-05,
      "loss": 2.4054,
      "step": 9840
    },
    {
      "epoch": 1.728373398841902,
      "grad_norm": 10.886502265930176,
      "learning_rate": 2.1208399134351056e-05,
      "loss": 2.6013,
      "step": 9850
    },
    {
      "epoch": 1.730128092647833,
      "grad_norm": 9.173192977905273,
      "learning_rate": 2.117915423758554e-05,
      "loss": 2.4865,
      "step": 9860
    },
    {
      "epoch": 1.7318827864537638,
      "grad_norm": 10.064229011535645,
      "learning_rate": 2.114990934082003e-05,
      "loss": 2.3944,
      "step": 9870
    },
    {
      "epoch": 1.7336374802596946,
      "grad_norm": 7.221165180206299,
      "learning_rate": 2.1120664444054513e-05,
      "loss": 2.4946,
      "step": 9880
    },
    {
      "epoch": 1.7353921740656255,
      "grad_norm": 8.685588836669922,
      "learning_rate": 2.1091419547289e-05,
      "loss": 2.6631,
      "step": 9890
    },
    {
      "epoch": 1.7371468678715565,
      "grad_norm": 6.956415176391602,
      "learning_rate": 2.1062174650523484e-05,
      "loss": 2.5839,
      "step": 9900
    },
    {
      "epoch": 1.7389015616774874,
      "grad_norm": 6.912266731262207,
      "learning_rate": 2.103292975375797e-05,
      "loss": 2.4677,
      "step": 9910
    },
    {
      "epoch": 1.7406562554834182,
      "grad_norm": 7.457016944885254,
      "learning_rate": 2.1003684856992455e-05,
      "loss": 2.4032,
      "step": 9920
    },
    {
      "epoch": 1.742410949289349,
      "grad_norm": 8.672785758972168,
      "learning_rate": 2.097443996022694e-05,
      "loss": 2.7474,
      "step": 9930
    },
    {
      "epoch": 1.7441656430952799,
      "grad_norm": 7.213974952697754,
      "learning_rate": 2.0945195063461426e-05,
      "loss": 2.4741,
      "step": 9940
    },
    {
      "epoch": 1.7459203369012108,
      "grad_norm": 10.736044883728027,
      "learning_rate": 2.0915950166695912e-05,
      "loss": 2.5641,
      "step": 9950
    },
    {
      "epoch": 1.7476750307071416,
      "grad_norm": 7.148845195770264,
      "learning_rate": 2.0886705269930398e-05,
      "loss": 2.32,
      "step": 9960
    },
    {
      "epoch": 1.7494297245130723,
      "grad_norm": 7.694695472717285,
      "learning_rate": 2.0857460373164883e-05,
      "loss": 2.6255,
      "step": 9970
    },
    {
      "epoch": 1.7511844183190033,
      "grad_norm": 10.008315086364746,
      "learning_rate": 2.082821547639937e-05,
      "loss": 2.5913,
      "step": 9980
    },
    {
      "epoch": 1.7529391121249343,
      "grad_norm": 7.911608695983887,
      "learning_rate": 2.0798970579633855e-05,
      "loss": 2.5683,
      "step": 9990
    },
    {
      "epoch": 1.7546938059308652,
      "grad_norm": 10.098422050476074,
      "learning_rate": 2.076972568286834e-05,
      "loss": 2.477,
      "step": 10000
    },
    {
      "epoch": 1.756448499736796,
      "grad_norm": 10.29045295715332,
      "learning_rate": 2.0740480786102826e-05,
      "loss": 2.6878,
      "step": 10010
    },
    {
      "epoch": 1.7582031935427267,
      "grad_norm": 8.330403327941895,
      "learning_rate": 2.071123588933731e-05,
      "loss": 2.4997,
      "step": 10020
    },
    {
      "epoch": 1.7599578873486577,
      "grad_norm": 7.915322780609131,
      "learning_rate": 2.0681990992571797e-05,
      "loss": 2.7554,
      "step": 10030
    },
    {
      "epoch": 1.7617125811545886,
      "grad_norm": 6.061832427978516,
      "learning_rate": 2.0652746095806283e-05,
      "loss": 2.5215,
      "step": 10040
    },
    {
      "epoch": 1.7634672749605194,
      "grad_norm": 8.190474510192871,
      "learning_rate": 2.062350119904077e-05,
      "loss": 2.5902,
      "step": 10050
    },
    {
      "epoch": 1.76522196876645,
      "grad_norm": 8.480025291442871,
      "learning_rate": 2.0594256302275254e-05,
      "loss": 2.3058,
      "step": 10060
    },
    {
      "epoch": 1.766976662572381,
      "grad_norm": 8.931754112243652,
      "learning_rate": 2.056501140550974e-05,
      "loss": 2.5979,
      "step": 10070
    },
    {
      "epoch": 1.768731356378312,
      "grad_norm": 7.744062900543213,
      "learning_rate": 2.0535766508744225e-05,
      "loss": 2.6083,
      "step": 10080
    },
    {
      "epoch": 1.770486050184243,
      "grad_norm": 10.172868728637695,
      "learning_rate": 2.050652161197871e-05,
      "loss": 2.5239,
      "step": 10090
    },
    {
      "epoch": 1.7722407439901737,
      "grad_norm": 8.858345031738281,
      "learning_rate": 2.0477276715213196e-05,
      "loss": 2.6116,
      "step": 10100
    },
    {
      "epoch": 1.7739954377961045,
      "grad_norm": 6.063527584075928,
      "learning_rate": 2.0448031818447682e-05,
      "loss": 2.4571,
      "step": 10110
    },
    {
      "epoch": 1.7757501316020354,
      "grad_norm": 9.729833602905273,
      "learning_rate": 2.0418786921682168e-05,
      "loss": 2.5309,
      "step": 10120
    },
    {
      "epoch": 1.7775048254079664,
      "grad_norm": 9.305211067199707,
      "learning_rate": 2.0389542024916653e-05,
      "loss": 2.3606,
      "step": 10130
    },
    {
      "epoch": 1.7792595192138971,
      "grad_norm": 8.214035034179688,
      "learning_rate": 2.036029712815114e-05,
      "loss": 2.4864,
      "step": 10140
    },
    {
      "epoch": 1.7810142130198279,
      "grad_norm": 11.041421890258789,
      "learning_rate": 2.0331052231385625e-05,
      "loss": 2.5666,
      "step": 10150
    },
    {
      "epoch": 1.7827689068257588,
      "grad_norm": 7.185957431793213,
      "learning_rate": 2.030180733462011e-05,
      "loss": 2.5847,
      "step": 10160
    },
    {
      "epoch": 1.7845236006316898,
      "grad_norm": 7.023129940032959,
      "learning_rate": 2.0272562437854596e-05,
      "loss": 2.5043,
      "step": 10170
    },
    {
      "epoch": 1.7862782944376208,
      "grad_norm": 6.219356536865234,
      "learning_rate": 2.0243317541089078e-05,
      "loss": 2.6081,
      "step": 10180
    },
    {
      "epoch": 1.7880329882435515,
      "grad_norm": 8.474349975585938,
      "learning_rate": 2.0214072644323567e-05,
      "loss": 2.2836,
      "step": 10190
    },
    {
      "epoch": 1.7897876820494822,
      "grad_norm": 7.856096267700195,
      "learning_rate": 2.0184827747558053e-05,
      "loss": 2.3994,
      "step": 10200
    },
    {
      "epoch": 1.7915423758554132,
      "grad_norm": 7.815897464752197,
      "learning_rate": 2.0155582850792538e-05,
      "loss": 2.3961,
      "step": 10210
    },
    {
      "epoch": 1.7932970696613442,
      "grad_norm": 9.219125747680664,
      "learning_rate": 2.012633795402702e-05,
      "loss": 2.5048,
      "step": 10220
    },
    {
      "epoch": 1.795051763467275,
      "grad_norm": 9.607881546020508,
      "learning_rate": 2.009709305726151e-05,
      "loss": 2.5189,
      "step": 10230
    },
    {
      "epoch": 1.7968064572732059,
      "grad_norm": 8.515260696411133,
      "learning_rate": 2.0067848160495995e-05,
      "loss": 2.764,
      "step": 10240
    },
    {
      "epoch": 1.7985611510791366,
      "grad_norm": 7.270952224731445,
      "learning_rate": 2.003860326373048e-05,
      "loss": 2.4081,
      "step": 10250
    },
    {
      "epoch": 1.8003158448850676,
      "grad_norm": 9.692498207092285,
      "learning_rate": 2.0009358366964963e-05,
      "loss": 2.353,
      "step": 10260
    },
    {
      "epoch": 1.8020705386909985,
      "grad_norm": 9.004814147949219,
      "learning_rate": 1.9980113470199452e-05,
      "loss": 2.4476,
      "step": 10270
    },
    {
      "epoch": 1.8038252324969293,
      "grad_norm": 9.653058052062988,
      "learning_rate": 1.9950868573433938e-05,
      "loss": 2.6374,
      "step": 10280
    },
    {
      "epoch": 1.80557992630286,
      "grad_norm": 7.945208549499512,
      "learning_rate": 1.9921623676668423e-05,
      "loss": 2.636,
      "step": 10290
    },
    {
      "epoch": 1.807334620108791,
      "grad_norm": 7.349155902862549,
      "learning_rate": 1.9892378779902905e-05,
      "loss": 2.6223,
      "step": 10300
    },
    {
      "epoch": 1.809089313914722,
      "grad_norm": 9.302567481994629,
      "learning_rate": 1.9863133883137394e-05,
      "loss": 2.4078,
      "step": 10310
    },
    {
      "epoch": 1.810844007720653,
      "grad_norm": 8.727519989013672,
      "learning_rate": 1.983388898637188e-05,
      "loss": 2.7366,
      "step": 10320
    },
    {
      "epoch": 1.8125987015265836,
      "grad_norm": 8.015674591064453,
      "learning_rate": 1.9804644089606362e-05,
      "loss": 2.4216,
      "step": 10330
    },
    {
      "epoch": 1.8143533953325144,
      "grad_norm": 8.324481964111328,
      "learning_rate": 1.9775399192840848e-05,
      "loss": 2.5219,
      "step": 10340
    },
    {
      "epoch": 1.8161080891384453,
      "grad_norm": 7.042218208312988,
      "learning_rate": 1.9746154296075337e-05,
      "loss": 2.4738,
      "step": 10350
    },
    {
      "epoch": 1.8178627829443763,
      "grad_norm": 7.3416337966918945,
      "learning_rate": 1.9716909399309823e-05,
      "loss": 2.5689,
      "step": 10360
    },
    {
      "epoch": 1.819617476750307,
      "grad_norm": 7.689963340759277,
      "learning_rate": 1.9687664502544305e-05,
      "loss": 2.7199,
      "step": 10370
    },
    {
      "epoch": 1.8213721705562378,
      "grad_norm": 6.799106597900391,
      "learning_rate": 1.965841960577879e-05,
      "loss": 2.4933,
      "step": 10380
    },
    {
      "epoch": 1.8231268643621688,
      "grad_norm": 6.787283897399902,
      "learning_rate": 1.962917470901328e-05,
      "loss": 2.49,
      "step": 10390
    },
    {
      "epoch": 1.8248815581680997,
      "grad_norm": 6.917415142059326,
      "learning_rate": 1.9599929812247765e-05,
      "loss": 2.515,
      "step": 10400
    },
    {
      "epoch": 1.8266362519740307,
      "grad_norm": 6.96771240234375,
      "learning_rate": 1.9570684915482247e-05,
      "loss": 2.6441,
      "step": 10410
    },
    {
      "epoch": 1.8283909457799614,
      "grad_norm": 7.317321300506592,
      "learning_rate": 1.9541440018716736e-05,
      "loss": 2.6217,
      "step": 10420
    },
    {
      "epoch": 1.8301456395858922,
      "grad_norm": 6.909133434295654,
      "learning_rate": 1.9512195121951222e-05,
      "loss": 2.7197,
      "step": 10430
    },
    {
      "epoch": 1.8319003333918231,
      "grad_norm": 9.898616790771484,
      "learning_rate": 1.9482950225185708e-05,
      "loss": 2.5984,
      "step": 10440
    },
    {
      "epoch": 1.833655027197754,
      "grad_norm": 7.620553016662598,
      "learning_rate": 1.945370532842019e-05,
      "loss": 2.6239,
      "step": 10450
    },
    {
      "epoch": 1.8354097210036848,
      "grad_norm": 9.837821006774902,
      "learning_rate": 1.942446043165468e-05,
      "loss": 2.6064,
      "step": 10460
    },
    {
      "epoch": 1.8371644148096156,
      "grad_norm": 7.087553024291992,
      "learning_rate": 1.9395215534889164e-05,
      "loss": 2.5163,
      "step": 10470
    },
    {
      "epoch": 1.8389191086155465,
      "grad_norm": 7.8760576248168945,
      "learning_rate": 1.9365970638123647e-05,
      "loss": 2.4552,
      "step": 10480
    },
    {
      "epoch": 1.8406738024214775,
      "grad_norm": 6.606070041656494,
      "learning_rate": 1.9336725741358132e-05,
      "loss": 2.7001,
      "step": 10490
    },
    {
      "epoch": 1.8424284962274085,
      "grad_norm": 8.13245964050293,
      "learning_rate": 1.930748084459262e-05,
      "loss": 2.4528,
      "step": 10500
    },
    {
      "epoch": 1.8441831900333392,
      "grad_norm": 6.773950576782227,
      "learning_rate": 1.9278235947827107e-05,
      "loss": 2.6901,
      "step": 10510
    },
    {
      "epoch": 1.84593788383927,
      "grad_norm": 10.728180885314941,
      "learning_rate": 1.924899105106159e-05,
      "loss": 2.2917,
      "step": 10520
    },
    {
      "epoch": 1.847692577645201,
      "grad_norm": 6.182598114013672,
      "learning_rate": 1.9219746154296075e-05,
      "loss": 2.5323,
      "step": 10530
    },
    {
      "epoch": 1.8494472714511319,
      "grad_norm": 6.1384735107421875,
      "learning_rate": 1.9190501257530564e-05,
      "loss": 2.4247,
      "step": 10540
    },
    {
      "epoch": 1.8512019652570626,
      "grad_norm": 7.382254600524902,
      "learning_rate": 1.916125636076505e-05,
      "loss": 2.6027,
      "step": 10550
    },
    {
      "epoch": 1.8529566590629936,
      "grad_norm": 7.919144630432129,
      "learning_rate": 1.913201146399953e-05,
      "loss": 2.5431,
      "step": 10560
    },
    {
      "epoch": 1.8547113528689243,
      "grad_norm": 8.865317344665527,
      "learning_rate": 1.9102766567234017e-05,
      "loss": 2.4419,
      "step": 10570
    },
    {
      "epoch": 1.8564660466748553,
      "grad_norm": 7.261765003204346,
      "learning_rate": 1.9073521670468506e-05,
      "loss": 2.442,
      "step": 10580
    },
    {
      "epoch": 1.8582207404807862,
      "grad_norm": 8.393131256103516,
      "learning_rate": 1.9044276773702992e-05,
      "loss": 2.77,
      "step": 10590
    },
    {
      "epoch": 1.859975434286717,
      "grad_norm": 8.172019004821777,
      "learning_rate": 1.9015031876937474e-05,
      "loss": 2.6115,
      "step": 10600
    },
    {
      "epoch": 1.8617301280926477,
      "grad_norm": 7.038435935974121,
      "learning_rate": 1.898578698017196e-05,
      "loss": 2.6258,
      "step": 10610
    },
    {
      "epoch": 1.8634848218985787,
      "grad_norm": 7.693148612976074,
      "learning_rate": 1.895654208340645e-05,
      "loss": 2.2815,
      "step": 10620
    },
    {
      "epoch": 1.8652395157045096,
      "grad_norm": 5.87446928024292,
      "learning_rate": 1.892729718664093e-05,
      "loss": 2.4947,
      "step": 10630
    },
    {
      "epoch": 1.8669942095104406,
      "grad_norm": 8.490228652954102,
      "learning_rate": 1.8898052289875417e-05,
      "loss": 2.3696,
      "step": 10640
    },
    {
      "epoch": 1.8687489033163713,
      "grad_norm": 5.9410905838012695,
      "learning_rate": 1.8868807393109902e-05,
      "loss": 2.5453,
      "step": 10650
    },
    {
      "epoch": 1.870503597122302,
      "grad_norm": 7.094889163970947,
      "learning_rate": 1.883956249634439e-05,
      "loss": 2.6012,
      "step": 10660
    },
    {
      "epoch": 1.872258290928233,
      "grad_norm": 8.478957176208496,
      "learning_rate": 1.8810317599578873e-05,
      "loss": 2.5694,
      "step": 10670
    },
    {
      "epoch": 1.874012984734164,
      "grad_norm": 8.85379695892334,
      "learning_rate": 1.878107270281336e-05,
      "loss": 2.348,
      "step": 10680
    },
    {
      "epoch": 1.8757676785400947,
      "grad_norm": 8.652161598205566,
      "learning_rate": 1.8751827806047845e-05,
      "loss": 2.3597,
      "step": 10690
    },
    {
      "epoch": 1.8775223723460255,
      "grad_norm": 8.647398948669434,
      "learning_rate": 1.8722582909282334e-05,
      "loss": 2.7677,
      "step": 10700
    },
    {
      "epoch": 1.8792770661519564,
      "grad_norm": 7.585400104522705,
      "learning_rate": 1.8693338012516816e-05,
      "loss": 2.8039,
      "step": 10710
    },
    {
      "epoch": 1.8810317599578874,
      "grad_norm": 7.09187126159668,
      "learning_rate": 1.86640931157513e-05,
      "loss": 2.4952,
      "step": 10720
    },
    {
      "epoch": 1.8827864537638184,
      "grad_norm": 6.655105113983154,
      "learning_rate": 1.8634848218985787e-05,
      "loss": 2.5754,
      "step": 10730
    },
    {
      "epoch": 1.884541147569749,
      "grad_norm": 9.913207054138184,
      "learning_rate": 1.8605603322220276e-05,
      "loss": 2.5242,
      "step": 10740
    },
    {
      "epoch": 1.8862958413756798,
      "grad_norm": 10.765034675598145,
      "learning_rate": 1.857635842545476e-05,
      "loss": 2.3441,
      "step": 10750
    },
    {
      "epoch": 1.8880505351816108,
      "grad_norm": 8.012516021728516,
      "learning_rate": 1.8550038018365796e-05,
      "loss": 2.4107,
      "step": 10760
    },
    {
      "epoch": 1.8898052289875418,
      "grad_norm": 6.954861640930176,
      "learning_rate": 1.8520793121600282e-05,
      "loss": 2.4802,
      "step": 10770
    },
    {
      "epoch": 1.8915599227934725,
      "grad_norm": 5.792262077331543,
      "learning_rate": 1.8491548224834767e-05,
      "loss": 2.3822,
      "step": 10780
    },
    {
      "epoch": 1.8933146165994033,
      "grad_norm": 6.600284576416016,
      "learning_rate": 1.8462303328069253e-05,
      "loss": 2.4229,
      "step": 10790
    },
    {
      "epoch": 1.8950693104053342,
      "grad_norm": 6.20832633972168,
      "learning_rate": 1.843305843130374e-05,
      "loss": 2.5496,
      "step": 10800
    },
    {
      "epoch": 1.8968240042112652,
      "grad_norm": 6.063787460327148,
      "learning_rate": 1.8403813534538224e-05,
      "loss": 2.3524,
      "step": 10810
    },
    {
      "epoch": 1.8985786980171961,
      "grad_norm": 6.806005954742432,
      "learning_rate": 1.837456863777271e-05,
      "loss": 2.4373,
      "step": 10820
    },
    {
      "epoch": 1.9003333918231269,
      "grad_norm": 7.582851409912109,
      "learning_rate": 1.8345323741007196e-05,
      "loss": 2.4107,
      "step": 10830
    },
    {
      "epoch": 1.9020880856290576,
      "grad_norm": 8.40916633605957,
      "learning_rate": 1.831607884424168e-05,
      "loss": 2.4068,
      "step": 10840
    },
    {
      "epoch": 1.9038427794349886,
      "grad_norm": 6.721252918243408,
      "learning_rate": 1.8286833947476167e-05,
      "loss": 2.3077,
      "step": 10850
    },
    {
      "epoch": 1.9055974732409195,
      "grad_norm": 6.162997245788574,
      "learning_rate": 1.8257589050710652e-05,
      "loss": 2.4637,
      "step": 10860
    },
    {
      "epoch": 1.9073521670468503,
      "grad_norm": 8.067547798156738,
      "learning_rate": 1.8228344153945138e-05,
      "loss": 2.3993,
      "step": 10870
    },
    {
      "epoch": 1.909106860852781,
      "grad_norm": 9.48062515258789,
      "learning_rate": 1.8199099257179624e-05,
      "loss": 2.2875,
      "step": 10880
    },
    {
      "epoch": 1.910861554658712,
      "grad_norm": 7.9606404304504395,
      "learning_rate": 1.816985436041411e-05,
      "loss": 2.4524,
      "step": 10890
    },
    {
      "epoch": 1.912616248464643,
      "grad_norm": 9.248359680175781,
      "learning_rate": 1.8140609463648595e-05,
      "loss": 2.6669,
      "step": 10900
    },
    {
      "epoch": 1.914370942270574,
      "grad_norm": 9.6509370803833,
      "learning_rate": 1.811136456688308e-05,
      "loss": 2.3359,
      "step": 10910
    },
    {
      "epoch": 1.9161256360765047,
      "grad_norm": 10.27426815032959,
      "learning_rate": 1.8082119670117563e-05,
      "loss": 2.6579,
      "step": 10920
    },
    {
      "epoch": 1.9178803298824354,
      "grad_norm": 8.673778533935547,
      "learning_rate": 1.8052874773352052e-05,
      "loss": 2.885,
      "step": 10930
    },
    {
      "epoch": 1.9196350236883664,
      "grad_norm": 7.945272445678711,
      "learning_rate": 1.8023629876586537e-05,
      "loss": 2.8535,
      "step": 10940
    },
    {
      "epoch": 1.9213897174942973,
      "grad_norm": 6.585977077484131,
      "learning_rate": 1.7994384979821023e-05,
      "loss": 2.3386,
      "step": 10950
    },
    {
      "epoch": 1.923144411300228,
      "grad_norm": 6.133057594299316,
      "learning_rate": 1.7965140083055505e-05,
      "loss": 2.4117,
      "step": 10960
    },
    {
      "epoch": 1.924899105106159,
      "grad_norm": 6.511083602905273,
      "learning_rate": 1.7935895186289994e-05,
      "loss": 2.4441,
      "step": 10970
    },
    {
      "epoch": 1.9266537989120898,
      "grad_norm": 7.189404010772705,
      "learning_rate": 1.790665028952448e-05,
      "loss": 2.6653,
      "step": 10980
    },
    {
      "epoch": 1.9284084927180207,
      "grad_norm": 10.276328086853027,
      "learning_rate": 1.7877405392758965e-05,
      "loss": 2.5369,
      "step": 10990
    },
    {
      "epoch": 1.9301631865239517,
      "grad_norm": 9.090600967407227,
      "learning_rate": 1.7848160495993448e-05,
      "loss": 2.5099,
      "step": 11000
    },
    {
      "epoch": 1.9319178803298824,
      "grad_norm": 7.648210048675537,
      "learning_rate": 1.7818915599227937e-05,
      "loss": 2.7149,
      "step": 11010
    },
    {
      "epoch": 1.9336725741358132,
      "grad_norm": 6.916133880615234,
      "learning_rate": 1.7789670702462422e-05,
      "loss": 2.1465,
      "step": 11020
    },
    {
      "epoch": 1.9354272679417441,
      "grad_norm": 7.835100173950195,
      "learning_rate": 1.7760425805696908e-05,
      "loss": 2.5167,
      "step": 11030
    },
    {
      "epoch": 1.937181961747675,
      "grad_norm": 8.451115608215332,
      "learning_rate": 1.773118090893139e-05,
      "loss": 2.5498,
      "step": 11040
    },
    {
      "epoch": 1.938936655553606,
      "grad_norm": 7.6191630363464355,
      "learning_rate": 1.770193601216588e-05,
      "loss": 2.7242,
      "step": 11050
    },
    {
      "epoch": 1.9406913493595368,
      "grad_norm": 6.568850517272949,
      "learning_rate": 1.7672691115400365e-05,
      "loss": 2.7063,
      "step": 11060
    },
    {
      "epoch": 1.9424460431654675,
      "grad_norm": 7.487425327301025,
      "learning_rate": 1.7643446218634847e-05,
      "loss": 2.6238,
      "step": 11070
    },
    {
      "epoch": 1.9442007369713985,
      "grad_norm": 8.114781379699707,
      "learning_rate": 1.7614201321869333e-05,
      "loss": 2.636,
      "step": 11080
    },
    {
      "epoch": 1.9459554307773295,
      "grad_norm": 7.122491359710693,
      "learning_rate": 1.758495642510382e-05,
      "loss": 2.5607,
      "step": 11090
    },
    {
      "epoch": 1.9477101245832602,
      "grad_norm": 9.571504592895508,
      "learning_rate": 1.7555711528338307e-05,
      "loss": 2.721,
      "step": 11100
    },
    {
      "epoch": 1.949464818389191,
      "grad_norm": 7.703037738800049,
      "learning_rate": 1.752646663157279e-05,
      "loss": 2.5824,
      "step": 11110
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 6.977794647216797,
      "learning_rate": 1.7497221734807275e-05,
      "loss": 2.507,
      "step": 11120
    },
    {
      "epoch": 1.9529742060010529,
      "grad_norm": 10.11352252960205,
      "learning_rate": 1.7467976838041764e-05,
      "loss": 2.6431,
      "step": 11130
    },
    {
      "epoch": 1.9547288998069838,
      "grad_norm": 7.805565357208252,
      "learning_rate": 1.743873194127625e-05,
      "loss": 2.285,
      "step": 11140
    },
    {
      "epoch": 1.9564835936129146,
      "grad_norm": 8.20887565612793,
      "learning_rate": 1.7409487044510732e-05,
      "loss": 2.7797,
      "step": 11150
    },
    {
      "epoch": 1.9582382874188453,
      "grad_norm": 9.898843765258789,
      "learning_rate": 1.7380242147745218e-05,
      "loss": 2.4311,
      "step": 11160
    },
    {
      "epoch": 1.9599929812247763,
      "grad_norm": 6.60793924331665,
      "learning_rate": 1.7350997250979707e-05,
      "loss": 2.1636,
      "step": 11170
    },
    {
      "epoch": 1.9617476750307072,
      "grad_norm": 7.949257850646973,
      "learning_rate": 1.7321752354214192e-05,
      "loss": 2.721,
      "step": 11180
    },
    {
      "epoch": 1.963502368836638,
      "grad_norm": 7.434246063232422,
      "learning_rate": 1.7292507457448674e-05,
      "loss": 2.5244,
      "step": 11190
    },
    {
      "epoch": 1.9652570626425687,
      "grad_norm": 6.651870250701904,
      "learning_rate": 1.726326256068316e-05,
      "loss": 2.4502,
      "step": 11200
    },
    {
      "epoch": 1.9670117564484997,
      "grad_norm": 8.395928382873535,
      "learning_rate": 1.723401766391765e-05,
      "loss": 2.374,
      "step": 11210
    },
    {
      "epoch": 1.9687664502544306,
      "grad_norm": 7.304534435272217,
      "learning_rate": 1.720477276715213e-05,
      "loss": 2.5704,
      "step": 11220
    },
    {
      "epoch": 1.9705211440603616,
      "grad_norm": 8.535252571105957,
      "learning_rate": 1.7175527870386617e-05,
      "loss": 2.3703,
      "step": 11230
    },
    {
      "epoch": 1.9722758378662923,
      "grad_norm": 9.42033576965332,
      "learning_rate": 1.7146282973621103e-05,
      "loss": 2.4191,
      "step": 11240
    },
    {
      "epoch": 1.974030531672223,
      "grad_norm": 10.903915405273438,
      "learning_rate": 1.711703807685559e-05,
      "loss": 2.2364,
      "step": 11250
    },
    {
      "epoch": 1.975785225478154,
      "grad_norm": 9.162212371826172,
      "learning_rate": 1.7087793180090074e-05,
      "loss": 2.4973,
      "step": 11260
    },
    {
      "epoch": 1.977539919284085,
      "grad_norm": 5.029208660125732,
      "learning_rate": 1.705854828332456e-05,
      "loss": 2.3518,
      "step": 11270
    },
    {
      "epoch": 1.9792946130900158,
      "grad_norm": 7.9763946533203125,
      "learning_rate": 1.7029303386559045e-05,
      "loss": 2.3236,
      "step": 11280
    },
    {
      "epoch": 1.9810493068959465,
      "grad_norm": 7.030273914337158,
      "learning_rate": 1.7000058489793534e-05,
      "loss": 2.5194,
      "step": 11290
    },
    {
      "epoch": 1.9828040007018775,
      "grad_norm": 11.34073257446289,
      "learning_rate": 1.6970813593028016e-05,
      "loss": 2.5974,
      "step": 11300
    },
    {
      "epoch": 1.9845586945078084,
      "grad_norm": 7.233656406402588,
      "learning_rate": 1.6941568696262502e-05,
      "loss": 2.3715,
      "step": 11310
    },
    {
      "epoch": 1.9863133883137394,
      "grad_norm": 7.994445323944092,
      "learning_rate": 1.6912323799496988e-05,
      "loss": 2.4895,
      "step": 11320
    },
    {
      "epoch": 1.9880680821196701,
      "grad_norm": 7.499844074249268,
      "learning_rate": 1.6883078902731477e-05,
      "loss": 2.2881,
      "step": 11330
    },
    {
      "epoch": 1.9898227759256009,
      "grad_norm": 10.068499565124512,
      "learning_rate": 1.685383400596596e-05,
      "loss": 2.6,
      "step": 11340
    },
    {
      "epoch": 1.9915774697315318,
      "grad_norm": 11.955753326416016,
      "learning_rate": 1.6824589109200444e-05,
      "loss": 2.2768,
      "step": 11350
    },
    {
      "epoch": 1.9933321635374628,
      "grad_norm": 8.500129699707031,
      "learning_rate": 1.679534421243493e-05,
      "loss": 2.4529,
      "step": 11360
    },
    {
      "epoch": 1.9950868573433935,
      "grad_norm": 7.667243957519531,
      "learning_rate": 1.676609931566942e-05,
      "loss": 2.4175,
      "step": 11370
    },
    {
      "epoch": 1.9968415511493245,
      "grad_norm": 8.57912540435791,
      "learning_rate": 1.67368544189039e-05,
      "loss": 2.5807,
      "step": 11380
    },
    {
      "epoch": 1.9985962449552552,
      "grad_norm": 8.496663093566895,
      "learning_rate": 1.6707609522138387e-05,
      "loss": 2.4102,
      "step": 11390
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.28472405018864616,
      "eval_f1_C01": 0.2057142857142857,
      "eval_f1_C02": 0.0,
      "eval_f1_C03": 0.0,
      "eval_f1_C04": 0.5080367393800229,
      "eval_f1_C05": 0.0,
      "eval_f1_C06": 0.0,
      "eval_f1_C07": 0.0,
      "eval_f1_C08": 0.0,
      "eval_f1_C09": 0.0,
      "eval_f1_C10": 0.0,
      "eval_f1_C11": 0.0,
      "eval_f1_C12": 0.0,
      "eval_f1_C13": 0.0,
      "eval_f1_C14": 0.5744161728825374,
      "eval_f1_C15": 0.0,
      "eval_f1_C16": 0.0,
      "eval_f1_C17": 0.0,
      "eval_f1_C18": 0.034912718204488775,
      "eval_f1_C19": 0.0,
      "eval_f1_C20": 0.31431897555296856,
      "eval_f1_C21": 0.00676818950930626,
      "eval_f1_C22": 0.0,
      "eval_f1_C23": 0.29288492910026354,
      "eval_f1_macro": 0.08421965262364667,
      "eval_loss": 2.474750518798828,
      "eval_precision_C01": 0.2452316076294278,
      "eval_precision_C02": 1.0,
      "eval_precision_C03": 1.0,
      "eval_precision_C04": 0.39900811541929665,
      "eval_precision_C05": 1.0,
      "eval_precision_C06": 1.0,
      "eval_precision_C07": 1.0,
      "eval_precision_C08": 1.0,
      "eval_precision_C09": 1.0,
      "eval_precision_C10": 1.0,
      "eval_precision_C11": 1.0,
      "eval_precision_C12": 1.0,
      "eval_precision_C13": 1.0,
      "eval_precision_C14": 0.4996967859308672,
      "eval_precision_C15": 1.0,
      "eval_precision_C16": 1.0,
      "eval_precision_C17": 1.0,
      "eval_precision_C18": 0.4117647058823529,
      "eval_precision_C19": 1.0,
      "eval_precision_C20": 0.2465753424657534,
      "eval_precision_C21": 0.5,
      "eval_precision_C22": 1.0,
      "eval_precision_C23": 0.19298825864064825,
      "eval_precision_global": 0.8041419485203628,
      "eval_recall_C01": 0.17716535433070865,
      "eval_recall_C02": 0.0,
      "eval_recall_C03": 0.0,
      "eval_recall_C04": 0.6990521327014217,
      "eval_recall_C05": 0.0,
      "eval_recall_C06": 0.0,
      "eval_recall_C07": 0.0,
      "eval_recall_C08": 0.0,
      "eval_recall_C09": 0.0,
      "eval_recall_C10": 0.0,
      "eval_recall_C11": 0.0,
      "eval_recall_C12": 0.0,
      "eval_recall_C13": 0.0,
      "eval_recall_C14": 0.6754098360655738,
      "eval_recall_C15": 0.0,
      "eval_recall_C16": 0.0,
      "eval_recall_C17": 0.0,
      "eval_recall_C18": 0.018229166666666668,
      "eval_recall_C19": 0.0,
      "eval_recall_C20": 0.4333868378812199,
      "eval_recall_C21": 0.0034071550255536627,
      "eval_recall_C22": 0.0,
      "eval_recall_C23": 0.6071800208116546,
      "eval_recall_global": 0.11364480449925211,
      "eval_runtime": 51.558,
      "eval_samples_per_second": 221.052,
      "eval_steps_per_second": 27.639,
      "step": 11398
    },
    {
      "epoch": 2.000350938761186,
      "grad_norm": 8.524356842041016,
      "learning_rate": 1.6678364625372873e-05,
      "loss": 2.472,
      "step": 11400
    },
    {
      "epoch": 2.002105632567117,
      "grad_norm": 6.566694736480713,
      "learning_rate": 1.6649119728607358e-05,
      "loss": 2.3424,
      "step": 11410
    },
    {
      "epoch": 2.003860326373048,
      "grad_norm": 7.0533013343811035,
      "learning_rate": 1.6619874831841844e-05,
      "loss": 2.5218,
      "step": 11420
    },
    {
      "epoch": 2.0056150201789786,
      "grad_norm": 6.268474578857422,
      "learning_rate": 1.659062993507633e-05,
      "loss": 2.2045,
      "step": 11430
    },
    {
      "epoch": 2.0073697139849096,
      "grad_norm": 12.90646743774414,
      "learning_rate": 1.6561385038310815e-05,
      "loss": 2.6621,
      "step": 11440
    },
    {
      "epoch": 2.0091244077908406,
      "grad_norm": 7.872183322906494,
      "learning_rate": 1.65321401415453e-05,
      "loss": 2.4385,
      "step": 11450
    },
    {
      "epoch": 2.0108791015967715,
      "grad_norm": 6.955444812774658,
      "learning_rate": 1.6502895244779786e-05,
      "loss": 2.6343,
      "step": 11460
    },
    {
      "epoch": 2.012633795402702,
      "grad_norm": 7.371109485626221,
      "learning_rate": 1.6473650348014272e-05,
      "loss": 2.4474,
      "step": 11470
    },
    {
      "epoch": 2.014388489208633,
      "grad_norm": 9.82899284362793,
      "learning_rate": 1.6444405451248758e-05,
      "loss": 2.5807,
      "step": 11480
    },
    {
      "epoch": 2.016143183014564,
      "grad_norm": 6.9988813400268555,
      "learning_rate": 1.6415160554483243e-05,
      "loss": 2.5293,
      "step": 11490
    },
    {
      "epoch": 2.017897876820495,
      "grad_norm": 6.915615558624268,
      "learning_rate": 1.638591565771773e-05,
      "loss": 2.3478,
      "step": 11500
    },
    {
      "epoch": 2.019652570626426,
      "grad_norm": 7.1961212158203125,
      "learning_rate": 1.6356670760952214e-05,
      "loss": 2.3107,
      "step": 11510
    },
    {
      "epoch": 2.0214072644323564,
      "grad_norm": 7.18261194229126,
      "learning_rate": 1.63274258641867e-05,
      "loss": 2.4934,
      "step": 11520
    },
    {
      "epoch": 2.0231619582382874,
      "grad_norm": 9.000630378723145,
      "learning_rate": 1.6298180967421186e-05,
      "loss": 2.5919,
      "step": 11530
    },
    {
      "epoch": 2.0249166520442183,
      "grad_norm": 7.249172687530518,
      "learning_rate": 1.626893607065567e-05,
      "loss": 2.4418,
      "step": 11540
    },
    {
      "epoch": 2.0266713458501493,
      "grad_norm": 9.43285846710205,
      "learning_rate": 1.6239691173890157e-05,
      "loss": 2.5747,
      "step": 11550
    },
    {
      "epoch": 2.02842603965608,
      "grad_norm": 7.83048152923584,
      "learning_rate": 1.6210446277124642e-05,
      "loss": 2.4302,
      "step": 11560
    },
    {
      "epoch": 2.030180733462011,
      "grad_norm": 7.203038215637207,
      "learning_rate": 1.6181201380359128e-05,
      "loss": 2.5879,
      "step": 11570
    },
    {
      "epoch": 2.0319354272679417,
      "grad_norm": 10.656036376953125,
      "learning_rate": 1.6151956483593614e-05,
      "loss": 2.5508,
      "step": 11580
    },
    {
      "epoch": 2.0336901210738727,
      "grad_norm": 8.163763999938965,
      "learning_rate": 1.61227115868281e-05,
      "loss": 2.8669,
      "step": 11590
    },
    {
      "epoch": 2.0354448148798037,
      "grad_norm": 8.401090621948242,
      "learning_rate": 1.6093466690062585e-05,
      "loss": 2.4587,
      "step": 11600
    },
    {
      "epoch": 2.037199508685734,
      "grad_norm": 7.716235637664795,
      "learning_rate": 1.606422179329707e-05,
      "loss": 2.4068,
      "step": 11610
    },
    {
      "epoch": 2.038954202491665,
      "grad_norm": 7.361355781555176,
      "learning_rate": 1.6034976896531556e-05,
      "loss": 2.546,
      "step": 11620
    },
    {
      "epoch": 2.040708896297596,
      "grad_norm": 7.103748798370361,
      "learning_rate": 1.6005731999766042e-05,
      "loss": 2.4096,
      "step": 11630
    },
    {
      "epoch": 2.042463590103527,
      "grad_norm": 6.376732349395752,
      "learning_rate": 1.5976487103000527e-05,
      "loss": 2.5736,
      "step": 11640
    },
    {
      "epoch": 2.0442182839094576,
      "grad_norm": 7.838223457336426,
      "learning_rate": 1.5947242206235013e-05,
      "loss": 2.3012,
      "step": 11650
    },
    {
      "epoch": 2.0459729777153886,
      "grad_norm": 9.60910415649414,
      "learning_rate": 1.59179973094695e-05,
      "loss": 2.4913,
      "step": 11660
    },
    {
      "epoch": 2.0477276715213195,
      "grad_norm": 10.926042556762695,
      "learning_rate": 1.5888752412703984e-05,
      "loss": 2.4181,
      "step": 11670
    },
    {
      "epoch": 2.0494823653272505,
      "grad_norm": 7.994182586669922,
      "learning_rate": 1.5859507515938467e-05,
      "loss": 2.5404,
      "step": 11680
    },
    {
      "epoch": 2.0512370591331814,
      "grad_norm": 9.924327850341797,
      "learning_rate": 1.5830262619172956e-05,
      "loss": 2.3744,
      "step": 11690
    },
    {
      "epoch": 2.052991752939112,
      "grad_norm": 7.719926834106445,
      "learning_rate": 1.580101772240744e-05,
      "loss": 2.7056,
      "step": 11700
    },
    {
      "epoch": 2.054746446745043,
      "grad_norm": 6.650829315185547,
      "learning_rate": 1.5771772825641927e-05,
      "loss": 2.3438,
      "step": 11710
    },
    {
      "epoch": 2.056501140550974,
      "grad_norm": 7.185796737670898,
      "learning_rate": 1.574252792887641e-05,
      "loss": 2.5991,
      "step": 11720
    },
    {
      "epoch": 2.058255834356905,
      "grad_norm": 7.506579399108887,
      "learning_rate": 1.5713283032110898e-05,
      "loss": 2.4251,
      "step": 11730
    },
    {
      "epoch": 2.0600105281628354,
      "grad_norm": 8.658587455749512,
      "learning_rate": 1.5684038135345384e-05,
      "loss": 2.4804,
      "step": 11740
    },
    {
      "epoch": 2.0617652219687663,
      "grad_norm": 9.754817008972168,
      "learning_rate": 1.565479323857987e-05,
      "loss": 2.6627,
      "step": 11750
    },
    {
      "epoch": 2.0635199157746973,
      "grad_norm": 9.110879898071289,
      "learning_rate": 1.562554834181435e-05,
      "loss": 2.7061,
      "step": 11760
    },
    {
      "epoch": 2.0652746095806283,
      "grad_norm": 6.640185356140137,
      "learning_rate": 1.559630344504884e-05,
      "loss": 2.3267,
      "step": 11770
    },
    {
      "epoch": 2.067029303386559,
      "grad_norm": 8.66254997253418,
      "learning_rate": 1.5567058548283326e-05,
      "loss": 2.6036,
      "step": 11780
    },
    {
      "epoch": 2.0687839971924897,
      "grad_norm": 7.800894737243652,
      "learning_rate": 1.5537813651517812e-05,
      "loss": 2.4709,
      "step": 11790
    },
    {
      "epoch": 2.0705386909984207,
      "grad_norm": 6.440817356109619,
      "learning_rate": 1.5508568754752294e-05,
      "loss": 2.3915,
      "step": 11800
    },
    {
      "epoch": 2.0722933848043517,
      "grad_norm": 9.235298156738281,
      "learning_rate": 1.5479323857986783e-05,
      "loss": 2.511,
      "step": 11810
    },
    {
      "epoch": 2.0740480786102826,
      "grad_norm": 7.130293846130371,
      "learning_rate": 1.545007896122127e-05,
      "loss": 2.5813,
      "step": 11820
    },
    {
      "epoch": 2.0758027724162136,
      "grad_norm": 8.058826446533203,
      "learning_rate": 1.542083406445575e-05,
      "loss": 2.7354,
      "step": 11830
    },
    {
      "epoch": 2.077557466222144,
      "grad_norm": 8.325188636779785,
      "learning_rate": 1.539158916769024e-05,
      "loss": 2.4152,
      "step": 11840
    },
    {
      "epoch": 2.079312160028075,
      "grad_norm": 10.338338851928711,
      "learning_rate": 1.5362344270924725e-05,
      "loss": 2.2985,
      "step": 11850
    },
    {
      "epoch": 2.081066853834006,
      "grad_norm": 11.158804893493652,
      "learning_rate": 1.533309937415921e-05,
      "loss": 2.6029,
      "step": 11860
    },
    {
      "epoch": 2.082821547639937,
      "grad_norm": 7.732171535491943,
      "learning_rate": 1.5303854477393693e-05,
      "loss": 2.4349,
      "step": 11870
    },
    {
      "epoch": 2.0845762414458675,
      "grad_norm": 7.588192462921143,
      "learning_rate": 1.5274609580628182e-05,
      "loss": 2.6457,
      "step": 11880
    },
    {
      "epoch": 2.0863309352517985,
      "grad_norm": 7.219654083251953,
      "learning_rate": 1.5245364683862668e-05,
      "loss": 2.3461,
      "step": 11890
    },
    {
      "epoch": 2.0880856290577294,
      "grad_norm": 7.78537654876709,
      "learning_rate": 1.5216119787097152e-05,
      "loss": 2.3593,
      "step": 11900
    },
    {
      "epoch": 2.0898403228636604,
      "grad_norm": 10.764347076416016,
      "learning_rate": 1.5186874890331638e-05,
      "loss": 2.6645,
      "step": 11910
    },
    {
      "epoch": 2.0915950166695914,
      "grad_norm": 8.725282669067383,
      "learning_rate": 1.5157629993566125e-05,
      "loss": 2.6416,
      "step": 11920
    },
    {
      "epoch": 2.093349710475522,
      "grad_norm": 9.072824478149414,
      "learning_rate": 1.512838509680061e-05,
      "loss": 2.4843,
      "step": 11930
    },
    {
      "epoch": 2.095104404281453,
      "grad_norm": 6.568950176239014,
      "learning_rate": 1.5099140200035094e-05,
      "loss": 2.3149,
      "step": 11940
    },
    {
      "epoch": 2.096859098087384,
      "grad_norm": 8.876712799072266,
      "learning_rate": 1.506989530326958e-05,
      "loss": 2.4094,
      "step": 11950
    },
    {
      "epoch": 2.0986137918933148,
      "grad_norm": 6.304048538208008,
      "learning_rate": 1.5040650406504067e-05,
      "loss": 2.3918,
      "step": 11960
    },
    {
      "epoch": 2.1003684856992453,
      "grad_norm": 10.291718482971191,
      "learning_rate": 1.5011405509738551e-05,
      "loss": 2.6662,
      "step": 11970
    },
    {
      "epoch": 2.1021231795051762,
      "grad_norm": 9.514418601989746,
      "learning_rate": 1.4982160612973037e-05,
      "loss": 2.4053,
      "step": 11980
    },
    {
      "epoch": 2.103877873311107,
      "grad_norm": 7.451307773590088,
      "learning_rate": 1.4952915716207523e-05,
      "loss": 2.4824,
      "step": 11990
    },
    {
      "epoch": 2.105632567117038,
      "grad_norm": 10.89095401763916,
      "learning_rate": 1.492367081944201e-05,
      "loss": 2.6092,
      "step": 12000
    },
    {
      "epoch": 2.107387260922969,
      "grad_norm": 8.18203353881836,
      "learning_rate": 1.4894425922676494e-05,
      "loss": 2.4433,
      "step": 12010
    },
    {
      "epoch": 2.1091419547288996,
      "grad_norm": 8.55337905883789,
      "learning_rate": 1.486518102591098e-05,
      "loss": 2.4969,
      "step": 12020
    },
    {
      "epoch": 2.1108966485348306,
      "grad_norm": 8.004520416259766,
      "learning_rate": 1.4835936129145463e-05,
      "loss": 2.4409,
      "step": 12030
    },
    {
      "epoch": 2.1126513423407616,
      "grad_norm": 8.334733009338379,
      "learning_rate": 1.4806691232379952e-05,
      "loss": 2.4776,
      "step": 12040
    },
    {
      "epoch": 2.1144060361466925,
      "grad_norm": 8.670736312866211,
      "learning_rate": 1.4777446335614436e-05,
      "loss": 2.5154,
      "step": 12050
    },
    {
      "epoch": 2.116160729952623,
      "grad_norm": 6.65322208404541,
      "learning_rate": 1.4748201438848922e-05,
      "loss": 2.4613,
      "step": 12060
    },
    {
      "epoch": 2.117915423758554,
      "grad_norm": 11.816288948059082,
      "learning_rate": 1.4718956542083406e-05,
      "loss": 2.5657,
      "step": 12070
    },
    {
      "epoch": 2.119670117564485,
      "grad_norm": 10.421085357666016,
      "learning_rate": 1.4689711645317895e-05,
      "loss": 2.7059,
      "step": 12080
    },
    {
      "epoch": 2.121424811370416,
      "grad_norm": 8.072163581848145,
      "learning_rate": 1.4660466748552379e-05,
      "loss": 2.6417,
      "step": 12090
    },
    {
      "epoch": 2.123179505176347,
      "grad_norm": 6.7279887199401855,
      "learning_rate": 1.4631221851786864e-05,
      "loss": 2.5148,
      "step": 12100
    },
    {
      "epoch": 2.1249341989822774,
      "grad_norm": 6.6043548583984375,
      "learning_rate": 1.4601976955021348e-05,
      "loss": 2.4976,
      "step": 12110
    },
    {
      "epoch": 2.1266888927882084,
      "grad_norm": 9.748534202575684,
      "learning_rate": 1.4572732058255836e-05,
      "loss": 2.3694,
      "step": 12120
    },
    {
      "epoch": 2.1284435865941393,
      "grad_norm": 8.63394546508789,
      "learning_rate": 1.4543487161490321e-05,
      "loss": 2.5086,
      "step": 12130
    },
    {
      "epoch": 2.1301982804000703,
      "grad_norm": 7.9435858726501465,
      "learning_rate": 1.4514242264724807e-05,
      "loss": 2.419,
      "step": 12140
    },
    {
      "epoch": 2.1319529742060013,
      "grad_norm": 7.422883033752441,
      "learning_rate": 1.448499736795929e-05,
      "loss": 2.3385,
      "step": 12150
    },
    {
      "epoch": 2.133707668011932,
      "grad_norm": 13.678696632385254,
      "learning_rate": 1.4455752471193778e-05,
      "loss": 2.3635,
      "step": 12160
    },
    {
      "epoch": 2.1354623618178628,
      "grad_norm": 14.548720359802246,
      "learning_rate": 1.4426507574428264e-05,
      "loss": 2.4837,
      "step": 12170
    },
    {
      "epoch": 2.1372170556237937,
      "grad_norm": 6.35319709777832,
      "learning_rate": 1.4397262677662748e-05,
      "loss": 2.6835,
      "step": 12180
    },
    {
      "epoch": 2.1389717494297247,
      "grad_norm": 7.25669002532959,
      "learning_rate": 1.4368017780897233e-05,
      "loss": 2.4989,
      "step": 12190
    },
    {
      "epoch": 2.140726443235655,
      "grad_norm": 12.123626708984375,
      "learning_rate": 1.433877288413172e-05,
      "loss": 2.4549,
      "step": 12200
    },
    {
      "epoch": 2.142481137041586,
      "grad_norm": 8.077717781066895,
      "learning_rate": 1.4309527987366206e-05,
      "loss": 2.8844,
      "step": 12210
    },
    {
      "epoch": 2.144235830847517,
      "grad_norm": 7.970901012420654,
      "learning_rate": 1.428028309060069e-05,
      "loss": 2.482,
      "step": 12220
    },
    {
      "epoch": 2.145990524653448,
      "grad_norm": 7.817970275878906,
      "learning_rate": 1.4251038193835176e-05,
      "loss": 2.3615,
      "step": 12230
    },
    {
      "epoch": 2.147745218459379,
      "grad_norm": 8.841432571411133,
      "learning_rate": 1.4221793297069663e-05,
      "loss": 2.6043,
      "step": 12240
    },
    {
      "epoch": 2.1494999122653096,
      "grad_norm": 9.816327095031738,
      "learning_rate": 1.4192548400304149e-05,
      "loss": 2.4644,
      "step": 12250
    },
    {
      "epoch": 2.1512546060712405,
      "grad_norm": 10.48484992980957,
      "learning_rate": 1.4163303503538633e-05,
      "loss": 2.3962,
      "step": 12260
    },
    {
      "epoch": 2.1530092998771715,
      "grad_norm": 9.168615341186523,
      "learning_rate": 1.4134058606773118e-05,
      "loss": 2.6308,
      "step": 12270
    },
    {
      "epoch": 2.1547639936831025,
      "grad_norm": 10.38460922241211,
      "learning_rate": 1.4104813710007606e-05,
      "loss": 2.38,
      "step": 12280
    },
    {
      "epoch": 2.156518687489033,
      "grad_norm": 9.321237564086914,
      "learning_rate": 1.4075568813242091e-05,
      "loss": 2.3971,
      "step": 12290
    },
    {
      "epoch": 2.158273381294964,
      "grad_norm": 6.359052658081055,
      "learning_rate": 1.4046323916476575e-05,
      "loss": 2.746,
      "step": 12300
    },
    {
      "epoch": 2.160028075100895,
      "grad_norm": 5.82257080078125,
      "learning_rate": 1.401707901971106e-05,
      "loss": 2.1606,
      "step": 12310
    },
    {
      "epoch": 2.161782768906826,
      "grad_norm": 10.125673294067383,
      "learning_rate": 1.3987834122945548e-05,
      "loss": 2.4296,
      "step": 12320
    },
    {
      "epoch": 2.163537462712757,
      "grad_norm": 9.211240768432617,
      "learning_rate": 1.3958589226180032e-05,
      "loss": 2.5031,
      "step": 12330
    },
    {
      "epoch": 2.1652921565186873,
      "grad_norm": 8.036304473876953,
      "learning_rate": 1.3929344329414518e-05,
      "loss": 2.6713,
      "step": 12340
    },
    {
      "epoch": 2.1670468503246183,
      "grad_norm": 9.800572395324707,
      "learning_rate": 1.3900099432649003e-05,
      "loss": 2.4614,
      "step": 12350
    },
    {
      "epoch": 2.1688015441305493,
      "grad_norm": 7.436674118041992,
      "learning_rate": 1.387085453588349e-05,
      "loss": 2.6258,
      "step": 12360
    },
    {
      "epoch": 2.1705562379364802,
      "grad_norm": 6.609721660614014,
      "learning_rate": 1.3841609639117974e-05,
      "loss": 2.564,
      "step": 12370
    },
    {
      "epoch": 2.1723109317424107,
      "grad_norm": 10.394583702087402,
      "learning_rate": 1.381236474235246e-05,
      "loss": 2.4398,
      "step": 12380
    },
    {
      "epoch": 2.1740656255483417,
      "grad_norm": 10.181536674499512,
      "learning_rate": 1.3783119845586944e-05,
      "loss": 2.479,
      "step": 12390
    },
    {
      "epoch": 2.1758203193542727,
      "grad_norm": 7.647341251373291,
      "learning_rate": 1.3753874948821433e-05,
      "loss": 2.4412,
      "step": 12400
    },
    {
      "epoch": 2.1775750131602036,
      "grad_norm": 7.602856159210205,
      "learning_rate": 1.3724630052055917e-05,
      "loss": 2.5306,
      "step": 12410
    },
    {
      "epoch": 2.1793297069661346,
      "grad_norm": 11.647733688354492,
      "learning_rate": 1.3695385155290403e-05,
      "loss": 2.3069,
      "step": 12420
    },
    {
      "epoch": 2.181084400772065,
      "grad_norm": 5.86280632019043,
      "learning_rate": 1.3666140258524886e-05,
      "loss": 2.3695,
      "step": 12430
    },
    {
      "epoch": 2.182839094577996,
      "grad_norm": 9.882661819458008,
      "learning_rate": 1.3636895361759375e-05,
      "loss": 2.6393,
      "step": 12440
    },
    {
      "epoch": 2.184593788383927,
      "grad_norm": 7.211832046508789,
      "learning_rate": 1.360765046499386e-05,
      "loss": 2.4936,
      "step": 12450
    },
    {
      "epoch": 2.186348482189858,
      "grad_norm": 6.778736114501953,
      "learning_rate": 1.3578405568228345e-05,
      "loss": 2.3664,
      "step": 12460
    },
    {
      "epoch": 2.1881031759957885,
      "grad_norm": 7.116447448730469,
      "learning_rate": 1.3549160671462829e-05,
      "loss": 2.3323,
      "step": 12470
    },
    {
      "epoch": 2.1898578698017195,
      "grad_norm": 7.075325012207031,
      "learning_rate": 1.3519915774697316e-05,
      "loss": 2.5523,
      "step": 12480
    },
    {
      "epoch": 2.1916125636076504,
      "grad_norm": 5.441455841064453,
      "learning_rate": 1.3490670877931802e-05,
      "loss": 2.6389,
      "step": 12490
    },
    {
      "epoch": 2.1933672574135814,
      "grad_norm": 7.582988739013672,
      "learning_rate": 1.3461425981166287e-05,
      "loss": 2.4139,
      "step": 12500
    },
    {
      "epoch": 2.1951219512195124,
      "grad_norm": 11.14409351348877,
      "learning_rate": 1.3432181084400771e-05,
      "loss": 2.6646,
      "step": 12510
    },
    {
      "epoch": 2.196876645025443,
      "grad_norm": 7.52761173248291,
      "learning_rate": 1.3402936187635259e-05,
      "loss": 2.4437,
      "step": 12520
    },
    {
      "epoch": 2.198631338831374,
      "grad_norm": 8.359395027160645,
      "learning_rate": 1.3373691290869744e-05,
      "loss": 2.2045,
      "step": 12530
    },
    {
      "epoch": 2.200386032637305,
      "grad_norm": 10.8268461227417,
      "learning_rate": 1.334444639410423e-05,
      "loss": 2.5289,
      "step": 12540
    },
    {
      "epoch": 2.2021407264432358,
      "grad_norm": 8.610021591186523,
      "learning_rate": 1.3315201497338714e-05,
      "loss": 2.4486,
      "step": 12550
    },
    {
      "epoch": 2.2038954202491663,
      "grad_norm": 7.463440895080566,
      "learning_rate": 1.3285956600573201e-05,
      "loss": 2.6197,
      "step": 12560
    },
    {
      "epoch": 2.2056501140550973,
      "grad_norm": 9.436086654663086,
      "learning_rate": 1.3256711703807687e-05,
      "loss": 2.6064,
      "step": 12570
    },
    {
      "epoch": 2.207404807861028,
      "grad_norm": 9.508285522460938,
      "learning_rate": 1.322746680704217e-05,
      "loss": 2.6536,
      "step": 12580
    },
    {
      "epoch": 2.209159501666959,
      "grad_norm": 9.688416481018066,
      "learning_rate": 1.3198221910276656e-05,
      "loss": 2.5563,
      "step": 12590
    },
    {
      "epoch": 2.21091419547289,
      "grad_norm": 7.232044696807861,
      "learning_rate": 1.3168977013511144e-05,
      "loss": 2.5375,
      "step": 12600
    },
    {
      "epoch": 2.2126688892788207,
      "grad_norm": 9.524553298950195,
      "learning_rate": 1.313973211674563e-05,
      "loss": 2.4898,
      "step": 12610
    },
    {
      "epoch": 2.2144235830847516,
      "grad_norm": 8.36982250213623,
      "learning_rate": 1.3110487219980113e-05,
      "loss": 2.4824,
      "step": 12620
    },
    {
      "epoch": 2.2161782768906826,
      "grad_norm": 6.973400592803955,
      "learning_rate": 1.3081242323214599e-05,
      "loss": 2.523,
      "step": 12630
    },
    {
      "epoch": 2.2179329706966135,
      "grad_norm": 9.75279426574707,
      "learning_rate": 1.3051997426449086e-05,
      "loss": 2.4883,
      "step": 12640
    },
    {
      "epoch": 2.2196876645025445,
      "grad_norm": 8.233349800109863,
      "learning_rate": 1.3022752529683572e-05,
      "loss": 2.2502,
      "step": 12650
    },
    {
      "epoch": 2.221442358308475,
      "grad_norm": 12.430377006530762,
      "learning_rate": 1.2993507632918056e-05,
      "loss": 2.3736,
      "step": 12660
    },
    {
      "epoch": 2.223197052114406,
      "grad_norm": 7.637608528137207,
      "learning_rate": 1.2964262736152541e-05,
      "loss": 2.6491,
      "step": 12670
    },
    {
      "epoch": 2.224951745920337,
      "grad_norm": 9.153775215148926,
      "learning_rate": 1.2935017839387029e-05,
      "loss": 2.2119,
      "step": 12680
    },
    {
      "epoch": 2.226706439726268,
      "grad_norm": 9.203614234924316,
      "learning_rate": 1.2905772942621514e-05,
      "loss": 2.754,
      "step": 12690
    },
    {
      "epoch": 2.2284611335321984,
      "grad_norm": 6.33034086227417,
      "learning_rate": 1.2876528045855998e-05,
      "loss": 2.3207,
      "step": 12700
    },
    {
      "epoch": 2.2302158273381294,
      "grad_norm": 9.653822898864746,
      "learning_rate": 1.2847283149090484e-05,
      "loss": 2.4381,
      "step": 12710
    },
    {
      "epoch": 2.2319705211440604,
      "grad_norm": 8.989803314208984,
      "learning_rate": 1.2818038252324971e-05,
      "loss": 2.4017,
      "step": 12720
    },
    {
      "epoch": 2.2337252149499913,
      "grad_norm": 7.301869869232178,
      "learning_rate": 1.2788793355559455e-05,
      "loss": 2.5449,
      "step": 12730
    },
    {
      "epoch": 2.2354799087559223,
      "grad_norm": 6.587071895599365,
      "learning_rate": 1.275954845879394e-05,
      "loss": 2.2676,
      "step": 12740
    },
    {
      "epoch": 2.237234602561853,
      "grad_norm": 7.247373580932617,
      "learning_rate": 1.2730303562028426e-05,
      "loss": 2.4872,
      "step": 12750
    },
    {
      "epoch": 2.2389892963677838,
      "grad_norm": 10.601374626159668,
      "learning_rate": 1.2703983154939464e-05,
      "loss": 2.7171,
      "step": 12760
    },
    {
      "epoch": 2.2407439901737147,
      "grad_norm": 9.117183685302734,
      "learning_rate": 1.2674738258173948e-05,
      "loss": 2.4378,
      "step": 12770
    },
    {
      "epoch": 2.2424986839796457,
      "grad_norm": 7.278252124786377,
      "learning_rate": 1.2645493361408434e-05,
      "loss": 2.3112,
      "step": 12780
    },
    {
      "epoch": 2.244253377785576,
      "grad_norm": 11.061447143554688,
      "learning_rate": 1.2616248464642921e-05,
      "loss": 2.4693,
      "step": 12790
    },
    {
      "epoch": 2.246008071591507,
      "grad_norm": 13.385661125183105,
      "learning_rate": 1.2587003567877407e-05,
      "loss": 2.5304,
      "step": 12800
    },
    {
      "epoch": 2.247762765397438,
      "grad_norm": 6.623979091644287,
      "learning_rate": 1.255775867111189e-05,
      "loss": 2.3815,
      "step": 12810
    },
    {
      "epoch": 2.249517459203369,
      "grad_norm": 9.787532806396484,
      "learning_rate": 1.2528513774346376e-05,
      "loss": 2.6401,
      "step": 12820
    },
    {
      "epoch": 2.2512721530093,
      "grad_norm": 9.10274600982666,
      "learning_rate": 1.2499268877580862e-05,
      "loss": 2.1731,
      "step": 12830
    },
    {
      "epoch": 2.2530268468152306,
      "grad_norm": 8.106328964233398,
      "learning_rate": 1.2470023980815349e-05,
      "loss": 2.5034,
      "step": 12840
    },
    {
      "epoch": 2.2547815406211615,
      "grad_norm": 9.647268295288086,
      "learning_rate": 1.2440779084049833e-05,
      "loss": 2.4422,
      "step": 12850
    },
    {
      "epoch": 2.2565362344270925,
      "grad_norm": 7.004984378814697,
      "learning_rate": 1.241153418728432e-05,
      "loss": 2.5436,
      "step": 12860
    },
    {
      "epoch": 2.2582909282330235,
      "grad_norm": 6.500669956207275,
      "learning_rate": 1.2382289290518804e-05,
      "loss": 2.5455,
      "step": 12870
    },
    {
      "epoch": 2.2600456220389544,
      "grad_norm": 11.874530792236328,
      "learning_rate": 1.2353044393753292e-05,
      "loss": 2.3497,
      "step": 12880
    },
    {
      "epoch": 2.261800315844885,
      "grad_norm": 9.607792854309082,
      "learning_rate": 1.2323799496987775e-05,
      "loss": 2.3957,
      "step": 12890
    },
    {
      "epoch": 2.263555009650816,
      "grad_norm": 8.690451622009277,
      "learning_rate": 1.2294554600222263e-05,
      "loss": 2.6601,
      "step": 12900
    },
    {
      "epoch": 2.265309703456747,
      "grad_norm": 10.172541618347168,
      "learning_rate": 1.2265309703456747e-05,
      "loss": 2.3914,
      "step": 12910
    },
    {
      "epoch": 2.267064397262678,
      "grad_norm": 8.298615455627441,
      "learning_rate": 1.2236064806691232e-05,
      "loss": 2.507,
      "step": 12920
    },
    {
      "epoch": 2.2688190910686084,
      "grad_norm": 10.586010932922363,
      "learning_rate": 1.2206819909925718e-05,
      "loss": 2.4377,
      "step": 12930
    },
    {
      "epoch": 2.2705737848745393,
      "grad_norm": 11.4739351272583,
      "learning_rate": 1.2177575013160204e-05,
      "loss": 2.4034,
      "step": 12940
    },
    {
      "epoch": 2.2723284786804703,
      "grad_norm": 8.619543075561523,
      "learning_rate": 1.214833011639469e-05,
      "loss": 2.6994,
      "step": 12950
    },
    {
      "epoch": 2.2740831724864012,
      "grad_norm": 11.005472183227539,
      "learning_rate": 1.2119085219629175e-05,
      "loss": 2.4914,
      "step": 12960
    },
    {
      "epoch": 2.275837866292332,
      "grad_norm": 8.531974792480469,
      "learning_rate": 1.208984032286366e-05,
      "loss": 2.5898,
      "step": 12970
    },
    {
      "epoch": 2.2775925600982627,
      "grad_norm": 8.968770027160645,
      "learning_rate": 1.2060595426098146e-05,
      "loss": 2.4264,
      "step": 12980
    },
    {
      "epoch": 2.2793472539041937,
      "grad_norm": 8.874627113342285,
      "learning_rate": 1.2031350529332632e-05,
      "loss": 2.381,
      "step": 12990
    },
    {
      "epoch": 2.2811019477101246,
      "grad_norm": 10.249902725219727,
      "learning_rate": 1.2002105632567117e-05,
      "loss": 2.4746,
      "step": 13000
    },
    {
      "epoch": 2.2828566415160556,
      "grad_norm": 8.33400821685791,
      "learning_rate": 1.1972860735801603e-05,
      "loss": 2.4021,
      "step": 13010
    },
    {
      "epoch": 2.284611335321986,
      "grad_norm": 5.945827484130859,
      "learning_rate": 1.1943615839036089e-05,
      "loss": 2.4057,
      "step": 13020
    },
    {
      "epoch": 2.286366029127917,
      "grad_norm": 6.894702911376953,
      "learning_rate": 1.1914370942270574e-05,
      "loss": 2.5399,
      "step": 13030
    },
    {
      "epoch": 2.288120722933848,
      "grad_norm": 9.680030822753906,
      "learning_rate": 1.188512604550506e-05,
      "loss": 2.4712,
      "step": 13040
    },
    {
      "epoch": 2.289875416739779,
      "grad_norm": 8.637577056884766,
      "learning_rate": 1.1855881148739545e-05,
      "loss": 2.5582,
      "step": 13050
    },
    {
      "epoch": 2.29163011054571,
      "grad_norm": 6.80185079574585,
      "learning_rate": 1.1826636251974031e-05,
      "loss": 2.3312,
      "step": 13060
    },
    {
      "epoch": 2.2933848043516405,
      "grad_norm": 8.457059860229492,
      "learning_rate": 1.1797391355208517e-05,
      "loss": 2.5539,
      "step": 13070
    },
    {
      "epoch": 2.2951394981575715,
      "grad_norm": 8.308317184448242,
      "learning_rate": 1.1768146458443002e-05,
      "loss": 2.4746,
      "step": 13080
    },
    {
      "epoch": 2.2968941919635024,
      "grad_norm": 6.863090515136719,
      "learning_rate": 1.1738901561677488e-05,
      "loss": 2.6126,
      "step": 13090
    },
    {
      "epoch": 2.2986488857694334,
      "grad_norm": 7.979316711425781,
      "learning_rate": 1.1709656664911973e-05,
      "loss": 2.3362,
      "step": 13100
    },
    {
      "epoch": 2.300403579575364,
      "grad_norm": 8.283356666564941,
      "learning_rate": 1.1680411768146459e-05,
      "loss": 2.53,
      "step": 13110
    },
    {
      "epoch": 2.302158273381295,
      "grad_norm": 9.744537353515625,
      "learning_rate": 1.1651166871380945e-05,
      "loss": 2.757,
      "step": 13120
    },
    {
      "epoch": 2.303912967187226,
      "grad_norm": 8.466458320617676,
      "learning_rate": 1.162192197461543e-05,
      "loss": 2.4744,
      "step": 13130
    },
    {
      "epoch": 2.305667660993157,
      "grad_norm": 11.382628440856934,
      "learning_rate": 1.1592677077849916e-05,
      "loss": 2.4966,
      "step": 13140
    },
    {
      "epoch": 2.3074223547990877,
      "grad_norm": 7.9143595695495605,
      "learning_rate": 1.15634321810844e-05,
      "loss": 2.3865,
      "step": 13150
    },
    {
      "epoch": 2.3091770486050183,
      "grad_norm": 7.6332173347473145,
      "learning_rate": 1.1534187284318887e-05,
      "loss": 2.2971,
      "step": 13160
    },
    {
      "epoch": 2.3109317424109492,
      "grad_norm": 7.578042030334473,
      "learning_rate": 1.1504942387553371e-05,
      "loss": 2.6262,
      "step": 13170
    },
    {
      "epoch": 2.31268643621688,
      "grad_norm": 6.888478755950928,
      "learning_rate": 1.1475697490787858e-05,
      "loss": 2.5818,
      "step": 13180
    },
    {
      "epoch": 2.314441130022811,
      "grad_norm": 10.755704879760742,
      "learning_rate": 1.1446452594022342e-05,
      "loss": 2.2774,
      "step": 13190
    },
    {
      "epoch": 2.3161958238287417,
      "grad_norm": 10.136178016662598,
      "learning_rate": 1.141720769725683e-05,
      "loss": 2.5521,
      "step": 13200
    },
    {
      "epoch": 2.3179505176346726,
      "grad_norm": 6.020205497741699,
      "learning_rate": 1.1387962800491314e-05,
      "loss": 2.3395,
      "step": 13210
    },
    {
      "epoch": 2.3197052114406036,
      "grad_norm": 9.627642631530762,
      "learning_rate": 1.1358717903725801e-05,
      "loss": 2.5726,
      "step": 13220
    },
    {
      "epoch": 2.3214599052465346,
      "grad_norm": 8.4550142288208,
      "learning_rate": 1.1329473006960285e-05,
      "loss": 2.6418,
      "step": 13230
    },
    {
      "epoch": 2.3232145990524655,
      "grad_norm": 8.585010528564453,
      "learning_rate": 1.1300228110194772e-05,
      "loss": 2.4764,
      "step": 13240
    },
    {
      "epoch": 2.324969292858396,
      "grad_norm": 6.550507545471191,
      "learning_rate": 1.1270983213429256e-05,
      "loss": 2.4516,
      "step": 13250
    },
    {
      "epoch": 2.326723986664327,
      "grad_norm": 7.959822654724121,
      "learning_rate": 1.1241738316663743e-05,
      "loss": 2.4855,
      "step": 13260
    },
    {
      "epoch": 2.328478680470258,
      "grad_norm": 8.400908470153809,
      "learning_rate": 1.1212493419898229e-05,
      "loss": 2.4811,
      "step": 13270
    },
    {
      "epoch": 2.330233374276189,
      "grad_norm": 9.315485000610352,
      "learning_rate": 1.1183248523132715e-05,
      "loss": 2.2429,
      "step": 13280
    },
    {
      "epoch": 2.3319880680821194,
      "grad_norm": 7.851945400238037,
      "learning_rate": 1.11540036263672e-05,
      "loss": 2.4887,
      "step": 13290
    },
    {
      "epoch": 2.3337427618880504,
      "grad_norm": 8.90501594543457,
      "learning_rate": 1.1124758729601684e-05,
      "loss": 2.3834,
      "step": 13300
    },
    {
      "epoch": 2.3354974556939814,
      "grad_norm": 7.477398872375488,
      "learning_rate": 1.1095513832836172e-05,
      "loss": 2.638,
      "step": 13310
    },
    {
      "epoch": 2.3372521494999123,
      "grad_norm": 8.398862838745117,
      "learning_rate": 1.1066268936070655e-05,
      "loss": 2.6222,
      "step": 13320
    },
    {
      "epoch": 2.3390068433058433,
      "grad_norm": 8.764005661010742,
      "learning_rate": 1.1037024039305143e-05,
      "loss": 2.2433,
      "step": 13330
    },
    {
      "epoch": 2.340761537111774,
      "grad_norm": 8.881515502929688,
      "learning_rate": 1.1007779142539627e-05,
      "loss": 2.3708,
      "step": 13340
    },
    {
      "epoch": 2.342516230917705,
      "grad_norm": 8.90768051147461,
      "learning_rate": 1.0978534245774114e-05,
      "loss": 2.4206,
      "step": 13350
    },
    {
      "epoch": 2.3442709247236357,
      "grad_norm": 7.105897903442383,
      "learning_rate": 1.0949289349008598e-05,
      "loss": 2.3786,
      "step": 13360
    },
    {
      "epoch": 2.3460256185295667,
      "grad_norm": 8.78073501586914,
      "learning_rate": 1.0920044452243085e-05,
      "loss": 2.509,
      "step": 13370
    },
    {
      "epoch": 2.347780312335497,
      "grad_norm": 7.678630828857422,
      "learning_rate": 1.089079955547757e-05,
      "loss": 2.2145,
      "step": 13380
    },
    {
      "epoch": 2.349535006141428,
      "grad_norm": 8.893643379211426,
      "learning_rate": 1.0861554658712057e-05,
      "loss": 2.5162,
      "step": 13390
    },
    {
      "epoch": 2.351289699947359,
      "grad_norm": 9.045323371887207,
      "learning_rate": 1.083230976194654e-05,
      "loss": 2.697,
      "step": 13400
    },
    {
      "epoch": 2.35304439375329,
      "grad_norm": 8.01448917388916,
      "learning_rate": 1.0803064865181028e-05,
      "loss": 2.5178,
      "step": 13410
    },
    {
      "epoch": 2.354799087559221,
      "grad_norm": 10.142297744750977,
      "learning_rate": 1.0773819968415512e-05,
      "loss": 2.3396,
      "step": 13420
    },
    {
      "epoch": 2.3565537813651516,
      "grad_norm": 12.524666786193848,
      "learning_rate": 1.0744575071649999e-05,
      "loss": 2.3982,
      "step": 13430
    },
    {
      "epoch": 2.3583084751710826,
      "grad_norm": 8.853271484375,
      "learning_rate": 1.0715330174884483e-05,
      "loss": 2.3704,
      "step": 13440
    },
    {
      "epoch": 2.3600631689770135,
      "grad_norm": 11.3908109664917,
      "learning_rate": 1.0686085278118969e-05,
      "loss": 2.5319,
      "step": 13450
    },
    {
      "epoch": 2.3618178627829445,
      "grad_norm": 11.73098087310791,
      "learning_rate": 1.0656840381353454e-05,
      "loss": 2.6458,
      "step": 13460
    },
    {
      "epoch": 2.363572556588875,
      "grad_norm": 8.369698524475098,
      "learning_rate": 1.062759548458794e-05,
      "loss": 2.5464,
      "step": 13470
    },
    {
      "epoch": 2.365327250394806,
      "grad_norm": 6.846827983856201,
      "learning_rate": 1.0598350587822425e-05,
      "loss": 2.4409,
      "step": 13480
    },
    {
      "epoch": 2.367081944200737,
      "grad_norm": 7.860296726226807,
      "learning_rate": 1.0569105691056911e-05,
      "loss": 2.45,
      "step": 13490
    },
    {
      "epoch": 2.368836638006668,
      "grad_norm": 12.4185209274292,
      "learning_rate": 1.0539860794291397e-05,
      "loss": 2.6097,
      "step": 13500
    },
    {
      "epoch": 2.370591331812599,
      "grad_norm": 7.388949394226074,
      "learning_rate": 1.0510615897525882e-05,
      "loss": 2.4611,
      "step": 13510
    },
    {
      "epoch": 2.37234602561853,
      "grad_norm": 6.50897741317749,
      "learning_rate": 1.0481371000760368e-05,
      "loss": 2.3691,
      "step": 13520
    },
    {
      "epoch": 2.3741007194244603,
      "grad_norm": 7.237418174743652,
      "learning_rate": 1.0452126103994854e-05,
      "loss": 2.6173,
      "step": 13530
    },
    {
      "epoch": 2.3758554132303913,
      "grad_norm": 8.997517585754395,
      "learning_rate": 1.0422881207229339e-05,
      "loss": 2.4309,
      "step": 13540
    },
    {
      "epoch": 2.3776101070363223,
      "grad_norm": 7.175768852233887,
      "learning_rate": 1.0393636310463825e-05,
      "loss": 2.4382,
      "step": 13550
    },
    {
      "epoch": 2.379364800842253,
      "grad_norm": 11.531755447387695,
      "learning_rate": 1.036439141369831e-05,
      "loss": 2.35,
      "step": 13560
    },
    {
      "epoch": 2.3811194946481837,
      "grad_norm": 11.61562442779541,
      "learning_rate": 1.0335146516932796e-05,
      "loss": 2.4865,
      "step": 13570
    },
    {
      "epoch": 2.3828741884541147,
      "grad_norm": 8.189356803894043,
      "learning_rate": 1.0305901620167282e-05,
      "loss": 2.5261,
      "step": 13580
    },
    {
      "epoch": 2.3846288822600457,
      "grad_norm": 7.543436527252197,
      "learning_rate": 1.0276656723401767e-05,
      "loss": 2.648,
      "step": 13590
    },
    {
      "epoch": 2.3863835760659766,
      "grad_norm": 11.0587158203125,
      "learning_rate": 1.0247411826636253e-05,
      "loss": 2.3257,
      "step": 13600
    },
    {
      "epoch": 2.3881382698719076,
      "grad_norm": 7.365039825439453,
      "learning_rate": 1.0218166929870738e-05,
      "loss": 2.373,
      "step": 13610
    },
    {
      "epoch": 2.389892963677838,
      "grad_norm": 8.526531219482422,
      "learning_rate": 1.0188922033105224e-05,
      "loss": 2.3368,
      "step": 13620
    },
    {
      "epoch": 2.391647657483769,
      "grad_norm": 11.45175552368164,
      "learning_rate": 1.015967713633971e-05,
      "loss": 2.5751,
      "step": 13630
    },
    {
      "epoch": 2.3934023512897,
      "grad_norm": 6.849976062774658,
      "learning_rate": 1.0130432239574195e-05,
      "loss": 2.4126,
      "step": 13640
    },
    {
      "epoch": 2.395157045095631,
      "grad_norm": 9.043408393859863,
      "learning_rate": 1.0101187342808681e-05,
      "loss": 2.8983,
      "step": 13650
    },
    {
      "epoch": 2.3969117389015615,
      "grad_norm": 10.927433967590332,
      "learning_rate": 1.0071942446043167e-05,
      "loss": 2.4244,
      "step": 13660
    },
    {
      "epoch": 2.3986664327074925,
      "grad_norm": 8.611252784729004,
      "learning_rate": 1.0042697549277652e-05,
      "loss": 2.4253,
      "step": 13670
    },
    {
      "epoch": 2.4004211265134234,
      "grad_norm": 7.678099155426025,
      "learning_rate": 1.0013452652512136e-05,
      "loss": 2.5383,
      "step": 13680
    },
    {
      "epoch": 2.4021758203193544,
      "grad_norm": 8.236473083496094,
      "learning_rate": 9.984207755746623e-06,
      "loss": 2.3742,
      "step": 13690
    },
    {
      "epoch": 2.4039305141252854,
      "grad_norm": 8.191566467285156,
      "learning_rate": 9.954962858981107e-06,
      "loss": 2.5308,
      "step": 13700
    },
    {
      "epoch": 2.405685207931216,
      "grad_norm": 9.448955535888672,
      "learning_rate": 9.925717962215595e-06,
      "loss": 2.4482,
      "step": 13710
    },
    {
      "epoch": 2.407439901737147,
      "grad_norm": 8.742284774780273,
      "learning_rate": 9.896473065450079e-06,
      "loss": 2.6645,
      "step": 13720
    },
    {
      "epoch": 2.409194595543078,
      "grad_norm": 10.344829559326172,
      "learning_rate": 9.867228168684566e-06,
      "loss": 2.5061,
      "step": 13730
    },
    {
      "epoch": 2.4109492893490088,
      "grad_norm": 8.654509544372559,
      "learning_rate": 9.83798327191905e-06,
      "loss": 2.6149,
      "step": 13740
    },
    {
      "epoch": 2.4127039831549393,
      "grad_norm": 8.838055610656738,
      "learning_rate": 9.808738375153537e-06,
      "loss": 2.6163,
      "step": 13750
    },
    {
      "epoch": 2.4144586769608702,
      "grad_norm": 9.809078216552734,
      "learning_rate": 9.779493478388021e-06,
      "loss": 2.5007,
      "step": 13760
    },
    {
      "epoch": 2.416213370766801,
      "grad_norm": 9.000716209411621,
      "learning_rate": 9.750248581622508e-06,
      "loss": 2.283,
      "step": 13770
    },
    {
      "epoch": 2.417968064572732,
      "grad_norm": 9.869659423828125,
      "learning_rate": 9.721003684856992e-06,
      "loss": 2.5237,
      "step": 13780
    },
    {
      "epoch": 2.419722758378663,
      "grad_norm": 7.0789995193481445,
      "learning_rate": 9.69175878809148e-06,
      "loss": 2.4752,
      "step": 13790
    },
    {
      "epoch": 2.4214774521845936,
      "grad_norm": 6.290640354156494,
      "learning_rate": 9.662513891325964e-06,
      "loss": 2.2476,
      "step": 13800
    },
    {
      "epoch": 2.4232321459905246,
      "grad_norm": 9.549306869506836,
      "learning_rate": 9.633268994560451e-06,
      "loss": 2.5704,
      "step": 13810
    },
    {
      "epoch": 2.4249868397964556,
      "grad_norm": 8.867992401123047,
      "learning_rate": 9.604024097794935e-06,
      "loss": 2.5214,
      "step": 13820
    },
    {
      "epoch": 2.4267415336023865,
      "grad_norm": 6.5872273445129395,
      "learning_rate": 9.57477920102942e-06,
      "loss": 2.3665,
      "step": 13830
    },
    {
      "epoch": 2.428496227408317,
      "grad_norm": 8.938096046447754,
      "learning_rate": 9.545534304263906e-06,
      "loss": 2.4366,
      "step": 13840
    },
    {
      "epoch": 2.430250921214248,
      "grad_norm": 12.27813720703125,
      "learning_rate": 9.516289407498392e-06,
      "loss": 2.2262,
      "step": 13850
    },
    {
      "epoch": 2.432005615020179,
      "grad_norm": 8.7827787399292,
      "learning_rate": 9.487044510732877e-06,
      "loss": 2.6312,
      "step": 13860
    },
    {
      "epoch": 2.43376030882611,
      "grad_norm": 9.384825706481934,
      "learning_rate": 9.457799613967363e-06,
      "loss": 2.4944,
      "step": 13870
    },
    {
      "epoch": 2.435515002632041,
      "grad_norm": 8.327624320983887,
      "learning_rate": 9.428554717201849e-06,
      "loss": 2.5356,
      "step": 13880
    },
    {
      "epoch": 2.4372696964379714,
      "grad_norm": 8.198604583740234,
      "learning_rate": 9.399309820436334e-06,
      "loss": 2.7064,
      "step": 13890
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 7.396657466888428,
      "learning_rate": 9.37006492367082e-06,
      "loss": 2.5655,
      "step": 13900
    },
    {
      "epoch": 2.4407790840498333,
      "grad_norm": 6.324214458465576,
      "learning_rate": 9.340820026905305e-06,
      "loss": 2.5002,
      "step": 13910
    },
    {
      "epoch": 2.4425337778557643,
      "grad_norm": 8.99838924407959,
      "learning_rate": 9.311575130139791e-06,
      "loss": 2.5356,
      "step": 13920
    },
    {
      "epoch": 2.444288471661695,
      "grad_norm": 7.994616508483887,
      "learning_rate": 9.282330233374277e-06,
      "loss": 2.2967,
      "step": 13930
    },
    {
      "epoch": 2.446043165467626,
      "grad_norm": 9.296357154846191,
      "learning_rate": 9.253085336608762e-06,
      "loss": 2.2631,
      "step": 13940
    },
    {
      "epoch": 2.4477978592735568,
      "grad_norm": 6.572951316833496,
      "learning_rate": 9.223840439843248e-06,
      "loss": 2.3117,
      "step": 13950
    },
    {
      "epoch": 2.4495525530794877,
      "grad_norm": 8.493037223815918,
      "learning_rate": 9.194595543077734e-06,
      "loss": 2.5096,
      "step": 13960
    },
    {
      "epoch": 2.4513072468854187,
      "grad_norm": 9.485112190246582,
      "learning_rate": 9.16535064631222e-06,
      "loss": 2.4532,
      "step": 13970
    },
    {
      "epoch": 2.453061940691349,
      "grad_norm": 8.665090560913086,
      "learning_rate": 9.136105749546705e-06,
      "loss": 2.3704,
      "step": 13980
    },
    {
      "epoch": 2.45481663449728,
      "grad_norm": 9.09191608428955,
      "learning_rate": 9.10686085278119e-06,
      "loss": 2.5898,
      "step": 13990
    },
    {
      "epoch": 2.456571328303211,
      "grad_norm": 9.907564163208008,
      "learning_rate": 9.077615956015676e-06,
      "loss": 2.3703,
      "step": 14000
    },
    {
      "epoch": 2.458326022109142,
      "grad_norm": 13.188003540039062,
      "learning_rate": 9.048371059250162e-06,
      "loss": 2.358,
      "step": 14010
    },
    {
      "epoch": 2.4600807159150726,
      "grad_norm": 8.38581657409668,
      "learning_rate": 9.019126162484647e-06,
      "loss": 2.4853,
      "step": 14020
    },
    {
      "epoch": 2.4618354097210036,
      "grad_norm": 8.907434463500977,
      "learning_rate": 8.989881265719133e-06,
      "loss": 2.3065,
      "step": 14030
    },
    {
      "epoch": 2.4635901035269345,
      "grad_norm": 9.75571060180664,
      "learning_rate": 8.960636368953619e-06,
      "loss": 2.5574,
      "step": 14040
    },
    {
      "epoch": 2.4653447973328655,
      "grad_norm": 11.713635444641113,
      "learning_rate": 8.931391472188104e-06,
      "loss": 2.3715,
      "step": 14050
    },
    {
      "epoch": 2.4670994911387965,
      "grad_norm": 10.368806838989258,
      "learning_rate": 8.902146575422588e-06,
      "loss": 2.567,
      "step": 14060
    },
    {
      "epoch": 2.468854184944727,
      "grad_norm": 8.13353443145752,
      "learning_rate": 8.872901678657075e-06,
      "loss": 2.2068,
      "step": 14070
    },
    {
      "epoch": 2.470608878750658,
      "grad_norm": 8.313493728637695,
      "learning_rate": 8.84365678189156e-06,
      "loss": 2.6075,
      "step": 14080
    },
    {
      "epoch": 2.472363572556589,
      "grad_norm": 8.065692901611328,
      "learning_rate": 8.814411885126047e-06,
      "loss": 2.4997,
      "step": 14090
    },
    {
      "epoch": 2.47411826636252,
      "grad_norm": 5.958445072174072,
      "learning_rate": 8.78516698836053e-06,
      "loss": 2.4916,
      "step": 14100
    },
    {
      "epoch": 2.4758729601684504,
      "grad_norm": 10.998282432556152,
      "learning_rate": 8.755922091595018e-06,
      "loss": 2.5298,
      "step": 14110
    },
    {
      "epoch": 2.4776276539743813,
      "grad_norm": 10.082894325256348,
      "learning_rate": 8.726677194829502e-06,
      "loss": 2.2665,
      "step": 14120
    },
    {
      "epoch": 2.4793823477803123,
      "grad_norm": 11.84870433807373,
      "learning_rate": 8.697432298063989e-06,
      "loss": 2.3011,
      "step": 14130
    },
    {
      "epoch": 2.4811370415862433,
      "grad_norm": 6.831900596618652,
      "learning_rate": 8.668187401298473e-06,
      "loss": 2.5869,
      "step": 14140
    },
    {
      "epoch": 2.4828917353921742,
      "grad_norm": 8.040821075439453,
      "learning_rate": 8.63894250453296e-06,
      "loss": 2.351,
      "step": 14150
    },
    {
      "epoch": 2.4846464291981047,
      "grad_norm": 10.025574684143066,
      "learning_rate": 8.609697607767444e-06,
      "loss": 2.5033,
      "step": 14160
    },
    {
      "epoch": 2.4864011230040357,
      "grad_norm": 7.207585334777832,
      "learning_rate": 8.580452711001932e-06,
      "loss": 2.478,
      "step": 14170
    },
    {
      "epoch": 2.4881558168099667,
      "grad_norm": 8.448648452758789,
      "learning_rate": 8.551207814236416e-06,
      "loss": 2.364,
      "step": 14180
    },
    {
      "epoch": 2.4899105106158976,
      "grad_norm": 8.264514923095703,
      "learning_rate": 8.521962917470903e-06,
      "loss": 2.5787,
      "step": 14190
    },
    {
      "epoch": 2.491665204421828,
      "grad_norm": 10.242830276489258,
      "learning_rate": 8.492718020705387e-06,
      "loss": 2.4265,
      "step": 14200
    },
    {
      "epoch": 2.493419898227759,
      "grad_norm": 8.54570198059082,
      "learning_rate": 8.463473123939872e-06,
      "loss": 2.3651,
      "step": 14210
    },
    {
      "epoch": 2.49517459203369,
      "grad_norm": 7.408640384674072,
      "learning_rate": 8.434228227174358e-06,
      "loss": 2.6272,
      "step": 14220
    },
    {
      "epoch": 2.496929285839621,
      "grad_norm": 8.334970474243164,
      "learning_rate": 8.404983330408844e-06,
      "loss": 2.4223,
      "step": 14230
    },
    {
      "epoch": 2.498683979645552,
      "grad_norm": 8.674464225769043,
      "learning_rate": 8.37573843364333e-06,
      "loss": 2.3855,
      "step": 14240
    },
    {
      "epoch": 2.500438673451483,
      "grad_norm": 9.287162780761719,
      "learning_rate": 8.346493536877815e-06,
      "loss": 2.6565,
      "step": 14250
    },
    {
      "epoch": 2.5021933672574135,
      "grad_norm": 8.621679306030273,
      "learning_rate": 8.3172486401123e-06,
      "loss": 2.4509,
      "step": 14260
    },
    {
      "epoch": 2.5039480610633444,
      "grad_norm": 7.337958335876465,
      "learning_rate": 8.288003743346786e-06,
      "loss": 2.5085,
      "step": 14270
    },
    {
      "epoch": 2.5057027548692754,
      "grad_norm": 11.115388870239258,
      "learning_rate": 8.258758846581272e-06,
      "loss": 2.7058,
      "step": 14280
    },
    {
      "epoch": 2.507457448675206,
      "grad_norm": 8.508142471313477,
      "learning_rate": 8.229513949815757e-06,
      "loss": 2.3904,
      "step": 14290
    },
    {
      "epoch": 2.509212142481137,
      "grad_norm": 9.126998901367188,
      "learning_rate": 8.200269053050243e-06,
      "loss": 2.729,
      "step": 14300
    },
    {
      "epoch": 2.510966836287068,
      "grad_norm": 8.214146614074707,
      "learning_rate": 8.171024156284729e-06,
      "loss": 2.4741,
      "step": 14310
    },
    {
      "epoch": 2.512721530092999,
      "grad_norm": 9.252058982849121,
      "learning_rate": 8.141779259519214e-06,
      "loss": 2.3793,
      "step": 14320
    },
    {
      "epoch": 2.5144762238989298,
      "grad_norm": 8.726460456848145,
      "learning_rate": 8.1125343627537e-06,
      "loss": 2.4021,
      "step": 14330
    },
    {
      "epoch": 2.5162309177048607,
      "grad_norm": 7.8330793380737305,
      "learning_rate": 8.083289465988185e-06,
      "loss": 2.4069,
      "step": 14340
    },
    {
      "epoch": 2.5179856115107913,
      "grad_norm": 7.800514221191406,
      "learning_rate": 8.054044569222671e-06,
      "loss": 2.5161,
      "step": 14350
    },
    {
      "epoch": 2.519740305316722,
      "grad_norm": 7.288799285888672,
      "learning_rate": 8.024799672457157e-06,
      "loss": 2.6429,
      "step": 14360
    },
    {
      "epoch": 2.521494999122653,
      "grad_norm": 9.89874267578125,
      "learning_rate": 7.995554775691642e-06,
      "loss": 2.3725,
      "step": 14370
    },
    {
      "epoch": 2.5232496929285837,
      "grad_norm": 9.360734939575195,
      "learning_rate": 7.966309878926128e-06,
      "loss": 2.5962,
      "step": 14380
    },
    {
      "epoch": 2.5250043867345147,
      "grad_norm": 9.754090309143066,
      "learning_rate": 7.937064982160614e-06,
      "loss": 2.4639,
      "step": 14390
    },
    {
      "epoch": 2.5267590805404456,
      "grad_norm": 12.478517532348633,
      "learning_rate": 7.9078200853951e-06,
      "loss": 2.4242,
      "step": 14400
    },
    {
      "epoch": 2.5285137743463766,
      "grad_norm": 8.686078071594238,
      "learning_rate": 7.878575188629585e-06,
      "loss": 2.3382,
      "step": 14410
    },
    {
      "epoch": 2.5302684681523075,
      "grad_norm": 8.456178665161133,
      "learning_rate": 7.84933029186407e-06,
      "loss": 2.3832,
      "step": 14420
    },
    {
      "epoch": 2.5320231619582385,
      "grad_norm": 10.018614768981934,
      "learning_rate": 7.820085395098556e-06,
      "loss": 2.5661,
      "step": 14430
    },
    {
      "epoch": 2.533777855764169,
      "grad_norm": 8.835572242736816,
      "learning_rate": 7.79084049833304e-06,
      "loss": 2.5417,
      "step": 14440
    },
    {
      "epoch": 2.5355325495701,
      "grad_norm": 9.085909843444824,
      "learning_rate": 7.761595601567527e-06,
      "loss": 2.3265,
      "step": 14450
    },
    {
      "epoch": 2.537287243376031,
      "grad_norm": 9.73375129699707,
      "learning_rate": 7.732350704802011e-06,
      "loss": 2.6018,
      "step": 14460
    },
    {
      "epoch": 2.5390419371819615,
      "grad_norm": 8.56855297088623,
      "learning_rate": 7.703105808036499e-06,
      "loss": 2.3167,
      "step": 14470
    },
    {
      "epoch": 2.5407966309878924,
      "grad_norm": 7.206392288208008,
      "learning_rate": 7.673860911270982e-06,
      "loss": 2.4036,
      "step": 14480
    },
    {
      "epoch": 2.5425513247938234,
      "grad_norm": 6.633063793182373,
      "learning_rate": 7.64461601450547e-06,
      "loss": 2.2635,
      "step": 14490
    },
    {
      "epoch": 2.5443060185997544,
      "grad_norm": 10.390796661376953,
      "learning_rate": 7.6153711177399545e-06,
      "loss": 2.3167,
      "step": 14500
    },
    {
      "epoch": 2.5460607124056853,
      "grad_norm": 9.596853256225586,
      "learning_rate": 7.58612622097444e-06,
      "loss": 2.423,
      "step": 14510
    },
    {
      "epoch": 2.5478154062116163,
      "grad_norm": 9.887350082397461,
      "learning_rate": 7.556881324208925e-06,
      "loss": 2.5449,
      "step": 14520
    },
    {
      "epoch": 2.549570100017547,
      "grad_norm": 10.928149223327637,
      "learning_rate": 7.527636427443411e-06,
      "loss": 2.5306,
      "step": 14530
    },
    {
      "epoch": 2.5513247938234778,
      "grad_norm": 9.384140968322754,
      "learning_rate": 7.498391530677896e-06,
      "loss": 2.3698,
      "step": 14540
    },
    {
      "epoch": 2.5530794876294087,
      "grad_norm": 7.790217399597168,
      "learning_rate": 7.469146633912383e-06,
      "loss": 2.728,
      "step": 14550
    },
    {
      "epoch": 2.5548341814353397,
      "grad_norm": 7.03154182434082,
      "learning_rate": 7.4399017371468674e-06,
      "loss": 2.4923,
      "step": 14560
    },
    {
      "epoch": 2.55658887524127,
      "grad_norm": 7.070491790771484,
      "learning_rate": 7.410656840381354e-06,
      "loss": 2.5933,
      "step": 14570
    },
    {
      "epoch": 2.558343569047201,
      "grad_norm": 7.273715019226074,
      "learning_rate": 7.381411943615839e-06,
      "loss": 2.4732,
      "step": 14580
    },
    {
      "epoch": 2.560098262853132,
      "grad_norm": 7.814031600952148,
      "learning_rate": 7.352167046850325e-06,
      "loss": 2.3669,
      "step": 14590
    },
    {
      "epoch": 2.561852956659063,
      "grad_norm": 9.074956893920898,
      "learning_rate": 7.32292215008481e-06,
      "loss": 2.4271,
      "step": 14600
    },
    {
      "epoch": 2.563607650464994,
      "grad_norm": 7.153980731964111,
      "learning_rate": 7.293677253319296e-06,
      "loss": 2.3462,
      "step": 14610
    },
    {
      "epoch": 2.5653623442709246,
      "grad_norm": 14.118101119995117,
      "learning_rate": 7.264432356553781e-06,
      "loss": 2.4911,
      "step": 14620
    },
    {
      "epoch": 2.5671170380768555,
      "grad_norm": 9.71121597290039,
      "learning_rate": 7.235187459788268e-06,
      "loss": 2.5861,
      "step": 14630
    },
    {
      "epoch": 2.5688717318827865,
      "grad_norm": 11.298120498657227,
      "learning_rate": 7.205942563022752e-06,
      "loss": 2.6002,
      "step": 14640
    },
    {
      "epoch": 2.5706264256887175,
      "grad_norm": 11.468791961669922,
      "learning_rate": 7.176697666257239e-06,
      "loss": 2.3443,
      "step": 14650
    },
    {
      "epoch": 2.572381119494648,
      "grad_norm": 8.727171897888184,
      "learning_rate": 7.147452769491724e-06,
      "loss": 2.4805,
      "step": 14660
    },
    {
      "epoch": 2.574135813300579,
      "grad_norm": 7.808493137359619,
      "learning_rate": 7.11820787272621e-06,
      "loss": 2.4026,
      "step": 14670
    },
    {
      "epoch": 2.57589050710651,
      "grad_norm": 10.262815475463867,
      "learning_rate": 7.088962975960695e-06,
      "loss": 2.4608,
      "step": 14680
    },
    {
      "epoch": 2.577645200912441,
      "grad_norm": 10.313952445983887,
      "learning_rate": 7.0597180791951805e-06,
      "loss": 2.5131,
      "step": 14690
    },
    {
      "epoch": 2.579399894718372,
      "grad_norm": 8.374090194702148,
      "learning_rate": 7.030473182429666e-06,
      "loss": 2.4427,
      "step": 14700
    },
    {
      "epoch": 2.5811545885243024,
      "grad_norm": 9.391357421875,
      "learning_rate": 7.001228285664152e-06,
      "loss": 2.5108,
      "step": 14710
    },
    {
      "epoch": 2.5829092823302333,
      "grad_norm": 11.356706619262695,
      "learning_rate": 6.9719833888986365e-06,
      "loss": 2.2536,
      "step": 14720
    },
    {
      "epoch": 2.5846639761361643,
      "grad_norm": 8.123409271240234,
      "learning_rate": 6.942738492133123e-06,
      "loss": 2.3685,
      "step": 14730
    },
    {
      "epoch": 2.5864186699420952,
      "grad_norm": 7.443994998931885,
      "learning_rate": 6.913493595367608e-06,
      "loss": 2.1356,
      "step": 14740
    },
    {
      "epoch": 2.5881733637480258,
      "grad_norm": 8.934300422668457,
      "learning_rate": 6.884248698602094e-06,
      "loss": 2.6201,
      "step": 14750
    },
    {
      "epoch": 2.5899280575539567,
      "grad_norm": 8.482352256774902,
      "learning_rate": 6.855003801836579e-06,
      "loss": 2.4037,
      "step": 14760
    },
    {
      "epoch": 2.5916827513598877,
      "grad_norm": 8.034249305725098,
      "learning_rate": 6.8257589050710655e-06,
      "loss": 2.3569,
      "step": 14770
    },
    {
      "epoch": 2.5934374451658186,
      "grad_norm": 7.887059688568115,
      "learning_rate": 6.79651400830555e-06,
      "loss": 2.3851,
      "step": 14780
    },
    {
      "epoch": 2.5951921389717496,
      "grad_norm": Infinity,
      "learning_rate": 6.770193601216589e-06,
      "loss": 2.2338,
      "step": 14790
    },
    {
      "epoch": 2.5969468327776806,
      "grad_norm": 8.788350105285645,
      "learning_rate": 6.740948704451074e-06,
      "loss": 2.2991,
      "step": 14800
    },
    {
      "epoch": 2.598701526583611,
      "grad_norm": 7.240419387817383,
      "learning_rate": 6.71170380768556e-06,
      "loss": 2.5723,
      "step": 14810
    },
    {
      "epoch": 2.600456220389542,
      "grad_norm": 9.40127182006836,
      "learning_rate": 6.682458910920045e-06,
      "loss": 2.3747,
      "step": 14820
    },
    {
      "epoch": 2.602210914195473,
      "grad_norm": 10.31748104095459,
      "learning_rate": 6.653214014154531e-06,
      "loss": 2.4628,
      "step": 14830
    },
    {
      "epoch": 2.6039656080014035,
      "grad_norm": 8.750615119934082,
      "learning_rate": 6.623969117389016e-06,
      "loss": 2.6266,
      "step": 14840
    },
    {
      "epoch": 2.6057203018073345,
      "grad_norm": 7.299401760101318,
      "learning_rate": 6.5947242206235026e-06,
      "loss": 2.4987,
      "step": 14850
    },
    {
      "epoch": 2.6074749956132655,
      "grad_norm": 11.632186889648438,
      "learning_rate": 6.565479323857987e-06,
      "loss": 2.4374,
      "step": 14860
    },
    {
      "epoch": 2.6092296894191964,
      "grad_norm": 8.568450927734375,
      "learning_rate": 6.536234427092473e-06,
      "loss": 2.4483,
      "step": 14870
    },
    {
      "epoch": 2.6109843832251274,
      "grad_norm": 8.297239303588867,
      "learning_rate": 6.506989530326959e-06,
      "loss": 2.5222,
      "step": 14880
    },
    {
      "epoch": 2.6127390770310583,
      "grad_norm": 7.621902942657471,
      "learning_rate": 6.477744633561444e-06,
      "loss": 2.5669,
      "step": 14890
    },
    {
      "epoch": 2.614493770836989,
      "grad_norm": 7.080008506774902,
      "learning_rate": 6.448499736795929e-06,
      "loss": 2.7692,
      "step": 14900
    },
    {
      "epoch": 2.61624846464292,
      "grad_norm": 7.602224349975586,
      "learning_rate": 6.4192548400304154e-06,
      "loss": 2.3427,
      "step": 14910
    },
    {
      "epoch": 2.618003158448851,
      "grad_norm": 8.053292274475098,
      "learning_rate": 6.3900099432649e-06,
      "loss": 2.6547,
      "step": 14920
    },
    {
      "epoch": 2.6197578522547813,
      "grad_norm": 10.607797622680664,
      "learning_rate": 6.360765046499387e-06,
      "loss": 2.5475,
      "step": 14930
    },
    {
      "epoch": 2.6215125460607123,
      "grad_norm": 12.637409210205078,
      "learning_rate": 6.3315201497338715e-06,
      "loss": 2.4665,
      "step": 14940
    },
    {
      "epoch": 2.6232672398666432,
      "grad_norm": 10.648067474365234,
      "learning_rate": 6.302275252968358e-06,
      "loss": 2.4248,
      "step": 14950
    },
    {
      "epoch": 2.625021933672574,
      "grad_norm": 10.284584999084473,
      "learning_rate": 6.273030356202843e-06,
      "loss": 2.3615,
      "step": 14960
    },
    {
      "epoch": 2.626776627478505,
      "grad_norm": 9.427643775939941,
      "learning_rate": 6.243785459437328e-06,
      "loss": 2.5979,
      "step": 14970
    },
    {
      "epoch": 2.628531321284436,
      "grad_norm": 9.565200805664062,
      "learning_rate": 6.214540562671814e-06,
      "loss": 2.4925,
      "step": 14980
    },
    {
      "epoch": 2.6302860150903666,
      "grad_norm": 6.747193336486816,
      "learning_rate": 6.1852956659062996e-06,
      "loss": 2.5413,
      "step": 14990
    },
    {
      "epoch": 2.6320407088962976,
      "grad_norm": 6.730859279632568,
      "learning_rate": 6.156050769140785e-06,
      "loss": 2.7256,
      "step": 15000
    },
    {
      "epoch": 2.6337954027022286,
      "grad_norm": 7.735109329223633,
      "learning_rate": 6.126805872375271e-06,
      "loss": 2.2511,
      "step": 15010
    },
    {
      "epoch": 2.635550096508159,
      "grad_norm": 8.480988502502441,
      "learning_rate": 6.0975609756097564e-06,
      "loss": 2.4376,
      "step": 15020
    },
    {
      "epoch": 2.63730479031409,
      "grad_norm": 8.26540470123291,
      "learning_rate": 6.068316078844242e-06,
      "loss": 2.6002,
      "step": 15030
    },
    {
      "epoch": 2.639059484120021,
      "grad_norm": 7.5234527587890625,
      "learning_rate": 6.039071182078728e-06,
      "loss": 2.2518,
      "step": 15040
    },
    {
      "epoch": 2.640814177925952,
      "grad_norm": 10.848958969116211,
      "learning_rate": 6.009826285313213e-06,
      "loss": 2.5576,
      "step": 15050
    },
    {
      "epoch": 2.642568871731883,
      "grad_norm": 8.134759902954102,
      "learning_rate": 5.980581388547699e-06,
      "loss": 2.4669,
      "step": 15060
    },
    {
      "epoch": 2.644323565537814,
      "grad_norm": 7.648292541503906,
      "learning_rate": 5.9513364917821845e-06,
      "loss": 2.3938,
      "step": 15070
    },
    {
      "epoch": 2.6460782593437444,
      "grad_norm": 7.524911403656006,
      "learning_rate": 5.922091595016669e-06,
      "loss": 2.5187,
      "step": 15080
    },
    {
      "epoch": 2.6478329531496754,
      "grad_norm": 9.317538261413574,
      "learning_rate": 5.892846698251155e-06,
      "loss": 2.2258,
      "step": 15090
    },
    {
      "epoch": 2.6495876469556063,
      "grad_norm": 8.219943046569824,
      "learning_rate": 5.8636018014856406e-06,
      "loss": 2.4506,
      "step": 15100
    },
    {
      "epoch": 2.651342340761537,
      "grad_norm": 6.119924068450928,
      "learning_rate": 5.834356904720126e-06,
      "loss": 2.3113,
      "step": 15110
    },
    {
      "epoch": 2.653097034567468,
      "grad_norm": 9.071462631225586,
      "learning_rate": 5.805112007954612e-06,
      "loss": 2.3751,
      "step": 15120
    },
    {
      "epoch": 2.654851728373399,
      "grad_norm": 7.899154186248779,
      "learning_rate": 5.775867111189097e-06,
      "loss": 2.3707,
      "step": 15130
    },
    {
      "epoch": 2.6566064221793297,
      "grad_norm": 10.887627601623535,
      "learning_rate": 5.746622214423583e-06,
      "loss": 2.5049,
      "step": 15140
    },
    {
      "epoch": 2.6583611159852607,
      "grad_norm": 7.1840033531188965,
      "learning_rate": 5.717377317658069e-06,
      "loss": 2.2252,
      "step": 15150
    },
    {
      "epoch": 2.6601158097911917,
      "grad_norm": 7.002256870269775,
      "learning_rate": 5.688132420892554e-06,
      "loss": 2.3993,
      "step": 15160
    },
    {
      "epoch": 2.661870503597122,
      "grad_norm": 9.105148315429688,
      "learning_rate": 5.65888752412704e-06,
      "loss": 2.4066,
      "step": 15170
    },
    {
      "epoch": 2.663625197403053,
      "grad_norm": 8.48957347869873,
      "learning_rate": 5.6296426273615255e-06,
      "loss": 2.6599,
      "step": 15180
    },
    {
      "epoch": 2.665379891208984,
      "grad_norm": 8.423202514648438,
      "learning_rate": 5.600397730596011e-06,
      "loss": 2.5595,
      "step": 15190
    },
    {
      "epoch": 2.6671345850149146,
      "grad_norm": 9.06076717376709,
      "learning_rate": 5.571152833830497e-06,
      "loss": 2.4789,
      "step": 15200
    },
    {
      "epoch": 2.6688892788208456,
      "grad_norm": 9.590036392211914,
      "learning_rate": 5.541907937064982e-06,
      "loss": 2.5985,
      "step": 15210
    },
    {
      "epoch": 2.6706439726267766,
      "grad_norm": 8.249414443969727,
      "learning_rate": 5.512663040299468e-06,
      "loss": 2.5919,
      "step": 15220
    },
    {
      "epoch": 2.6723986664327075,
      "grad_norm": 9.124754905700684,
      "learning_rate": 5.483418143533954e-06,
      "loss": 2.337,
      "step": 15230
    },
    {
      "epoch": 2.6741533602386385,
      "grad_norm": 8.1058931350708,
      "learning_rate": 5.454173246768439e-06,
      "loss": 2.4349,
      "step": 15240
    },
    {
      "epoch": 2.6759080540445694,
      "grad_norm": 6.889133930206299,
      "learning_rate": 5.424928350002925e-06,
      "loss": 2.2754,
      "step": 15250
    },
    {
      "epoch": 2.6776627478505,
      "grad_norm": 9.26036262512207,
      "learning_rate": 5.3956834532374105e-06,
      "loss": 2.471,
      "step": 15260
    },
    {
      "epoch": 2.679417441656431,
      "grad_norm": 8.852865219116211,
      "learning_rate": 5.366438556471895e-06,
      "loss": 2.4514,
      "step": 15270
    },
    {
      "epoch": 2.681172135462362,
      "grad_norm": 9.778197288513184,
      "learning_rate": 5.337193659706381e-06,
      "loss": 2.5813,
      "step": 15280
    },
    {
      "epoch": 2.682926829268293,
      "grad_norm": 8.484532356262207,
      "learning_rate": 5.3079487629408665e-06,
      "loss": 2.1842,
      "step": 15290
    },
    {
      "epoch": 2.6846815230742234,
      "grad_norm": 8.821457862854004,
      "learning_rate": 5.278703866175352e-06,
      "loss": 2.4038,
      "step": 15300
    },
    {
      "epoch": 2.6864362168801543,
      "grad_norm": 9.783447265625,
      "learning_rate": 5.249458969409838e-06,
      "loss": 2.6843,
      "step": 15310
    },
    {
      "epoch": 2.6881909106860853,
      "grad_norm": 8.741825103759766,
      "learning_rate": 5.220214072644323e-06,
      "loss": 2.5113,
      "step": 15320
    },
    {
      "epoch": 2.6899456044920163,
      "grad_norm": 7.786653518676758,
      "learning_rate": 5.190969175878809e-06,
      "loss": 2.3541,
      "step": 15330
    },
    {
      "epoch": 2.691700298297947,
      "grad_norm": 7.331043720245361,
      "learning_rate": 5.161724279113295e-06,
      "loss": 2.2638,
      "step": 15340
    },
    {
      "epoch": 2.6934549921038777,
      "grad_norm": 8.946433067321777,
      "learning_rate": 5.13247938234778e-06,
      "loss": 2.5943,
      "step": 15350
    },
    {
      "epoch": 2.6952096859098087,
      "grad_norm": 7.39707612991333,
      "learning_rate": 5.103234485582266e-06,
      "loss": 2.5055,
      "step": 15360
    },
    {
      "epoch": 2.6969643797157397,
      "grad_norm": 8.050268173217773,
      "learning_rate": 5.0739895888167515e-06,
      "loss": 2.4004,
      "step": 15370
    },
    {
      "epoch": 2.6987190735216706,
      "grad_norm": 10.089035034179688,
      "learning_rate": 5.044744692051237e-06,
      "loss": 2.4513,
      "step": 15380
    },
    {
      "epoch": 2.700473767327601,
      "grad_norm": 9.513641357421875,
      "learning_rate": 5.015499795285723e-06,
      "loss": 2.3892,
      "step": 15390
    },
    {
      "epoch": 2.702228461133532,
      "grad_norm": 8.04715347290039,
      "learning_rate": 4.986254898520208e-06,
      "loss": 2.6193,
      "step": 15400
    },
    {
      "epoch": 2.703983154939463,
      "grad_norm": 13.937725067138672,
      "learning_rate": 4.957010001754694e-06,
      "loss": 2.3112,
      "step": 15410
    },
    {
      "epoch": 2.705737848745394,
      "grad_norm": 12.008194923400879,
      "learning_rate": 4.92776510498918e-06,
      "loss": 2.3997,
      "step": 15420
    },
    {
      "epoch": 2.707492542551325,
      "grad_norm": 9.92639446258545,
      "learning_rate": 4.898520208223665e-06,
      "loss": 2.0807,
      "step": 15430
    },
    {
      "epoch": 2.7092472363572555,
      "grad_norm": 7.7595601081848145,
      "learning_rate": 4.869275311458151e-06,
      "loss": 2.3634,
      "step": 15440
    },
    {
      "epoch": 2.7110019301631865,
      "grad_norm": 7.201369285583496,
      "learning_rate": 4.8400304146926365e-06,
      "loss": 2.5336,
      "step": 15450
    },
    {
      "epoch": 2.7127566239691174,
      "grad_norm": 7.636270999908447,
      "learning_rate": 4.810785517927121e-06,
      "loss": 2.4456,
      "step": 15460
    },
    {
      "epoch": 2.7145113177750484,
      "grad_norm": 7.187307834625244,
      "learning_rate": 4.781540621161607e-06,
      "loss": 2.4542,
      "step": 15470
    },
    {
      "epoch": 2.716266011580979,
      "grad_norm": 6.724223613739014,
      "learning_rate": 4.7522957243960925e-06,
      "loss": 2.2242,
      "step": 15480
    },
    {
      "epoch": 2.71802070538691,
      "grad_norm": 10.924382209777832,
      "learning_rate": 4.723050827630578e-06,
      "loss": 2.5324,
      "step": 15490
    },
    {
      "epoch": 2.719775399192841,
      "grad_norm": 11.399385452270508,
      "learning_rate": 4.693805930865064e-06,
      "loss": 2.5597,
      "step": 15500
    },
    {
      "epoch": 2.721530092998772,
      "grad_norm": 9.115069389343262,
      "learning_rate": 4.66456103409955e-06,
      "loss": 2.3893,
      "step": 15510
    },
    {
      "epoch": 2.7232847868047028,
      "grad_norm": 8.00876522064209,
      "learning_rate": 4.635316137334036e-06,
      "loss": 2.7942,
      "step": 15520
    },
    {
      "epoch": 2.7250394806106333,
      "grad_norm": 8.361346244812012,
      "learning_rate": 4.606071240568521e-06,
      "loss": 2.5586,
      "step": 15530
    },
    {
      "epoch": 2.7267941744165642,
      "grad_norm": 8.916932106018066,
      "learning_rate": 4.576826343803007e-06,
      "loss": 2.4624,
      "step": 15540
    },
    {
      "epoch": 2.728548868222495,
      "grad_norm": 7.698461055755615,
      "learning_rate": 4.547581447037493e-06,
      "loss": 2.4953,
      "step": 15550
    },
    {
      "epoch": 2.730303562028426,
      "grad_norm": 7.76003360748291,
      "learning_rate": 4.518336550271978e-06,
      "loss": 2.4788,
      "step": 15560
    },
    {
      "epoch": 2.7320582558343567,
      "grad_norm": 10.076143264770508,
      "learning_rate": 4.489091653506464e-06,
      "loss": 2.6614,
      "step": 15570
    },
    {
      "epoch": 2.7338129496402876,
      "grad_norm": 8.557962417602539,
      "learning_rate": 4.4598467567409495e-06,
      "loss": 2.4457,
      "step": 15580
    },
    {
      "epoch": 2.7355676434462186,
      "grad_norm": 10.122832298278809,
      "learning_rate": 4.430601859975435e-06,
      "loss": 2.5063,
      "step": 15590
    },
    {
      "epoch": 2.7373223372521496,
      "grad_norm": 8.969073295593262,
      "learning_rate": 4.401356963209921e-06,
      "loss": 2.697,
      "step": 15600
    },
    {
      "epoch": 2.7390770310580805,
      "grad_norm": 12.334283828735352,
      "learning_rate": 4.3721120664444055e-06,
      "loss": 2.4613,
      "step": 15610
    },
    {
      "epoch": 2.7408317248640115,
      "grad_norm": 7.985081195831299,
      "learning_rate": 4.342867169678891e-06,
      "loss": 2.3923,
      "step": 15620
    },
    {
      "epoch": 2.742586418669942,
      "grad_norm": 11.19275951385498,
      "learning_rate": 4.313622272913377e-06,
      "loss": 2.5714,
      "step": 15630
    },
    {
      "epoch": 2.744341112475873,
      "grad_norm": 7.176344394683838,
      "learning_rate": 4.284377376147862e-06,
      "loss": 2.6111,
      "step": 15640
    },
    {
      "epoch": 2.746095806281804,
      "grad_norm": 8.14975643157959,
      "learning_rate": 4.255132479382348e-06,
      "loss": 2.4378,
      "step": 15650
    },
    {
      "epoch": 2.7478505000877345,
      "grad_norm": 9.262359619140625,
      "learning_rate": 4.225887582616834e-06,
      "loss": 2.5428,
      "step": 15660
    },
    {
      "epoch": 2.7496051938936654,
      "grad_norm": 10.071765899658203,
      "learning_rate": 4.196642685851319e-06,
      "loss": 2.6227,
      "step": 15670
    },
    {
      "epoch": 2.7513598876995964,
      "grad_norm": 8.588567733764648,
      "learning_rate": 4.167397789085805e-06,
      "loss": 2.5449,
      "step": 15680
    },
    {
      "epoch": 2.7531145815055273,
      "grad_norm": 7.7157769203186035,
      "learning_rate": 4.1381528923202905e-06,
      "loss": 2.383,
      "step": 15690
    },
    {
      "epoch": 2.7548692753114583,
      "grad_norm": 12.113027572631836,
      "learning_rate": 4.108907995554776e-06,
      "loss": 2.3902,
      "step": 15700
    },
    {
      "epoch": 2.7566239691173893,
      "grad_norm": 11.533771514892578,
      "learning_rate": 4.079663098789262e-06,
      "loss": 2.4812,
      "step": 15710
    },
    {
      "epoch": 2.75837866292332,
      "grad_norm": 8.508556365966797,
      "learning_rate": 4.050418202023747e-06,
      "loss": 2.5757,
      "step": 15720
    },
    {
      "epoch": 2.7601333567292508,
      "grad_norm": 9.611898422241211,
      "learning_rate": 4.021173305258233e-06,
      "loss": 2.5294,
      "step": 15730
    },
    {
      "epoch": 2.7618880505351817,
      "grad_norm": 9.012395858764648,
      "learning_rate": 3.991928408492719e-06,
      "loss": 2.582,
      "step": 15740
    },
    {
      "epoch": 2.7636427443411122,
      "grad_norm": 7.559260845184326,
      "learning_rate": 3.962683511727204e-06,
      "loss": 2.5767,
      "step": 15750
    },
    {
      "epoch": 2.765397438147043,
      "grad_norm": 8.879234313964844,
      "learning_rate": 3.93343861496169e-06,
      "loss": 2.5334,
      "step": 15760
    },
    {
      "epoch": 2.767152131952974,
      "grad_norm": 7.046725749969482,
      "learning_rate": 3.9041937181961755e-06,
      "loss": 2.6399,
      "step": 15770
    },
    {
      "epoch": 2.768906825758905,
      "grad_norm": 6.321941375732422,
      "learning_rate": 3.874948821430661e-06,
      "loss": 2.2792,
      "step": 15780
    },
    {
      "epoch": 2.770661519564836,
      "grad_norm": 10.200860023498535,
      "learning_rate": 3.845703924665147e-06,
      "loss": 2.4762,
      "step": 15790
    },
    {
      "epoch": 2.772416213370767,
      "grad_norm": 9.926106452941895,
      "learning_rate": 3.8164590278996315e-06,
      "loss": 2.4354,
      "step": 15800
    },
    {
      "epoch": 2.7741709071766976,
      "grad_norm": 7.027896404266357,
      "learning_rate": 3.7872141311341175e-06,
      "loss": 2.5235,
      "step": 15810
    },
    {
      "epoch": 2.7759256009826285,
      "grad_norm": 7.560113906860352,
      "learning_rate": 3.757969234368603e-06,
      "loss": 2.4069,
      "step": 15820
    },
    {
      "epoch": 2.7776802947885595,
      "grad_norm": 6.251443386077881,
      "learning_rate": 3.728724337603089e-06,
      "loss": 2.3754,
      "step": 15830
    },
    {
      "epoch": 2.77943498859449,
      "grad_norm": 7.13029670715332,
      "learning_rate": 3.6994794408375744e-06,
      "loss": 2.4759,
      "step": 15840
    },
    {
      "epoch": 2.781189682400421,
      "grad_norm": 7.033872604370117,
      "learning_rate": 3.6702345440720596e-06,
      "loss": 2.3771,
      "step": 15850
    },
    {
      "epoch": 2.782944376206352,
      "grad_norm": 10.99267864227295,
      "learning_rate": 3.6409896473065452e-06,
      "loss": 2.3916,
      "step": 15860
    },
    {
      "epoch": 2.784699070012283,
      "grad_norm": 7.425596237182617,
      "learning_rate": 3.611744750541031e-06,
      "loss": 2.4818,
      "step": 15870
    },
    {
      "epoch": 2.786453763818214,
      "grad_norm": 7.532220840454102,
      "learning_rate": 3.5824998537755165e-06,
      "loss": 2.1757,
      "step": 15880
    },
    {
      "epoch": 2.788208457624145,
      "grad_norm": 8.725998878479004,
      "learning_rate": 3.553254957010002e-06,
      "loss": 2.4799,
      "step": 15890
    },
    {
      "epoch": 2.7899631514300753,
      "grad_norm": 7.032027721405029,
      "learning_rate": 3.5240100602444877e-06,
      "loss": 2.394,
      "step": 15900
    },
    {
      "epoch": 2.7917178452360063,
      "grad_norm": 9.73123550415039,
      "learning_rate": 3.4947651634789733e-06,
      "loss": 2.2941,
      "step": 15910
    },
    {
      "epoch": 2.7934725390419373,
      "grad_norm": 9.38502311706543,
      "learning_rate": 3.465520266713459e-06,
      "loss": 2.4859,
      "step": 15920
    },
    {
      "epoch": 2.795227232847868,
      "grad_norm": 9.024696350097656,
      "learning_rate": 3.4362753699479446e-06,
      "loss": 2.5599,
      "step": 15930
    },
    {
      "epoch": 2.7969819266537987,
      "grad_norm": 8.813329696655273,
      "learning_rate": 3.4070304731824298e-06,
      "loss": 2.2957,
      "step": 15940
    },
    {
      "epoch": 2.7987366204597297,
      "grad_norm": 10.042885780334473,
      "learning_rate": 3.3777855764169154e-06,
      "loss": 2.4703,
      "step": 15950
    },
    {
      "epoch": 2.8004913142656607,
      "grad_norm": 7.0260796546936035,
      "learning_rate": 3.348540679651401e-06,
      "loss": 2.3531,
      "step": 15960
    },
    {
      "epoch": 2.8022460080715916,
      "grad_norm": 7.380734443664551,
      "learning_rate": 3.3192957828858866e-06,
      "loss": 2.4345,
      "step": 15970
    },
    {
      "epoch": 2.8040007018775226,
      "grad_norm": 9.071864128112793,
      "learning_rate": 3.2900508861203723e-06,
      "loss": 2.4861,
      "step": 15980
    },
    {
      "epoch": 2.805755395683453,
      "grad_norm": 7.366601467132568,
      "learning_rate": 3.260805989354858e-06,
      "loss": 2.6019,
      "step": 15990
    },
    {
      "epoch": 2.807510089489384,
      "grad_norm": 8.087577819824219,
      "learning_rate": 3.2315610925893435e-06,
      "loss": 2.5494,
      "step": 16000
    },
    {
      "epoch": 2.809264783295315,
      "grad_norm": 8.83800983428955,
      "learning_rate": 3.202316195823829e-06,
      "loss": 2.5113,
      "step": 16010
    },
    {
      "epoch": 2.8110194771012456,
      "grad_norm": 7.267480850219727,
      "learning_rate": 3.1730712990583147e-06,
      "loss": 2.3424,
      "step": 16020
    },
    {
      "epoch": 2.8127741709071765,
      "grad_norm": 10.561253547668457,
      "learning_rate": 3.1438264022928004e-06,
      "loss": 2.6687,
      "step": 16030
    },
    {
      "epoch": 2.8145288647131075,
      "grad_norm": 11.477388381958008,
      "learning_rate": 3.1145815055272856e-06,
      "loss": 2.4122,
      "step": 16040
    },
    {
      "epoch": 2.8162835585190384,
      "grad_norm": 9.075119018554688,
      "learning_rate": 3.085336608761771e-06,
      "loss": 2.5843,
      "step": 16050
    },
    {
      "epoch": 2.8180382523249694,
      "grad_norm": 6.5634446144104,
      "learning_rate": 3.056091711996257e-06,
      "loss": 2.4549,
      "step": 16060
    },
    {
      "epoch": 2.8197929461309004,
      "grad_norm": 6.65757942199707,
      "learning_rate": 3.0268468152307424e-06,
      "loss": 2.1724,
      "step": 16070
    },
    {
      "epoch": 2.821547639936831,
      "grad_norm": 8.197671890258789,
      "learning_rate": 2.997601918465228e-06,
      "loss": 2.5438,
      "step": 16080
    },
    {
      "epoch": 2.823302333742762,
      "grad_norm": 8.462438583374023,
      "learning_rate": 2.9683570216997137e-06,
      "loss": 2.4384,
      "step": 16090
    },
    {
      "epoch": 2.825057027548693,
      "grad_norm": 7.998929977416992,
      "learning_rate": 2.9391121249341993e-06,
      "loss": 2.4641,
      "step": 16100
    },
    {
      "epoch": 2.8268117213546238,
      "grad_norm": 9.405584335327148,
      "learning_rate": 2.909867228168685e-06,
      "loss": 2.5213,
      "step": 16110
    },
    {
      "epoch": 2.8285664151605543,
      "grad_norm": 8.773065567016602,
      "learning_rate": 2.8806223314031705e-06,
      "loss": 2.5723,
      "step": 16120
    },
    {
      "epoch": 2.8303211089664853,
      "grad_norm": 9.03992748260498,
      "learning_rate": 2.8513774346376557e-06,
      "loss": 2.3905,
      "step": 16130
    },
    {
      "epoch": 2.832075802772416,
      "grad_norm": 5.697781562805176,
      "learning_rate": 2.8221325378721414e-06,
      "loss": 2.5692,
      "step": 16140
    },
    {
      "epoch": 2.833830496578347,
      "grad_norm": 8.172561645507812,
      "learning_rate": 2.792887641106627e-06,
      "loss": 2.0963,
      "step": 16150
    },
    {
      "epoch": 2.835585190384278,
      "grad_norm": 9.544204711914062,
      "learning_rate": 2.7636427443411126e-06,
      "loss": 2.4142,
      "step": 16160
    },
    {
      "epoch": 2.8373398841902087,
      "grad_norm": 9.973404884338379,
      "learning_rate": 2.7343978475755982e-06,
      "loss": 2.61,
      "step": 16170
    },
    {
      "epoch": 2.8390945779961396,
      "grad_norm": 10.777446746826172,
      "learning_rate": 2.705152950810084e-06,
      "loss": 2.4152,
      "step": 16180
    },
    {
      "epoch": 2.8408492718020706,
      "grad_norm": 8.790327072143555,
      "learning_rate": 2.6759080540445695e-06,
      "loss": 2.4363,
      "step": 16190
    },
    {
      "epoch": 2.8426039656080015,
      "grad_norm": 7.776834011077881,
      "learning_rate": 2.646663157279055e-06,
      "loss": 2.3601,
      "step": 16200
    },
    {
      "epoch": 2.844358659413932,
      "grad_norm": 7.195651054382324,
      "learning_rate": 2.6174182605135407e-06,
      "loss": 2.2544,
      "step": 16210
    },
    {
      "epoch": 2.846113353219863,
      "grad_norm": 7.125692367553711,
      "learning_rate": 2.5881733637480263e-06,
      "loss": 2.3728,
      "step": 16220
    },
    {
      "epoch": 2.847868047025794,
      "grad_norm": 8.722291946411133,
      "learning_rate": 2.5589284669825115e-06,
      "loss": 2.5599,
      "step": 16230
    },
    {
      "epoch": 2.849622740831725,
      "grad_norm": 6.3996076583862305,
      "learning_rate": 2.529683570216997e-06,
      "loss": 2.411,
      "step": 16240
    },
    {
      "epoch": 2.851377434637656,
      "grad_norm": 8.370420455932617,
      "learning_rate": 2.5004386734514828e-06,
      "loss": 2.2795,
      "step": 16250
    },
    {
      "epoch": 2.8531321284435864,
      "grad_norm": 10.149697303771973,
      "learning_rate": 2.4711937766859684e-06,
      "loss": 2.5243,
      "step": 16260
    },
    {
      "epoch": 2.8548868222495174,
      "grad_norm": 9.32416820526123,
      "learning_rate": 2.441948879920454e-06,
      "loss": 2.5049,
      "step": 16270
    },
    {
      "epoch": 2.8566415160554484,
      "grad_norm": 11.065910339355469,
      "learning_rate": 2.4127039831549396e-06,
      "loss": 2.2626,
      "step": 16280
    },
    {
      "epoch": 2.8583962098613793,
      "grad_norm": 7.341986656188965,
      "learning_rate": 2.3834590863894253e-06,
      "loss": 2.4853,
      "step": 16290
    },
    {
      "epoch": 2.86015090366731,
      "grad_norm": 8.809333801269531,
      "learning_rate": 2.354214189623911e-06,
      "loss": 2.6697,
      "step": 16300
    },
    {
      "epoch": 2.861905597473241,
      "grad_norm": 8.35319709777832,
      "learning_rate": 2.3249692928583965e-06,
      "loss": 2.3888,
      "step": 16310
    },
    {
      "epoch": 2.8636602912791718,
      "grad_norm": 7.942260265350342,
      "learning_rate": 2.2957243960928817e-06,
      "loss": 2.655,
      "step": 16320
    },
    {
      "epoch": 2.8654149850851027,
      "grad_norm": 7.799402236938477,
      "learning_rate": 2.2664794993273673e-06,
      "loss": 2.5621,
      "step": 16330
    },
    {
      "epoch": 2.8671696788910337,
      "grad_norm": 9.60640811920166,
      "learning_rate": 2.237234602561853e-06,
      "loss": 2.7597,
      "step": 16340
    },
    {
      "epoch": 2.8689243726969647,
      "grad_norm": 6.049283504486084,
      "learning_rate": 2.2079897057963386e-06,
      "loss": 2.4029,
      "step": 16350
    },
    {
      "epoch": 2.870679066502895,
      "grad_norm": 7.710054874420166,
      "learning_rate": 2.178744809030824e-06,
      "loss": 2.4017,
      "step": 16360
    },
    {
      "epoch": 2.872433760308826,
      "grad_norm": 10.913874626159668,
      "learning_rate": 2.14949991226531e-06,
      "loss": 2.3852,
      "step": 16370
    },
    {
      "epoch": 2.874188454114757,
      "grad_norm": 7.849503040313721,
      "learning_rate": 2.1202550154997954e-06,
      "loss": 2.5218,
      "step": 16380
    },
    {
      "epoch": 2.8759431479206876,
      "grad_norm": 10.033183097839355,
      "learning_rate": 2.091010118734281e-06,
      "loss": 2.3264,
      "step": 16390
    },
    {
      "epoch": 2.8776978417266186,
      "grad_norm": 7.74798583984375,
      "learning_rate": 2.0617652219687667e-06,
      "loss": 2.3878,
      "step": 16400
    },
    {
      "epoch": 2.8794525355325495,
      "grad_norm": 6.762147426605225,
      "learning_rate": 2.0325203252032523e-06,
      "loss": 2.5724,
      "step": 16410
    },
    {
      "epoch": 2.8812072293384805,
      "grad_norm": 9.82525634765625,
      "learning_rate": 2.0032754284377375e-06,
      "loss": 2.5189,
      "step": 16420
    },
    {
      "epoch": 2.8829619231444115,
      "grad_norm": 8.4909086227417,
      "learning_rate": 1.974030531672223e-06,
      "loss": 2.512,
      "step": 16430
    },
    {
      "epoch": 2.8847166169503424,
      "grad_norm": 6.446767330169678,
      "learning_rate": 1.9447856349067087e-06,
      "loss": 2.4785,
      "step": 16440
    },
    {
      "epoch": 2.886471310756273,
      "grad_norm": 8.475700378417969,
      "learning_rate": 1.9155407381411943e-06,
      "loss": 2.425,
      "step": 16450
    },
    {
      "epoch": 2.888226004562204,
      "grad_norm": 7.30882453918457,
      "learning_rate": 1.88629584137568e-06,
      "loss": 2.5837,
      "step": 16460
    },
    {
      "epoch": 2.889980698368135,
      "grad_norm": 7.553532123565674,
      "learning_rate": 1.8570509446101656e-06,
      "loss": 2.4303,
      "step": 16470
    },
    {
      "epoch": 2.8917353921740654,
      "grad_norm": 9.331151962280273,
      "learning_rate": 1.8278060478446512e-06,
      "loss": 2.8838,
      "step": 16480
    },
    {
      "epoch": 2.8934900859799964,
      "grad_norm": 11.668113708496094,
      "learning_rate": 1.7985611510791366e-06,
      "loss": 2.5915,
      "step": 16490
    },
    {
      "epoch": 2.8952447797859273,
      "grad_norm": 8.233171463012695,
      "learning_rate": 1.7693162543136222e-06,
      "loss": 2.4041,
      "step": 16500
    },
    {
      "epoch": 2.8969994735918583,
      "grad_norm": 9.082279205322266,
      "learning_rate": 1.7400713575481079e-06,
      "loss": 2.6061,
      "step": 16510
    },
    {
      "epoch": 2.8987541673977892,
      "grad_norm": 7.405715465545654,
      "learning_rate": 1.7108264607825935e-06,
      "loss": 2.3364,
      "step": 16520
    },
    {
      "epoch": 2.90050886120372,
      "grad_norm": 7.378285884857178,
      "learning_rate": 1.681581564017079e-06,
      "loss": 2.3585,
      "step": 16530
    },
    {
      "epoch": 2.9022635550096507,
      "grad_norm": 6.584386825561523,
      "learning_rate": 1.6523366672515645e-06,
      "loss": 2.4778,
      "step": 16540
    },
    {
      "epoch": 2.9040182488155817,
      "grad_norm": 10.097792625427246,
      "learning_rate": 1.6230917704860501e-06,
      "loss": 2.4927,
      "step": 16550
    },
    {
      "epoch": 2.9057729426215126,
      "grad_norm": 6.790286064147949,
      "learning_rate": 1.5938468737205358e-06,
      "loss": 2.3199,
      "step": 16560
    },
    {
      "epoch": 2.907527636427443,
      "grad_norm": 6.450170516967773,
      "learning_rate": 1.5646019769550214e-06,
      "loss": 2.4552,
      "step": 16570
    },
    {
      "epoch": 2.909282330233374,
      "grad_norm": 8.5759916305542,
      "learning_rate": 1.535357080189507e-06,
      "loss": 2.7145,
      "step": 16580
    },
    {
      "epoch": 2.911037024039305,
      "grad_norm": 8.021474838256836,
      "learning_rate": 1.5061121834239926e-06,
      "loss": 2.4683,
      "step": 16590
    },
    {
      "epoch": 2.912791717845236,
      "grad_norm": 6.5735063552856445,
      "learning_rate": 1.4768672866584782e-06,
      "loss": 2.5568,
      "step": 16600
    },
    {
      "epoch": 2.914546411651167,
      "grad_norm": 6.220189571380615,
      "learning_rate": 1.4476223898929639e-06,
      "loss": 2.3343,
      "step": 16610
    },
    {
      "epoch": 2.916301105457098,
      "grad_norm": 10.161458969116211,
      "learning_rate": 1.4183774931274493e-06,
      "loss": 2.4061,
      "step": 16620
    },
    {
      "epoch": 2.9180557992630285,
      "grad_norm": 8.383151054382324,
      "learning_rate": 1.389132596361935e-06,
      "loss": 2.5056,
      "step": 16630
    },
    {
      "epoch": 2.9198104930689595,
      "grad_norm": 7.259553909301758,
      "learning_rate": 1.3598876995964205e-06,
      "loss": 2.3555,
      "step": 16640
    },
    {
      "epoch": 2.9215651868748904,
      "grad_norm": 7.941014766693115,
      "learning_rate": 1.3306428028309061e-06,
      "loss": 2.4644,
      "step": 16650
    },
    {
      "epoch": 2.923319880680821,
      "grad_norm": 8.290373802185059,
      "learning_rate": 1.3013979060653918e-06,
      "loss": 2.4374,
      "step": 16660
    },
    {
      "epoch": 2.925074574486752,
      "grad_norm": 8.695359230041504,
      "learning_rate": 1.2721530092998772e-06,
      "loss": 2.2953,
      "step": 16670
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 12.646394729614258,
      "learning_rate": 1.2429081125343628e-06,
      "loss": 2.3207,
      "step": 16680
    },
    {
      "epoch": 2.928583962098614,
      "grad_norm": 10.73214054107666,
      "learning_rate": 1.2136632157688484e-06,
      "loss": 2.625,
      "step": 16690
    },
    {
      "epoch": 2.930338655904545,
      "grad_norm": 8.101505279541016,
      "learning_rate": 1.184418319003334e-06,
      "loss": 2.5064,
      "step": 16700
    },
    {
      "epoch": 2.9320933497104757,
      "grad_norm": 7.6449713706970215,
      "learning_rate": 1.1551734222378197e-06,
      "loss": 2.7296,
      "step": 16710
    },
    {
      "epoch": 2.9338480435164063,
      "grad_norm": 10.117011070251465,
      "learning_rate": 1.125928525472305e-06,
      "loss": 2.6481,
      "step": 16720
    },
    {
      "epoch": 2.9356027373223372,
      "grad_norm": 7.972126483917236,
      "learning_rate": 1.0966836287067907e-06,
      "loss": 2.5743,
      "step": 16730
    },
    {
      "epoch": 2.937357431128268,
      "grad_norm": 10.649182319641113,
      "learning_rate": 1.0674387319412763e-06,
      "loss": 2.5366,
      "step": 16740
    },
    {
      "epoch": 2.9391121249341987,
      "grad_norm": 8.935691833496094,
      "learning_rate": 1.038193835175762e-06,
      "loss": 2.5474,
      "step": 16750
    },
    {
      "epoch": 2.9408668187401297,
      "grad_norm": 8.64745807647705,
      "learning_rate": 1.0089489384102473e-06,
      "loss": 2.2997,
      "step": 16760
    },
    {
      "epoch": 2.9426215125460606,
      "grad_norm": 7.42073917388916,
      "learning_rate": 9.79704041644733e-07,
      "loss": 2.3857,
      "step": 16770
    },
    {
      "epoch": 2.9443762063519916,
      "grad_norm": 8.383439064025879,
      "learning_rate": 9.504591448792186e-07,
      "loss": 2.513,
      "step": 16780
    },
    {
      "epoch": 2.9461309001579226,
      "grad_norm": 7.071073055267334,
      "learning_rate": 9.212142481137042e-07,
      "loss": 2.5369,
      "step": 16790
    },
    {
      "epoch": 2.9478855939638535,
      "grad_norm": 8.76984691619873,
      "learning_rate": 8.919693513481897e-07,
      "loss": 2.6437,
      "step": 16800
    },
    {
      "epoch": 2.949640287769784,
      "grad_norm": 8.201861381530762,
      "learning_rate": 8.627244545826753e-07,
      "loss": 2.615,
      "step": 16810
    },
    {
      "epoch": 2.951394981575715,
      "grad_norm": 9.72220516204834,
      "learning_rate": 8.334795578171609e-07,
      "loss": 2.4796,
      "step": 16820
    },
    {
      "epoch": 2.953149675381646,
      "grad_norm": 7.324934005737305,
      "learning_rate": 8.042346610516465e-07,
      "loss": 2.6331,
      "step": 16830
    },
    {
      "epoch": 2.954904369187577,
      "grad_norm": 7.6757073402404785,
      "learning_rate": 7.749897642861321e-07,
      "loss": 2.4466,
      "step": 16840
    },
    {
      "epoch": 2.9566590629935074,
      "grad_norm": 7.785146713256836,
      "learning_rate": 7.457448675206177e-07,
      "loss": 2.5724,
      "step": 16850
    },
    {
      "epoch": 2.9584137567994384,
      "grad_norm": 7.714850902557373,
      "learning_rate": 7.164999707551033e-07,
      "loss": 2.7359,
      "step": 16860
    },
    {
      "epoch": 2.9601684506053694,
      "grad_norm": 6.8255743980407715,
      "learning_rate": 6.901795636661403e-07,
      "loss": 2.4136,
      "step": 16870
    },
    {
      "epoch": 2.9619231444113003,
      "grad_norm": 6.937267303466797,
      "learning_rate": 6.60934666900626e-07,
      "loss": 2.6592,
      "step": 16880
    },
    {
      "epoch": 2.9636778382172313,
      "grad_norm": 8.687468528747559,
      "learning_rate": 6.316897701351115e-07,
      "loss": 2.7617,
      "step": 16890
    },
    {
      "epoch": 2.965432532023162,
      "grad_norm": 8.452341079711914,
      "learning_rate": 6.024448733695971e-07,
      "loss": 2.4468,
      "step": 16900
    },
    {
      "epoch": 2.967187225829093,
      "grad_norm": 10.339627265930176,
      "learning_rate": 5.731999766040826e-07,
      "loss": 2.6181,
      "step": 16910
    },
    {
      "epoch": 2.9689419196350237,
      "grad_norm": 8.447347640991211,
      "learning_rate": 5.439550798385682e-07,
      "loss": 2.3791,
      "step": 16920
    },
    {
      "epoch": 2.9706966134409547,
      "grad_norm": 6.579084873199463,
      "learning_rate": 5.147101830730537e-07,
      "loss": 2.384,
      "step": 16930
    },
    {
      "epoch": 2.972451307246885,
      "grad_norm": 7.384524822235107,
      "learning_rate": 4.854652863075394e-07,
      "loss": 2.3098,
      "step": 16940
    },
    {
      "epoch": 2.974206001052816,
      "grad_norm": 8.607905387878418,
      "learning_rate": 4.5622038954202493e-07,
      "loss": 2.3538,
      "step": 16950
    },
    {
      "epoch": 2.975960694858747,
      "grad_norm": 8.766918182373047,
      "learning_rate": 4.269754927765105e-07,
      "loss": 2.3616,
      "step": 16960
    },
    {
      "epoch": 2.977715388664678,
      "grad_norm": 7.715925216674805,
      "learning_rate": 3.9773059601099607e-07,
      "loss": 2.5429,
      "step": 16970
    },
    {
      "epoch": 2.979470082470609,
      "grad_norm": 8.79754638671875,
      "learning_rate": 3.684856992454817e-07,
      "loss": 2.6442,
      "step": 16980
    },
    {
      "epoch": 2.9812247762765396,
      "grad_norm": 7.556107044219971,
      "learning_rate": 3.3924080247996726e-07,
      "loss": 2.6152,
      "step": 16990
    },
    {
      "epoch": 2.9829794700824706,
      "grad_norm": 6.878061771392822,
      "learning_rate": 3.099959057144529e-07,
      "loss": 2.4241,
      "step": 17000
    },
    {
      "epoch": 2.9847341638884015,
      "grad_norm": 10.859997749328613,
      "learning_rate": 2.8075100894893845e-07,
      "loss": 2.6636,
      "step": 17010
    },
    {
      "epoch": 2.9864888576943325,
      "grad_norm": 9.031464576721191,
      "learning_rate": 2.51506112183424e-07,
      "loss": 2.5781,
      "step": 17020
    },
    {
      "epoch": 2.988243551500263,
      "grad_norm": 9.22603988647461,
      "learning_rate": 2.2226121541790959e-07,
      "loss": 2.6077,
      "step": 17030
    },
    {
      "epoch": 2.989998245306194,
      "grad_norm": 7.437325954437256,
      "learning_rate": 1.9301631865239518e-07,
      "loss": 2.3694,
      "step": 17040
    },
    {
      "epoch": 2.991752939112125,
      "grad_norm": 7.453976631164551,
      "learning_rate": 1.6377142188688075e-07,
      "loss": 2.2914,
      "step": 17050
    },
    {
      "epoch": 2.993507632918056,
      "grad_norm": 12.309062004089355,
      "learning_rate": 1.3452652512136632e-07,
      "loss": 2.3162,
      "step": 17060
    },
    {
      "epoch": 2.995262326723987,
      "grad_norm": 6.768568515777588,
      "learning_rate": 1.052816283558519e-07,
      "loss": 2.3845,
      "step": 17070
    },
    {
      "epoch": 2.9970170205299174,
      "grad_norm": 7.628850936889648,
      "learning_rate": 7.603673159033748e-08,
      "loss": 2.4952,
      "step": 17080
    },
    {
      "epoch": 2.9987717143358483,
      "grad_norm": 7.849569320678711,
      "learning_rate": 4.6791834824823075e-08,
      "loss": 2.5208,
      "step": 17090
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.28823374572255855,
      "eval_f1_C01": 0.22519083969465647,
      "eval_f1_C02": 0.0,
      "eval_f1_C03": 0.0,
      "eval_f1_C04": 0.5195791934541204,
      "eval_f1_C05": 0.0,
      "eval_f1_C06": 0.0,
      "eval_f1_C07": 0.0,
      "eval_f1_C08": 0.0,
      "eval_f1_C09": 0.0,
      "eval_f1_C10": 0.0,
      "eval_f1_C11": 0.0,
      "eval_f1_C12": 0.0039447731755424065,
      "eval_f1_C13": 0.0,
      "eval_f1_C14": 0.5719310587407668,
      "eval_f1_C15": 0.0,
      "eval_f1_C16": 0.0,
      "eval_f1_C17": 0.0,
      "eval_f1_C18": 0.07494145199063232,
      "eval_f1_C19": 0.0,
      "eval_f1_C20": 0.328515111695138,
      "eval_f1_C21": 0.10689170182841069,
      "eval_f1_C22": 0.0,
      "eval_f1_C23": 0.2925230109696129,
      "eval_f1_macro": 0.09232683224125567,
      "eval_loss": 2.4415299892425537,
      "eval_precision_C01": 0.21851851851851853,
      "eval_precision_C02": 1.0,
      "eval_precision_C03": 1.0,
      "eval_precision_C04": 0.41233766233766234,
      "eval_precision_C05": 1.0,
      "eval_precision_C06": 1.0,
      "eval_precision_C07": 1.0,
      "eval_precision_C08": 1.0,
      "eval_precision_C09": 1.0,
      "eval_precision_C10": 1.0,
      "eval_precision_C11": 1.0,
      "eval_precision_C12": 0.3333333333333333,
      "eval_precision_C13": 1.0,
      "eval_precision_C14": 0.5009242144177449,
      "eval_precision_C15": 1.0,
      "eval_precision_C16": 1.0,
      "eval_precision_C17": 1.0,
      "eval_precision_C18": 0.37209302325581395,
      "eval_precision_C19": 1.0,
      "eval_precision_C20": 0.27808676307007785,
      "eval_precision_C21": 0.3064516129032258,
      "eval_precision_C22": 1.0,
      "eval_precision_C23": 0.1930437676818106,
      "eval_precision_global": 0.7658603867616603,
      "eval_recall_C01": 0.23228346456692914,
      "eval_recall_C02": 0.0,
      "eval_recall_C03": 0.0,
      "eval_recall_C04": 0.7022116903633492,
      "eval_recall_C05": 0.0,
      "eval_recall_C06": 0.0,
      "eval_recall_C07": 0.0,
      "eval_recall_C08": 0.0,
      "eval_recall_C09": 0.0,
      "eval_recall_C10": 0.0,
      "eval_recall_C11": 0.0,
      "eval_recall_C12": 0.001984126984126984,
      "eval_recall_C13": 0.0,
      "eval_recall_C14": 0.6663934426229509,
      "eval_recall_C15": 0.0,
      "eval_recall_C16": 0.0,
      "eval_recall_C17": 0.0,
      "eval_recall_C18": 0.041666666666666664,
      "eval_recall_C19": 0.0,
      "eval_recall_C20": 0.4012841091492777,
      "eval_recall_C21": 0.06473594548551959,
      "eval_recall_C22": 0.0,
      "eval_recall_C23": 0.603537981269511,
      "eval_recall_global": 0.11800423596123177,
      "eval_runtime": 51.4018,
      "eval_samples_per_second": 221.724,
      "eval_steps_per_second": 27.723,
      "step": 17097
    }
  ],
  "logging_steps": 10,
  "max_steps": 17097,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.65627715383936e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
