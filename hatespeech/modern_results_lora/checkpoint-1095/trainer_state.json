{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1095,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0091324200913242,
      "grad_norm": 5.764733791351318,
      "learning_rate": 4.984779299847793e-05,
      "loss": 1.1781,
      "step": 10
    },
    {
      "epoch": 0.0182648401826484,
      "grad_norm": 2.436617136001587,
      "learning_rate": 4.969558599695586e-05,
      "loss": 0.5632,
      "step": 20
    },
    {
      "epoch": 0.0273972602739726,
      "grad_norm": 5.719903945922852,
      "learning_rate": 4.9543378995433794e-05,
      "loss": 0.4185,
      "step": 30
    },
    {
      "epoch": 0.0365296803652968,
      "grad_norm": 1.004781723022461,
      "learning_rate": 4.939117199391172e-05,
      "loss": 0.5267,
      "step": 40
    },
    {
      "epoch": 0.045662100456621,
      "grad_norm": 0.7854910492897034,
      "learning_rate": 4.923896499238965e-05,
      "loss": 0.3572,
      "step": 50
    },
    {
      "epoch": 0.0547945205479452,
      "grad_norm": 3.592888593673706,
      "learning_rate": 4.908675799086758e-05,
      "loss": 0.6624,
      "step": 60
    },
    {
      "epoch": 0.0639269406392694,
      "grad_norm": 1.4343537092208862,
      "learning_rate": 4.8934550989345515e-05,
      "loss": 0.4892,
      "step": 70
    },
    {
      "epoch": 0.0730593607305936,
      "grad_norm": 8.628373146057129,
      "learning_rate": 4.878234398782344e-05,
      "loss": 0.4583,
      "step": 80
    },
    {
      "epoch": 0.0821917808219178,
      "grad_norm": 2.372424840927124,
      "learning_rate": 4.863013698630137e-05,
      "loss": 0.5622,
      "step": 90
    },
    {
      "epoch": 0.091324200913242,
      "grad_norm": 2.0125808715820312,
      "learning_rate": 4.84779299847793e-05,
      "loss": 0.4336,
      "step": 100
    },
    {
      "epoch": 0.1004566210045662,
      "grad_norm": 2.1987414360046387,
      "learning_rate": 4.8325722983257235e-05,
      "loss": 0.5767,
      "step": 110
    },
    {
      "epoch": 0.1095890410958904,
      "grad_norm": 2.3950655460357666,
      "learning_rate": 4.8173515981735164e-05,
      "loss": 0.3194,
      "step": 120
    },
    {
      "epoch": 0.1187214611872146,
      "grad_norm": 1.5012563467025757,
      "learning_rate": 4.802130898021309e-05,
      "loss": 0.2457,
      "step": 130
    },
    {
      "epoch": 0.1278538812785388,
      "grad_norm": 0.9848641157150269,
      "learning_rate": 4.786910197869102e-05,
      "loss": 0.4722,
      "step": 140
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 5.501198768615723,
      "learning_rate": 4.7716894977168955e-05,
      "loss": 0.772,
      "step": 150
    },
    {
      "epoch": 0.1461187214611872,
      "grad_norm": 7.525727272033691,
      "learning_rate": 4.7564687975646884e-05,
      "loss": 0.6815,
      "step": 160
    },
    {
      "epoch": 0.1552511415525114,
      "grad_norm": 3.9026007652282715,
      "learning_rate": 4.741248097412481e-05,
      "loss": 0.5422,
      "step": 170
    },
    {
      "epoch": 0.1643835616438356,
      "grad_norm": 2.3730521202087402,
      "learning_rate": 4.726027397260274e-05,
      "loss": 0.4432,
      "step": 180
    },
    {
      "epoch": 0.1735159817351598,
      "grad_norm": 2.0919034481048584,
      "learning_rate": 4.710806697108067e-05,
      "loss": 0.3779,
      "step": 190
    },
    {
      "epoch": 0.182648401826484,
      "grad_norm": 4.37285852432251,
      "learning_rate": 4.6955859969558604e-05,
      "loss": 0.4991,
      "step": 200
    },
    {
      "epoch": 0.1917808219178082,
      "grad_norm": 1.8473596572875977,
      "learning_rate": 4.680365296803653e-05,
      "loss": 0.4493,
      "step": 210
    },
    {
      "epoch": 0.2009132420091324,
      "grad_norm": 2.571307420730591,
      "learning_rate": 4.665144596651446e-05,
      "loss": 0.4797,
      "step": 220
    },
    {
      "epoch": 0.2100456621004566,
      "grad_norm": 3.253429889678955,
      "learning_rate": 4.649923896499239e-05,
      "loss": 0.8208,
      "step": 230
    },
    {
      "epoch": 0.2191780821917808,
      "grad_norm": 1.8835606575012207,
      "learning_rate": 4.6347031963470325e-05,
      "loss": 0.3439,
      "step": 240
    },
    {
      "epoch": 0.228310502283105,
      "grad_norm": 5.245028495788574,
      "learning_rate": 4.619482496194825e-05,
      "loss": 0.5117,
      "step": 250
    },
    {
      "epoch": 0.2374429223744292,
      "grad_norm": 2.241241216659546,
      "learning_rate": 4.604261796042618e-05,
      "loss": 0.3563,
      "step": 260
    },
    {
      "epoch": 0.2465753424657534,
      "grad_norm": 3.8060293197631836,
      "learning_rate": 4.589041095890411e-05,
      "loss": 0.3667,
      "step": 270
    },
    {
      "epoch": 0.2557077625570776,
      "grad_norm": 2.7781898975372314,
      "learning_rate": 4.5738203957382045e-05,
      "loss": 0.4001,
      "step": 280
    },
    {
      "epoch": 0.2648401826484018,
      "grad_norm": 1.6057904958724976,
      "learning_rate": 4.5585996955859973e-05,
      "loss": 0.3013,
      "step": 290
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 3.481186866760254,
      "learning_rate": 4.54337899543379e-05,
      "loss": 0.4545,
      "step": 300
    },
    {
      "epoch": 0.2831050228310502,
      "grad_norm": 2.209181547164917,
      "learning_rate": 4.528158295281583e-05,
      "loss": 0.6136,
      "step": 310
    },
    {
      "epoch": 0.2922374429223744,
      "grad_norm": 2.4162909984588623,
      "learning_rate": 4.512937595129376e-05,
      "loss": 0.4162,
      "step": 320
    },
    {
      "epoch": 0.3013698630136986,
      "grad_norm": 5.911314010620117,
      "learning_rate": 4.4977168949771694e-05,
      "loss": 0.4856,
      "step": 330
    },
    {
      "epoch": 0.3105022831050228,
      "grad_norm": 3.0444045066833496,
      "learning_rate": 4.482496194824962e-05,
      "loss": 0.3174,
      "step": 340
    },
    {
      "epoch": 0.319634703196347,
      "grad_norm": 0.979099690914154,
      "learning_rate": 4.467275494672755e-05,
      "loss": 0.3502,
      "step": 350
    },
    {
      "epoch": 0.3287671232876712,
      "grad_norm": 3.108412504196167,
      "learning_rate": 4.452054794520548e-05,
      "loss": 0.6241,
      "step": 360
    },
    {
      "epoch": 0.3378995433789954,
      "grad_norm": 2.5400776863098145,
      "learning_rate": 4.4368340943683414e-05,
      "loss": 0.4746,
      "step": 370
    },
    {
      "epoch": 0.3470319634703196,
      "grad_norm": 2.12054705619812,
      "learning_rate": 4.421613394216134e-05,
      "loss": 0.6568,
      "step": 380
    },
    {
      "epoch": 0.3561643835616438,
      "grad_norm": 2.4121391773223877,
      "learning_rate": 4.406392694063927e-05,
      "loss": 0.5373,
      "step": 390
    },
    {
      "epoch": 0.365296803652968,
      "grad_norm": 2.157649040222168,
      "learning_rate": 4.39117199391172e-05,
      "loss": 0.4037,
      "step": 400
    },
    {
      "epoch": 0.3744292237442922,
      "grad_norm": 2.4147603511810303,
      "learning_rate": 4.3759512937595135e-05,
      "loss": 0.3606,
      "step": 410
    },
    {
      "epoch": 0.3835616438356164,
      "grad_norm": 1.5403311252593994,
      "learning_rate": 4.360730593607306e-05,
      "loss": 0.3522,
      "step": 420
    },
    {
      "epoch": 0.3926940639269406,
      "grad_norm": 3.4712796211242676,
      "learning_rate": 4.345509893455099e-05,
      "loss": 0.5384,
      "step": 430
    },
    {
      "epoch": 0.4018264840182648,
      "grad_norm": 3.574594020843506,
      "learning_rate": 4.330289193302892e-05,
      "loss": 0.3047,
      "step": 440
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 1.088865876197815,
      "learning_rate": 4.3150684931506855e-05,
      "loss": 0.6517,
      "step": 450
    },
    {
      "epoch": 0.4200913242009132,
      "grad_norm": 1.3922253847122192,
      "learning_rate": 4.299847792998478e-05,
      "loss": 0.365,
      "step": 460
    },
    {
      "epoch": 0.4292237442922374,
      "grad_norm": 1.047322154045105,
      "learning_rate": 4.284627092846271e-05,
      "loss": 0.4123,
      "step": 470
    },
    {
      "epoch": 0.4383561643835616,
      "grad_norm": 6.0662407875061035,
      "learning_rate": 4.269406392694064e-05,
      "loss": 0.5206,
      "step": 480
    },
    {
      "epoch": 0.4474885844748858,
      "grad_norm": 2.3779296875,
      "learning_rate": 4.254185692541857e-05,
      "loss": 0.4229,
      "step": 490
    },
    {
      "epoch": 0.45662100456621,
      "grad_norm": 4.853570938110352,
      "learning_rate": 4.2389649923896504e-05,
      "loss": 0.3396,
      "step": 500
    },
    {
      "epoch": 0.4657534246575342,
      "grad_norm": 5.261697769165039,
      "learning_rate": 4.223744292237443e-05,
      "loss": 0.3332,
      "step": 510
    },
    {
      "epoch": 0.4748858447488584,
      "grad_norm": 3.860701322555542,
      "learning_rate": 4.208523592085236e-05,
      "loss": 0.5272,
      "step": 520
    },
    {
      "epoch": 0.4840182648401826,
      "grad_norm": 2.6627798080444336,
      "learning_rate": 4.193302891933029e-05,
      "loss": 0.4492,
      "step": 530
    },
    {
      "epoch": 0.4931506849315068,
      "grad_norm": 2.9266114234924316,
      "learning_rate": 4.1780821917808224e-05,
      "loss": 0.4871,
      "step": 540
    },
    {
      "epoch": 0.502283105022831,
      "grad_norm": 1.2428011894226074,
      "learning_rate": 4.162861491628615e-05,
      "loss": 0.197,
      "step": 550
    },
    {
      "epoch": 0.5114155251141552,
      "grad_norm": 6.849915504455566,
      "learning_rate": 4.147640791476408e-05,
      "loss": 0.5149,
      "step": 560
    },
    {
      "epoch": 0.5205479452054794,
      "grad_norm": 5.782258987426758,
      "learning_rate": 4.132420091324201e-05,
      "loss": 0.3452,
      "step": 570
    },
    {
      "epoch": 0.5296803652968036,
      "grad_norm": 0.831027626991272,
      "learning_rate": 4.1171993911719944e-05,
      "loss": 0.6857,
      "step": 580
    },
    {
      "epoch": 0.5388127853881278,
      "grad_norm": 1.9537763595581055,
      "learning_rate": 4.101978691019787e-05,
      "loss": 0.3424,
      "step": 590
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 5.599656105041504,
      "learning_rate": 4.08675799086758e-05,
      "loss": 0.3949,
      "step": 600
    },
    {
      "epoch": 0.5570776255707762,
      "grad_norm": 1.728446364402771,
      "learning_rate": 4.071537290715373e-05,
      "loss": 0.4469,
      "step": 610
    },
    {
      "epoch": 0.5662100456621004,
      "grad_norm": 5.917515277862549,
      "learning_rate": 4.0563165905631665e-05,
      "loss": 0.3367,
      "step": 620
    },
    {
      "epoch": 0.5753424657534246,
      "grad_norm": 1.8780170679092407,
      "learning_rate": 4.041095890410959e-05,
      "loss": 0.5115,
      "step": 630
    },
    {
      "epoch": 0.5844748858447488,
      "grad_norm": 2.5909252166748047,
      "learning_rate": 4.025875190258752e-05,
      "loss": 0.36,
      "step": 640
    },
    {
      "epoch": 0.593607305936073,
      "grad_norm": 2.507908582687378,
      "learning_rate": 4.010654490106545e-05,
      "loss": 0.6,
      "step": 650
    },
    {
      "epoch": 0.6027397260273972,
      "grad_norm": 2.5865373611450195,
      "learning_rate": 3.995433789954338e-05,
      "loss": 0.4252,
      "step": 660
    },
    {
      "epoch": 0.6118721461187214,
      "grad_norm": 3.520864248275757,
      "learning_rate": 3.9802130898021314e-05,
      "loss": 0.5569,
      "step": 670
    },
    {
      "epoch": 0.6210045662100456,
      "grad_norm": 2.2555739879608154,
      "learning_rate": 3.964992389649924e-05,
      "loss": 0.3635,
      "step": 680
    },
    {
      "epoch": 0.6301369863013698,
      "grad_norm": 1.6071314811706543,
      "learning_rate": 3.949771689497717e-05,
      "loss": 0.3888,
      "step": 690
    },
    {
      "epoch": 0.639269406392694,
      "grad_norm": 3.3402698040008545,
      "learning_rate": 3.93455098934551e-05,
      "loss": 0.6157,
      "step": 700
    },
    {
      "epoch": 0.6484018264840182,
      "grad_norm": 5.3154215812683105,
      "learning_rate": 3.9193302891933034e-05,
      "loss": 0.382,
      "step": 710
    },
    {
      "epoch": 0.6575342465753424,
      "grad_norm": 1.3434995412826538,
      "learning_rate": 3.904109589041096e-05,
      "loss": 0.417,
      "step": 720
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.8490011692047119,
      "learning_rate": 3.888888888888889e-05,
      "loss": 0.3938,
      "step": 730
    },
    {
      "epoch": 0.6757990867579908,
      "grad_norm": 1.3805787563323975,
      "learning_rate": 3.873668188736682e-05,
      "loss": 0.4668,
      "step": 740
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 2.668084144592285,
      "learning_rate": 3.8584474885844754e-05,
      "loss": 0.4649,
      "step": 750
    },
    {
      "epoch": 0.6940639269406392,
      "grad_norm": 4.917397499084473,
      "learning_rate": 3.843226788432268e-05,
      "loss": 0.3521,
      "step": 760
    },
    {
      "epoch": 0.7031963470319634,
      "grad_norm": 3.2532198429107666,
      "learning_rate": 3.828006088280061e-05,
      "loss": 0.277,
      "step": 770
    },
    {
      "epoch": 0.7123287671232876,
      "grad_norm": 3.4182302951812744,
      "learning_rate": 3.812785388127854e-05,
      "loss": 0.4187,
      "step": 780
    },
    {
      "epoch": 0.7214611872146118,
      "grad_norm": 1.894112467765808,
      "learning_rate": 3.797564687975647e-05,
      "loss": 0.7904,
      "step": 790
    },
    {
      "epoch": 0.730593607305936,
      "grad_norm": 1.9929389953613281,
      "learning_rate": 3.78234398782344e-05,
      "loss": 0.4763,
      "step": 800
    },
    {
      "epoch": 0.7397260273972602,
      "grad_norm": 2.2545082569122314,
      "learning_rate": 3.767123287671233e-05,
      "loss": 0.5097,
      "step": 810
    },
    {
      "epoch": 0.7488584474885844,
      "grad_norm": 4.622438907623291,
      "learning_rate": 3.751902587519026e-05,
      "loss": 0.5246,
      "step": 820
    },
    {
      "epoch": 0.7579908675799086,
      "grad_norm": 2.241492509841919,
      "learning_rate": 3.736681887366819e-05,
      "loss": 0.4514,
      "step": 830
    },
    {
      "epoch": 0.7671232876712328,
      "grad_norm": 3.2742772102355957,
      "learning_rate": 3.7214611872146123e-05,
      "loss": 0.426,
      "step": 840
    },
    {
      "epoch": 0.776255707762557,
      "grad_norm": 4.013513565063477,
      "learning_rate": 3.706240487062405e-05,
      "loss": 0.6039,
      "step": 850
    },
    {
      "epoch": 0.7853881278538812,
      "grad_norm": 2.1800153255462646,
      "learning_rate": 3.691019786910198e-05,
      "loss": 0.4992,
      "step": 860
    },
    {
      "epoch": 0.7945205479452054,
      "grad_norm": 2.318580389022827,
      "learning_rate": 3.675799086757991e-05,
      "loss": 0.4532,
      "step": 870
    },
    {
      "epoch": 0.8036529680365296,
      "grad_norm": 6.134377479553223,
      "learning_rate": 3.6605783866057844e-05,
      "loss": 0.6513,
      "step": 880
    },
    {
      "epoch": 0.8127853881278538,
      "grad_norm": 3.850567102432251,
      "learning_rate": 3.645357686453577e-05,
      "loss": 0.4239,
      "step": 890
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 2.7840640544891357,
      "learning_rate": 3.63013698630137e-05,
      "loss": 0.4472,
      "step": 900
    },
    {
      "epoch": 0.8310502283105022,
      "grad_norm": 2.4859445095062256,
      "learning_rate": 3.614916286149163e-05,
      "loss": 0.5086,
      "step": 910
    },
    {
      "epoch": 0.8401826484018264,
      "grad_norm": 2.4027302265167236,
      "learning_rate": 3.5996955859969564e-05,
      "loss": 0.4635,
      "step": 920
    },
    {
      "epoch": 0.8493150684931506,
      "grad_norm": 3.4102776050567627,
      "learning_rate": 3.584474885844749e-05,
      "loss": 0.3019,
      "step": 930
    },
    {
      "epoch": 0.8584474885844748,
      "grad_norm": 5.376007556915283,
      "learning_rate": 3.569254185692542e-05,
      "loss": 0.6734,
      "step": 940
    },
    {
      "epoch": 0.867579908675799,
      "grad_norm": 2.698889970779419,
      "learning_rate": 3.554033485540335e-05,
      "loss": 0.4886,
      "step": 950
    },
    {
      "epoch": 0.8767123287671232,
      "grad_norm": 2.655712127685547,
      "learning_rate": 3.538812785388128e-05,
      "loss": 0.5503,
      "step": 960
    },
    {
      "epoch": 0.8858447488584474,
      "grad_norm": 2.042450189590454,
      "learning_rate": 3.523592085235921e-05,
      "loss": 0.3903,
      "step": 970
    },
    {
      "epoch": 0.8949771689497716,
      "grad_norm": 2.3970847129821777,
      "learning_rate": 3.508371385083714e-05,
      "loss": 0.3998,
      "step": 980
    },
    {
      "epoch": 0.9041095890410958,
      "grad_norm": 1.8317621946334839,
      "learning_rate": 3.493150684931507e-05,
      "loss": 0.2733,
      "step": 990
    },
    {
      "epoch": 0.91324200913242,
      "grad_norm": 4.95844030380249,
      "learning_rate": 3.4779299847793e-05,
      "loss": 0.5046,
      "step": 1000
    },
    {
      "epoch": 0.9223744292237442,
      "grad_norm": 2.5660552978515625,
      "learning_rate": 3.462709284627093e-05,
      "loss": 0.4048,
      "step": 1010
    },
    {
      "epoch": 0.9315068493150684,
      "grad_norm": 4.510872840881348,
      "learning_rate": 3.447488584474886e-05,
      "loss": 0.4649,
      "step": 1020
    },
    {
      "epoch": 0.9406392694063926,
      "grad_norm": 2.8183722496032715,
      "learning_rate": 3.432267884322679e-05,
      "loss": 0.4198,
      "step": 1030
    },
    {
      "epoch": 0.9497716894977168,
      "grad_norm": 2.114328145980835,
      "learning_rate": 3.417047184170472e-05,
      "loss": 0.4025,
      "step": 1040
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 5.912996292114258,
      "learning_rate": 3.4018264840182654e-05,
      "loss": 0.5845,
      "step": 1050
    },
    {
      "epoch": 0.9680365296803652,
      "grad_norm": 4.500508785247803,
      "learning_rate": 3.386605783866058e-05,
      "loss": 0.3938,
      "step": 1060
    },
    {
      "epoch": 0.9771689497716894,
      "grad_norm": 2.324503183364868,
      "learning_rate": 3.371385083713851e-05,
      "loss": 0.6372,
      "step": 1070
    },
    {
      "epoch": 0.9863013698630136,
      "grad_norm": 2.3568296432495117,
      "learning_rate": 3.356164383561644e-05,
      "loss": 0.5111,
      "step": 1080
    },
    {
      "epoch": 0.9954337899543378,
      "grad_norm": 2.26467227935791,
      "learning_rate": 3.3409436834094374e-05,
      "loss": 0.6652,
      "step": 1090
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8597533120146186,
      "eval_f1_hate": 0.0,
      "eval_f1_idk/skip": 0.0,
      "eval_f1_macro": 0.23114713829525915,
      "eval_f1_noHate": 0.9245885531810366,
      "eval_f1_relation": 0.0,
      "eval_loss": 0.4586680829524994,
      "eval_precision_global": 0.9649383280036546,
      "eval_precision_hate": 1.0,
      "eval_precision_idk/skip": 1.0,
      "eval_precision_noHate": 0.8597533120146186,
      "eval_precision_relation": 1.0,
      "eval_recall_global": 0.25,
      "eval_recall_hate": 0.0,
      "eval_recall_idk/skip": 0.0,
      "eval_recall_noHate": 1.0,
      "eval_recall_relation": 0.0,
      "eval_runtime": 111.5688,
      "eval_samples_per_second": 19.62,
      "eval_steps_per_second": 2.456,
      "step": 1095
    }
  ],
  "logging_steps": 10,
  "max_steps": 3285,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4240395828142080.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
