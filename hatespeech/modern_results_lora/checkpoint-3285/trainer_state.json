{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3285,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0091324200913242,
      "grad_norm": 5.764733791351318,
      "learning_rate": 4.984779299847793e-05,
      "loss": 1.1781,
      "step": 10
    },
    {
      "epoch": 0.0182648401826484,
      "grad_norm": 2.436617136001587,
      "learning_rate": 4.969558599695586e-05,
      "loss": 0.5632,
      "step": 20
    },
    {
      "epoch": 0.0273972602739726,
      "grad_norm": 5.719903945922852,
      "learning_rate": 4.9543378995433794e-05,
      "loss": 0.4185,
      "step": 30
    },
    {
      "epoch": 0.0365296803652968,
      "grad_norm": 1.004781723022461,
      "learning_rate": 4.939117199391172e-05,
      "loss": 0.5267,
      "step": 40
    },
    {
      "epoch": 0.045662100456621,
      "grad_norm": 0.7854910492897034,
      "learning_rate": 4.923896499238965e-05,
      "loss": 0.3572,
      "step": 50
    },
    {
      "epoch": 0.0547945205479452,
      "grad_norm": 3.592888593673706,
      "learning_rate": 4.908675799086758e-05,
      "loss": 0.6624,
      "step": 60
    },
    {
      "epoch": 0.0639269406392694,
      "grad_norm": 1.4343537092208862,
      "learning_rate": 4.8934550989345515e-05,
      "loss": 0.4892,
      "step": 70
    },
    {
      "epoch": 0.0730593607305936,
      "grad_norm": 8.628373146057129,
      "learning_rate": 4.878234398782344e-05,
      "loss": 0.4583,
      "step": 80
    },
    {
      "epoch": 0.0821917808219178,
      "grad_norm": 2.372424840927124,
      "learning_rate": 4.863013698630137e-05,
      "loss": 0.5622,
      "step": 90
    },
    {
      "epoch": 0.091324200913242,
      "grad_norm": 2.0125808715820312,
      "learning_rate": 4.84779299847793e-05,
      "loss": 0.4336,
      "step": 100
    },
    {
      "epoch": 0.1004566210045662,
      "grad_norm": 2.1987414360046387,
      "learning_rate": 4.8325722983257235e-05,
      "loss": 0.5767,
      "step": 110
    },
    {
      "epoch": 0.1095890410958904,
      "grad_norm": 2.3950655460357666,
      "learning_rate": 4.8173515981735164e-05,
      "loss": 0.3194,
      "step": 120
    },
    {
      "epoch": 0.1187214611872146,
      "grad_norm": 1.5012563467025757,
      "learning_rate": 4.802130898021309e-05,
      "loss": 0.2457,
      "step": 130
    },
    {
      "epoch": 0.1278538812785388,
      "grad_norm": 0.9848641157150269,
      "learning_rate": 4.786910197869102e-05,
      "loss": 0.4722,
      "step": 140
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 5.501198768615723,
      "learning_rate": 4.7716894977168955e-05,
      "loss": 0.772,
      "step": 150
    },
    {
      "epoch": 0.1461187214611872,
      "grad_norm": 7.525727272033691,
      "learning_rate": 4.7564687975646884e-05,
      "loss": 0.6815,
      "step": 160
    },
    {
      "epoch": 0.1552511415525114,
      "grad_norm": 3.9026007652282715,
      "learning_rate": 4.741248097412481e-05,
      "loss": 0.5422,
      "step": 170
    },
    {
      "epoch": 0.1643835616438356,
      "grad_norm": 2.3730521202087402,
      "learning_rate": 4.726027397260274e-05,
      "loss": 0.4432,
      "step": 180
    },
    {
      "epoch": 0.1735159817351598,
      "grad_norm": 2.0919034481048584,
      "learning_rate": 4.710806697108067e-05,
      "loss": 0.3779,
      "step": 190
    },
    {
      "epoch": 0.182648401826484,
      "grad_norm": 4.37285852432251,
      "learning_rate": 4.6955859969558604e-05,
      "loss": 0.4991,
      "step": 200
    },
    {
      "epoch": 0.1917808219178082,
      "grad_norm": 1.8473596572875977,
      "learning_rate": 4.680365296803653e-05,
      "loss": 0.4493,
      "step": 210
    },
    {
      "epoch": 0.2009132420091324,
      "grad_norm": 2.571307420730591,
      "learning_rate": 4.665144596651446e-05,
      "loss": 0.4797,
      "step": 220
    },
    {
      "epoch": 0.2100456621004566,
      "grad_norm": 3.253429889678955,
      "learning_rate": 4.649923896499239e-05,
      "loss": 0.8208,
      "step": 230
    },
    {
      "epoch": 0.2191780821917808,
      "grad_norm": 1.8835606575012207,
      "learning_rate": 4.6347031963470325e-05,
      "loss": 0.3439,
      "step": 240
    },
    {
      "epoch": 0.228310502283105,
      "grad_norm": 5.245028495788574,
      "learning_rate": 4.619482496194825e-05,
      "loss": 0.5117,
      "step": 250
    },
    {
      "epoch": 0.2374429223744292,
      "grad_norm": 2.241241216659546,
      "learning_rate": 4.604261796042618e-05,
      "loss": 0.3563,
      "step": 260
    },
    {
      "epoch": 0.2465753424657534,
      "grad_norm": 3.8060293197631836,
      "learning_rate": 4.589041095890411e-05,
      "loss": 0.3667,
      "step": 270
    },
    {
      "epoch": 0.2557077625570776,
      "grad_norm": 2.7781898975372314,
      "learning_rate": 4.5738203957382045e-05,
      "loss": 0.4001,
      "step": 280
    },
    {
      "epoch": 0.2648401826484018,
      "grad_norm": 1.6057904958724976,
      "learning_rate": 4.5585996955859973e-05,
      "loss": 0.3013,
      "step": 290
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 3.481186866760254,
      "learning_rate": 4.54337899543379e-05,
      "loss": 0.4545,
      "step": 300
    },
    {
      "epoch": 0.2831050228310502,
      "grad_norm": 2.209181547164917,
      "learning_rate": 4.528158295281583e-05,
      "loss": 0.6136,
      "step": 310
    },
    {
      "epoch": 0.2922374429223744,
      "grad_norm": 2.4162909984588623,
      "learning_rate": 4.512937595129376e-05,
      "loss": 0.4162,
      "step": 320
    },
    {
      "epoch": 0.3013698630136986,
      "grad_norm": 5.911314010620117,
      "learning_rate": 4.4977168949771694e-05,
      "loss": 0.4856,
      "step": 330
    },
    {
      "epoch": 0.3105022831050228,
      "grad_norm": 3.0444045066833496,
      "learning_rate": 4.482496194824962e-05,
      "loss": 0.3174,
      "step": 340
    },
    {
      "epoch": 0.319634703196347,
      "grad_norm": 0.979099690914154,
      "learning_rate": 4.467275494672755e-05,
      "loss": 0.3502,
      "step": 350
    },
    {
      "epoch": 0.3287671232876712,
      "grad_norm": 3.108412504196167,
      "learning_rate": 4.452054794520548e-05,
      "loss": 0.6241,
      "step": 360
    },
    {
      "epoch": 0.3378995433789954,
      "grad_norm": 2.5400776863098145,
      "learning_rate": 4.4368340943683414e-05,
      "loss": 0.4746,
      "step": 370
    },
    {
      "epoch": 0.3470319634703196,
      "grad_norm": 2.12054705619812,
      "learning_rate": 4.421613394216134e-05,
      "loss": 0.6568,
      "step": 380
    },
    {
      "epoch": 0.3561643835616438,
      "grad_norm": 2.4121391773223877,
      "learning_rate": 4.406392694063927e-05,
      "loss": 0.5373,
      "step": 390
    },
    {
      "epoch": 0.365296803652968,
      "grad_norm": 2.157649040222168,
      "learning_rate": 4.39117199391172e-05,
      "loss": 0.4037,
      "step": 400
    },
    {
      "epoch": 0.3744292237442922,
      "grad_norm": 2.4147603511810303,
      "learning_rate": 4.3759512937595135e-05,
      "loss": 0.3606,
      "step": 410
    },
    {
      "epoch": 0.3835616438356164,
      "grad_norm": 1.5403311252593994,
      "learning_rate": 4.360730593607306e-05,
      "loss": 0.3522,
      "step": 420
    },
    {
      "epoch": 0.3926940639269406,
      "grad_norm": 3.4712796211242676,
      "learning_rate": 4.345509893455099e-05,
      "loss": 0.5384,
      "step": 430
    },
    {
      "epoch": 0.4018264840182648,
      "grad_norm": 3.574594020843506,
      "learning_rate": 4.330289193302892e-05,
      "loss": 0.3047,
      "step": 440
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 1.088865876197815,
      "learning_rate": 4.3150684931506855e-05,
      "loss": 0.6517,
      "step": 450
    },
    {
      "epoch": 0.4200913242009132,
      "grad_norm": 1.3922253847122192,
      "learning_rate": 4.299847792998478e-05,
      "loss": 0.365,
      "step": 460
    },
    {
      "epoch": 0.4292237442922374,
      "grad_norm": 1.047322154045105,
      "learning_rate": 4.284627092846271e-05,
      "loss": 0.4123,
      "step": 470
    },
    {
      "epoch": 0.4383561643835616,
      "grad_norm": 6.0662407875061035,
      "learning_rate": 4.269406392694064e-05,
      "loss": 0.5206,
      "step": 480
    },
    {
      "epoch": 0.4474885844748858,
      "grad_norm": 2.3779296875,
      "learning_rate": 4.254185692541857e-05,
      "loss": 0.4229,
      "step": 490
    },
    {
      "epoch": 0.45662100456621,
      "grad_norm": 4.853570938110352,
      "learning_rate": 4.2389649923896504e-05,
      "loss": 0.3396,
      "step": 500
    },
    {
      "epoch": 0.4657534246575342,
      "grad_norm": 5.261697769165039,
      "learning_rate": 4.223744292237443e-05,
      "loss": 0.3332,
      "step": 510
    },
    {
      "epoch": 0.4748858447488584,
      "grad_norm": 3.860701322555542,
      "learning_rate": 4.208523592085236e-05,
      "loss": 0.5272,
      "step": 520
    },
    {
      "epoch": 0.4840182648401826,
      "grad_norm": 2.6627798080444336,
      "learning_rate": 4.193302891933029e-05,
      "loss": 0.4492,
      "step": 530
    },
    {
      "epoch": 0.4931506849315068,
      "grad_norm": 2.9266114234924316,
      "learning_rate": 4.1780821917808224e-05,
      "loss": 0.4871,
      "step": 540
    },
    {
      "epoch": 0.502283105022831,
      "grad_norm": 1.2428011894226074,
      "learning_rate": 4.162861491628615e-05,
      "loss": 0.197,
      "step": 550
    },
    {
      "epoch": 0.5114155251141552,
      "grad_norm": 6.849915504455566,
      "learning_rate": 4.147640791476408e-05,
      "loss": 0.5149,
      "step": 560
    },
    {
      "epoch": 0.5205479452054794,
      "grad_norm": 5.782258987426758,
      "learning_rate": 4.132420091324201e-05,
      "loss": 0.3452,
      "step": 570
    },
    {
      "epoch": 0.5296803652968036,
      "grad_norm": 0.831027626991272,
      "learning_rate": 4.1171993911719944e-05,
      "loss": 0.6857,
      "step": 580
    },
    {
      "epoch": 0.5388127853881278,
      "grad_norm": 1.9537763595581055,
      "learning_rate": 4.101978691019787e-05,
      "loss": 0.3424,
      "step": 590
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 5.599656105041504,
      "learning_rate": 4.08675799086758e-05,
      "loss": 0.3949,
      "step": 600
    },
    {
      "epoch": 0.5570776255707762,
      "grad_norm": 1.728446364402771,
      "learning_rate": 4.071537290715373e-05,
      "loss": 0.4469,
      "step": 610
    },
    {
      "epoch": 0.5662100456621004,
      "grad_norm": 5.917515277862549,
      "learning_rate": 4.0563165905631665e-05,
      "loss": 0.3367,
      "step": 620
    },
    {
      "epoch": 0.5753424657534246,
      "grad_norm": 1.8780170679092407,
      "learning_rate": 4.041095890410959e-05,
      "loss": 0.5115,
      "step": 630
    },
    {
      "epoch": 0.5844748858447488,
      "grad_norm": 2.5909252166748047,
      "learning_rate": 4.025875190258752e-05,
      "loss": 0.36,
      "step": 640
    },
    {
      "epoch": 0.593607305936073,
      "grad_norm": 2.507908582687378,
      "learning_rate": 4.010654490106545e-05,
      "loss": 0.6,
      "step": 650
    },
    {
      "epoch": 0.6027397260273972,
      "grad_norm": 2.5865373611450195,
      "learning_rate": 3.995433789954338e-05,
      "loss": 0.4252,
      "step": 660
    },
    {
      "epoch": 0.6118721461187214,
      "grad_norm": 3.520864248275757,
      "learning_rate": 3.9802130898021314e-05,
      "loss": 0.5569,
      "step": 670
    },
    {
      "epoch": 0.6210045662100456,
      "grad_norm": 2.2555739879608154,
      "learning_rate": 3.964992389649924e-05,
      "loss": 0.3635,
      "step": 680
    },
    {
      "epoch": 0.6301369863013698,
      "grad_norm": 1.6071314811706543,
      "learning_rate": 3.949771689497717e-05,
      "loss": 0.3888,
      "step": 690
    },
    {
      "epoch": 0.639269406392694,
      "grad_norm": 3.3402698040008545,
      "learning_rate": 3.93455098934551e-05,
      "loss": 0.6157,
      "step": 700
    },
    {
      "epoch": 0.6484018264840182,
      "grad_norm": 5.3154215812683105,
      "learning_rate": 3.9193302891933034e-05,
      "loss": 0.382,
      "step": 710
    },
    {
      "epoch": 0.6575342465753424,
      "grad_norm": 1.3434995412826538,
      "learning_rate": 3.904109589041096e-05,
      "loss": 0.417,
      "step": 720
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.8490011692047119,
      "learning_rate": 3.888888888888889e-05,
      "loss": 0.3938,
      "step": 730
    },
    {
      "epoch": 0.6757990867579908,
      "grad_norm": 1.3805787563323975,
      "learning_rate": 3.873668188736682e-05,
      "loss": 0.4668,
      "step": 740
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 2.668084144592285,
      "learning_rate": 3.8584474885844754e-05,
      "loss": 0.4649,
      "step": 750
    },
    {
      "epoch": 0.6940639269406392,
      "grad_norm": 4.917397499084473,
      "learning_rate": 3.843226788432268e-05,
      "loss": 0.3521,
      "step": 760
    },
    {
      "epoch": 0.7031963470319634,
      "grad_norm": 3.2532198429107666,
      "learning_rate": 3.828006088280061e-05,
      "loss": 0.277,
      "step": 770
    },
    {
      "epoch": 0.7123287671232876,
      "grad_norm": 3.4182302951812744,
      "learning_rate": 3.812785388127854e-05,
      "loss": 0.4187,
      "step": 780
    },
    {
      "epoch": 0.7214611872146118,
      "grad_norm": 1.894112467765808,
      "learning_rate": 3.797564687975647e-05,
      "loss": 0.7904,
      "step": 790
    },
    {
      "epoch": 0.730593607305936,
      "grad_norm": 1.9929389953613281,
      "learning_rate": 3.78234398782344e-05,
      "loss": 0.4763,
      "step": 800
    },
    {
      "epoch": 0.7397260273972602,
      "grad_norm": 2.2545082569122314,
      "learning_rate": 3.767123287671233e-05,
      "loss": 0.5097,
      "step": 810
    },
    {
      "epoch": 0.7488584474885844,
      "grad_norm": 4.622438907623291,
      "learning_rate": 3.751902587519026e-05,
      "loss": 0.5246,
      "step": 820
    },
    {
      "epoch": 0.7579908675799086,
      "grad_norm": 2.241492509841919,
      "learning_rate": 3.736681887366819e-05,
      "loss": 0.4514,
      "step": 830
    },
    {
      "epoch": 0.7671232876712328,
      "grad_norm": 3.2742772102355957,
      "learning_rate": 3.7214611872146123e-05,
      "loss": 0.426,
      "step": 840
    },
    {
      "epoch": 0.776255707762557,
      "grad_norm": 4.013513565063477,
      "learning_rate": 3.706240487062405e-05,
      "loss": 0.6039,
      "step": 850
    },
    {
      "epoch": 0.7853881278538812,
      "grad_norm": 2.1800153255462646,
      "learning_rate": 3.691019786910198e-05,
      "loss": 0.4992,
      "step": 860
    },
    {
      "epoch": 0.7945205479452054,
      "grad_norm": 2.318580389022827,
      "learning_rate": 3.675799086757991e-05,
      "loss": 0.4532,
      "step": 870
    },
    {
      "epoch": 0.8036529680365296,
      "grad_norm": 6.134377479553223,
      "learning_rate": 3.6605783866057844e-05,
      "loss": 0.6513,
      "step": 880
    },
    {
      "epoch": 0.8127853881278538,
      "grad_norm": 3.850567102432251,
      "learning_rate": 3.645357686453577e-05,
      "loss": 0.4239,
      "step": 890
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 2.7840640544891357,
      "learning_rate": 3.63013698630137e-05,
      "loss": 0.4472,
      "step": 900
    },
    {
      "epoch": 0.8310502283105022,
      "grad_norm": 2.4859445095062256,
      "learning_rate": 3.614916286149163e-05,
      "loss": 0.5086,
      "step": 910
    },
    {
      "epoch": 0.8401826484018264,
      "grad_norm": 2.4027302265167236,
      "learning_rate": 3.5996955859969564e-05,
      "loss": 0.4635,
      "step": 920
    },
    {
      "epoch": 0.8493150684931506,
      "grad_norm": 3.4102776050567627,
      "learning_rate": 3.584474885844749e-05,
      "loss": 0.3019,
      "step": 930
    },
    {
      "epoch": 0.8584474885844748,
      "grad_norm": 5.376007556915283,
      "learning_rate": 3.569254185692542e-05,
      "loss": 0.6734,
      "step": 940
    },
    {
      "epoch": 0.867579908675799,
      "grad_norm": 2.698889970779419,
      "learning_rate": 3.554033485540335e-05,
      "loss": 0.4886,
      "step": 950
    },
    {
      "epoch": 0.8767123287671232,
      "grad_norm": 2.655712127685547,
      "learning_rate": 3.538812785388128e-05,
      "loss": 0.5503,
      "step": 960
    },
    {
      "epoch": 0.8858447488584474,
      "grad_norm": 2.042450189590454,
      "learning_rate": 3.523592085235921e-05,
      "loss": 0.3903,
      "step": 970
    },
    {
      "epoch": 0.8949771689497716,
      "grad_norm": 2.3970847129821777,
      "learning_rate": 3.508371385083714e-05,
      "loss": 0.3998,
      "step": 980
    },
    {
      "epoch": 0.9041095890410958,
      "grad_norm": 1.8317621946334839,
      "learning_rate": 3.493150684931507e-05,
      "loss": 0.2733,
      "step": 990
    },
    {
      "epoch": 0.91324200913242,
      "grad_norm": 4.95844030380249,
      "learning_rate": 3.4779299847793e-05,
      "loss": 0.5046,
      "step": 1000
    },
    {
      "epoch": 0.9223744292237442,
      "grad_norm": 2.5660552978515625,
      "learning_rate": 3.462709284627093e-05,
      "loss": 0.4048,
      "step": 1010
    },
    {
      "epoch": 0.9315068493150684,
      "grad_norm": 4.510872840881348,
      "learning_rate": 3.447488584474886e-05,
      "loss": 0.4649,
      "step": 1020
    },
    {
      "epoch": 0.9406392694063926,
      "grad_norm": 2.8183722496032715,
      "learning_rate": 3.432267884322679e-05,
      "loss": 0.4198,
      "step": 1030
    },
    {
      "epoch": 0.9497716894977168,
      "grad_norm": 2.114328145980835,
      "learning_rate": 3.417047184170472e-05,
      "loss": 0.4025,
      "step": 1040
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 5.912996292114258,
      "learning_rate": 3.4018264840182654e-05,
      "loss": 0.5845,
      "step": 1050
    },
    {
      "epoch": 0.9680365296803652,
      "grad_norm": 4.500508785247803,
      "learning_rate": 3.386605783866058e-05,
      "loss": 0.3938,
      "step": 1060
    },
    {
      "epoch": 0.9771689497716894,
      "grad_norm": 2.324503183364868,
      "learning_rate": 3.371385083713851e-05,
      "loss": 0.6372,
      "step": 1070
    },
    {
      "epoch": 0.9863013698630136,
      "grad_norm": 2.3568296432495117,
      "learning_rate": 3.356164383561644e-05,
      "loss": 0.5111,
      "step": 1080
    },
    {
      "epoch": 0.9954337899543378,
      "grad_norm": 2.26467227935791,
      "learning_rate": 3.3409436834094374e-05,
      "loss": 0.6652,
      "step": 1090
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8597533120146186,
      "eval_f1_hate": 0.0,
      "eval_f1_idk/skip": 0.0,
      "eval_f1_macro": 0.23114713829525915,
      "eval_f1_noHate": 0.9245885531810366,
      "eval_f1_relation": 0.0,
      "eval_loss": 0.4586680829524994,
      "eval_precision_global": 0.9649383280036546,
      "eval_precision_hate": 1.0,
      "eval_precision_idk/skip": 1.0,
      "eval_precision_noHate": 0.8597533120146186,
      "eval_precision_relation": 1.0,
      "eval_recall_global": 0.25,
      "eval_recall_hate": 0.0,
      "eval_recall_idk/skip": 0.0,
      "eval_recall_noHate": 1.0,
      "eval_recall_relation": 0.0,
      "eval_runtime": 111.5688,
      "eval_samples_per_second": 19.62,
      "eval_steps_per_second": 2.456,
      "step": 1095
    },
    {
      "epoch": 1.004566210045662,
      "grad_norm": 1.7939413785934448,
      "learning_rate": 3.32572298325723e-05,
      "loss": 0.3475,
      "step": 1100
    },
    {
      "epoch": 1.0136986301369864,
      "grad_norm": 2.3409016132354736,
      "learning_rate": 3.310502283105023e-05,
      "loss": 0.4952,
      "step": 1110
    },
    {
      "epoch": 1.0228310502283104,
      "grad_norm": 1.036000370979309,
      "learning_rate": 3.295281582952816e-05,
      "loss": 0.4576,
      "step": 1120
    },
    {
      "epoch": 1.0319634703196348,
      "grad_norm": 1.142340898513794,
      "learning_rate": 3.280060882800609e-05,
      "loss": 0.2911,
      "step": 1130
    },
    {
      "epoch": 1.0410958904109588,
      "grad_norm": 1.0710312128067017,
      "learning_rate": 3.264840182648402e-05,
      "loss": 0.5143,
      "step": 1140
    },
    {
      "epoch": 1.0502283105022832,
      "grad_norm": 0.7216975092887878,
      "learning_rate": 3.249619482496195e-05,
      "loss": 0.2675,
      "step": 1150
    },
    {
      "epoch": 1.0593607305936072,
      "grad_norm": 0.57040935754776,
      "learning_rate": 3.234398782343988e-05,
      "loss": 0.3774,
      "step": 1160
    },
    {
      "epoch": 1.0684931506849316,
      "grad_norm": 5.39246129989624,
      "learning_rate": 3.219178082191781e-05,
      "loss": 0.5575,
      "step": 1170
    },
    {
      "epoch": 1.0776255707762556,
      "grad_norm": 2.493762969970703,
      "learning_rate": 3.203957382039574e-05,
      "loss": 0.5524,
      "step": 1180
    },
    {
      "epoch": 1.08675799086758,
      "grad_norm": 4.961030960083008,
      "learning_rate": 3.188736681887367e-05,
      "loss": 0.3865,
      "step": 1190
    },
    {
      "epoch": 1.095890410958904,
      "grad_norm": 4.000412464141846,
      "learning_rate": 3.17351598173516e-05,
      "loss": 0.5016,
      "step": 1200
    },
    {
      "epoch": 1.1050228310502284,
      "grad_norm": 3.149400234222412,
      "learning_rate": 3.158295281582953e-05,
      "loss": 0.5454,
      "step": 1210
    },
    {
      "epoch": 1.1141552511415524,
      "grad_norm": 1.9568474292755127,
      "learning_rate": 3.1430745814307464e-05,
      "loss": 0.356,
      "step": 1220
    },
    {
      "epoch": 1.1232876712328768,
      "grad_norm": 10.033867835998535,
      "learning_rate": 3.127853881278539e-05,
      "loss": 0.4801,
      "step": 1230
    },
    {
      "epoch": 1.1324200913242009,
      "grad_norm": 3.642991304397583,
      "learning_rate": 3.112633181126332e-05,
      "loss": 0.4932,
      "step": 1240
    },
    {
      "epoch": 1.1415525114155252,
      "grad_norm": 2.384519100189209,
      "learning_rate": 3.097412480974125e-05,
      "loss": 0.3255,
      "step": 1250
    },
    {
      "epoch": 1.1506849315068493,
      "grad_norm": 2.0460705757141113,
      "learning_rate": 3.082191780821918e-05,
      "loss": 0.3658,
      "step": 1260
    },
    {
      "epoch": 1.1598173515981736,
      "grad_norm": 8.626426696777344,
      "learning_rate": 3.066971080669711e-05,
      "loss": 0.4706,
      "step": 1270
    },
    {
      "epoch": 1.1689497716894977,
      "grad_norm": 2.1887106895446777,
      "learning_rate": 3.051750380517504e-05,
      "loss": 0.3398,
      "step": 1280
    },
    {
      "epoch": 1.178082191780822,
      "grad_norm": 4.865962505340576,
      "learning_rate": 3.036529680365297e-05,
      "loss": 0.5093,
      "step": 1290
    },
    {
      "epoch": 1.187214611872146,
      "grad_norm": 2.473673105239868,
      "learning_rate": 3.02130898021309e-05,
      "loss": 0.5062,
      "step": 1300
    },
    {
      "epoch": 1.1963470319634704,
      "grad_norm": 4.687769889831543,
      "learning_rate": 3.006088280060883e-05,
      "loss": 0.3616,
      "step": 1310
    },
    {
      "epoch": 1.2054794520547945,
      "grad_norm": 6.55504846572876,
      "learning_rate": 2.990867579908676e-05,
      "loss": 0.4734,
      "step": 1320
    },
    {
      "epoch": 1.2146118721461188,
      "grad_norm": 5.971541404724121,
      "learning_rate": 2.975646879756469e-05,
      "loss": 0.4144,
      "step": 1330
    },
    {
      "epoch": 1.2237442922374429,
      "grad_norm": 5.757138252258301,
      "learning_rate": 2.960426179604262e-05,
      "loss": 0.4391,
      "step": 1340
    },
    {
      "epoch": 1.2328767123287672,
      "grad_norm": 6.313765525817871,
      "learning_rate": 2.945205479452055e-05,
      "loss": 0.4488,
      "step": 1350
    },
    {
      "epoch": 1.2420091324200913,
      "grad_norm": 2.432166814804077,
      "learning_rate": 2.929984779299848e-05,
      "loss": 0.3963,
      "step": 1360
    },
    {
      "epoch": 1.2511415525114156,
      "grad_norm": 1.2760546207427979,
      "learning_rate": 2.914764079147641e-05,
      "loss": 0.44,
      "step": 1370
    },
    {
      "epoch": 1.2602739726027397,
      "grad_norm": 2.281508207321167,
      "learning_rate": 2.8995433789954342e-05,
      "loss": 0.3723,
      "step": 1380
    },
    {
      "epoch": 1.269406392694064,
      "grad_norm": 7.333834648132324,
      "learning_rate": 2.884322678843227e-05,
      "loss": 0.6875,
      "step": 1390
    },
    {
      "epoch": 1.278538812785388,
      "grad_norm": 2.1903011798858643,
      "learning_rate": 2.8691019786910202e-05,
      "loss": 0.3235,
      "step": 1400
    },
    {
      "epoch": 1.2876712328767124,
      "grad_norm": 4.404420375823975,
      "learning_rate": 2.853881278538813e-05,
      "loss": 0.4167,
      "step": 1410
    },
    {
      "epoch": 1.2968036529680365,
      "grad_norm": 2.180072546005249,
      "learning_rate": 2.838660578386606e-05,
      "loss": 0.318,
      "step": 1420
    },
    {
      "epoch": 1.3059360730593608,
      "grad_norm": 2.7482314109802246,
      "learning_rate": 2.823439878234399e-05,
      "loss": 0.5653,
      "step": 1430
    },
    {
      "epoch": 1.3150684931506849,
      "grad_norm": 2.44889760017395,
      "learning_rate": 2.808219178082192e-05,
      "loss": 0.4835,
      "step": 1440
    },
    {
      "epoch": 1.3242009132420092,
      "grad_norm": 4.776541709899902,
      "learning_rate": 2.792998477929985e-05,
      "loss": 0.5758,
      "step": 1450
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 2.1494498252868652,
      "learning_rate": 2.777777777777778e-05,
      "loss": 0.4222,
      "step": 1460
    },
    {
      "epoch": 1.3424657534246576,
      "grad_norm": 3.9124350547790527,
      "learning_rate": 2.762557077625571e-05,
      "loss": 0.5217,
      "step": 1470
    },
    {
      "epoch": 1.3515981735159817,
      "grad_norm": 2.0004444122314453,
      "learning_rate": 2.747336377473364e-05,
      "loss": 0.4382,
      "step": 1480
    },
    {
      "epoch": 1.360730593607306,
      "grad_norm": 2.3097474575042725,
      "learning_rate": 2.732115677321157e-05,
      "loss": 0.3642,
      "step": 1490
    },
    {
      "epoch": 1.36986301369863,
      "grad_norm": 2.277977228164673,
      "learning_rate": 2.71689497716895e-05,
      "loss": 0.4699,
      "step": 1500
    },
    {
      "epoch": 1.3789954337899544,
      "grad_norm": 2.4746851921081543,
      "learning_rate": 2.701674277016743e-05,
      "loss": 0.4934,
      "step": 1510
    },
    {
      "epoch": 1.3881278538812785,
      "grad_norm": 9.87077522277832,
      "learning_rate": 2.686453576864536e-05,
      "loss": 0.7005,
      "step": 1520
    },
    {
      "epoch": 1.3972602739726028,
      "grad_norm": 9.732364654541016,
      "learning_rate": 2.671232876712329e-05,
      "loss": 0.5142,
      "step": 1530
    },
    {
      "epoch": 1.4063926940639269,
      "grad_norm": 1.9765045642852783,
      "learning_rate": 2.656012176560122e-05,
      "loss": 0.449,
      "step": 1540
    },
    {
      "epoch": 1.4155251141552512,
      "grad_norm": 2.656970500946045,
      "learning_rate": 2.640791476407915e-05,
      "loss": 0.4224,
      "step": 1550
    },
    {
      "epoch": 1.4246575342465753,
      "grad_norm": 4.1895341873168945,
      "learning_rate": 2.625570776255708e-05,
      "loss": 0.4269,
      "step": 1560
    },
    {
      "epoch": 1.4337899543378996,
      "grad_norm": 2.1294970512390137,
      "learning_rate": 2.6103500761035012e-05,
      "loss": 0.2552,
      "step": 1570
    },
    {
      "epoch": 1.4429223744292237,
      "grad_norm": 1.249374508857727,
      "learning_rate": 2.595129375951294e-05,
      "loss": 0.3636,
      "step": 1580
    },
    {
      "epoch": 1.452054794520548,
      "grad_norm": 2.2504751682281494,
      "learning_rate": 2.579908675799087e-05,
      "loss": 0.5303,
      "step": 1590
    },
    {
      "epoch": 1.461187214611872,
      "grad_norm": 1.8770390748977661,
      "learning_rate": 2.56468797564688e-05,
      "loss": 0.5322,
      "step": 1600
    },
    {
      "epoch": 1.4703196347031964,
      "grad_norm": 3.5693111419677734,
      "learning_rate": 2.549467275494673e-05,
      "loss": 0.4007,
      "step": 1610
    },
    {
      "epoch": 1.4794520547945205,
      "grad_norm": 3.405364990234375,
      "learning_rate": 2.534246575342466e-05,
      "loss": 0.5462,
      "step": 1620
    },
    {
      "epoch": 1.4885844748858448,
      "grad_norm": 5.415417194366455,
      "learning_rate": 2.519025875190259e-05,
      "loss": 0.3769,
      "step": 1630
    },
    {
      "epoch": 1.4977168949771689,
      "grad_norm": 2.798953056335449,
      "learning_rate": 2.503805175038052e-05,
      "loss": 0.3198,
      "step": 1640
    },
    {
      "epoch": 1.5068493150684932,
      "grad_norm": 4.94481897354126,
      "learning_rate": 2.4885844748858446e-05,
      "loss": 0.4477,
      "step": 1650
    },
    {
      "epoch": 1.5159817351598175,
      "grad_norm": 2.5483813285827637,
      "learning_rate": 2.4733637747336378e-05,
      "loss": 0.4494,
      "step": 1660
    },
    {
      "epoch": 1.5251141552511416,
      "grad_norm": 2.5218937397003174,
      "learning_rate": 2.4581430745814306e-05,
      "loss": 0.2446,
      "step": 1670
    },
    {
      "epoch": 1.5342465753424657,
      "grad_norm": 1.3277006149291992,
      "learning_rate": 2.4429223744292238e-05,
      "loss": 0.2005,
      "step": 1680
    },
    {
      "epoch": 1.54337899543379,
      "grad_norm": 0.7459893822669983,
      "learning_rate": 2.4277016742770166e-05,
      "loss": 0.5296,
      "step": 1690
    },
    {
      "epoch": 1.5525114155251143,
      "grad_norm": 0.37566402554512024,
      "learning_rate": 2.4124809741248098e-05,
      "loss": 0.268,
      "step": 1700
    },
    {
      "epoch": 1.5616438356164384,
      "grad_norm": 3.5912489891052246,
      "learning_rate": 2.3972602739726026e-05,
      "loss": 0.6536,
      "step": 1710
    },
    {
      "epoch": 1.5707762557077625,
      "grad_norm": 6.358153343200684,
      "learning_rate": 2.3820395738203958e-05,
      "loss": 0.5157,
      "step": 1720
    },
    {
      "epoch": 1.5799086757990868,
      "grad_norm": 3.694597005844116,
      "learning_rate": 2.3668188736681887e-05,
      "loss": 0.5974,
      "step": 1730
    },
    {
      "epoch": 1.589041095890411,
      "grad_norm": 2.2409212589263916,
      "learning_rate": 2.351598173515982e-05,
      "loss": 0.6816,
      "step": 1740
    },
    {
      "epoch": 1.5981735159817352,
      "grad_norm": 1.9426168203353882,
      "learning_rate": 2.3363774733637747e-05,
      "loss": 0.3793,
      "step": 1750
    },
    {
      "epoch": 1.6073059360730593,
      "grad_norm": 2.638720750808716,
      "learning_rate": 2.321156773211568e-05,
      "loss": 0.4555,
      "step": 1760
    },
    {
      "epoch": 1.6164383561643836,
      "grad_norm": 2.1486825942993164,
      "learning_rate": 2.3059360730593607e-05,
      "loss": 0.2402,
      "step": 1770
    },
    {
      "epoch": 1.625570776255708,
      "grad_norm": 2.144379138946533,
      "learning_rate": 2.290715372907154e-05,
      "loss": 0.3159,
      "step": 1780
    },
    {
      "epoch": 1.634703196347032,
      "grad_norm": 3.904815196990967,
      "learning_rate": 2.2754946727549467e-05,
      "loss": 0.4231,
      "step": 1790
    },
    {
      "epoch": 1.643835616438356,
      "grad_norm": 1.8221369981765747,
      "learning_rate": 2.2602739726027396e-05,
      "loss": 0.5574,
      "step": 1800
    },
    {
      "epoch": 1.6529680365296804,
      "grad_norm": 3.023078203201294,
      "learning_rate": 2.2450532724505327e-05,
      "loss": 0.5859,
      "step": 1810
    },
    {
      "epoch": 1.6621004566210047,
      "grad_norm": 4.2152791023254395,
      "learning_rate": 2.2298325722983256e-05,
      "loss": 0.6533,
      "step": 1820
    },
    {
      "epoch": 1.6712328767123288,
      "grad_norm": 2.5346248149871826,
      "learning_rate": 2.2146118721461187e-05,
      "loss": 0.2602,
      "step": 1830
    },
    {
      "epoch": 1.6803652968036529,
      "grad_norm": 1.6469688415527344,
      "learning_rate": 2.1993911719939116e-05,
      "loss": 0.4173,
      "step": 1840
    },
    {
      "epoch": 1.6894977168949772,
      "grad_norm": 5.0946364402771,
      "learning_rate": 2.1841704718417048e-05,
      "loss": 0.4853,
      "step": 1850
    },
    {
      "epoch": 1.6986301369863015,
      "grad_norm": 1.1745328903198242,
      "learning_rate": 2.1689497716894976e-05,
      "loss": 0.4294,
      "step": 1860
    },
    {
      "epoch": 1.7077625570776256,
      "grad_norm": 1.2721672058105469,
      "learning_rate": 2.1537290715372908e-05,
      "loss": 0.3198,
      "step": 1870
    },
    {
      "epoch": 1.7168949771689497,
      "grad_norm": 2.0459835529327393,
      "learning_rate": 2.1385083713850836e-05,
      "loss": 0.4337,
      "step": 1880
    },
    {
      "epoch": 1.726027397260274,
      "grad_norm": 5.399624824523926,
      "learning_rate": 2.1232876712328768e-05,
      "loss": 0.3922,
      "step": 1890
    },
    {
      "epoch": 1.7351598173515983,
      "grad_norm": 1.9014322757720947,
      "learning_rate": 2.1080669710806696e-05,
      "loss": 0.3405,
      "step": 1900
    },
    {
      "epoch": 1.7442922374429224,
      "grad_norm": 1.3585858345031738,
      "learning_rate": 2.0928462709284628e-05,
      "loss": 0.2527,
      "step": 1910
    },
    {
      "epoch": 1.7534246575342465,
      "grad_norm": 2.2049450874328613,
      "learning_rate": 2.0776255707762557e-05,
      "loss": 0.5254,
      "step": 1920
    },
    {
      "epoch": 1.7625570776255708,
      "grad_norm": 0.8977378010749817,
      "learning_rate": 2.062404870624049e-05,
      "loss": 0.4627,
      "step": 1930
    },
    {
      "epoch": 1.771689497716895,
      "grad_norm": 5.752340316772461,
      "learning_rate": 2.0471841704718417e-05,
      "loss": 0.616,
      "step": 1940
    },
    {
      "epoch": 1.7808219178082192,
      "grad_norm": 1.2610440254211426,
      "learning_rate": 2.0319634703196345e-05,
      "loss": 0.6137,
      "step": 1950
    },
    {
      "epoch": 1.7899543378995433,
      "grad_norm": 4.436743259429932,
      "learning_rate": 2.0167427701674277e-05,
      "loss": 0.4806,
      "step": 1960
    },
    {
      "epoch": 1.7990867579908676,
      "grad_norm": 2.829638957977295,
      "learning_rate": 2.0015220700152205e-05,
      "loss": 0.4468,
      "step": 1970
    },
    {
      "epoch": 1.808219178082192,
      "grad_norm": 4.745106220245361,
      "learning_rate": 1.9863013698630137e-05,
      "loss": 0.3436,
      "step": 1980
    },
    {
      "epoch": 1.817351598173516,
      "grad_norm": 3.335690498352051,
      "learning_rate": 1.9710806697108066e-05,
      "loss": 0.5684,
      "step": 1990
    },
    {
      "epoch": 1.82648401826484,
      "grad_norm": 1.7064300775527954,
      "learning_rate": 1.9558599695585997e-05,
      "loss": 0.3777,
      "step": 2000
    },
    {
      "epoch": 1.8356164383561644,
      "grad_norm": 1.9403191804885864,
      "learning_rate": 1.9406392694063926e-05,
      "loss": 0.4493,
      "step": 2010
    },
    {
      "epoch": 1.8447488584474887,
      "grad_norm": 3.34503173828125,
      "learning_rate": 1.9254185692541858e-05,
      "loss": 0.4782,
      "step": 2020
    },
    {
      "epoch": 1.8538812785388128,
      "grad_norm": 4.12899112701416,
      "learning_rate": 1.9101978691019786e-05,
      "loss": 0.4269,
      "step": 2030
    },
    {
      "epoch": 1.8630136986301369,
      "grad_norm": 2.091839075088501,
      "learning_rate": 1.8949771689497718e-05,
      "loss": 0.4841,
      "step": 2040
    },
    {
      "epoch": 1.8721461187214612,
      "grad_norm": 1.7386481761932373,
      "learning_rate": 1.8797564687975646e-05,
      "loss": 0.3431,
      "step": 2050
    },
    {
      "epoch": 1.8812785388127855,
      "grad_norm": 2.3469924926757812,
      "learning_rate": 1.8645357686453578e-05,
      "loss": 0.4537,
      "step": 2060
    },
    {
      "epoch": 1.8904109589041096,
      "grad_norm": 4.005034923553467,
      "learning_rate": 1.8493150684931506e-05,
      "loss": 0.5378,
      "step": 2070
    },
    {
      "epoch": 1.8995433789954337,
      "grad_norm": 3.45361590385437,
      "learning_rate": 1.8340943683409438e-05,
      "loss": 0.5186,
      "step": 2080
    },
    {
      "epoch": 1.908675799086758,
      "grad_norm": 2.048825263977051,
      "learning_rate": 1.8188736681887367e-05,
      "loss": 0.3626,
      "step": 2090
    },
    {
      "epoch": 1.9178082191780823,
      "grad_norm": 4.851515769958496,
      "learning_rate": 1.80365296803653e-05,
      "loss": 0.4355,
      "step": 2100
    },
    {
      "epoch": 1.9269406392694064,
      "grad_norm": 1.1750516891479492,
      "learning_rate": 1.7884322678843227e-05,
      "loss": 0.2501,
      "step": 2110
    },
    {
      "epoch": 1.9360730593607305,
      "grad_norm": 1.3029587268829346,
      "learning_rate": 1.7732115677321155e-05,
      "loss": 0.4773,
      "step": 2120
    },
    {
      "epoch": 1.9452054794520548,
      "grad_norm": 0.5232364535331726,
      "learning_rate": 1.7579908675799087e-05,
      "loss": 0.353,
      "step": 2130
    },
    {
      "epoch": 1.954337899543379,
      "grad_norm": 2.827425718307495,
      "learning_rate": 1.7427701674277015e-05,
      "loss": 0.6064,
      "step": 2140
    },
    {
      "epoch": 1.9634703196347032,
      "grad_norm": 2.5618934631347656,
      "learning_rate": 1.7275494672754947e-05,
      "loss": 0.6285,
      "step": 2150
    },
    {
      "epoch": 1.9726027397260273,
      "grad_norm": 2.4005424976348877,
      "learning_rate": 1.7123287671232875e-05,
      "loss": 0.6237,
      "step": 2160
    },
    {
      "epoch": 1.9817351598173516,
      "grad_norm": 2.1189446449279785,
      "learning_rate": 1.6971080669710807e-05,
      "loss": 0.3539,
      "step": 2170
    },
    {
      "epoch": 1.990867579908676,
      "grad_norm": 4.934134483337402,
      "learning_rate": 1.6818873668188736e-05,
      "loss": 0.4753,
      "step": 2180
    },
    {
      "epoch": 2.0,
      "grad_norm": 4.582617282867432,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.4998,
      "step": 2190
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8597533120146186,
      "eval_f1_hate": 0.007722007722007722,
      "eval_f1_idk/skip": 0.0,
      "eval_f1_macro": 0.23312519219598865,
      "eval_f1_noHate": 0.9247787610619469,
      "eval_f1_relation": 0.0,
      "eval_loss": 0.4494031071662903,
      "eval_precision_global": 0.7984522720341567,
      "eval_precision_hate": 0.3333333333333333,
      "eval_precision_idk/skip": 1.0,
      "eval_precision_noHate": 0.8604757548032936,
      "eval_precision_relation": 1.0,
      "eval_recall_global": 0.2508437250929862,
      "eval_recall_hate": 0.00390625,
      "eval_recall_idk/skip": 0.0,
      "eval_recall_noHate": 0.9994686503719448,
      "eval_recall_relation": 0.0,
      "eval_runtime": 111.5089,
      "eval_samples_per_second": 19.631,
      "eval_steps_per_second": 2.457,
      "step": 2190
    },
    {
      "epoch": 2.009132420091324,
      "grad_norm": 3.122997522354126,
      "learning_rate": 1.6514459665144596e-05,
      "loss": 0.4207,
      "step": 2200
    },
    {
      "epoch": 2.018264840182648,
      "grad_norm": 3.683199644088745,
      "learning_rate": 1.6362252663622528e-05,
      "loss": 0.5016,
      "step": 2210
    },
    {
      "epoch": 2.0273972602739727,
      "grad_norm": 4.229711055755615,
      "learning_rate": 1.6210045662100456e-05,
      "loss": 0.5204,
      "step": 2220
    },
    {
      "epoch": 2.036529680365297,
      "grad_norm": 3.725194215774536,
      "learning_rate": 1.6057838660578388e-05,
      "loss": 0.446,
      "step": 2230
    },
    {
      "epoch": 2.045662100456621,
      "grad_norm": 1.892792820930481,
      "learning_rate": 1.5905631659056316e-05,
      "loss": 0.4151,
      "step": 2240
    },
    {
      "epoch": 2.0547945205479454,
      "grad_norm": 2.581727981567383,
      "learning_rate": 1.5753424657534248e-05,
      "loss": 0.4451,
      "step": 2250
    },
    {
      "epoch": 2.0639269406392695,
      "grad_norm": 2.40104341506958,
      "learning_rate": 1.5601217656012176e-05,
      "loss": 0.3693,
      "step": 2260
    },
    {
      "epoch": 2.0730593607305936,
      "grad_norm": 4.446810245513916,
      "learning_rate": 1.5449010654490105e-05,
      "loss": 0.3306,
      "step": 2270
    },
    {
      "epoch": 2.0821917808219177,
      "grad_norm": 3.4434337615966797,
      "learning_rate": 1.5296803652968037e-05,
      "loss": 0.3402,
      "step": 2280
    },
    {
      "epoch": 2.091324200913242,
      "grad_norm": 3.5899078845977783,
      "learning_rate": 1.5144596651445967e-05,
      "loss": 0.4128,
      "step": 2290
    },
    {
      "epoch": 2.1004566210045663,
      "grad_norm": 5.166606903076172,
      "learning_rate": 1.4992389649923897e-05,
      "loss": 0.4654,
      "step": 2300
    },
    {
      "epoch": 2.1095890410958904,
      "grad_norm": 1.5501272678375244,
      "learning_rate": 1.4840182648401827e-05,
      "loss": 0.4388,
      "step": 2310
    },
    {
      "epoch": 2.1187214611872145,
      "grad_norm": 1.7986449003219604,
      "learning_rate": 1.4687975646879757e-05,
      "loss": 0.4915,
      "step": 2320
    },
    {
      "epoch": 2.127853881278539,
      "grad_norm": 1.3725579977035522,
      "learning_rate": 1.4535768645357687e-05,
      "loss": 0.3659,
      "step": 2330
    },
    {
      "epoch": 2.136986301369863,
      "grad_norm": 2.4308536052703857,
      "learning_rate": 1.4383561643835617e-05,
      "loss": 0.4918,
      "step": 2340
    },
    {
      "epoch": 2.146118721461187,
      "grad_norm": 4.224341869354248,
      "learning_rate": 1.4231354642313546e-05,
      "loss": 0.4747,
      "step": 2350
    },
    {
      "epoch": 2.1552511415525113,
      "grad_norm": 2.8451993465423584,
      "learning_rate": 1.4079147640791476e-05,
      "loss": 0.4559,
      "step": 2360
    },
    {
      "epoch": 2.1643835616438354,
      "grad_norm": 2.5098953247070312,
      "learning_rate": 1.3926940639269406e-05,
      "loss": 0.4575,
      "step": 2370
    },
    {
      "epoch": 2.17351598173516,
      "grad_norm": 2.822180986404419,
      "learning_rate": 1.3774733637747336e-05,
      "loss": 0.3829,
      "step": 2380
    },
    {
      "epoch": 2.182648401826484,
      "grad_norm": 5.029664516448975,
      "learning_rate": 1.3622526636225266e-05,
      "loss": 0.2999,
      "step": 2390
    },
    {
      "epoch": 2.191780821917808,
      "grad_norm": 2.3818700313568115,
      "learning_rate": 1.3470319634703196e-05,
      "loss": 0.515,
      "step": 2400
    },
    {
      "epoch": 2.2009132420091326,
      "grad_norm": 5.006175518035889,
      "learning_rate": 1.3318112633181126e-05,
      "loss": 0.349,
      "step": 2410
    },
    {
      "epoch": 2.2100456621004567,
      "grad_norm": 2.274348258972168,
      "learning_rate": 1.3165905631659056e-05,
      "loss": 0.4448,
      "step": 2420
    },
    {
      "epoch": 2.219178082191781,
      "grad_norm": 3.3674051761627197,
      "learning_rate": 1.3013698630136986e-05,
      "loss": 0.409,
      "step": 2430
    },
    {
      "epoch": 2.228310502283105,
      "grad_norm": 3.3269200325012207,
      "learning_rate": 1.2861491628614916e-05,
      "loss": 0.5685,
      "step": 2440
    },
    {
      "epoch": 2.237442922374429,
      "grad_norm": 2.3794491291046143,
      "learning_rate": 1.2709284627092847e-05,
      "loss": 0.5403,
      "step": 2450
    },
    {
      "epoch": 2.2465753424657535,
      "grad_norm": 2.1113550662994385,
      "learning_rate": 1.2557077625570777e-05,
      "loss": 0.4994,
      "step": 2460
    },
    {
      "epoch": 2.2557077625570776,
      "grad_norm": 2.7219719886779785,
      "learning_rate": 1.2404870624048707e-05,
      "loss": 0.4918,
      "step": 2470
    },
    {
      "epoch": 2.2648401826484017,
      "grad_norm": 6.674935817718506,
      "learning_rate": 1.2252663622526637e-05,
      "loss": 0.4018,
      "step": 2480
    },
    {
      "epoch": 2.2739726027397262,
      "grad_norm": 4.260508060455322,
      "learning_rate": 1.2100456621004567e-05,
      "loss": 0.5053,
      "step": 2490
    },
    {
      "epoch": 2.2831050228310503,
      "grad_norm": 2.535223960876465,
      "learning_rate": 1.1948249619482495e-05,
      "loss": 0.5536,
      "step": 2500
    },
    {
      "epoch": 2.2922374429223744,
      "grad_norm": 3.1395058631896973,
      "learning_rate": 1.1796042617960425e-05,
      "loss": 0.5102,
      "step": 2510
    },
    {
      "epoch": 2.3013698630136985,
      "grad_norm": 2.101473331451416,
      "learning_rate": 1.1643835616438355e-05,
      "loss": 0.4289,
      "step": 2520
    },
    {
      "epoch": 2.3105022831050226,
      "grad_norm": 2.379716396331787,
      "learning_rate": 1.1491628614916286e-05,
      "loss": 0.3479,
      "step": 2530
    },
    {
      "epoch": 2.319634703196347,
      "grad_norm": 3.0275049209594727,
      "learning_rate": 1.1339421613394216e-05,
      "loss": 0.5122,
      "step": 2540
    },
    {
      "epoch": 2.328767123287671,
      "grad_norm": 4.106378555297852,
      "learning_rate": 1.1187214611872146e-05,
      "loss": 0.5409,
      "step": 2550
    },
    {
      "epoch": 2.3378995433789953,
      "grad_norm": 1.925279974937439,
      "learning_rate": 1.1035007610350076e-05,
      "loss": 0.3063,
      "step": 2560
    },
    {
      "epoch": 2.34703196347032,
      "grad_norm": 1.8691918849945068,
      "learning_rate": 1.0882800608828006e-05,
      "loss": 0.5008,
      "step": 2570
    },
    {
      "epoch": 2.356164383561644,
      "grad_norm": 2.330195188522339,
      "learning_rate": 1.0730593607305936e-05,
      "loss": 0.5686,
      "step": 2580
    },
    {
      "epoch": 2.365296803652968,
      "grad_norm": 3.7086682319641113,
      "learning_rate": 1.0578386605783866e-05,
      "loss": 0.3088,
      "step": 2590
    },
    {
      "epoch": 2.374429223744292,
      "grad_norm": 2.22609543800354,
      "learning_rate": 1.0426179604261796e-05,
      "loss": 0.5107,
      "step": 2600
    },
    {
      "epoch": 2.383561643835616,
      "grad_norm": 2.9040961265563965,
      "learning_rate": 1.0273972602739726e-05,
      "loss": 0.4585,
      "step": 2610
    },
    {
      "epoch": 2.3926940639269407,
      "grad_norm": 12.671747207641602,
      "learning_rate": 1.0121765601217656e-05,
      "loss": 0.4234,
      "step": 2620
    },
    {
      "epoch": 2.401826484018265,
      "grad_norm": 2.3707878589630127,
      "learning_rate": 9.969558599695586e-06,
      "loss": 0.302,
      "step": 2630
    },
    {
      "epoch": 2.410958904109589,
      "grad_norm": 4.517814636230469,
      "learning_rate": 9.817351598173517e-06,
      "loss": 0.3415,
      "step": 2640
    },
    {
      "epoch": 2.4200913242009134,
      "grad_norm": 5.934787273406982,
      "learning_rate": 9.665144596651447e-06,
      "loss": 0.4912,
      "step": 2650
    },
    {
      "epoch": 2.4292237442922375,
      "grad_norm": 2.3189055919647217,
      "learning_rate": 9.512937595129375e-06,
      "loss": 0.4002,
      "step": 2660
    },
    {
      "epoch": 2.4383561643835616,
      "grad_norm": 2.286371946334839,
      "learning_rate": 9.360730593607305e-06,
      "loss": 0.393,
      "step": 2670
    },
    {
      "epoch": 2.4474885844748857,
      "grad_norm": 3.170295000076294,
      "learning_rate": 9.208523592085235e-06,
      "loss": 0.2916,
      "step": 2680
    },
    {
      "epoch": 2.45662100456621,
      "grad_norm": 3.1193580627441406,
      "learning_rate": 9.056316590563165e-06,
      "loss": 0.262,
      "step": 2690
    },
    {
      "epoch": 2.4657534246575343,
      "grad_norm": 1.9041025638580322,
      "learning_rate": 8.904109589041095e-06,
      "loss": 0.2016,
      "step": 2700
    },
    {
      "epoch": 2.4748858447488584,
      "grad_norm": 12.019113540649414,
      "learning_rate": 8.751902587519026e-06,
      "loss": 0.723,
      "step": 2710
    },
    {
      "epoch": 2.4840182648401825,
      "grad_norm": 5.420868396759033,
      "learning_rate": 8.599695585996956e-06,
      "loss": 0.3331,
      "step": 2720
    },
    {
      "epoch": 2.493150684931507,
      "grad_norm": 1.839930534362793,
      "learning_rate": 8.447488584474886e-06,
      "loss": 0.1702,
      "step": 2730
    },
    {
      "epoch": 2.502283105022831,
      "grad_norm": 1.7357128858566284,
      "learning_rate": 8.295281582952816e-06,
      "loss": 0.4223,
      "step": 2740
    },
    {
      "epoch": 2.5114155251141552,
      "grad_norm": 5.033404350280762,
      "learning_rate": 8.143074581430746e-06,
      "loss": 0.672,
      "step": 2750
    },
    {
      "epoch": 2.5205479452054793,
      "grad_norm": 2.285170793533325,
      "learning_rate": 7.990867579908676e-06,
      "loss": 0.4733,
      "step": 2760
    },
    {
      "epoch": 2.5296803652968034,
      "grad_norm": 0.7583867311477661,
      "learning_rate": 7.838660578386606e-06,
      "loss": 0.2569,
      "step": 2770
    },
    {
      "epoch": 2.538812785388128,
      "grad_norm": 2.8995413780212402,
      "learning_rate": 7.686453576864536e-06,
      "loss": 0.3396,
      "step": 2780
    },
    {
      "epoch": 2.547945205479452,
      "grad_norm": 9.071980476379395,
      "learning_rate": 7.5342465753424655e-06,
      "loss": 0.4072,
      "step": 2790
    },
    {
      "epoch": 2.557077625570776,
      "grad_norm": 1.776383876800537,
      "learning_rate": 7.3820395738203955e-06,
      "loss": 0.3709,
      "step": 2800
    },
    {
      "epoch": 2.5662100456621006,
      "grad_norm": 3.4673895835876465,
      "learning_rate": 7.229832572298326e-06,
      "loss": 0.5801,
      "step": 2810
    },
    {
      "epoch": 2.5753424657534247,
      "grad_norm": 8.842192649841309,
      "learning_rate": 7.077625570776256e-06,
      "loss": 0.3178,
      "step": 2820
    },
    {
      "epoch": 2.584474885844749,
      "grad_norm": 3.4340155124664307,
      "learning_rate": 6.925418569254186e-06,
      "loss": 0.4415,
      "step": 2830
    },
    {
      "epoch": 2.593607305936073,
      "grad_norm": 0.7375269532203674,
      "learning_rate": 6.773211567732116e-06,
      "loss": 0.3063,
      "step": 2840
    },
    {
      "epoch": 2.602739726027397,
      "grad_norm": 1.4380875825881958,
      "learning_rate": 6.621004566210046e-06,
      "loss": 0.3453,
      "step": 2850
    },
    {
      "epoch": 2.6118721461187215,
      "grad_norm": 3.283637285232544,
      "learning_rate": 6.468797564687975e-06,
      "loss": 0.3362,
      "step": 2860
    },
    {
      "epoch": 2.6210045662100456,
      "grad_norm": 1.6967482566833496,
      "learning_rate": 6.316590563165905e-06,
      "loss": 0.4827,
      "step": 2870
    },
    {
      "epoch": 2.6301369863013697,
      "grad_norm": 0.8709425330162048,
      "learning_rate": 6.1643835616438354e-06,
      "loss": 0.3778,
      "step": 2880
    },
    {
      "epoch": 2.6392694063926943,
      "grad_norm": 5.50696325302124,
      "learning_rate": 6.0121765601217655e-06,
      "loss": 0.5668,
      "step": 2890
    },
    {
      "epoch": 2.6484018264840183,
      "grad_norm": 5.674956321716309,
      "learning_rate": 5.859969558599696e-06,
      "loss": 0.4784,
      "step": 2900
    },
    {
      "epoch": 2.6575342465753424,
      "grad_norm": 3.014261484146118,
      "learning_rate": 5.707762557077626e-06,
      "loss": 0.3515,
      "step": 2910
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.9985891580581665,
      "learning_rate": 5.555555555555556e-06,
      "loss": 0.441,
      "step": 2920
    },
    {
      "epoch": 2.6757990867579906,
      "grad_norm": 3.3361735343933105,
      "learning_rate": 5.403348554033486e-06,
      "loss": 0.2627,
      "step": 2930
    },
    {
      "epoch": 2.684931506849315,
      "grad_norm": 2.9214160442352295,
      "learning_rate": 5.251141552511415e-06,
      "loss": 0.5683,
      "step": 2940
    },
    {
      "epoch": 2.6940639269406392,
      "grad_norm": 3.580357313156128,
      "learning_rate": 5.098934550989345e-06,
      "loss": 0.421,
      "step": 2950
    },
    {
      "epoch": 2.7031963470319633,
      "grad_norm": 7.401581287384033,
      "learning_rate": 4.946727549467275e-06,
      "loss": 0.5234,
      "step": 2960
    },
    {
      "epoch": 2.712328767123288,
      "grad_norm": 2.4447484016418457,
      "learning_rate": 4.7945205479452054e-06,
      "loss": 0.5744,
      "step": 2970
    },
    {
      "epoch": 2.721461187214612,
      "grad_norm": 5.7386603355407715,
      "learning_rate": 4.6423135464231355e-06,
      "loss": 0.3318,
      "step": 2980
    },
    {
      "epoch": 2.730593607305936,
      "grad_norm": 2.893920660018921,
      "learning_rate": 4.490106544901066e-06,
      "loss": 0.4068,
      "step": 2990
    },
    {
      "epoch": 2.73972602739726,
      "grad_norm": 2.534308433532715,
      "learning_rate": 4.337899543378996e-06,
      "loss": 0.5364,
      "step": 3000
    },
    {
      "epoch": 2.748858447488584,
      "grad_norm": 2.8395512104034424,
      "learning_rate": 4.185692541856925e-06,
      "loss": 0.3489,
      "step": 3010
    },
    {
      "epoch": 2.7579908675799087,
      "grad_norm": 1.865065097808838,
      "learning_rate": 4.033485540334855e-06,
      "loss": 0.3198,
      "step": 3020
    },
    {
      "epoch": 2.767123287671233,
      "grad_norm": 5.4067463874816895,
      "learning_rate": 3.881278538812785e-06,
      "loss": 0.5835,
      "step": 3030
    },
    {
      "epoch": 2.776255707762557,
      "grad_norm": 4.196545124053955,
      "learning_rate": 3.7290715372907152e-06,
      "loss": 0.2526,
      "step": 3040
    },
    {
      "epoch": 2.7853881278538815,
      "grad_norm": 8.786758422851562,
      "learning_rate": 3.5768645357686453e-06,
      "loss": 0.459,
      "step": 3050
    },
    {
      "epoch": 2.7945205479452055,
      "grad_norm": 5.686471939086914,
      "learning_rate": 3.4246575342465754e-06,
      "loss": 0.5846,
      "step": 3060
    },
    {
      "epoch": 2.8036529680365296,
      "grad_norm": 5.4303507804870605,
      "learning_rate": 3.272450532724505e-06,
      "loss": 0.2459,
      "step": 3070
    },
    {
      "epoch": 2.8127853881278537,
      "grad_norm": 2.4180166721343994,
      "learning_rate": 3.120243531202435e-06,
      "loss": 0.2643,
      "step": 3080
    },
    {
      "epoch": 2.821917808219178,
      "grad_norm": 2.947209596633911,
      "learning_rate": 2.9680365296803653e-06,
      "loss": 0.2642,
      "step": 3090
    },
    {
      "epoch": 2.8310502283105023,
      "grad_norm": 2.8470253944396973,
      "learning_rate": 2.8158295281582954e-06,
      "loss": 0.5467,
      "step": 3100
    },
    {
      "epoch": 2.8401826484018264,
      "grad_norm": 1.5741698741912842,
      "learning_rate": 2.663622526636225e-06,
      "loss": 0.2813,
      "step": 3110
    },
    {
      "epoch": 2.8493150684931505,
      "grad_norm": 2.978539228439331,
      "learning_rate": 2.511415525114155e-06,
      "loss": 0.272,
      "step": 3120
    },
    {
      "epoch": 2.858447488584475,
      "grad_norm": 1.8533977270126343,
      "learning_rate": 2.3592085235920852e-06,
      "loss": 0.4434,
      "step": 3130
    },
    {
      "epoch": 2.867579908675799,
      "grad_norm": 3.577821969985962,
      "learning_rate": 2.2070015220700153e-06,
      "loss": 0.63,
      "step": 3140
    },
    {
      "epoch": 2.8767123287671232,
      "grad_norm": 5.5385518074035645,
      "learning_rate": 2.054794520547945e-06,
      "loss": 0.4146,
      "step": 3150
    },
    {
      "epoch": 2.8858447488584473,
      "grad_norm": 1.532224178314209,
      "learning_rate": 1.902587519025875e-06,
      "loss": 0.2894,
      "step": 3160
    },
    {
      "epoch": 2.8949771689497714,
      "grad_norm": 2.416238784790039,
      "learning_rate": 1.7503805175038052e-06,
      "loss": 0.5695,
      "step": 3170
    },
    {
      "epoch": 2.904109589041096,
      "grad_norm": 2.368701696395874,
      "learning_rate": 1.598173515981735e-06,
      "loss": 0.4717,
      "step": 3180
    },
    {
      "epoch": 2.91324200913242,
      "grad_norm": 1.2995342016220093,
      "learning_rate": 1.4459665144596652e-06,
      "loss": 0.3841,
      "step": 3190
    },
    {
      "epoch": 2.922374429223744,
      "grad_norm": 1.9868863821029663,
      "learning_rate": 1.293759512937595e-06,
      "loss": 0.1736,
      "step": 3200
    },
    {
      "epoch": 2.9315068493150687,
      "grad_norm": 5.234294891357422,
      "learning_rate": 1.1415525114155251e-06,
      "loss": 0.4752,
      "step": 3210
    },
    {
      "epoch": 2.9406392694063928,
      "grad_norm": 3.030346393585205,
      "learning_rate": 9.89345509893455e-07,
      "loss": 0.444,
      "step": 3220
    },
    {
      "epoch": 2.949771689497717,
      "grad_norm": 4.460147857666016,
      "learning_rate": 8.371385083713851e-07,
      "loss": 0.5474,
      "step": 3230
    },
    {
      "epoch": 2.958904109589041,
      "grad_norm": 8.749507904052734,
      "learning_rate": 6.849315068493151e-07,
      "loss": 0.445,
      "step": 3240
    },
    {
      "epoch": 2.968036529680365,
      "grad_norm": 2.2286741733551025,
      "learning_rate": 5.327245053272451e-07,
      "loss": 0.37,
      "step": 3250
    },
    {
      "epoch": 2.9771689497716896,
      "grad_norm": 3.373434066772461,
      "learning_rate": 3.8051750380517503e-07,
      "loss": 0.6585,
      "step": 3260
    },
    {
      "epoch": 2.9863013698630136,
      "grad_norm": 4.913638114929199,
      "learning_rate": 2.2831050228310502e-07,
      "loss": 0.3587,
      "step": 3270
    },
    {
      "epoch": 2.9954337899543377,
      "grad_norm": 1.861515998840332,
      "learning_rate": 7.6103500761035e-08,
      "loss": 0.421,
      "step": 3280
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.8597533120146186,
      "eval_f1_hate": 0.0,
      "eval_f1_idk/skip": 0.0,
      "eval_f1_macro": 0.23114713829525915,
      "eval_f1_noHate": 0.9245885531810366,
      "eval_f1_relation": 0.0,
      "eval_loss": 0.4783695340156555,
      "eval_precision_global": 0.9649383280036546,
      "eval_precision_hate": 1.0,
      "eval_precision_idk/skip": 1.0,
      "eval_precision_noHate": 0.8597533120146186,
      "eval_precision_relation": 1.0,
      "eval_recall_global": 0.25,
      "eval_recall_hate": 0.0,
      "eval_recall_idk/skip": 0.0,
      "eval_recall_noHate": 1.0,
      "eval_recall_relation": 0.0,
      "eval_runtime": 111.503,
      "eval_samples_per_second": 19.632,
      "eval_steps_per_second": 2.457,
      "step": 3285
    }
  ],
  "logging_steps": 10,
  "max_steps": 3285,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.272118748442624e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
