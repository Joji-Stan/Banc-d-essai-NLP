{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1095,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0091324200913242,
      "grad_norm": 4.2014241218566895,
      "learning_rate": 4.984779299847793e-05,
      "loss": 1.4092,
      "step": 10
    },
    {
      "epoch": 0.0182648401826484,
      "grad_norm": 4.149141788482666,
      "learning_rate": 4.969558599695586e-05,
      "loss": 1.3238,
      "step": 20
    },
    {
      "epoch": 0.0273972602739726,
      "grad_norm": 4.970274448394775,
      "learning_rate": 4.9543378995433794e-05,
      "loss": 1.2415,
      "step": 30
    },
    {
      "epoch": 0.0365296803652968,
      "grad_norm": 4.783473014831543,
      "learning_rate": 4.939117199391172e-05,
      "loss": 1.0942,
      "step": 40
    },
    {
      "epoch": 0.045662100456621,
      "grad_norm": 3.472869634628296,
      "learning_rate": 4.923896499238965e-05,
      "loss": 0.9998,
      "step": 50
    },
    {
      "epoch": 0.0547945205479452,
      "grad_norm": 3.1168034076690674,
      "learning_rate": 4.908675799086758e-05,
      "loss": 0.8516,
      "step": 60
    },
    {
      "epoch": 0.0639269406392694,
      "grad_norm": 2.4794931411743164,
      "learning_rate": 4.8934550989345515e-05,
      "loss": 0.7077,
      "step": 70
    },
    {
      "epoch": 0.0730593607305936,
      "grad_norm": 1.394862413406372,
      "learning_rate": 4.878234398782344e-05,
      "loss": 0.573,
      "step": 80
    },
    {
      "epoch": 0.0821917808219178,
      "grad_norm": 2.526909112930298,
      "learning_rate": 4.863013698630137e-05,
      "loss": 0.5617,
      "step": 90
    },
    {
      "epoch": 0.091324200913242,
      "grad_norm": 8.289301872253418,
      "learning_rate": 4.84779299847793e-05,
      "loss": 0.6276,
      "step": 100
    },
    {
      "epoch": 0.1004566210045662,
      "grad_norm": 3.2346248626708984,
      "learning_rate": 4.8325722983257235e-05,
      "loss": 0.6523,
      "step": 110
    },
    {
      "epoch": 0.1095890410958904,
      "grad_norm": 2.560805320739746,
      "learning_rate": 4.8173515981735164e-05,
      "loss": 0.3892,
      "step": 120
    },
    {
      "epoch": 0.1187214611872146,
      "grad_norm": 2.987116575241089,
      "learning_rate": 4.802130898021309e-05,
      "loss": 0.6495,
      "step": 130
    },
    {
      "epoch": 0.1278538812785388,
      "grad_norm": 3.038130044937134,
      "learning_rate": 4.786910197869102e-05,
      "loss": 0.6337,
      "step": 140
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 5.149893283843994,
      "learning_rate": 4.7716894977168955e-05,
      "loss": 0.4634,
      "step": 150
    },
    {
      "epoch": 0.1461187214611872,
      "grad_norm": 3.445244073867798,
      "learning_rate": 4.7564687975646884e-05,
      "loss": 0.4522,
      "step": 160
    },
    {
      "epoch": 0.1552511415525114,
      "grad_norm": 2.870965003967285,
      "learning_rate": 4.741248097412481e-05,
      "loss": 0.3908,
      "step": 170
    },
    {
      "epoch": 0.1643835616438356,
      "grad_norm": 7.03959321975708,
      "learning_rate": 4.726027397260274e-05,
      "loss": 0.3304,
      "step": 180
    },
    {
      "epoch": 0.1735159817351598,
      "grad_norm": 3.2255845069885254,
      "learning_rate": 4.710806697108067e-05,
      "loss": 0.4624,
      "step": 190
    },
    {
      "epoch": 0.182648401826484,
      "grad_norm": 2.61588454246521,
      "learning_rate": 4.6955859969558604e-05,
      "loss": 0.3659,
      "step": 200
    },
    {
      "epoch": 0.1917808219178082,
      "grad_norm": 4.456578254699707,
      "learning_rate": 4.680365296803653e-05,
      "loss": 0.3655,
      "step": 210
    },
    {
      "epoch": 0.2009132420091324,
      "grad_norm": 5.7224440574646,
      "learning_rate": 4.665144596651446e-05,
      "loss": 0.3439,
      "step": 220
    },
    {
      "epoch": 0.2100456621004566,
      "grad_norm": 1.495111346244812,
      "learning_rate": 4.649923896499239e-05,
      "loss": 0.3132,
      "step": 230
    },
    {
      "epoch": 0.2191780821917808,
      "grad_norm": 3.848186492919922,
      "learning_rate": 4.6347031963470325e-05,
      "loss": 0.412,
      "step": 240
    },
    {
      "epoch": 0.228310502283105,
      "grad_norm": 3.3290815353393555,
      "learning_rate": 4.619482496194825e-05,
      "loss": 0.3121,
      "step": 250
    },
    {
      "epoch": 0.2374429223744292,
      "grad_norm": 1.6857166290283203,
      "learning_rate": 4.604261796042618e-05,
      "loss": 0.2836,
      "step": 260
    },
    {
      "epoch": 0.2465753424657534,
      "grad_norm": 0.7483834624290466,
      "learning_rate": 4.589041095890411e-05,
      "loss": 0.4508,
      "step": 270
    },
    {
      "epoch": 0.2557077625570776,
      "grad_norm": 15.452421188354492,
      "learning_rate": 4.5738203957382045e-05,
      "loss": 0.4947,
      "step": 280
    },
    {
      "epoch": 0.2648401826484018,
      "grad_norm": 3.2019295692443848,
      "learning_rate": 4.5585996955859973e-05,
      "loss": 0.3868,
      "step": 290
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 2.713031530380249,
      "learning_rate": 4.54337899543379e-05,
      "loss": 0.3957,
      "step": 300
    },
    {
      "epoch": 0.2831050228310502,
      "grad_norm": 8.713400840759277,
      "learning_rate": 4.528158295281583e-05,
      "loss": 0.4065,
      "step": 310
    },
    {
      "epoch": 0.2922374429223744,
      "grad_norm": 0.9665752053260803,
      "learning_rate": 4.512937595129376e-05,
      "loss": 0.4704,
      "step": 320
    },
    {
      "epoch": 0.3013698630136986,
      "grad_norm": 1.5282211303710938,
      "learning_rate": 4.4977168949771694e-05,
      "loss": 0.6581,
      "step": 330
    },
    {
      "epoch": 0.3105022831050228,
      "grad_norm": 3.5258123874664307,
      "learning_rate": 4.482496194824962e-05,
      "loss": 0.2639,
      "step": 340
    },
    {
      "epoch": 0.319634703196347,
      "grad_norm": 3.5934760570526123,
      "learning_rate": 4.467275494672755e-05,
      "loss": 0.2484,
      "step": 350
    },
    {
      "epoch": 0.3287671232876712,
      "grad_norm": 2.534461259841919,
      "learning_rate": 4.452054794520548e-05,
      "loss": 0.5359,
      "step": 360
    },
    {
      "epoch": 0.3378995433789954,
      "grad_norm": 4.455851078033447,
      "learning_rate": 4.4368340943683414e-05,
      "loss": 0.4696,
      "step": 370
    },
    {
      "epoch": 0.3470319634703196,
      "grad_norm": 2.4211103916168213,
      "learning_rate": 4.421613394216134e-05,
      "loss": 0.4243,
      "step": 380
    },
    {
      "epoch": 0.3561643835616438,
      "grad_norm": 2.8859012126922607,
      "learning_rate": 4.406392694063927e-05,
      "loss": 0.471,
      "step": 390
    },
    {
      "epoch": 0.365296803652968,
      "grad_norm": 12.153661727905273,
      "learning_rate": 4.39117199391172e-05,
      "loss": 0.4516,
      "step": 400
    },
    {
      "epoch": 0.3744292237442922,
      "grad_norm": 7.018897533416748,
      "learning_rate": 4.3759512937595135e-05,
      "loss": 0.5746,
      "step": 410
    },
    {
      "epoch": 0.3835616438356164,
      "grad_norm": 2.0375819206237793,
      "learning_rate": 4.360730593607306e-05,
      "loss": 0.1443,
      "step": 420
    },
    {
      "epoch": 0.3926940639269406,
      "grad_norm": 3.4404189586639404,
      "learning_rate": 4.345509893455099e-05,
      "loss": 0.43,
      "step": 430
    },
    {
      "epoch": 0.4018264840182648,
      "grad_norm": 8.660131454467773,
      "learning_rate": 4.330289193302892e-05,
      "loss": 0.8187,
      "step": 440
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 2.531592845916748,
      "learning_rate": 4.3150684931506855e-05,
      "loss": 0.3468,
      "step": 450
    },
    {
      "epoch": 0.4200913242009132,
      "grad_norm": 2.712364435195923,
      "learning_rate": 4.299847792998478e-05,
      "loss": 0.3988,
      "step": 460
    },
    {
      "epoch": 0.4292237442922374,
      "grad_norm": 1.8181850910186768,
      "learning_rate": 4.284627092846271e-05,
      "loss": 0.449,
      "step": 470
    },
    {
      "epoch": 0.4383561643835616,
      "grad_norm": 1.7195039987564087,
      "learning_rate": 4.269406392694064e-05,
      "loss": 0.2727,
      "step": 480
    },
    {
      "epoch": 0.4474885844748858,
      "grad_norm": 2.748134136199951,
      "learning_rate": 4.254185692541857e-05,
      "loss": 0.3758,
      "step": 490
    },
    {
      "epoch": 0.45662100456621,
      "grad_norm": 5.917792320251465,
      "learning_rate": 4.2389649923896504e-05,
      "loss": 0.3246,
      "step": 500
    },
    {
      "epoch": 0.4657534246575342,
      "grad_norm": 2.3957009315490723,
      "learning_rate": 4.223744292237443e-05,
      "loss": 0.5519,
      "step": 510
    },
    {
      "epoch": 0.4748858447488584,
      "grad_norm": 2.0162081718444824,
      "learning_rate": 4.208523592085236e-05,
      "loss": 0.3067,
      "step": 520
    },
    {
      "epoch": 0.4840182648401826,
      "grad_norm": 7.277093887329102,
      "learning_rate": 4.193302891933029e-05,
      "loss": 0.2649,
      "step": 530
    },
    {
      "epoch": 0.4931506849315068,
      "grad_norm": 0.6217344999313354,
      "learning_rate": 4.1780821917808224e-05,
      "loss": 0.1976,
      "step": 540
    },
    {
      "epoch": 0.502283105022831,
      "grad_norm": 1.422882318496704,
      "learning_rate": 4.162861491628615e-05,
      "loss": 0.2015,
      "step": 550
    },
    {
      "epoch": 0.5114155251141552,
      "grad_norm": 1.3135812282562256,
      "learning_rate": 4.147640791476408e-05,
      "loss": 0.2779,
      "step": 560
    },
    {
      "epoch": 0.5205479452054794,
      "grad_norm": 14.464576721191406,
      "learning_rate": 4.132420091324201e-05,
      "loss": 0.4883,
      "step": 570
    },
    {
      "epoch": 0.5296803652968036,
      "grad_norm": 4.937425136566162,
      "learning_rate": 4.1171993911719944e-05,
      "loss": 0.5045,
      "step": 580
    },
    {
      "epoch": 0.5388127853881278,
      "grad_norm": 2.497363805770874,
      "learning_rate": 4.101978691019787e-05,
      "loss": 0.383,
      "step": 590
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 4.553092002868652,
      "learning_rate": 4.08675799086758e-05,
      "loss": 0.2887,
      "step": 600
    },
    {
      "epoch": 0.5570776255707762,
      "grad_norm": 3.252288579940796,
      "learning_rate": 4.071537290715373e-05,
      "loss": 0.2456,
      "step": 610
    },
    {
      "epoch": 0.5662100456621004,
      "grad_norm": 1.4945292472839355,
      "learning_rate": 4.0563165905631665e-05,
      "loss": 0.5277,
      "step": 620
    },
    {
      "epoch": 0.5753424657534246,
      "grad_norm": 5.297628402709961,
      "learning_rate": 4.041095890410959e-05,
      "loss": 0.589,
      "step": 630
    },
    {
      "epoch": 0.5844748858447488,
      "grad_norm": 5.739619731903076,
      "learning_rate": 4.025875190258752e-05,
      "loss": 0.3895,
      "step": 640
    },
    {
      "epoch": 0.593607305936073,
      "grad_norm": 3.931476354598999,
      "learning_rate": 4.010654490106545e-05,
      "loss": 0.4483,
      "step": 650
    },
    {
      "epoch": 0.6027397260273972,
      "grad_norm": 5.132766246795654,
      "learning_rate": 3.995433789954338e-05,
      "loss": 0.338,
      "step": 660
    },
    {
      "epoch": 0.6118721461187214,
      "grad_norm": 4.865106105804443,
      "learning_rate": 3.9802130898021314e-05,
      "loss": 0.3937,
      "step": 670
    },
    {
      "epoch": 0.6210045662100456,
      "grad_norm": 2.0048723220825195,
      "learning_rate": 3.964992389649924e-05,
      "loss": 0.3914,
      "step": 680
    },
    {
      "epoch": 0.6301369863013698,
      "grad_norm": 4.006353378295898,
      "learning_rate": 3.949771689497717e-05,
      "loss": 0.5664,
      "step": 690
    },
    {
      "epoch": 0.639269406392694,
      "grad_norm": 4.143347263336182,
      "learning_rate": 3.93455098934551e-05,
      "loss": 0.1821,
      "step": 700
    },
    {
      "epoch": 0.6484018264840182,
      "grad_norm": 0.5014417767524719,
      "learning_rate": 3.9193302891933034e-05,
      "loss": 0.2695,
      "step": 710
    },
    {
      "epoch": 0.6575342465753424,
      "grad_norm": 7.656728267669678,
      "learning_rate": 3.904109589041096e-05,
      "loss": 0.7182,
      "step": 720
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 3.5674729347229004,
      "learning_rate": 3.888888888888889e-05,
      "loss": 0.4191,
      "step": 730
    },
    {
      "epoch": 0.6757990867579908,
      "grad_norm": 3.0391266345977783,
      "learning_rate": 3.873668188736682e-05,
      "loss": 0.4172,
      "step": 740
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 1.8672640323638916,
      "learning_rate": 3.8584474885844754e-05,
      "loss": 0.3501,
      "step": 750
    },
    {
      "epoch": 0.6940639269406392,
      "grad_norm": 2.1507303714752197,
      "learning_rate": 3.843226788432268e-05,
      "loss": 0.2292,
      "step": 760
    },
    {
      "epoch": 0.7031963470319634,
      "grad_norm": 5.020025253295898,
      "learning_rate": 3.828006088280061e-05,
      "loss": 0.443,
      "step": 770
    },
    {
      "epoch": 0.7123287671232876,
      "grad_norm": 8.281349182128906,
      "learning_rate": 3.812785388127854e-05,
      "loss": 0.4447,
      "step": 780
    },
    {
      "epoch": 0.7214611872146118,
      "grad_norm": 2.182190418243408,
      "learning_rate": 3.797564687975647e-05,
      "loss": 0.2514,
      "step": 790
    },
    {
      "epoch": 0.730593607305936,
      "grad_norm": 13.718620300292969,
      "learning_rate": 3.78234398782344e-05,
      "loss": 0.5834,
      "step": 800
    },
    {
      "epoch": 0.7397260273972602,
      "grad_norm": 2.225736618041992,
      "learning_rate": 3.767123287671233e-05,
      "loss": 0.4969,
      "step": 810
    },
    {
      "epoch": 0.7488584474885844,
      "grad_norm": 7.196774959564209,
      "learning_rate": 3.751902587519026e-05,
      "loss": 0.4007,
      "step": 820
    },
    {
      "epoch": 0.7579908675799086,
      "grad_norm": 6.489071846008301,
      "learning_rate": 3.736681887366819e-05,
      "loss": 0.3576,
      "step": 830
    },
    {
      "epoch": 0.7671232876712328,
      "grad_norm": 5.737024784088135,
      "learning_rate": 3.7214611872146123e-05,
      "loss": 0.4381,
      "step": 840
    },
    {
      "epoch": 0.776255707762557,
      "grad_norm": 4.6021504402160645,
      "learning_rate": 3.706240487062405e-05,
      "loss": 0.2426,
      "step": 850
    },
    {
      "epoch": 0.7853881278538812,
      "grad_norm": 2.569338321685791,
      "learning_rate": 3.691019786910198e-05,
      "loss": 0.3163,
      "step": 860
    },
    {
      "epoch": 0.7945205479452054,
      "grad_norm": 0.5997539758682251,
      "learning_rate": 3.675799086757991e-05,
      "loss": 0.2816,
      "step": 870
    },
    {
      "epoch": 0.8036529680365296,
      "grad_norm": 1.7195467948913574,
      "learning_rate": 3.6605783866057844e-05,
      "loss": 0.4755,
      "step": 880
    },
    {
      "epoch": 0.8127853881278538,
      "grad_norm": 0.6913968324661255,
      "learning_rate": 3.645357686453577e-05,
      "loss": 0.1824,
      "step": 890
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 0.5214787721633911,
      "learning_rate": 3.63013698630137e-05,
      "loss": 0.7159,
      "step": 900
    },
    {
      "epoch": 0.8310502283105022,
      "grad_norm": 3.387037754058838,
      "learning_rate": 3.614916286149163e-05,
      "loss": 0.6324,
      "step": 910
    },
    {
      "epoch": 0.8401826484018264,
      "grad_norm": 0.6512325406074524,
      "learning_rate": 3.5996955859969564e-05,
      "loss": 0.385,
      "step": 920
    },
    {
      "epoch": 0.8493150684931506,
      "grad_norm": 3.3542628288269043,
      "learning_rate": 3.584474885844749e-05,
      "loss": 0.288,
      "step": 930
    },
    {
      "epoch": 0.8584474885844748,
      "grad_norm": 5.149443626403809,
      "learning_rate": 3.569254185692542e-05,
      "loss": 0.5914,
      "step": 940
    },
    {
      "epoch": 0.867579908675799,
      "grad_norm": 10.313617706298828,
      "learning_rate": 3.554033485540335e-05,
      "loss": 0.3933,
      "step": 950
    },
    {
      "epoch": 0.8767123287671232,
      "grad_norm": 2.1160686016082764,
      "learning_rate": 3.538812785388128e-05,
      "loss": 0.4591,
      "step": 960
    },
    {
      "epoch": 0.8858447488584474,
      "grad_norm": 4.683087348937988,
      "learning_rate": 3.523592085235921e-05,
      "loss": 0.4178,
      "step": 970
    },
    {
      "epoch": 0.8949771689497716,
      "grad_norm": 3.4736366271972656,
      "learning_rate": 3.508371385083714e-05,
      "loss": 0.3218,
      "step": 980
    },
    {
      "epoch": 0.9041095890410958,
      "grad_norm": 4.114583969116211,
      "learning_rate": 3.493150684931507e-05,
      "loss": 0.5066,
      "step": 990
    },
    {
      "epoch": 0.91324200913242,
      "grad_norm": 4.443102836608887,
      "learning_rate": 3.4779299847793e-05,
      "loss": 0.3163,
      "step": 1000
    },
    {
      "epoch": 0.9223744292237442,
      "grad_norm": 3.668541669845581,
      "learning_rate": 3.462709284627093e-05,
      "loss": 0.37,
      "step": 1010
    },
    {
      "epoch": 0.9315068493150684,
      "grad_norm": 3.1004445552825928,
      "learning_rate": 3.447488584474886e-05,
      "loss": 0.3316,
      "step": 1020
    },
    {
      "epoch": 0.9406392694063926,
      "grad_norm": 1.8595644235610962,
      "learning_rate": 3.432267884322679e-05,
      "loss": 0.3757,
      "step": 1030
    },
    {
      "epoch": 0.9497716894977168,
      "grad_norm": 0.7095105648040771,
      "learning_rate": 3.417047184170472e-05,
      "loss": 0.1853,
      "step": 1040
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 3.82582688331604,
      "learning_rate": 3.4018264840182654e-05,
      "loss": 0.2836,
      "step": 1050
    },
    {
      "epoch": 0.9680365296803652,
      "grad_norm": 6.456411361694336,
      "learning_rate": 3.386605783866058e-05,
      "loss": 0.4701,
      "step": 1060
    },
    {
      "epoch": 0.9771689497716894,
      "grad_norm": 1.0287468433380127,
      "learning_rate": 3.371385083713851e-05,
      "loss": 0.3622,
      "step": 1070
    },
    {
      "epoch": 0.9863013698630136,
      "grad_norm": 10.451523780822754,
      "learning_rate": 3.356164383561644e-05,
      "loss": 0.3881,
      "step": 1080
    },
    {
      "epoch": 0.9954337899543378,
      "grad_norm": 4.582371711730957,
      "learning_rate": 3.3409436834094374e-05,
      "loss": 0.4833,
      "step": 1090
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8917313841936958,
      "eval_f1_hate": 0.4340175953079179,
      "eval_f1_idk/skip": 0.0,
      "eval_f1_macro": 0.343724839708743,
      "eval_f1_noHate": 0.9408817635270541,
      "eval_f1_relation": 0.0,
      "eval_loss": 0.38222378492355347,
      "eval_precision_global": 0.9080246386707637,
      "eval_precision_hate": 0.7326732673267327,
      "eval_precision_idk/skip": 1.0,
      "eval_precision_noHate": 0.8994252873563219,
      "eval_precision_relation": 1.0,
      "eval_recall_global": 0.32366946778711486,
      "eval_recall_hate": 0.30833333333333335,
      "eval_recall_idk/skip": 0.0,
      "eval_recall_noHate": 0.9863445378151261,
      "eval_recall_relation": 0.0,
      "eval_runtime": 13.3652,
      "eval_samples_per_second": 163.784,
      "eval_steps_per_second": 20.501,
      "step": 1095
    }
  ],
  "logging_steps": 10,
  "max_steps": 3285,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 236499430813104.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
