{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3285,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0091324200913242,
      "grad_norm": 4.2014241218566895,
      "learning_rate": 4.984779299847793e-05,
      "loss": 1.4092,
      "step": 10
    },
    {
      "epoch": 0.0182648401826484,
      "grad_norm": 4.149141788482666,
      "learning_rate": 4.969558599695586e-05,
      "loss": 1.3238,
      "step": 20
    },
    {
      "epoch": 0.0273972602739726,
      "grad_norm": 4.970274448394775,
      "learning_rate": 4.9543378995433794e-05,
      "loss": 1.2415,
      "step": 30
    },
    {
      "epoch": 0.0365296803652968,
      "grad_norm": 4.783473014831543,
      "learning_rate": 4.939117199391172e-05,
      "loss": 1.0942,
      "step": 40
    },
    {
      "epoch": 0.045662100456621,
      "grad_norm": 3.472869634628296,
      "learning_rate": 4.923896499238965e-05,
      "loss": 0.9998,
      "step": 50
    },
    {
      "epoch": 0.0547945205479452,
      "grad_norm": 3.1168034076690674,
      "learning_rate": 4.908675799086758e-05,
      "loss": 0.8516,
      "step": 60
    },
    {
      "epoch": 0.0639269406392694,
      "grad_norm": 2.4794931411743164,
      "learning_rate": 4.8934550989345515e-05,
      "loss": 0.7077,
      "step": 70
    },
    {
      "epoch": 0.0730593607305936,
      "grad_norm": 1.394862413406372,
      "learning_rate": 4.878234398782344e-05,
      "loss": 0.573,
      "step": 80
    },
    {
      "epoch": 0.0821917808219178,
      "grad_norm": 2.526909112930298,
      "learning_rate": 4.863013698630137e-05,
      "loss": 0.5617,
      "step": 90
    },
    {
      "epoch": 0.091324200913242,
      "grad_norm": 8.289301872253418,
      "learning_rate": 4.84779299847793e-05,
      "loss": 0.6276,
      "step": 100
    },
    {
      "epoch": 0.1004566210045662,
      "grad_norm": 3.2346248626708984,
      "learning_rate": 4.8325722983257235e-05,
      "loss": 0.6523,
      "step": 110
    },
    {
      "epoch": 0.1095890410958904,
      "grad_norm": 2.560805320739746,
      "learning_rate": 4.8173515981735164e-05,
      "loss": 0.3892,
      "step": 120
    },
    {
      "epoch": 0.1187214611872146,
      "grad_norm": 2.987116575241089,
      "learning_rate": 4.802130898021309e-05,
      "loss": 0.6495,
      "step": 130
    },
    {
      "epoch": 0.1278538812785388,
      "grad_norm": 3.038130044937134,
      "learning_rate": 4.786910197869102e-05,
      "loss": 0.6337,
      "step": 140
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 5.149893283843994,
      "learning_rate": 4.7716894977168955e-05,
      "loss": 0.4634,
      "step": 150
    },
    {
      "epoch": 0.1461187214611872,
      "grad_norm": 3.445244073867798,
      "learning_rate": 4.7564687975646884e-05,
      "loss": 0.4522,
      "step": 160
    },
    {
      "epoch": 0.1552511415525114,
      "grad_norm": 2.870965003967285,
      "learning_rate": 4.741248097412481e-05,
      "loss": 0.3908,
      "step": 170
    },
    {
      "epoch": 0.1643835616438356,
      "grad_norm": 7.03959321975708,
      "learning_rate": 4.726027397260274e-05,
      "loss": 0.3304,
      "step": 180
    },
    {
      "epoch": 0.1735159817351598,
      "grad_norm": 3.2255845069885254,
      "learning_rate": 4.710806697108067e-05,
      "loss": 0.4624,
      "step": 190
    },
    {
      "epoch": 0.182648401826484,
      "grad_norm": 2.61588454246521,
      "learning_rate": 4.6955859969558604e-05,
      "loss": 0.3659,
      "step": 200
    },
    {
      "epoch": 0.1917808219178082,
      "grad_norm": 4.456578254699707,
      "learning_rate": 4.680365296803653e-05,
      "loss": 0.3655,
      "step": 210
    },
    {
      "epoch": 0.2009132420091324,
      "grad_norm": 5.7224440574646,
      "learning_rate": 4.665144596651446e-05,
      "loss": 0.3439,
      "step": 220
    },
    {
      "epoch": 0.2100456621004566,
      "grad_norm": 1.495111346244812,
      "learning_rate": 4.649923896499239e-05,
      "loss": 0.3132,
      "step": 230
    },
    {
      "epoch": 0.2191780821917808,
      "grad_norm": 3.848186492919922,
      "learning_rate": 4.6347031963470325e-05,
      "loss": 0.412,
      "step": 240
    },
    {
      "epoch": 0.228310502283105,
      "grad_norm": 3.3290815353393555,
      "learning_rate": 4.619482496194825e-05,
      "loss": 0.3121,
      "step": 250
    },
    {
      "epoch": 0.2374429223744292,
      "grad_norm": 1.6857166290283203,
      "learning_rate": 4.604261796042618e-05,
      "loss": 0.2836,
      "step": 260
    },
    {
      "epoch": 0.2465753424657534,
      "grad_norm": 0.7483834624290466,
      "learning_rate": 4.589041095890411e-05,
      "loss": 0.4508,
      "step": 270
    },
    {
      "epoch": 0.2557077625570776,
      "grad_norm": 15.452421188354492,
      "learning_rate": 4.5738203957382045e-05,
      "loss": 0.4947,
      "step": 280
    },
    {
      "epoch": 0.2648401826484018,
      "grad_norm": 3.2019295692443848,
      "learning_rate": 4.5585996955859973e-05,
      "loss": 0.3868,
      "step": 290
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 2.713031530380249,
      "learning_rate": 4.54337899543379e-05,
      "loss": 0.3957,
      "step": 300
    },
    {
      "epoch": 0.2831050228310502,
      "grad_norm": 8.713400840759277,
      "learning_rate": 4.528158295281583e-05,
      "loss": 0.4065,
      "step": 310
    },
    {
      "epoch": 0.2922374429223744,
      "grad_norm": 0.9665752053260803,
      "learning_rate": 4.512937595129376e-05,
      "loss": 0.4704,
      "step": 320
    },
    {
      "epoch": 0.3013698630136986,
      "grad_norm": 1.5282211303710938,
      "learning_rate": 4.4977168949771694e-05,
      "loss": 0.6581,
      "step": 330
    },
    {
      "epoch": 0.3105022831050228,
      "grad_norm": 3.5258123874664307,
      "learning_rate": 4.482496194824962e-05,
      "loss": 0.2639,
      "step": 340
    },
    {
      "epoch": 0.319634703196347,
      "grad_norm": 3.5934760570526123,
      "learning_rate": 4.467275494672755e-05,
      "loss": 0.2484,
      "step": 350
    },
    {
      "epoch": 0.3287671232876712,
      "grad_norm": 2.534461259841919,
      "learning_rate": 4.452054794520548e-05,
      "loss": 0.5359,
      "step": 360
    },
    {
      "epoch": 0.3378995433789954,
      "grad_norm": 4.455851078033447,
      "learning_rate": 4.4368340943683414e-05,
      "loss": 0.4696,
      "step": 370
    },
    {
      "epoch": 0.3470319634703196,
      "grad_norm": 2.4211103916168213,
      "learning_rate": 4.421613394216134e-05,
      "loss": 0.4243,
      "step": 380
    },
    {
      "epoch": 0.3561643835616438,
      "grad_norm": 2.8859012126922607,
      "learning_rate": 4.406392694063927e-05,
      "loss": 0.471,
      "step": 390
    },
    {
      "epoch": 0.365296803652968,
      "grad_norm": 12.153661727905273,
      "learning_rate": 4.39117199391172e-05,
      "loss": 0.4516,
      "step": 400
    },
    {
      "epoch": 0.3744292237442922,
      "grad_norm": 7.018897533416748,
      "learning_rate": 4.3759512937595135e-05,
      "loss": 0.5746,
      "step": 410
    },
    {
      "epoch": 0.3835616438356164,
      "grad_norm": 2.0375819206237793,
      "learning_rate": 4.360730593607306e-05,
      "loss": 0.1443,
      "step": 420
    },
    {
      "epoch": 0.3926940639269406,
      "grad_norm": 3.4404189586639404,
      "learning_rate": 4.345509893455099e-05,
      "loss": 0.43,
      "step": 430
    },
    {
      "epoch": 0.4018264840182648,
      "grad_norm": 8.660131454467773,
      "learning_rate": 4.330289193302892e-05,
      "loss": 0.8187,
      "step": 440
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 2.531592845916748,
      "learning_rate": 4.3150684931506855e-05,
      "loss": 0.3468,
      "step": 450
    },
    {
      "epoch": 0.4200913242009132,
      "grad_norm": 2.712364435195923,
      "learning_rate": 4.299847792998478e-05,
      "loss": 0.3988,
      "step": 460
    },
    {
      "epoch": 0.4292237442922374,
      "grad_norm": 1.8181850910186768,
      "learning_rate": 4.284627092846271e-05,
      "loss": 0.449,
      "step": 470
    },
    {
      "epoch": 0.4383561643835616,
      "grad_norm": 1.7195039987564087,
      "learning_rate": 4.269406392694064e-05,
      "loss": 0.2727,
      "step": 480
    },
    {
      "epoch": 0.4474885844748858,
      "grad_norm": 2.748134136199951,
      "learning_rate": 4.254185692541857e-05,
      "loss": 0.3758,
      "step": 490
    },
    {
      "epoch": 0.45662100456621,
      "grad_norm": 5.917792320251465,
      "learning_rate": 4.2389649923896504e-05,
      "loss": 0.3246,
      "step": 500
    },
    {
      "epoch": 0.4657534246575342,
      "grad_norm": 2.3957009315490723,
      "learning_rate": 4.223744292237443e-05,
      "loss": 0.5519,
      "step": 510
    },
    {
      "epoch": 0.4748858447488584,
      "grad_norm": 2.0162081718444824,
      "learning_rate": 4.208523592085236e-05,
      "loss": 0.3067,
      "step": 520
    },
    {
      "epoch": 0.4840182648401826,
      "grad_norm": 7.277093887329102,
      "learning_rate": 4.193302891933029e-05,
      "loss": 0.2649,
      "step": 530
    },
    {
      "epoch": 0.4931506849315068,
      "grad_norm": 0.6217344999313354,
      "learning_rate": 4.1780821917808224e-05,
      "loss": 0.1976,
      "step": 540
    },
    {
      "epoch": 0.502283105022831,
      "grad_norm": 1.422882318496704,
      "learning_rate": 4.162861491628615e-05,
      "loss": 0.2015,
      "step": 550
    },
    {
      "epoch": 0.5114155251141552,
      "grad_norm": 1.3135812282562256,
      "learning_rate": 4.147640791476408e-05,
      "loss": 0.2779,
      "step": 560
    },
    {
      "epoch": 0.5205479452054794,
      "grad_norm": 14.464576721191406,
      "learning_rate": 4.132420091324201e-05,
      "loss": 0.4883,
      "step": 570
    },
    {
      "epoch": 0.5296803652968036,
      "grad_norm": 4.937425136566162,
      "learning_rate": 4.1171993911719944e-05,
      "loss": 0.5045,
      "step": 580
    },
    {
      "epoch": 0.5388127853881278,
      "grad_norm": 2.497363805770874,
      "learning_rate": 4.101978691019787e-05,
      "loss": 0.383,
      "step": 590
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 4.553092002868652,
      "learning_rate": 4.08675799086758e-05,
      "loss": 0.2887,
      "step": 600
    },
    {
      "epoch": 0.5570776255707762,
      "grad_norm": 3.252288579940796,
      "learning_rate": 4.071537290715373e-05,
      "loss": 0.2456,
      "step": 610
    },
    {
      "epoch": 0.5662100456621004,
      "grad_norm": 1.4945292472839355,
      "learning_rate": 4.0563165905631665e-05,
      "loss": 0.5277,
      "step": 620
    },
    {
      "epoch": 0.5753424657534246,
      "grad_norm": 5.297628402709961,
      "learning_rate": 4.041095890410959e-05,
      "loss": 0.589,
      "step": 630
    },
    {
      "epoch": 0.5844748858447488,
      "grad_norm": 5.739619731903076,
      "learning_rate": 4.025875190258752e-05,
      "loss": 0.3895,
      "step": 640
    },
    {
      "epoch": 0.593607305936073,
      "grad_norm": 3.931476354598999,
      "learning_rate": 4.010654490106545e-05,
      "loss": 0.4483,
      "step": 650
    },
    {
      "epoch": 0.6027397260273972,
      "grad_norm": 5.132766246795654,
      "learning_rate": 3.995433789954338e-05,
      "loss": 0.338,
      "step": 660
    },
    {
      "epoch": 0.6118721461187214,
      "grad_norm": 4.865106105804443,
      "learning_rate": 3.9802130898021314e-05,
      "loss": 0.3937,
      "step": 670
    },
    {
      "epoch": 0.6210045662100456,
      "grad_norm": 2.0048723220825195,
      "learning_rate": 3.964992389649924e-05,
      "loss": 0.3914,
      "step": 680
    },
    {
      "epoch": 0.6301369863013698,
      "grad_norm": 4.006353378295898,
      "learning_rate": 3.949771689497717e-05,
      "loss": 0.5664,
      "step": 690
    },
    {
      "epoch": 0.639269406392694,
      "grad_norm": 4.143347263336182,
      "learning_rate": 3.93455098934551e-05,
      "loss": 0.1821,
      "step": 700
    },
    {
      "epoch": 0.6484018264840182,
      "grad_norm": 0.5014417767524719,
      "learning_rate": 3.9193302891933034e-05,
      "loss": 0.2695,
      "step": 710
    },
    {
      "epoch": 0.6575342465753424,
      "grad_norm": 7.656728267669678,
      "learning_rate": 3.904109589041096e-05,
      "loss": 0.7182,
      "step": 720
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 3.5674729347229004,
      "learning_rate": 3.888888888888889e-05,
      "loss": 0.4191,
      "step": 730
    },
    {
      "epoch": 0.6757990867579908,
      "grad_norm": 3.0391266345977783,
      "learning_rate": 3.873668188736682e-05,
      "loss": 0.4172,
      "step": 740
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 1.8672640323638916,
      "learning_rate": 3.8584474885844754e-05,
      "loss": 0.3501,
      "step": 750
    },
    {
      "epoch": 0.6940639269406392,
      "grad_norm": 2.1507303714752197,
      "learning_rate": 3.843226788432268e-05,
      "loss": 0.2292,
      "step": 760
    },
    {
      "epoch": 0.7031963470319634,
      "grad_norm": 5.020025253295898,
      "learning_rate": 3.828006088280061e-05,
      "loss": 0.443,
      "step": 770
    },
    {
      "epoch": 0.7123287671232876,
      "grad_norm": 8.281349182128906,
      "learning_rate": 3.812785388127854e-05,
      "loss": 0.4447,
      "step": 780
    },
    {
      "epoch": 0.7214611872146118,
      "grad_norm": 2.182190418243408,
      "learning_rate": 3.797564687975647e-05,
      "loss": 0.2514,
      "step": 790
    },
    {
      "epoch": 0.730593607305936,
      "grad_norm": 13.718620300292969,
      "learning_rate": 3.78234398782344e-05,
      "loss": 0.5834,
      "step": 800
    },
    {
      "epoch": 0.7397260273972602,
      "grad_norm": 2.225736618041992,
      "learning_rate": 3.767123287671233e-05,
      "loss": 0.4969,
      "step": 810
    },
    {
      "epoch": 0.7488584474885844,
      "grad_norm": 7.196774959564209,
      "learning_rate": 3.751902587519026e-05,
      "loss": 0.4007,
      "step": 820
    },
    {
      "epoch": 0.7579908675799086,
      "grad_norm": 6.489071846008301,
      "learning_rate": 3.736681887366819e-05,
      "loss": 0.3576,
      "step": 830
    },
    {
      "epoch": 0.7671232876712328,
      "grad_norm": 5.737024784088135,
      "learning_rate": 3.7214611872146123e-05,
      "loss": 0.4381,
      "step": 840
    },
    {
      "epoch": 0.776255707762557,
      "grad_norm": 4.6021504402160645,
      "learning_rate": 3.706240487062405e-05,
      "loss": 0.2426,
      "step": 850
    },
    {
      "epoch": 0.7853881278538812,
      "grad_norm": 2.569338321685791,
      "learning_rate": 3.691019786910198e-05,
      "loss": 0.3163,
      "step": 860
    },
    {
      "epoch": 0.7945205479452054,
      "grad_norm": 0.5997539758682251,
      "learning_rate": 3.675799086757991e-05,
      "loss": 0.2816,
      "step": 870
    },
    {
      "epoch": 0.8036529680365296,
      "grad_norm": 1.7195467948913574,
      "learning_rate": 3.6605783866057844e-05,
      "loss": 0.4755,
      "step": 880
    },
    {
      "epoch": 0.8127853881278538,
      "grad_norm": 0.6913968324661255,
      "learning_rate": 3.645357686453577e-05,
      "loss": 0.1824,
      "step": 890
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 0.5214787721633911,
      "learning_rate": 3.63013698630137e-05,
      "loss": 0.7159,
      "step": 900
    },
    {
      "epoch": 0.8310502283105022,
      "grad_norm": 3.387037754058838,
      "learning_rate": 3.614916286149163e-05,
      "loss": 0.6324,
      "step": 910
    },
    {
      "epoch": 0.8401826484018264,
      "grad_norm": 0.6512325406074524,
      "learning_rate": 3.5996955859969564e-05,
      "loss": 0.385,
      "step": 920
    },
    {
      "epoch": 0.8493150684931506,
      "grad_norm": 3.3542628288269043,
      "learning_rate": 3.584474885844749e-05,
      "loss": 0.288,
      "step": 930
    },
    {
      "epoch": 0.8584474885844748,
      "grad_norm": 5.149443626403809,
      "learning_rate": 3.569254185692542e-05,
      "loss": 0.5914,
      "step": 940
    },
    {
      "epoch": 0.867579908675799,
      "grad_norm": 10.313617706298828,
      "learning_rate": 3.554033485540335e-05,
      "loss": 0.3933,
      "step": 950
    },
    {
      "epoch": 0.8767123287671232,
      "grad_norm": 2.1160686016082764,
      "learning_rate": 3.538812785388128e-05,
      "loss": 0.4591,
      "step": 960
    },
    {
      "epoch": 0.8858447488584474,
      "grad_norm": 4.683087348937988,
      "learning_rate": 3.523592085235921e-05,
      "loss": 0.4178,
      "step": 970
    },
    {
      "epoch": 0.8949771689497716,
      "grad_norm": 3.4736366271972656,
      "learning_rate": 3.508371385083714e-05,
      "loss": 0.3218,
      "step": 980
    },
    {
      "epoch": 0.9041095890410958,
      "grad_norm": 4.114583969116211,
      "learning_rate": 3.493150684931507e-05,
      "loss": 0.5066,
      "step": 990
    },
    {
      "epoch": 0.91324200913242,
      "grad_norm": 4.443102836608887,
      "learning_rate": 3.4779299847793e-05,
      "loss": 0.3163,
      "step": 1000
    },
    {
      "epoch": 0.9223744292237442,
      "grad_norm": 3.668541669845581,
      "learning_rate": 3.462709284627093e-05,
      "loss": 0.37,
      "step": 1010
    },
    {
      "epoch": 0.9315068493150684,
      "grad_norm": 3.1004445552825928,
      "learning_rate": 3.447488584474886e-05,
      "loss": 0.3316,
      "step": 1020
    },
    {
      "epoch": 0.9406392694063926,
      "grad_norm": 1.8595644235610962,
      "learning_rate": 3.432267884322679e-05,
      "loss": 0.3757,
      "step": 1030
    },
    {
      "epoch": 0.9497716894977168,
      "grad_norm": 0.7095105648040771,
      "learning_rate": 3.417047184170472e-05,
      "loss": 0.1853,
      "step": 1040
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 3.82582688331604,
      "learning_rate": 3.4018264840182654e-05,
      "loss": 0.2836,
      "step": 1050
    },
    {
      "epoch": 0.9680365296803652,
      "grad_norm": 6.456411361694336,
      "learning_rate": 3.386605783866058e-05,
      "loss": 0.4701,
      "step": 1060
    },
    {
      "epoch": 0.9771689497716894,
      "grad_norm": 1.0287468433380127,
      "learning_rate": 3.371385083713851e-05,
      "loss": 0.3622,
      "step": 1070
    },
    {
      "epoch": 0.9863013698630136,
      "grad_norm": 10.451523780822754,
      "learning_rate": 3.356164383561644e-05,
      "loss": 0.3881,
      "step": 1080
    },
    {
      "epoch": 0.9954337899543378,
      "grad_norm": 4.582371711730957,
      "learning_rate": 3.3409436834094374e-05,
      "loss": 0.4833,
      "step": 1090
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8917313841936958,
      "eval_f1_hate": 0.4340175953079179,
      "eval_f1_idk/skip": 0.0,
      "eval_f1_macro": 0.343724839708743,
      "eval_f1_noHate": 0.9408817635270541,
      "eval_f1_relation": 0.0,
      "eval_loss": 0.38222378492355347,
      "eval_precision_global": 0.9080246386707637,
      "eval_precision_hate": 0.7326732673267327,
      "eval_precision_idk/skip": 1.0,
      "eval_precision_noHate": 0.8994252873563219,
      "eval_precision_relation": 1.0,
      "eval_recall_global": 0.32366946778711486,
      "eval_recall_hate": 0.30833333333333335,
      "eval_recall_idk/skip": 0.0,
      "eval_recall_noHate": 0.9863445378151261,
      "eval_recall_relation": 0.0,
      "eval_runtime": 13.3652,
      "eval_samples_per_second": 163.784,
      "eval_steps_per_second": 20.501,
      "step": 1095
    },
    {
      "epoch": 1.004566210045662,
      "grad_norm": 3.0400333404541016,
      "learning_rate": 3.32572298325723e-05,
      "loss": 0.222,
      "step": 1100
    },
    {
      "epoch": 1.0136986301369864,
      "grad_norm": 0.3845624327659607,
      "learning_rate": 3.310502283105023e-05,
      "loss": 0.3422,
      "step": 1110
    },
    {
      "epoch": 1.0228310502283104,
      "grad_norm": 4.231970310211182,
      "learning_rate": 3.295281582952816e-05,
      "loss": 0.3363,
      "step": 1120
    },
    {
      "epoch": 1.0319634703196348,
      "grad_norm": 4.221740245819092,
      "learning_rate": 3.280060882800609e-05,
      "loss": 0.2572,
      "step": 1130
    },
    {
      "epoch": 1.0410958904109588,
      "grad_norm": 11.211963653564453,
      "learning_rate": 3.264840182648402e-05,
      "loss": 0.5303,
      "step": 1140
    },
    {
      "epoch": 1.0502283105022832,
      "grad_norm": 7.743516445159912,
      "learning_rate": 3.249619482496195e-05,
      "loss": 0.4717,
      "step": 1150
    },
    {
      "epoch": 1.0593607305936072,
      "grad_norm": 3.173330307006836,
      "learning_rate": 3.234398782343988e-05,
      "loss": 0.5244,
      "step": 1160
    },
    {
      "epoch": 1.0684931506849316,
      "grad_norm": 3.236485242843628,
      "learning_rate": 3.219178082191781e-05,
      "loss": 0.1922,
      "step": 1170
    },
    {
      "epoch": 1.0776255707762556,
      "grad_norm": 2.8704609870910645,
      "learning_rate": 3.203957382039574e-05,
      "loss": 0.4234,
      "step": 1180
    },
    {
      "epoch": 1.08675799086758,
      "grad_norm": 5.497557163238525,
      "learning_rate": 3.188736681887367e-05,
      "loss": 0.3451,
      "step": 1190
    },
    {
      "epoch": 1.095890410958904,
      "grad_norm": 0.5379366278648376,
      "learning_rate": 3.17351598173516e-05,
      "loss": 0.1344,
      "step": 1200
    },
    {
      "epoch": 1.1050228310502284,
      "grad_norm": 17.18410301208496,
      "learning_rate": 3.158295281582953e-05,
      "loss": 0.2706,
      "step": 1210
    },
    {
      "epoch": 1.1141552511415524,
      "grad_norm": 3.8860371112823486,
      "learning_rate": 3.1430745814307464e-05,
      "loss": 0.545,
      "step": 1220
    },
    {
      "epoch": 1.1232876712328768,
      "grad_norm": 3.093757152557373,
      "learning_rate": 3.127853881278539e-05,
      "loss": 0.3125,
      "step": 1230
    },
    {
      "epoch": 1.1324200913242009,
      "grad_norm": 3.3328843116760254,
      "learning_rate": 3.112633181126332e-05,
      "loss": 0.3047,
      "step": 1240
    },
    {
      "epoch": 1.1415525114155252,
      "grad_norm": 0.5910763144493103,
      "learning_rate": 3.097412480974125e-05,
      "loss": 0.5011,
      "step": 1250
    },
    {
      "epoch": 1.1506849315068493,
      "grad_norm": 0.6705597639083862,
      "learning_rate": 3.082191780821918e-05,
      "loss": 0.5788,
      "step": 1260
    },
    {
      "epoch": 1.1598173515981736,
      "grad_norm": 6.111064910888672,
      "learning_rate": 3.066971080669711e-05,
      "loss": 0.2385,
      "step": 1270
    },
    {
      "epoch": 1.1689497716894977,
      "grad_norm": 10.224773406982422,
      "learning_rate": 3.051750380517504e-05,
      "loss": 0.2632,
      "step": 1280
    },
    {
      "epoch": 1.178082191780822,
      "grad_norm": 1.3307323455810547,
      "learning_rate": 3.036529680365297e-05,
      "loss": 0.4386,
      "step": 1290
    },
    {
      "epoch": 1.187214611872146,
      "grad_norm": 4.205552577972412,
      "learning_rate": 3.02130898021309e-05,
      "loss": 0.166,
      "step": 1300
    },
    {
      "epoch": 1.1963470319634704,
      "grad_norm": 4.061017036437988,
      "learning_rate": 3.006088280060883e-05,
      "loss": 0.5329,
      "step": 1310
    },
    {
      "epoch": 1.2054794520547945,
      "grad_norm": 9.687250137329102,
      "learning_rate": 2.990867579908676e-05,
      "loss": 0.3321,
      "step": 1320
    },
    {
      "epoch": 1.2146118721461188,
      "grad_norm": 2.892995834350586,
      "learning_rate": 2.975646879756469e-05,
      "loss": 0.3298,
      "step": 1330
    },
    {
      "epoch": 1.2237442922374429,
      "grad_norm": 7.165471076965332,
      "learning_rate": 2.960426179604262e-05,
      "loss": 0.3651,
      "step": 1340
    },
    {
      "epoch": 1.2328767123287672,
      "grad_norm": 0.5170530676841736,
      "learning_rate": 2.945205479452055e-05,
      "loss": 0.4577,
      "step": 1350
    },
    {
      "epoch": 1.2420091324200913,
      "grad_norm": 3.8310301303863525,
      "learning_rate": 2.929984779299848e-05,
      "loss": 0.5097,
      "step": 1360
    },
    {
      "epoch": 1.2511415525114156,
      "grad_norm": 8.320622444152832,
      "learning_rate": 2.914764079147641e-05,
      "loss": 0.5556,
      "step": 1370
    },
    {
      "epoch": 1.2602739726027397,
      "grad_norm": 1.4610373973846436,
      "learning_rate": 2.8995433789954342e-05,
      "loss": 0.2856,
      "step": 1380
    },
    {
      "epoch": 1.269406392694064,
      "grad_norm": 3.7877790927886963,
      "learning_rate": 2.884322678843227e-05,
      "loss": 0.2191,
      "step": 1390
    },
    {
      "epoch": 1.278538812785388,
      "grad_norm": 3.9603960514068604,
      "learning_rate": 2.8691019786910202e-05,
      "loss": 0.396,
      "step": 1400
    },
    {
      "epoch": 1.2876712328767124,
      "grad_norm": 4.958068370819092,
      "learning_rate": 2.853881278538813e-05,
      "loss": 0.454,
      "step": 1410
    },
    {
      "epoch": 1.2968036529680365,
      "grad_norm": 11.898917198181152,
      "learning_rate": 2.838660578386606e-05,
      "loss": 0.5186,
      "step": 1420
    },
    {
      "epoch": 1.3059360730593608,
      "grad_norm": 0.6699023246765137,
      "learning_rate": 2.823439878234399e-05,
      "loss": 0.2383,
      "step": 1430
    },
    {
      "epoch": 1.3150684931506849,
      "grad_norm": 13.283489227294922,
      "learning_rate": 2.808219178082192e-05,
      "loss": 0.4086,
      "step": 1440
    },
    {
      "epoch": 1.3242009132420092,
      "grad_norm": 0.6436524987220764,
      "learning_rate": 2.792998477929985e-05,
      "loss": 0.3357,
      "step": 1450
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 15.899924278259277,
      "learning_rate": 2.777777777777778e-05,
      "loss": 0.2311,
      "step": 1460
    },
    {
      "epoch": 1.3424657534246576,
      "grad_norm": 4.042705059051514,
      "learning_rate": 2.762557077625571e-05,
      "loss": 0.23,
      "step": 1470
    },
    {
      "epoch": 1.3515981735159817,
      "grad_norm": 3.901237964630127,
      "learning_rate": 2.747336377473364e-05,
      "loss": 0.38,
      "step": 1480
    },
    {
      "epoch": 1.360730593607306,
      "grad_norm": 9.368206024169922,
      "learning_rate": 2.732115677321157e-05,
      "loss": 0.5356,
      "step": 1490
    },
    {
      "epoch": 1.36986301369863,
      "grad_norm": 11.471354484558105,
      "learning_rate": 2.71689497716895e-05,
      "loss": 0.5198,
      "step": 1500
    },
    {
      "epoch": 1.3789954337899544,
      "grad_norm": 1.8336173295974731,
      "learning_rate": 2.701674277016743e-05,
      "loss": 0.3458,
      "step": 1510
    },
    {
      "epoch": 1.3881278538812785,
      "grad_norm": 1.56838059425354,
      "learning_rate": 2.686453576864536e-05,
      "loss": 0.4461,
      "step": 1520
    },
    {
      "epoch": 1.3972602739726028,
      "grad_norm": 6.287276744842529,
      "learning_rate": 2.671232876712329e-05,
      "loss": 0.5814,
      "step": 1530
    },
    {
      "epoch": 1.4063926940639269,
      "grad_norm": 5.450047492980957,
      "learning_rate": 2.656012176560122e-05,
      "loss": 0.3737,
      "step": 1540
    },
    {
      "epoch": 1.4155251141552512,
      "grad_norm": 1.9170119762420654,
      "learning_rate": 2.640791476407915e-05,
      "loss": 0.3313,
      "step": 1550
    },
    {
      "epoch": 1.4246575342465753,
      "grad_norm": 0.8975473642349243,
      "learning_rate": 2.625570776255708e-05,
      "loss": 0.3079,
      "step": 1560
    },
    {
      "epoch": 1.4337899543378996,
      "grad_norm": 6.543036937713623,
      "learning_rate": 2.6103500761035012e-05,
      "loss": 0.3114,
      "step": 1570
    },
    {
      "epoch": 1.4429223744292237,
      "grad_norm": 0.9636650085449219,
      "learning_rate": 2.595129375951294e-05,
      "loss": 0.3503,
      "step": 1580
    },
    {
      "epoch": 1.452054794520548,
      "grad_norm": 7.719810962677002,
      "learning_rate": 2.579908675799087e-05,
      "loss": 0.3979,
      "step": 1590
    },
    {
      "epoch": 1.461187214611872,
      "grad_norm": 7.913337230682373,
      "learning_rate": 2.56468797564688e-05,
      "loss": 0.3471,
      "step": 1600
    },
    {
      "epoch": 1.4703196347031964,
      "grad_norm": 1.5591356754302979,
      "learning_rate": 2.549467275494673e-05,
      "loss": 0.1851,
      "step": 1610
    },
    {
      "epoch": 1.4794520547945205,
      "grad_norm": 5.20556640625,
      "learning_rate": 2.534246575342466e-05,
      "loss": 0.3932,
      "step": 1620
    },
    {
      "epoch": 1.4885844748858448,
      "grad_norm": 0.6447371244430542,
      "learning_rate": 2.519025875190259e-05,
      "loss": 0.286,
      "step": 1630
    },
    {
      "epoch": 1.4977168949771689,
      "grad_norm": 4.751617431640625,
      "learning_rate": 2.503805175038052e-05,
      "loss": 0.3789,
      "step": 1640
    },
    {
      "epoch": 1.5068493150684932,
      "grad_norm": 4.476944446563721,
      "learning_rate": 2.4885844748858446e-05,
      "loss": 0.6007,
      "step": 1650
    },
    {
      "epoch": 1.5159817351598175,
      "grad_norm": 6.40022087097168,
      "learning_rate": 2.4733637747336378e-05,
      "loss": 0.3689,
      "step": 1660
    },
    {
      "epoch": 1.5251141552511416,
      "grad_norm": 3.4018747806549072,
      "learning_rate": 2.4581430745814306e-05,
      "loss": 0.3356,
      "step": 1670
    },
    {
      "epoch": 1.5342465753424657,
      "grad_norm": 0.4293583631515503,
      "learning_rate": 2.4429223744292238e-05,
      "loss": 0.4435,
      "step": 1680
    },
    {
      "epoch": 1.54337899543379,
      "grad_norm": 1.73989737033844,
      "learning_rate": 2.4277016742770166e-05,
      "loss": 0.3521,
      "step": 1690
    },
    {
      "epoch": 1.5525114155251143,
      "grad_norm": 5.923254013061523,
      "learning_rate": 2.4124809741248098e-05,
      "loss": 0.3069,
      "step": 1700
    },
    {
      "epoch": 1.5616438356164384,
      "grad_norm": 3.6125378608703613,
      "learning_rate": 2.3972602739726026e-05,
      "loss": 0.5473,
      "step": 1710
    },
    {
      "epoch": 1.5707762557077625,
      "grad_norm": 8.213759422302246,
      "learning_rate": 2.3820395738203958e-05,
      "loss": 0.3414,
      "step": 1720
    },
    {
      "epoch": 1.5799086757990868,
      "grad_norm": 4.682781219482422,
      "learning_rate": 2.3668188736681887e-05,
      "loss": 0.2706,
      "step": 1730
    },
    {
      "epoch": 1.589041095890411,
      "grad_norm": 5.255059719085693,
      "learning_rate": 2.351598173515982e-05,
      "loss": 0.4048,
      "step": 1740
    },
    {
      "epoch": 1.5981735159817352,
      "grad_norm": 3.975907802581787,
      "learning_rate": 2.3363774733637747e-05,
      "loss": 0.442,
      "step": 1750
    },
    {
      "epoch": 1.6073059360730593,
      "grad_norm": 14.055427551269531,
      "learning_rate": 2.321156773211568e-05,
      "loss": 0.4312,
      "step": 1760
    },
    {
      "epoch": 1.6164383561643836,
      "grad_norm": 5.384091377258301,
      "learning_rate": 2.3059360730593607e-05,
      "loss": 0.4199,
      "step": 1770
    },
    {
      "epoch": 1.625570776255708,
      "grad_norm": 3.971848964691162,
      "learning_rate": 2.290715372907154e-05,
      "loss": 0.2119,
      "step": 1780
    },
    {
      "epoch": 1.634703196347032,
      "grad_norm": 6.963213920593262,
      "learning_rate": 2.2754946727549467e-05,
      "loss": 0.4687,
      "step": 1790
    },
    {
      "epoch": 1.643835616438356,
      "grad_norm": 9.165295600891113,
      "learning_rate": 2.2602739726027396e-05,
      "loss": 0.3066,
      "step": 1800
    },
    {
      "epoch": 1.6529680365296804,
      "grad_norm": 6.147500514984131,
      "learning_rate": 2.2450532724505327e-05,
      "loss": 0.4627,
      "step": 1810
    },
    {
      "epoch": 1.6621004566210047,
      "grad_norm": 6.483113765716553,
      "learning_rate": 2.2298325722983256e-05,
      "loss": 0.2675,
      "step": 1820
    },
    {
      "epoch": 1.6712328767123288,
      "grad_norm": 0.41939520835876465,
      "learning_rate": 2.2146118721461187e-05,
      "loss": 0.2717,
      "step": 1830
    },
    {
      "epoch": 1.6803652968036529,
      "grad_norm": 21.9180965423584,
      "learning_rate": 2.1993911719939116e-05,
      "loss": 0.4234,
      "step": 1840
    },
    {
      "epoch": 1.6894977168949772,
      "grad_norm": 9.225008964538574,
      "learning_rate": 2.1841704718417048e-05,
      "loss": 0.2094,
      "step": 1850
    },
    {
      "epoch": 1.6986301369863015,
      "grad_norm": 0.3402805030345917,
      "learning_rate": 2.1689497716894976e-05,
      "loss": 0.3454,
      "step": 1860
    },
    {
      "epoch": 1.7077625570776256,
      "grad_norm": 10.377140045166016,
      "learning_rate": 2.1537290715372908e-05,
      "loss": 0.2238,
      "step": 1870
    },
    {
      "epoch": 1.7168949771689497,
      "grad_norm": 9.097557067871094,
      "learning_rate": 2.1385083713850836e-05,
      "loss": 0.3308,
      "step": 1880
    },
    {
      "epoch": 1.726027397260274,
      "grad_norm": 1.4065568447113037,
      "learning_rate": 2.1232876712328768e-05,
      "loss": 0.2213,
      "step": 1890
    },
    {
      "epoch": 1.7351598173515983,
      "grad_norm": 5.801243305206299,
      "learning_rate": 2.1080669710806696e-05,
      "loss": 0.3694,
      "step": 1900
    },
    {
      "epoch": 1.7442922374429224,
      "grad_norm": 4.0137104988098145,
      "learning_rate": 2.0928462709284628e-05,
      "loss": 0.3053,
      "step": 1910
    },
    {
      "epoch": 1.7534246575342465,
      "grad_norm": 0.32009443640708923,
      "learning_rate": 2.0776255707762557e-05,
      "loss": 0.3622,
      "step": 1920
    },
    {
      "epoch": 1.7625570776255708,
      "grad_norm": 10.47020149230957,
      "learning_rate": 2.062404870624049e-05,
      "loss": 0.2743,
      "step": 1930
    },
    {
      "epoch": 1.771689497716895,
      "grad_norm": 2.0995447635650635,
      "learning_rate": 2.0471841704718417e-05,
      "loss": 0.212,
      "step": 1940
    },
    {
      "epoch": 1.7808219178082192,
      "grad_norm": 7.796110153198242,
      "learning_rate": 2.0319634703196345e-05,
      "loss": 0.3333,
      "step": 1950
    },
    {
      "epoch": 1.7899543378995433,
      "grad_norm": 8.493135452270508,
      "learning_rate": 2.0167427701674277e-05,
      "loss": 0.3254,
      "step": 1960
    },
    {
      "epoch": 1.7990867579908676,
      "grad_norm": 4.386829853057861,
      "learning_rate": 2.0015220700152205e-05,
      "loss": 0.2394,
      "step": 1970
    },
    {
      "epoch": 1.808219178082192,
      "grad_norm": 5.166952610015869,
      "learning_rate": 1.9863013698630137e-05,
      "loss": 0.401,
      "step": 1980
    },
    {
      "epoch": 1.817351598173516,
      "grad_norm": 10.313128471374512,
      "learning_rate": 1.9710806697108066e-05,
      "loss": 0.3607,
      "step": 1990
    },
    {
      "epoch": 1.82648401826484,
      "grad_norm": 0.5529770851135254,
      "learning_rate": 1.9558599695585997e-05,
      "loss": 0.6125,
      "step": 2000
    },
    {
      "epoch": 1.8356164383561644,
      "grad_norm": 0.3939063847064972,
      "learning_rate": 1.9406392694063926e-05,
      "loss": 0.2895,
      "step": 2010
    },
    {
      "epoch": 1.8447488584474887,
      "grad_norm": 5.846743106842041,
      "learning_rate": 1.9254185692541858e-05,
      "loss": 0.4426,
      "step": 2020
    },
    {
      "epoch": 1.8538812785388128,
      "grad_norm": 1.998340129852295,
      "learning_rate": 1.9101978691019786e-05,
      "loss": 0.2993,
      "step": 2030
    },
    {
      "epoch": 1.8630136986301369,
      "grad_norm": 9.049518585205078,
      "learning_rate": 1.8949771689497718e-05,
      "loss": 0.3049,
      "step": 2040
    },
    {
      "epoch": 1.8721461187214612,
      "grad_norm": 0.5510962605476379,
      "learning_rate": 1.8797564687975646e-05,
      "loss": 0.2789,
      "step": 2050
    },
    {
      "epoch": 1.8812785388127855,
      "grad_norm": 1.2402961254119873,
      "learning_rate": 1.8645357686453578e-05,
      "loss": 0.4075,
      "step": 2060
    },
    {
      "epoch": 1.8904109589041096,
      "grad_norm": 0.7977315783500671,
      "learning_rate": 1.8493150684931506e-05,
      "loss": 0.3516,
      "step": 2070
    },
    {
      "epoch": 1.8995433789954337,
      "grad_norm": 6.818615436553955,
      "learning_rate": 1.8340943683409438e-05,
      "loss": 0.3414,
      "step": 2080
    },
    {
      "epoch": 1.908675799086758,
      "grad_norm": 1.3460421562194824,
      "learning_rate": 1.8188736681887367e-05,
      "loss": 0.2216,
      "step": 2090
    },
    {
      "epoch": 1.9178082191780823,
      "grad_norm": 12.860871315002441,
      "learning_rate": 1.80365296803653e-05,
      "loss": 0.4873,
      "step": 2100
    },
    {
      "epoch": 1.9269406392694064,
      "grad_norm": 9.842572212219238,
      "learning_rate": 1.7884322678843227e-05,
      "loss": 0.3907,
      "step": 2110
    },
    {
      "epoch": 1.9360730593607305,
      "grad_norm": 10.042411804199219,
      "learning_rate": 1.7732115677321155e-05,
      "loss": 0.2042,
      "step": 2120
    },
    {
      "epoch": 1.9452054794520548,
      "grad_norm": 11.531584739685059,
      "learning_rate": 1.7579908675799087e-05,
      "loss": 0.3353,
      "step": 2130
    },
    {
      "epoch": 1.954337899543379,
      "grad_norm": 0.45052847266197205,
      "learning_rate": 1.7427701674277015e-05,
      "loss": 0.2063,
      "step": 2140
    },
    {
      "epoch": 1.9634703196347032,
      "grad_norm": 7.354028701782227,
      "learning_rate": 1.7275494672754947e-05,
      "loss": 0.5363,
      "step": 2150
    },
    {
      "epoch": 1.9726027397260273,
      "grad_norm": 0.5128328204154968,
      "learning_rate": 1.7123287671232875e-05,
      "loss": 0.3625,
      "step": 2160
    },
    {
      "epoch": 1.9817351598173516,
      "grad_norm": 0.5243252515792847,
      "learning_rate": 1.6971080669710807e-05,
      "loss": 0.3739,
      "step": 2170
    },
    {
      "epoch": 1.990867579908676,
      "grad_norm": 0.6587308049201965,
      "learning_rate": 1.6818873668188736e-05,
      "loss": 0.4656,
      "step": 2180
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.5550092458724976,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.2053,
      "step": 2190
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8985838282320694,
      "eval_f1_hate": 0.5274151436031331,
      "eval_f1_idk/skip": 0.0,
      "eval_f1_macro": 0.36805631754635293,
      "eval_f1_noHate": 0.9448101265822785,
      "eval_f1_relation": 0.0,
      "eval_loss": 0.38912519812583923,
      "eval_precision_global": 0.9045792916760659,
      "eval_precision_hate": 0.7062937062937062,
      "eval_precision_idk/skip": 1.0,
      "eval_precision_noHate": 0.9120234604105572,
      "eval_precision_relation": 1.0,
      "eval_recall_global": 0.350218837535014,
      "eval_recall_hate": 0.42083333333333334,
      "eval_recall_idk/skip": 0.0,
      "eval_recall_noHate": 0.9800420168067226,
      "eval_recall_relation": 0.0,
      "eval_runtime": 13.1181,
      "eval_samples_per_second": 166.869,
      "eval_steps_per_second": 20.887,
      "step": 2190
    },
    {
      "epoch": 2.009132420091324,
      "grad_norm": 0.31768301129341125,
      "learning_rate": 1.6514459665144596e-05,
      "loss": 0.2886,
      "step": 2200
    },
    {
      "epoch": 2.018264840182648,
      "grad_norm": 0.3345673382282257,
      "learning_rate": 1.6362252663622528e-05,
      "loss": 0.3739,
      "step": 2210
    },
    {
      "epoch": 2.0273972602739727,
      "grad_norm": 4.279358863830566,
      "learning_rate": 1.6210045662100456e-05,
      "loss": 0.3657,
      "step": 2220
    },
    {
      "epoch": 2.036529680365297,
      "grad_norm": 21.67007064819336,
      "learning_rate": 1.6057838660578388e-05,
      "loss": 0.3597,
      "step": 2230
    },
    {
      "epoch": 2.045662100456621,
      "grad_norm": 0.7450743913650513,
      "learning_rate": 1.5905631659056316e-05,
      "loss": 0.3523,
      "step": 2240
    },
    {
      "epoch": 2.0547945205479454,
      "grad_norm": 18.894332885742188,
      "learning_rate": 1.5753424657534248e-05,
      "loss": 0.2187,
      "step": 2250
    },
    {
      "epoch": 2.0639269406392695,
      "grad_norm": 7.687872886657715,
      "learning_rate": 1.5601217656012176e-05,
      "loss": 0.1575,
      "step": 2260
    },
    {
      "epoch": 2.0730593607305936,
      "grad_norm": 1.0911600589752197,
      "learning_rate": 1.5449010654490105e-05,
      "loss": 0.2571,
      "step": 2270
    },
    {
      "epoch": 2.0821917808219177,
      "grad_norm": 3.825803279876709,
      "learning_rate": 1.5296803652968037e-05,
      "loss": 0.485,
      "step": 2280
    },
    {
      "epoch": 2.091324200913242,
      "grad_norm": 0.4518949091434479,
      "learning_rate": 1.5144596651445967e-05,
      "loss": 0.5392,
      "step": 2290
    },
    {
      "epoch": 2.1004566210045663,
      "grad_norm": 0.7554592490196228,
      "learning_rate": 1.4992389649923897e-05,
      "loss": 0.5009,
      "step": 2300
    },
    {
      "epoch": 2.1095890410958904,
      "grad_norm": 5.287062168121338,
      "learning_rate": 1.4840182648401827e-05,
      "loss": 0.5763,
      "step": 2310
    },
    {
      "epoch": 2.1187214611872145,
      "grad_norm": 1.3488969802856445,
      "learning_rate": 1.4687975646879757e-05,
      "loss": 0.3251,
      "step": 2320
    },
    {
      "epoch": 2.127853881278539,
      "grad_norm": 11.767745971679688,
      "learning_rate": 1.4535768645357687e-05,
      "loss": 0.3315,
      "step": 2330
    },
    {
      "epoch": 2.136986301369863,
      "grad_norm": 5.733077526092529,
      "learning_rate": 1.4383561643835617e-05,
      "loss": 0.2454,
      "step": 2340
    },
    {
      "epoch": 2.146118721461187,
      "grad_norm": 4.701776027679443,
      "learning_rate": 1.4231354642313546e-05,
      "loss": 0.248,
      "step": 2350
    },
    {
      "epoch": 2.1552511415525113,
      "grad_norm": 4.751380920410156,
      "learning_rate": 1.4079147640791476e-05,
      "loss": 0.3524,
      "step": 2360
    },
    {
      "epoch": 2.1643835616438354,
      "grad_norm": 0.3431362211704254,
      "learning_rate": 1.3926940639269406e-05,
      "loss": 0.3792,
      "step": 2370
    },
    {
      "epoch": 2.17351598173516,
      "grad_norm": 7.046202182769775,
      "learning_rate": 1.3774733637747336e-05,
      "loss": 0.32,
      "step": 2380
    },
    {
      "epoch": 2.182648401826484,
      "grad_norm": 0.4504570960998535,
      "learning_rate": 1.3622526636225266e-05,
      "loss": 0.3236,
      "step": 2390
    },
    {
      "epoch": 2.191780821917808,
      "grad_norm": 5.490010738372803,
      "learning_rate": 1.3470319634703196e-05,
      "loss": 0.4877,
      "step": 2400
    },
    {
      "epoch": 2.2009132420091326,
      "grad_norm": 5.010889053344727,
      "learning_rate": 1.3318112633181126e-05,
      "loss": 0.523,
      "step": 2410
    },
    {
      "epoch": 2.2100456621004567,
      "grad_norm": 0.4352956712245941,
      "learning_rate": 1.3165905631659056e-05,
      "loss": 0.3846,
      "step": 2420
    },
    {
      "epoch": 2.219178082191781,
      "grad_norm": 3.773210048675537,
      "learning_rate": 1.3013698630136986e-05,
      "loss": 0.3261,
      "step": 2430
    },
    {
      "epoch": 2.228310502283105,
      "grad_norm": 4.2814764976501465,
      "learning_rate": 1.2861491628614916e-05,
      "loss": 0.3745,
      "step": 2440
    },
    {
      "epoch": 2.237442922374429,
      "grad_norm": 7.716864585876465,
      "learning_rate": 1.2709284627092847e-05,
      "loss": 0.4479,
      "step": 2450
    },
    {
      "epoch": 2.2465753424657535,
      "grad_norm": 1.7223451137542725,
      "learning_rate": 1.2557077625570777e-05,
      "loss": 0.2803,
      "step": 2460
    },
    {
      "epoch": 2.2557077625570776,
      "grad_norm": 3.597302198410034,
      "learning_rate": 1.2404870624048707e-05,
      "loss": 0.3249,
      "step": 2470
    },
    {
      "epoch": 2.2648401826484017,
      "grad_norm": 3.740170478820801,
      "learning_rate": 1.2252663622526637e-05,
      "loss": 0.4243,
      "step": 2480
    },
    {
      "epoch": 2.2739726027397262,
      "grad_norm": 12.198412895202637,
      "learning_rate": 1.2100456621004567e-05,
      "loss": 0.4259,
      "step": 2490
    },
    {
      "epoch": 2.2831050228310503,
      "grad_norm": 5.934516906738281,
      "learning_rate": 1.1948249619482495e-05,
      "loss": 0.402,
      "step": 2500
    },
    {
      "epoch": 2.2922374429223744,
      "grad_norm": 5.706295013427734,
      "learning_rate": 1.1796042617960425e-05,
      "loss": 0.236,
      "step": 2510
    },
    {
      "epoch": 2.3013698630136985,
      "grad_norm": 0.6418824195861816,
      "learning_rate": 1.1643835616438355e-05,
      "loss": 0.3266,
      "step": 2520
    },
    {
      "epoch": 2.3105022831050226,
      "grad_norm": 0.7572808265686035,
      "learning_rate": 1.1491628614916286e-05,
      "loss": 0.2362,
      "step": 2530
    },
    {
      "epoch": 2.319634703196347,
      "grad_norm": 3.9686503410339355,
      "learning_rate": 1.1339421613394216e-05,
      "loss": 0.3913,
      "step": 2540
    },
    {
      "epoch": 2.328767123287671,
      "grad_norm": 3.540386438369751,
      "learning_rate": 1.1187214611872146e-05,
      "loss": 0.2998,
      "step": 2550
    },
    {
      "epoch": 2.3378995433789953,
      "grad_norm": 5.5109429359436035,
      "learning_rate": 1.1035007610350076e-05,
      "loss": 0.2735,
      "step": 2560
    },
    {
      "epoch": 2.34703196347032,
      "grad_norm": 15.501723289489746,
      "learning_rate": 1.0882800608828006e-05,
      "loss": 0.1083,
      "step": 2570
    },
    {
      "epoch": 2.356164383561644,
      "grad_norm": 0.32602590322494507,
      "learning_rate": 1.0730593607305936e-05,
      "loss": 0.3303,
      "step": 2580
    },
    {
      "epoch": 2.365296803652968,
      "grad_norm": 8.278346061706543,
      "learning_rate": 1.0578386605783866e-05,
      "loss": 0.487,
      "step": 2590
    },
    {
      "epoch": 2.374429223744292,
      "grad_norm": 14.828980445861816,
      "learning_rate": 1.0426179604261796e-05,
      "loss": 0.3542,
      "step": 2600
    },
    {
      "epoch": 2.383561643835616,
      "grad_norm": 3.4446232318878174,
      "learning_rate": 1.0273972602739726e-05,
      "loss": 0.406,
      "step": 2610
    },
    {
      "epoch": 2.3926940639269407,
      "grad_norm": 4.943584442138672,
      "learning_rate": 1.0121765601217656e-05,
      "loss": 0.5146,
      "step": 2620
    },
    {
      "epoch": 2.401826484018265,
      "grad_norm": 1.7254685163497925,
      "learning_rate": 9.969558599695586e-06,
      "loss": 0.317,
      "step": 2630
    },
    {
      "epoch": 2.410958904109589,
      "grad_norm": 1.1940560340881348,
      "learning_rate": 9.817351598173517e-06,
      "loss": 0.2731,
      "step": 2640
    },
    {
      "epoch": 2.4200913242009134,
      "grad_norm": 0.7557370066642761,
      "learning_rate": 9.665144596651447e-06,
      "loss": 0.3546,
      "step": 2650
    },
    {
      "epoch": 2.4292237442922375,
      "grad_norm": 3.8258254528045654,
      "learning_rate": 9.512937595129375e-06,
      "loss": 0.248,
      "step": 2660
    },
    {
      "epoch": 2.4383561643835616,
      "grad_norm": 0.5387856960296631,
      "learning_rate": 9.360730593607305e-06,
      "loss": 0.2819,
      "step": 2670
    },
    {
      "epoch": 2.4474885844748857,
      "grad_norm": 9.426626205444336,
      "learning_rate": 9.208523592085235e-06,
      "loss": 0.4504,
      "step": 2680
    },
    {
      "epoch": 2.45662100456621,
      "grad_norm": 5.084839344024658,
      "learning_rate": 9.056316590563165e-06,
      "loss": 0.1787,
      "step": 2690
    },
    {
      "epoch": 2.4657534246575343,
      "grad_norm": 8.392438888549805,
      "learning_rate": 8.904109589041095e-06,
      "loss": 0.2906,
      "step": 2700
    },
    {
      "epoch": 2.4748858447488584,
      "grad_norm": 4.534856796264648,
      "learning_rate": 8.751902587519026e-06,
      "loss": 0.2163,
      "step": 2710
    },
    {
      "epoch": 2.4840182648401825,
      "grad_norm": 8.238226890563965,
      "learning_rate": 8.599695585996956e-06,
      "loss": 0.3642,
      "step": 2720
    },
    {
      "epoch": 2.493150684931507,
      "grad_norm": 0.7184130549430847,
      "learning_rate": 8.447488584474886e-06,
      "loss": 0.1746,
      "step": 2730
    },
    {
      "epoch": 2.502283105022831,
      "grad_norm": 0.5858363509178162,
      "learning_rate": 8.295281582952816e-06,
      "loss": 0.2641,
      "step": 2740
    },
    {
      "epoch": 2.5114155251141552,
      "grad_norm": 0.33157312870025635,
      "learning_rate": 8.143074581430746e-06,
      "loss": 0.464,
      "step": 2750
    },
    {
      "epoch": 2.5205479452054793,
      "grad_norm": 0.2941094934940338,
      "learning_rate": 7.990867579908676e-06,
      "loss": 0.2701,
      "step": 2760
    },
    {
      "epoch": 2.5296803652968034,
      "grad_norm": 2.8862085342407227,
      "learning_rate": 7.838660578386606e-06,
      "loss": 0.2587,
      "step": 2770
    },
    {
      "epoch": 2.538812785388128,
      "grad_norm": 0.29081642627716064,
      "learning_rate": 7.686453576864536e-06,
      "loss": 0.3084,
      "step": 2780
    },
    {
      "epoch": 2.547945205479452,
      "grad_norm": 0.47347211837768555,
      "learning_rate": 7.5342465753424655e-06,
      "loss": 0.2423,
      "step": 2790
    },
    {
      "epoch": 2.557077625570776,
      "grad_norm": 0.24898511171340942,
      "learning_rate": 7.3820395738203955e-06,
      "loss": 0.3069,
      "step": 2800
    },
    {
      "epoch": 2.5662100456621006,
      "grad_norm": 3.022677183151245,
      "learning_rate": 7.229832572298326e-06,
      "loss": 0.334,
      "step": 2810
    },
    {
      "epoch": 2.5753424657534247,
      "grad_norm": 19.499427795410156,
      "learning_rate": 7.077625570776256e-06,
      "loss": 0.4321,
      "step": 2820
    },
    {
      "epoch": 2.584474885844749,
      "grad_norm": 9.938504219055176,
      "learning_rate": 6.925418569254186e-06,
      "loss": 0.1454,
      "step": 2830
    },
    {
      "epoch": 2.593607305936073,
      "grad_norm": 4.917142391204834,
      "learning_rate": 6.773211567732116e-06,
      "loss": 0.4062,
      "step": 2840
    },
    {
      "epoch": 2.602739726027397,
      "grad_norm": 15.49856185913086,
      "learning_rate": 6.621004566210046e-06,
      "loss": 0.3984,
      "step": 2850
    },
    {
      "epoch": 2.6118721461187215,
      "grad_norm": 4.078102111816406,
      "learning_rate": 6.468797564687975e-06,
      "loss": 0.2179,
      "step": 2860
    },
    {
      "epoch": 2.6210045662100456,
      "grad_norm": 0.45599886775016785,
      "learning_rate": 6.316590563165905e-06,
      "loss": 0.185,
      "step": 2870
    },
    {
      "epoch": 2.6301369863013697,
      "grad_norm": 6.463504791259766,
      "learning_rate": 6.1643835616438354e-06,
      "loss": 0.4442,
      "step": 2880
    },
    {
      "epoch": 2.6392694063926943,
      "grad_norm": 4.539641857147217,
      "learning_rate": 6.0121765601217655e-06,
      "loss": 0.497,
      "step": 2890
    },
    {
      "epoch": 2.6484018264840183,
      "grad_norm": 5.533836364746094,
      "learning_rate": 5.859969558599696e-06,
      "loss": 0.4561,
      "step": 2900
    },
    {
      "epoch": 2.6575342465753424,
      "grad_norm": 9.4226655960083,
      "learning_rate": 5.707762557077626e-06,
      "loss": 0.3037,
      "step": 2910
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 4.718020439147949,
      "learning_rate": 5.555555555555556e-06,
      "loss": 0.6456,
      "step": 2920
    },
    {
      "epoch": 2.6757990867579906,
      "grad_norm": 0.7564858198165894,
      "learning_rate": 5.403348554033486e-06,
      "loss": 0.2658,
      "step": 2930
    },
    {
      "epoch": 2.684931506849315,
      "grad_norm": 4.455593585968018,
      "learning_rate": 5.251141552511415e-06,
      "loss": 0.164,
      "step": 2940
    },
    {
      "epoch": 2.6940639269406392,
      "grad_norm": 0.289989709854126,
      "learning_rate": 5.098934550989345e-06,
      "loss": 0.3609,
      "step": 2950
    },
    {
      "epoch": 2.7031963470319633,
      "grad_norm": 0.4883960783481598,
      "learning_rate": 4.946727549467275e-06,
      "loss": 0.1853,
      "step": 2960
    },
    {
      "epoch": 2.712328767123288,
      "grad_norm": 4.3404998779296875,
      "learning_rate": 4.7945205479452054e-06,
      "loss": 0.3171,
      "step": 2970
    },
    {
      "epoch": 2.721461187214612,
      "grad_norm": 0.7176561951637268,
      "learning_rate": 4.6423135464231355e-06,
      "loss": 0.3786,
      "step": 2980
    },
    {
      "epoch": 2.730593607305936,
      "grad_norm": 1.3520712852478027,
      "learning_rate": 4.490106544901066e-06,
      "loss": 0.1493,
      "step": 2990
    },
    {
      "epoch": 2.73972602739726,
      "grad_norm": 9.404271125793457,
      "learning_rate": 4.337899543378996e-06,
      "loss": 0.1359,
      "step": 3000
    },
    {
      "epoch": 2.748858447488584,
      "grad_norm": 24.37031364440918,
      "learning_rate": 4.185692541856925e-06,
      "loss": 0.281,
      "step": 3010
    },
    {
      "epoch": 2.7579908675799087,
      "grad_norm": 9.409422874450684,
      "learning_rate": 4.033485540334855e-06,
      "loss": 0.354,
      "step": 3020
    },
    {
      "epoch": 2.767123287671233,
      "grad_norm": 6.524041175842285,
      "learning_rate": 3.881278538812785e-06,
      "loss": 0.4656,
      "step": 3030
    },
    {
      "epoch": 2.776255707762557,
      "grad_norm": 12.786317825317383,
      "learning_rate": 3.7290715372907152e-06,
      "loss": 0.1993,
      "step": 3040
    },
    {
      "epoch": 2.7853881278538815,
      "grad_norm": 11.878960609436035,
      "learning_rate": 3.5768645357686453e-06,
      "loss": 0.3828,
      "step": 3050
    },
    {
      "epoch": 2.7945205479452055,
      "grad_norm": 0.4217653274536133,
      "learning_rate": 3.4246575342465754e-06,
      "loss": 0.2643,
      "step": 3060
    },
    {
      "epoch": 2.8036529680365296,
      "grad_norm": 0.4466277062892914,
      "learning_rate": 3.272450532724505e-06,
      "loss": 0.3261,
      "step": 3070
    },
    {
      "epoch": 2.8127853881278537,
      "grad_norm": 0.6448724269866943,
      "learning_rate": 3.120243531202435e-06,
      "loss": 0.3106,
      "step": 3080
    },
    {
      "epoch": 2.821917808219178,
      "grad_norm": 1.6072198152542114,
      "learning_rate": 2.9680365296803653e-06,
      "loss": 0.3685,
      "step": 3090
    },
    {
      "epoch": 2.8310502283105023,
      "grad_norm": 7.8538618087768555,
      "learning_rate": 2.8158295281582954e-06,
      "loss": 0.4026,
      "step": 3100
    },
    {
      "epoch": 2.8401826484018264,
      "grad_norm": 9.146856307983398,
      "learning_rate": 2.663622526636225e-06,
      "loss": 0.579,
      "step": 3110
    },
    {
      "epoch": 2.8493150684931505,
      "grad_norm": 0.3704872131347656,
      "learning_rate": 2.511415525114155e-06,
      "loss": 0.2823,
      "step": 3120
    },
    {
      "epoch": 2.858447488584475,
      "grad_norm": 7.327301025390625,
      "learning_rate": 2.3592085235920852e-06,
      "loss": 0.2298,
      "step": 3130
    },
    {
      "epoch": 2.867579908675799,
      "grad_norm": 0.4687500596046448,
      "learning_rate": 2.2070015220700153e-06,
      "loss": 0.6456,
      "step": 3140
    },
    {
      "epoch": 2.8767123287671232,
      "grad_norm": 4.687405586242676,
      "learning_rate": 2.054794520547945e-06,
      "loss": 0.4132,
      "step": 3150
    },
    {
      "epoch": 2.8858447488584473,
      "grad_norm": 9.385603904724121,
      "learning_rate": 1.902587519025875e-06,
      "loss": 0.2702,
      "step": 3160
    },
    {
      "epoch": 2.8949771689497714,
      "grad_norm": 0.42763248085975647,
      "learning_rate": 1.7503805175038052e-06,
      "loss": 0.3437,
      "step": 3170
    },
    {
      "epoch": 2.904109589041096,
      "grad_norm": 5.364828586578369,
      "learning_rate": 1.598173515981735e-06,
      "loss": 0.2719,
      "step": 3180
    },
    {
      "epoch": 2.91324200913242,
      "grad_norm": 0.4051724970340729,
      "learning_rate": 1.4459665144596652e-06,
      "loss": 0.3882,
      "step": 3190
    },
    {
      "epoch": 2.922374429223744,
      "grad_norm": 3.3279871940612793,
      "learning_rate": 1.293759512937595e-06,
      "loss": 0.3523,
      "step": 3200
    },
    {
      "epoch": 2.9315068493150687,
      "grad_norm": 5.563653945922852,
      "learning_rate": 1.1415525114155251e-06,
      "loss": 0.3853,
      "step": 3210
    },
    {
      "epoch": 2.9406392694063928,
      "grad_norm": 3.909595489501953,
      "learning_rate": 9.89345509893455e-07,
      "loss": 0.3768,
      "step": 3220
    },
    {
      "epoch": 2.949771689497717,
      "grad_norm": 14.019461631774902,
      "learning_rate": 8.371385083713851e-07,
      "loss": 0.1911,
      "step": 3230
    },
    {
      "epoch": 2.958904109589041,
      "grad_norm": 1.3747994899749756,
      "learning_rate": 6.849315068493151e-07,
      "loss": 0.4736,
      "step": 3240
    },
    {
      "epoch": 2.968036529680365,
      "grad_norm": 5.884377479553223,
      "learning_rate": 5.327245053272451e-07,
      "loss": 0.1153,
      "step": 3250
    },
    {
      "epoch": 2.9771689497716896,
      "grad_norm": 8.745347023010254,
      "learning_rate": 3.8051750380517503e-07,
      "loss": 0.5155,
      "step": 3260
    },
    {
      "epoch": 2.9863013698630136,
      "grad_norm": 3.5183987617492676,
      "learning_rate": 2.2831050228310502e-07,
      "loss": 0.3021,
      "step": 3270
    },
    {
      "epoch": 2.9954337899543377,
      "grad_norm": 14.451106071472168,
      "learning_rate": 7.6103500761035e-08,
      "loss": 0.5842,
      "step": 3280
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.8912745545911375,
      "eval_f1_hate": 0.5466970387243736,
      "eval_f1_idk/skip": 0.0,
      "eval_f1_macro": 0.3717795498711294,
      "eval_f1_noHate": 0.9404211607601438,
      "eval_f1_relation": 0.0,
      "eval_loss": 0.3820092976093292,
      "eval_precision_global": 0.8807788944723618,
      "eval_precision_hate": 0.6030150753768844,
      "eval_precision_idk/skip": 1.0,
      "eval_precision_noHate": 0.9201005025125628,
      "eval_precision_relation": 1.0,
      "eval_recall_global": 0.36541491596638653,
      "eval_recall_hate": 0.5,
      "eval_recall_idk/skip": 0.0,
      "eval_recall_noHate": 0.9616596638655462,
      "eval_recall_relation": 0.0,
      "eval_runtime": 13.1124,
      "eval_samples_per_second": 166.942,
      "eval_steps_per_second": 20.896,
      "step": 3285
    }
  ],
  "logging_steps": 10,
  "max_steps": 3285,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 711516719617056.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
