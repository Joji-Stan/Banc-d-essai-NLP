{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2190,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0091324200913242,
      "grad_norm": 4.2014241218566895,
      "learning_rate": 4.984779299847793e-05,
      "loss": 1.4092,
      "step": 10
    },
    {
      "epoch": 0.0182648401826484,
      "grad_norm": 4.149141788482666,
      "learning_rate": 4.969558599695586e-05,
      "loss": 1.3238,
      "step": 20
    },
    {
      "epoch": 0.0273972602739726,
      "grad_norm": 4.970274448394775,
      "learning_rate": 4.9543378995433794e-05,
      "loss": 1.2415,
      "step": 30
    },
    {
      "epoch": 0.0365296803652968,
      "grad_norm": 4.783473014831543,
      "learning_rate": 4.939117199391172e-05,
      "loss": 1.0942,
      "step": 40
    },
    {
      "epoch": 0.045662100456621,
      "grad_norm": 3.472869634628296,
      "learning_rate": 4.923896499238965e-05,
      "loss": 0.9998,
      "step": 50
    },
    {
      "epoch": 0.0547945205479452,
      "grad_norm": 3.1168034076690674,
      "learning_rate": 4.908675799086758e-05,
      "loss": 0.8516,
      "step": 60
    },
    {
      "epoch": 0.0639269406392694,
      "grad_norm": 2.4794931411743164,
      "learning_rate": 4.8934550989345515e-05,
      "loss": 0.7077,
      "step": 70
    },
    {
      "epoch": 0.0730593607305936,
      "grad_norm": 1.394862413406372,
      "learning_rate": 4.878234398782344e-05,
      "loss": 0.573,
      "step": 80
    },
    {
      "epoch": 0.0821917808219178,
      "grad_norm": 2.526909112930298,
      "learning_rate": 4.863013698630137e-05,
      "loss": 0.5617,
      "step": 90
    },
    {
      "epoch": 0.091324200913242,
      "grad_norm": 8.289301872253418,
      "learning_rate": 4.84779299847793e-05,
      "loss": 0.6276,
      "step": 100
    },
    {
      "epoch": 0.1004566210045662,
      "grad_norm": 3.2346248626708984,
      "learning_rate": 4.8325722983257235e-05,
      "loss": 0.6523,
      "step": 110
    },
    {
      "epoch": 0.1095890410958904,
      "grad_norm": 2.560805320739746,
      "learning_rate": 4.8173515981735164e-05,
      "loss": 0.3892,
      "step": 120
    },
    {
      "epoch": 0.1187214611872146,
      "grad_norm": 2.987116575241089,
      "learning_rate": 4.802130898021309e-05,
      "loss": 0.6495,
      "step": 130
    },
    {
      "epoch": 0.1278538812785388,
      "grad_norm": 3.038130044937134,
      "learning_rate": 4.786910197869102e-05,
      "loss": 0.6337,
      "step": 140
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 5.149893283843994,
      "learning_rate": 4.7716894977168955e-05,
      "loss": 0.4634,
      "step": 150
    },
    {
      "epoch": 0.1461187214611872,
      "grad_norm": 3.445244073867798,
      "learning_rate": 4.7564687975646884e-05,
      "loss": 0.4522,
      "step": 160
    },
    {
      "epoch": 0.1552511415525114,
      "grad_norm": 2.870965003967285,
      "learning_rate": 4.741248097412481e-05,
      "loss": 0.3908,
      "step": 170
    },
    {
      "epoch": 0.1643835616438356,
      "grad_norm": 7.03959321975708,
      "learning_rate": 4.726027397260274e-05,
      "loss": 0.3304,
      "step": 180
    },
    {
      "epoch": 0.1735159817351598,
      "grad_norm": 3.2255845069885254,
      "learning_rate": 4.710806697108067e-05,
      "loss": 0.4624,
      "step": 190
    },
    {
      "epoch": 0.182648401826484,
      "grad_norm": 2.61588454246521,
      "learning_rate": 4.6955859969558604e-05,
      "loss": 0.3659,
      "step": 200
    },
    {
      "epoch": 0.1917808219178082,
      "grad_norm": 4.456578254699707,
      "learning_rate": 4.680365296803653e-05,
      "loss": 0.3655,
      "step": 210
    },
    {
      "epoch": 0.2009132420091324,
      "grad_norm": 5.7224440574646,
      "learning_rate": 4.665144596651446e-05,
      "loss": 0.3439,
      "step": 220
    },
    {
      "epoch": 0.2100456621004566,
      "grad_norm": 1.495111346244812,
      "learning_rate": 4.649923896499239e-05,
      "loss": 0.3132,
      "step": 230
    },
    {
      "epoch": 0.2191780821917808,
      "grad_norm": 3.848186492919922,
      "learning_rate": 4.6347031963470325e-05,
      "loss": 0.412,
      "step": 240
    },
    {
      "epoch": 0.228310502283105,
      "grad_norm": 3.3290815353393555,
      "learning_rate": 4.619482496194825e-05,
      "loss": 0.3121,
      "step": 250
    },
    {
      "epoch": 0.2374429223744292,
      "grad_norm": 1.6857166290283203,
      "learning_rate": 4.604261796042618e-05,
      "loss": 0.2836,
      "step": 260
    },
    {
      "epoch": 0.2465753424657534,
      "grad_norm": 0.7483834624290466,
      "learning_rate": 4.589041095890411e-05,
      "loss": 0.4508,
      "step": 270
    },
    {
      "epoch": 0.2557077625570776,
      "grad_norm": 15.452421188354492,
      "learning_rate": 4.5738203957382045e-05,
      "loss": 0.4947,
      "step": 280
    },
    {
      "epoch": 0.2648401826484018,
      "grad_norm": 3.2019295692443848,
      "learning_rate": 4.5585996955859973e-05,
      "loss": 0.3868,
      "step": 290
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 2.713031530380249,
      "learning_rate": 4.54337899543379e-05,
      "loss": 0.3957,
      "step": 300
    },
    {
      "epoch": 0.2831050228310502,
      "grad_norm": 8.713400840759277,
      "learning_rate": 4.528158295281583e-05,
      "loss": 0.4065,
      "step": 310
    },
    {
      "epoch": 0.2922374429223744,
      "grad_norm": 0.9665752053260803,
      "learning_rate": 4.512937595129376e-05,
      "loss": 0.4704,
      "step": 320
    },
    {
      "epoch": 0.3013698630136986,
      "grad_norm": 1.5282211303710938,
      "learning_rate": 4.4977168949771694e-05,
      "loss": 0.6581,
      "step": 330
    },
    {
      "epoch": 0.3105022831050228,
      "grad_norm": 3.5258123874664307,
      "learning_rate": 4.482496194824962e-05,
      "loss": 0.2639,
      "step": 340
    },
    {
      "epoch": 0.319634703196347,
      "grad_norm": 3.5934760570526123,
      "learning_rate": 4.467275494672755e-05,
      "loss": 0.2484,
      "step": 350
    },
    {
      "epoch": 0.3287671232876712,
      "grad_norm": 2.534461259841919,
      "learning_rate": 4.452054794520548e-05,
      "loss": 0.5359,
      "step": 360
    },
    {
      "epoch": 0.3378995433789954,
      "grad_norm": 4.455851078033447,
      "learning_rate": 4.4368340943683414e-05,
      "loss": 0.4696,
      "step": 370
    },
    {
      "epoch": 0.3470319634703196,
      "grad_norm": 2.4211103916168213,
      "learning_rate": 4.421613394216134e-05,
      "loss": 0.4243,
      "step": 380
    },
    {
      "epoch": 0.3561643835616438,
      "grad_norm": 2.8859012126922607,
      "learning_rate": 4.406392694063927e-05,
      "loss": 0.471,
      "step": 390
    },
    {
      "epoch": 0.365296803652968,
      "grad_norm": 12.153661727905273,
      "learning_rate": 4.39117199391172e-05,
      "loss": 0.4516,
      "step": 400
    },
    {
      "epoch": 0.3744292237442922,
      "grad_norm": 7.018897533416748,
      "learning_rate": 4.3759512937595135e-05,
      "loss": 0.5746,
      "step": 410
    },
    {
      "epoch": 0.3835616438356164,
      "grad_norm": 2.0375819206237793,
      "learning_rate": 4.360730593607306e-05,
      "loss": 0.1443,
      "step": 420
    },
    {
      "epoch": 0.3926940639269406,
      "grad_norm": 3.4404189586639404,
      "learning_rate": 4.345509893455099e-05,
      "loss": 0.43,
      "step": 430
    },
    {
      "epoch": 0.4018264840182648,
      "grad_norm": 8.660131454467773,
      "learning_rate": 4.330289193302892e-05,
      "loss": 0.8187,
      "step": 440
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 2.531592845916748,
      "learning_rate": 4.3150684931506855e-05,
      "loss": 0.3468,
      "step": 450
    },
    {
      "epoch": 0.4200913242009132,
      "grad_norm": 2.712364435195923,
      "learning_rate": 4.299847792998478e-05,
      "loss": 0.3988,
      "step": 460
    },
    {
      "epoch": 0.4292237442922374,
      "grad_norm": 1.8181850910186768,
      "learning_rate": 4.284627092846271e-05,
      "loss": 0.449,
      "step": 470
    },
    {
      "epoch": 0.4383561643835616,
      "grad_norm": 1.7195039987564087,
      "learning_rate": 4.269406392694064e-05,
      "loss": 0.2727,
      "step": 480
    },
    {
      "epoch": 0.4474885844748858,
      "grad_norm": 2.748134136199951,
      "learning_rate": 4.254185692541857e-05,
      "loss": 0.3758,
      "step": 490
    },
    {
      "epoch": 0.45662100456621,
      "grad_norm": 5.917792320251465,
      "learning_rate": 4.2389649923896504e-05,
      "loss": 0.3246,
      "step": 500
    },
    {
      "epoch": 0.4657534246575342,
      "grad_norm": 2.3957009315490723,
      "learning_rate": 4.223744292237443e-05,
      "loss": 0.5519,
      "step": 510
    },
    {
      "epoch": 0.4748858447488584,
      "grad_norm": 2.0162081718444824,
      "learning_rate": 4.208523592085236e-05,
      "loss": 0.3067,
      "step": 520
    },
    {
      "epoch": 0.4840182648401826,
      "grad_norm": 7.277093887329102,
      "learning_rate": 4.193302891933029e-05,
      "loss": 0.2649,
      "step": 530
    },
    {
      "epoch": 0.4931506849315068,
      "grad_norm": 0.6217344999313354,
      "learning_rate": 4.1780821917808224e-05,
      "loss": 0.1976,
      "step": 540
    },
    {
      "epoch": 0.502283105022831,
      "grad_norm": 1.422882318496704,
      "learning_rate": 4.162861491628615e-05,
      "loss": 0.2015,
      "step": 550
    },
    {
      "epoch": 0.5114155251141552,
      "grad_norm": 1.3135812282562256,
      "learning_rate": 4.147640791476408e-05,
      "loss": 0.2779,
      "step": 560
    },
    {
      "epoch": 0.5205479452054794,
      "grad_norm": 14.464576721191406,
      "learning_rate": 4.132420091324201e-05,
      "loss": 0.4883,
      "step": 570
    },
    {
      "epoch": 0.5296803652968036,
      "grad_norm": 4.937425136566162,
      "learning_rate": 4.1171993911719944e-05,
      "loss": 0.5045,
      "step": 580
    },
    {
      "epoch": 0.5388127853881278,
      "grad_norm": 2.497363805770874,
      "learning_rate": 4.101978691019787e-05,
      "loss": 0.383,
      "step": 590
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 4.553092002868652,
      "learning_rate": 4.08675799086758e-05,
      "loss": 0.2887,
      "step": 600
    },
    {
      "epoch": 0.5570776255707762,
      "grad_norm": 3.252288579940796,
      "learning_rate": 4.071537290715373e-05,
      "loss": 0.2456,
      "step": 610
    },
    {
      "epoch": 0.5662100456621004,
      "grad_norm": 1.4945292472839355,
      "learning_rate": 4.0563165905631665e-05,
      "loss": 0.5277,
      "step": 620
    },
    {
      "epoch": 0.5753424657534246,
      "grad_norm": 5.297628402709961,
      "learning_rate": 4.041095890410959e-05,
      "loss": 0.589,
      "step": 630
    },
    {
      "epoch": 0.5844748858447488,
      "grad_norm": 5.739619731903076,
      "learning_rate": 4.025875190258752e-05,
      "loss": 0.3895,
      "step": 640
    },
    {
      "epoch": 0.593607305936073,
      "grad_norm": 3.931476354598999,
      "learning_rate": 4.010654490106545e-05,
      "loss": 0.4483,
      "step": 650
    },
    {
      "epoch": 0.6027397260273972,
      "grad_norm": 5.132766246795654,
      "learning_rate": 3.995433789954338e-05,
      "loss": 0.338,
      "step": 660
    },
    {
      "epoch": 0.6118721461187214,
      "grad_norm": 4.865106105804443,
      "learning_rate": 3.9802130898021314e-05,
      "loss": 0.3937,
      "step": 670
    },
    {
      "epoch": 0.6210045662100456,
      "grad_norm": 2.0048723220825195,
      "learning_rate": 3.964992389649924e-05,
      "loss": 0.3914,
      "step": 680
    },
    {
      "epoch": 0.6301369863013698,
      "grad_norm": 4.006353378295898,
      "learning_rate": 3.949771689497717e-05,
      "loss": 0.5664,
      "step": 690
    },
    {
      "epoch": 0.639269406392694,
      "grad_norm": 4.143347263336182,
      "learning_rate": 3.93455098934551e-05,
      "loss": 0.1821,
      "step": 700
    },
    {
      "epoch": 0.6484018264840182,
      "grad_norm": 0.5014417767524719,
      "learning_rate": 3.9193302891933034e-05,
      "loss": 0.2695,
      "step": 710
    },
    {
      "epoch": 0.6575342465753424,
      "grad_norm": 7.656728267669678,
      "learning_rate": 3.904109589041096e-05,
      "loss": 0.7182,
      "step": 720
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 3.5674729347229004,
      "learning_rate": 3.888888888888889e-05,
      "loss": 0.4191,
      "step": 730
    },
    {
      "epoch": 0.6757990867579908,
      "grad_norm": 3.0391266345977783,
      "learning_rate": 3.873668188736682e-05,
      "loss": 0.4172,
      "step": 740
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 1.8672640323638916,
      "learning_rate": 3.8584474885844754e-05,
      "loss": 0.3501,
      "step": 750
    },
    {
      "epoch": 0.6940639269406392,
      "grad_norm": 2.1507303714752197,
      "learning_rate": 3.843226788432268e-05,
      "loss": 0.2292,
      "step": 760
    },
    {
      "epoch": 0.7031963470319634,
      "grad_norm": 5.020025253295898,
      "learning_rate": 3.828006088280061e-05,
      "loss": 0.443,
      "step": 770
    },
    {
      "epoch": 0.7123287671232876,
      "grad_norm": 8.281349182128906,
      "learning_rate": 3.812785388127854e-05,
      "loss": 0.4447,
      "step": 780
    },
    {
      "epoch": 0.7214611872146118,
      "grad_norm": 2.182190418243408,
      "learning_rate": 3.797564687975647e-05,
      "loss": 0.2514,
      "step": 790
    },
    {
      "epoch": 0.730593607305936,
      "grad_norm": 13.718620300292969,
      "learning_rate": 3.78234398782344e-05,
      "loss": 0.5834,
      "step": 800
    },
    {
      "epoch": 0.7397260273972602,
      "grad_norm": 2.225736618041992,
      "learning_rate": 3.767123287671233e-05,
      "loss": 0.4969,
      "step": 810
    },
    {
      "epoch": 0.7488584474885844,
      "grad_norm": 7.196774959564209,
      "learning_rate": 3.751902587519026e-05,
      "loss": 0.4007,
      "step": 820
    },
    {
      "epoch": 0.7579908675799086,
      "grad_norm": 6.489071846008301,
      "learning_rate": 3.736681887366819e-05,
      "loss": 0.3576,
      "step": 830
    },
    {
      "epoch": 0.7671232876712328,
      "grad_norm": 5.737024784088135,
      "learning_rate": 3.7214611872146123e-05,
      "loss": 0.4381,
      "step": 840
    },
    {
      "epoch": 0.776255707762557,
      "grad_norm": 4.6021504402160645,
      "learning_rate": 3.706240487062405e-05,
      "loss": 0.2426,
      "step": 850
    },
    {
      "epoch": 0.7853881278538812,
      "grad_norm": 2.569338321685791,
      "learning_rate": 3.691019786910198e-05,
      "loss": 0.3163,
      "step": 860
    },
    {
      "epoch": 0.7945205479452054,
      "grad_norm": 0.5997539758682251,
      "learning_rate": 3.675799086757991e-05,
      "loss": 0.2816,
      "step": 870
    },
    {
      "epoch": 0.8036529680365296,
      "grad_norm": 1.7195467948913574,
      "learning_rate": 3.6605783866057844e-05,
      "loss": 0.4755,
      "step": 880
    },
    {
      "epoch": 0.8127853881278538,
      "grad_norm": 0.6913968324661255,
      "learning_rate": 3.645357686453577e-05,
      "loss": 0.1824,
      "step": 890
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 0.5214787721633911,
      "learning_rate": 3.63013698630137e-05,
      "loss": 0.7159,
      "step": 900
    },
    {
      "epoch": 0.8310502283105022,
      "grad_norm": 3.387037754058838,
      "learning_rate": 3.614916286149163e-05,
      "loss": 0.6324,
      "step": 910
    },
    {
      "epoch": 0.8401826484018264,
      "grad_norm": 0.6512325406074524,
      "learning_rate": 3.5996955859969564e-05,
      "loss": 0.385,
      "step": 920
    },
    {
      "epoch": 0.8493150684931506,
      "grad_norm": 3.3542628288269043,
      "learning_rate": 3.584474885844749e-05,
      "loss": 0.288,
      "step": 930
    },
    {
      "epoch": 0.8584474885844748,
      "grad_norm": 5.149443626403809,
      "learning_rate": 3.569254185692542e-05,
      "loss": 0.5914,
      "step": 940
    },
    {
      "epoch": 0.867579908675799,
      "grad_norm": 10.313617706298828,
      "learning_rate": 3.554033485540335e-05,
      "loss": 0.3933,
      "step": 950
    },
    {
      "epoch": 0.8767123287671232,
      "grad_norm": 2.1160686016082764,
      "learning_rate": 3.538812785388128e-05,
      "loss": 0.4591,
      "step": 960
    },
    {
      "epoch": 0.8858447488584474,
      "grad_norm": 4.683087348937988,
      "learning_rate": 3.523592085235921e-05,
      "loss": 0.4178,
      "step": 970
    },
    {
      "epoch": 0.8949771689497716,
      "grad_norm": 3.4736366271972656,
      "learning_rate": 3.508371385083714e-05,
      "loss": 0.3218,
      "step": 980
    },
    {
      "epoch": 0.9041095890410958,
      "grad_norm": 4.114583969116211,
      "learning_rate": 3.493150684931507e-05,
      "loss": 0.5066,
      "step": 990
    },
    {
      "epoch": 0.91324200913242,
      "grad_norm": 4.443102836608887,
      "learning_rate": 3.4779299847793e-05,
      "loss": 0.3163,
      "step": 1000
    },
    {
      "epoch": 0.9223744292237442,
      "grad_norm": 3.668541669845581,
      "learning_rate": 3.462709284627093e-05,
      "loss": 0.37,
      "step": 1010
    },
    {
      "epoch": 0.9315068493150684,
      "grad_norm": 3.1004445552825928,
      "learning_rate": 3.447488584474886e-05,
      "loss": 0.3316,
      "step": 1020
    },
    {
      "epoch": 0.9406392694063926,
      "grad_norm": 1.8595644235610962,
      "learning_rate": 3.432267884322679e-05,
      "loss": 0.3757,
      "step": 1030
    },
    {
      "epoch": 0.9497716894977168,
      "grad_norm": 0.7095105648040771,
      "learning_rate": 3.417047184170472e-05,
      "loss": 0.1853,
      "step": 1040
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 3.82582688331604,
      "learning_rate": 3.4018264840182654e-05,
      "loss": 0.2836,
      "step": 1050
    },
    {
      "epoch": 0.9680365296803652,
      "grad_norm": 6.456411361694336,
      "learning_rate": 3.386605783866058e-05,
      "loss": 0.4701,
      "step": 1060
    },
    {
      "epoch": 0.9771689497716894,
      "grad_norm": 1.0287468433380127,
      "learning_rate": 3.371385083713851e-05,
      "loss": 0.3622,
      "step": 1070
    },
    {
      "epoch": 0.9863013698630136,
      "grad_norm": 10.451523780822754,
      "learning_rate": 3.356164383561644e-05,
      "loss": 0.3881,
      "step": 1080
    },
    {
      "epoch": 0.9954337899543378,
      "grad_norm": 4.582371711730957,
      "learning_rate": 3.3409436834094374e-05,
      "loss": 0.4833,
      "step": 1090
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8917313841936958,
      "eval_f1_hate": 0.4340175953079179,
      "eval_f1_idk/skip": 0.0,
      "eval_f1_macro": 0.343724839708743,
      "eval_f1_noHate": 0.9408817635270541,
      "eval_f1_relation": 0.0,
      "eval_loss": 0.38222378492355347,
      "eval_precision_global": 0.9080246386707637,
      "eval_precision_hate": 0.7326732673267327,
      "eval_precision_idk/skip": 1.0,
      "eval_precision_noHate": 0.8994252873563219,
      "eval_precision_relation": 1.0,
      "eval_recall_global": 0.32366946778711486,
      "eval_recall_hate": 0.30833333333333335,
      "eval_recall_idk/skip": 0.0,
      "eval_recall_noHate": 0.9863445378151261,
      "eval_recall_relation": 0.0,
      "eval_runtime": 13.3652,
      "eval_samples_per_second": 163.784,
      "eval_steps_per_second": 20.501,
      "step": 1095
    },
    {
      "epoch": 1.004566210045662,
      "grad_norm": 3.0400333404541016,
      "learning_rate": 3.32572298325723e-05,
      "loss": 0.222,
      "step": 1100
    },
    {
      "epoch": 1.0136986301369864,
      "grad_norm": 0.3845624327659607,
      "learning_rate": 3.310502283105023e-05,
      "loss": 0.3422,
      "step": 1110
    },
    {
      "epoch": 1.0228310502283104,
      "grad_norm": 4.231970310211182,
      "learning_rate": 3.295281582952816e-05,
      "loss": 0.3363,
      "step": 1120
    },
    {
      "epoch": 1.0319634703196348,
      "grad_norm": 4.221740245819092,
      "learning_rate": 3.280060882800609e-05,
      "loss": 0.2572,
      "step": 1130
    },
    {
      "epoch": 1.0410958904109588,
      "grad_norm": 11.211963653564453,
      "learning_rate": 3.264840182648402e-05,
      "loss": 0.5303,
      "step": 1140
    },
    {
      "epoch": 1.0502283105022832,
      "grad_norm": 7.743516445159912,
      "learning_rate": 3.249619482496195e-05,
      "loss": 0.4717,
      "step": 1150
    },
    {
      "epoch": 1.0593607305936072,
      "grad_norm": 3.173330307006836,
      "learning_rate": 3.234398782343988e-05,
      "loss": 0.5244,
      "step": 1160
    },
    {
      "epoch": 1.0684931506849316,
      "grad_norm": 3.236485242843628,
      "learning_rate": 3.219178082191781e-05,
      "loss": 0.1922,
      "step": 1170
    },
    {
      "epoch": 1.0776255707762556,
      "grad_norm": 2.8704609870910645,
      "learning_rate": 3.203957382039574e-05,
      "loss": 0.4234,
      "step": 1180
    },
    {
      "epoch": 1.08675799086758,
      "grad_norm": 5.497557163238525,
      "learning_rate": 3.188736681887367e-05,
      "loss": 0.3451,
      "step": 1190
    },
    {
      "epoch": 1.095890410958904,
      "grad_norm": 0.5379366278648376,
      "learning_rate": 3.17351598173516e-05,
      "loss": 0.1344,
      "step": 1200
    },
    {
      "epoch": 1.1050228310502284,
      "grad_norm": 17.18410301208496,
      "learning_rate": 3.158295281582953e-05,
      "loss": 0.2706,
      "step": 1210
    },
    {
      "epoch": 1.1141552511415524,
      "grad_norm": 3.8860371112823486,
      "learning_rate": 3.1430745814307464e-05,
      "loss": 0.545,
      "step": 1220
    },
    {
      "epoch": 1.1232876712328768,
      "grad_norm": 3.093757152557373,
      "learning_rate": 3.127853881278539e-05,
      "loss": 0.3125,
      "step": 1230
    },
    {
      "epoch": 1.1324200913242009,
      "grad_norm": 3.3328843116760254,
      "learning_rate": 3.112633181126332e-05,
      "loss": 0.3047,
      "step": 1240
    },
    {
      "epoch": 1.1415525114155252,
      "grad_norm": 0.5910763144493103,
      "learning_rate": 3.097412480974125e-05,
      "loss": 0.5011,
      "step": 1250
    },
    {
      "epoch": 1.1506849315068493,
      "grad_norm": 0.6705597639083862,
      "learning_rate": 3.082191780821918e-05,
      "loss": 0.5788,
      "step": 1260
    },
    {
      "epoch": 1.1598173515981736,
      "grad_norm": 6.111064910888672,
      "learning_rate": 3.066971080669711e-05,
      "loss": 0.2385,
      "step": 1270
    },
    {
      "epoch": 1.1689497716894977,
      "grad_norm": 10.224773406982422,
      "learning_rate": 3.051750380517504e-05,
      "loss": 0.2632,
      "step": 1280
    },
    {
      "epoch": 1.178082191780822,
      "grad_norm": 1.3307323455810547,
      "learning_rate": 3.036529680365297e-05,
      "loss": 0.4386,
      "step": 1290
    },
    {
      "epoch": 1.187214611872146,
      "grad_norm": 4.205552577972412,
      "learning_rate": 3.02130898021309e-05,
      "loss": 0.166,
      "step": 1300
    },
    {
      "epoch": 1.1963470319634704,
      "grad_norm": 4.061017036437988,
      "learning_rate": 3.006088280060883e-05,
      "loss": 0.5329,
      "step": 1310
    },
    {
      "epoch": 1.2054794520547945,
      "grad_norm": 9.687250137329102,
      "learning_rate": 2.990867579908676e-05,
      "loss": 0.3321,
      "step": 1320
    },
    {
      "epoch": 1.2146118721461188,
      "grad_norm": 2.892995834350586,
      "learning_rate": 2.975646879756469e-05,
      "loss": 0.3298,
      "step": 1330
    },
    {
      "epoch": 1.2237442922374429,
      "grad_norm": 7.165471076965332,
      "learning_rate": 2.960426179604262e-05,
      "loss": 0.3651,
      "step": 1340
    },
    {
      "epoch": 1.2328767123287672,
      "grad_norm": 0.5170530676841736,
      "learning_rate": 2.945205479452055e-05,
      "loss": 0.4577,
      "step": 1350
    },
    {
      "epoch": 1.2420091324200913,
      "grad_norm": 3.8310301303863525,
      "learning_rate": 2.929984779299848e-05,
      "loss": 0.5097,
      "step": 1360
    },
    {
      "epoch": 1.2511415525114156,
      "grad_norm": 8.320622444152832,
      "learning_rate": 2.914764079147641e-05,
      "loss": 0.5556,
      "step": 1370
    },
    {
      "epoch": 1.2602739726027397,
      "grad_norm": 1.4610373973846436,
      "learning_rate": 2.8995433789954342e-05,
      "loss": 0.2856,
      "step": 1380
    },
    {
      "epoch": 1.269406392694064,
      "grad_norm": 3.7877790927886963,
      "learning_rate": 2.884322678843227e-05,
      "loss": 0.2191,
      "step": 1390
    },
    {
      "epoch": 1.278538812785388,
      "grad_norm": 3.9603960514068604,
      "learning_rate": 2.8691019786910202e-05,
      "loss": 0.396,
      "step": 1400
    },
    {
      "epoch": 1.2876712328767124,
      "grad_norm": 4.958068370819092,
      "learning_rate": 2.853881278538813e-05,
      "loss": 0.454,
      "step": 1410
    },
    {
      "epoch": 1.2968036529680365,
      "grad_norm": 11.898917198181152,
      "learning_rate": 2.838660578386606e-05,
      "loss": 0.5186,
      "step": 1420
    },
    {
      "epoch": 1.3059360730593608,
      "grad_norm": 0.6699023246765137,
      "learning_rate": 2.823439878234399e-05,
      "loss": 0.2383,
      "step": 1430
    },
    {
      "epoch": 1.3150684931506849,
      "grad_norm": 13.283489227294922,
      "learning_rate": 2.808219178082192e-05,
      "loss": 0.4086,
      "step": 1440
    },
    {
      "epoch": 1.3242009132420092,
      "grad_norm": 0.6436524987220764,
      "learning_rate": 2.792998477929985e-05,
      "loss": 0.3357,
      "step": 1450
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 15.899924278259277,
      "learning_rate": 2.777777777777778e-05,
      "loss": 0.2311,
      "step": 1460
    },
    {
      "epoch": 1.3424657534246576,
      "grad_norm": 4.042705059051514,
      "learning_rate": 2.762557077625571e-05,
      "loss": 0.23,
      "step": 1470
    },
    {
      "epoch": 1.3515981735159817,
      "grad_norm": 3.901237964630127,
      "learning_rate": 2.747336377473364e-05,
      "loss": 0.38,
      "step": 1480
    },
    {
      "epoch": 1.360730593607306,
      "grad_norm": 9.368206024169922,
      "learning_rate": 2.732115677321157e-05,
      "loss": 0.5356,
      "step": 1490
    },
    {
      "epoch": 1.36986301369863,
      "grad_norm": 11.471354484558105,
      "learning_rate": 2.71689497716895e-05,
      "loss": 0.5198,
      "step": 1500
    },
    {
      "epoch": 1.3789954337899544,
      "grad_norm": 1.8336173295974731,
      "learning_rate": 2.701674277016743e-05,
      "loss": 0.3458,
      "step": 1510
    },
    {
      "epoch": 1.3881278538812785,
      "grad_norm": 1.56838059425354,
      "learning_rate": 2.686453576864536e-05,
      "loss": 0.4461,
      "step": 1520
    },
    {
      "epoch": 1.3972602739726028,
      "grad_norm": 6.287276744842529,
      "learning_rate": 2.671232876712329e-05,
      "loss": 0.5814,
      "step": 1530
    },
    {
      "epoch": 1.4063926940639269,
      "grad_norm": 5.450047492980957,
      "learning_rate": 2.656012176560122e-05,
      "loss": 0.3737,
      "step": 1540
    },
    {
      "epoch": 1.4155251141552512,
      "grad_norm": 1.9170119762420654,
      "learning_rate": 2.640791476407915e-05,
      "loss": 0.3313,
      "step": 1550
    },
    {
      "epoch": 1.4246575342465753,
      "grad_norm": 0.8975473642349243,
      "learning_rate": 2.625570776255708e-05,
      "loss": 0.3079,
      "step": 1560
    },
    {
      "epoch": 1.4337899543378996,
      "grad_norm": 6.543036937713623,
      "learning_rate": 2.6103500761035012e-05,
      "loss": 0.3114,
      "step": 1570
    },
    {
      "epoch": 1.4429223744292237,
      "grad_norm": 0.9636650085449219,
      "learning_rate": 2.595129375951294e-05,
      "loss": 0.3503,
      "step": 1580
    },
    {
      "epoch": 1.452054794520548,
      "grad_norm": 7.719810962677002,
      "learning_rate": 2.579908675799087e-05,
      "loss": 0.3979,
      "step": 1590
    },
    {
      "epoch": 1.461187214611872,
      "grad_norm": 7.913337230682373,
      "learning_rate": 2.56468797564688e-05,
      "loss": 0.3471,
      "step": 1600
    },
    {
      "epoch": 1.4703196347031964,
      "grad_norm": 1.5591356754302979,
      "learning_rate": 2.549467275494673e-05,
      "loss": 0.1851,
      "step": 1610
    },
    {
      "epoch": 1.4794520547945205,
      "grad_norm": 5.20556640625,
      "learning_rate": 2.534246575342466e-05,
      "loss": 0.3932,
      "step": 1620
    },
    {
      "epoch": 1.4885844748858448,
      "grad_norm": 0.6447371244430542,
      "learning_rate": 2.519025875190259e-05,
      "loss": 0.286,
      "step": 1630
    },
    {
      "epoch": 1.4977168949771689,
      "grad_norm": 4.751617431640625,
      "learning_rate": 2.503805175038052e-05,
      "loss": 0.3789,
      "step": 1640
    },
    {
      "epoch": 1.5068493150684932,
      "grad_norm": 4.476944446563721,
      "learning_rate": 2.4885844748858446e-05,
      "loss": 0.6007,
      "step": 1650
    },
    {
      "epoch": 1.5159817351598175,
      "grad_norm": 6.40022087097168,
      "learning_rate": 2.4733637747336378e-05,
      "loss": 0.3689,
      "step": 1660
    },
    {
      "epoch": 1.5251141552511416,
      "grad_norm": 3.4018747806549072,
      "learning_rate": 2.4581430745814306e-05,
      "loss": 0.3356,
      "step": 1670
    },
    {
      "epoch": 1.5342465753424657,
      "grad_norm": 0.4293583631515503,
      "learning_rate": 2.4429223744292238e-05,
      "loss": 0.4435,
      "step": 1680
    },
    {
      "epoch": 1.54337899543379,
      "grad_norm": 1.73989737033844,
      "learning_rate": 2.4277016742770166e-05,
      "loss": 0.3521,
      "step": 1690
    },
    {
      "epoch": 1.5525114155251143,
      "grad_norm": 5.923254013061523,
      "learning_rate": 2.4124809741248098e-05,
      "loss": 0.3069,
      "step": 1700
    },
    {
      "epoch": 1.5616438356164384,
      "grad_norm": 3.6125378608703613,
      "learning_rate": 2.3972602739726026e-05,
      "loss": 0.5473,
      "step": 1710
    },
    {
      "epoch": 1.5707762557077625,
      "grad_norm": 8.213759422302246,
      "learning_rate": 2.3820395738203958e-05,
      "loss": 0.3414,
      "step": 1720
    },
    {
      "epoch": 1.5799086757990868,
      "grad_norm": 4.682781219482422,
      "learning_rate": 2.3668188736681887e-05,
      "loss": 0.2706,
      "step": 1730
    },
    {
      "epoch": 1.589041095890411,
      "grad_norm": 5.255059719085693,
      "learning_rate": 2.351598173515982e-05,
      "loss": 0.4048,
      "step": 1740
    },
    {
      "epoch": 1.5981735159817352,
      "grad_norm": 3.975907802581787,
      "learning_rate": 2.3363774733637747e-05,
      "loss": 0.442,
      "step": 1750
    },
    {
      "epoch": 1.6073059360730593,
      "grad_norm": 14.055427551269531,
      "learning_rate": 2.321156773211568e-05,
      "loss": 0.4312,
      "step": 1760
    },
    {
      "epoch": 1.6164383561643836,
      "grad_norm": 5.384091377258301,
      "learning_rate": 2.3059360730593607e-05,
      "loss": 0.4199,
      "step": 1770
    },
    {
      "epoch": 1.625570776255708,
      "grad_norm": 3.971848964691162,
      "learning_rate": 2.290715372907154e-05,
      "loss": 0.2119,
      "step": 1780
    },
    {
      "epoch": 1.634703196347032,
      "grad_norm": 6.963213920593262,
      "learning_rate": 2.2754946727549467e-05,
      "loss": 0.4687,
      "step": 1790
    },
    {
      "epoch": 1.643835616438356,
      "grad_norm": 9.165295600891113,
      "learning_rate": 2.2602739726027396e-05,
      "loss": 0.3066,
      "step": 1800
    },
    {
      "epoch": 1.6529680365296804,
      "grad_norm": 6.147500514984131,
      "learning_rate": 2.2450532724505327e-05,
      "loss": 0.4627,
      "step": 1810
    },
    {
      "epoch": 1.6621004566210047,
      "grad_norm": 6.483113765716553,
      "learning_rate": 2.2298325722983256e-05,
      "loss": 0.2675,
      "step": 1820
    },
    {
      "epoch": 1.6712328767123288,
      "grad_norm": 0.41939520835876465,
      "learning_rate": 2.2146118721461187e-05,
      "loss": 0.2717,
      "step": 1830
    },
    {
      "epoch": 1.6803652968036529,
      "grad_norm": 21.9180965423584,
      "learning_rate": 2.1993911719939116e-05,
      "loss": 0.4234,
      "step": 1840
    },
    {
      "epoch": 1.6894977168949772,
      "grad_norm": 9.225008964538574,
      "learning_rate": 2.1841704718417048e-05,
      "loss": 0.2094,
      "step": 1850
    },
    {
      "epoch": 1.6986301369863015,
      "grad_norm": 0.3402805030345917,
      "learning_rate": 2.1689497716894976e-05,
      "loss": 0.3454,
      "step": 1860
    },
    {
      "epoch": 1.7077625570776256,
      "grad_norm": 10.377140045166016,
      "learning_rate": 2.1537290715372908e-05,
      "loss": 0.2238,
      "step": 1870
    },
    {
      "epoch": 1.7168949771689497,
      "grad_norm": 9.097557067871094,
      "learning_rate": 2.1385083713850836e-05,
      "loss": 0.3308,
      "step": 1880
    },
    {
      "epoch": 1.726027397260274,
      "grad_norm": 1.4065568447113037,
      "learning_rate": 2.1232876712328768e-05,
      "loss": 0.2213,
      "step": 1890
    },
    {
      "epoch": 1.7351598173515983,
      "grad_norm": 5.801243305206299,
      "learning_rate": 2.1080669710806696e-05,
      "loss": 0.3694,
      "step": 1900
    },
    {
      "epoch": 1.7442922374429224,
      "grad_norm": 4.0137104988098145,
      "learning_rate": 2.0928462709284628e-05,
      "loss": 0.3053,
      "step": 1910
    },
    {
      "epoch": 1.7534246575342465,
      "grad_norm": 0.32009443640708923,
      "learning_rate": 2.0776255707762557e-05,
      "loss": 0.3622,
      "step": 1920
    },
    {
      "epoch": 1.7625570776255708,
      "grad_norm": 10.47020149230957,
      "learning_rate": 2.062404870624049e-05,
      "loss": 0.2743,
      "step": 1930
    },
    {
      "epoch": 1.771689497716895,
      "grad_norm": 2.0995447635650635,
      "learning_rate": 2.0471841704718417e-05,
      "loss": 0.212,
      "step": 1940
    },
    {
      "epoch": 1.7808219178082192,
      "grad_norm": 7.796110153198242,
      "learning_rate": 2.0319634703196345e-05,
      "loss": 0.3333,
      "step": 1950
    },
    {
      "epoch": 1.7899543378995433,
      "grad_norm": 8.493135452270508,
      "learning_rate": 2.0167427701674277e-05,
      "loss": 0.3254,
      "step": 1960
    },
    {
      "epoch": 1.7990867579908676,
      "grad_norm": 4.386829853057861,
      "learning_rate": 2.0015220700152205e-05,
      "loss": 0.2394,
      "step": 1970
    },
    {
      "epoch": 1.808219178082192,
      "grad_norm": 5.166952610015869,
      "learning_rate": 1.9863013698630137e-05,
      "loss": 0.401,
      "step": 1980
    },
    {
      "epoch": 1.817351598173516,
      "grad_norm": 10.313128471374512,
      "learning_rate": 1.9710806697108066e-05,
      "loss": 0.3607,
      "step": 1990
    },
    {
      "epoch": 1.82648401826484,
      "grad_norm": 0.5529770851135254,
      "learning_rate": 1.9558599695585997e-05,
      "loss": 0.6125,
      "step": 2000
    },
    {
      "epoch": 1.8356164383561644,
      "grad_norm": 0.3939063847064972,
      "learning_rate": 1.9406392694063926e-05,
      "loss": 0.2895,
      "step": 2010
    },
    {
      "epoch": 1.8447488584474887,
      "grad_norm": 5.846743106842041,
      "learning_rate": 1.9254185692541858e-05,
      "loss": 0.4426,
      "step": 2020
    },
    {
      "epoch": 1.8538812785388128,
      "grad_norm": 1.998340129852295,
      "learning_rate": 1.9101978691019786e-05,
      "loss": 0.2993,
      "step": 2030
    },
    {
      "epoch": 1.8630136986301369,
      "grad_norm": 9.049518585205078,
      "learning_rate": 1.8949771689497718e-05,
      "loss": 0.3049,
      "step": 2040
    },
    {
      "epoch": 1.8721461187214612,
      "grad_norm": 0.5510962605476379,
      "learning_rate": 1.8797564687975646e-05,
      "loss": 0.2789,
      "step": 2050
    },
    {
      "epoch": 1.8812785388127855,
      "grad_norm": 1.2402961254119873,
      "learning_rate": 1.8645357686453578e-05,
      "loss": 0.4075,
      "step": 2060
    },
    {
      "epoch": 1.8904109589041096,
      "grad_norm": 0.7977315783500671,
      "learning_rate": 1.8493150684931506e-05,
      "loss": 0.3516,
      "step": 2070
    },
    {
      "epoch": 1.8995433789954337,
      "grad_norm": 6.818615436553955,
      "learning_rate": 1.8340943683409438e-05,
      "loss": 0.3414,
      "step": 2080
    },
    {
      "epoch": 1.908675799086758,
      "grad_norm": 1.3460421562194824,
      "learning_rate": 1.8188736681887367e-05,
      "loss": 0.2216,
      "step": 2090
    },
    {
      "epoch": 1.9178082191780823,
      "grad_norm": 12.860871315002441,
      "learning_rate": 1.80365296803653e-05,
      "loss": 0.4873,
      "step": 2100
    },
    {
      "epoch": 1.9269406392694064,
      "grad_norm": 9.842572212219238,
      "learning_rate": 1.7884322678843227e-05,
      "loss": 0.3907,
      "step": 2110
    },
    {
      "epoch": 1.9360730593607305,
      "grad_norm": 10.042411804199219,
      "learning_rate": 1.7732115677321155e-05,
      "loss": 0.2042,
      "step": 2120
    },
    {
      "epoch": 1.9452054794520548,
      "grad_norm": 11.531584739685059,
      "learning_rate": 1.7579908675799087e-05,
      "loss": 0.3353,
      "step": 2130
    },
    {
      "epoch": 1.954337899543379,
      "grad_norm": 0.45052847266197205,
      "learning_rate": 1.7427701674277015e-05,
      "loss": 0.2063,
      "step": 2140
    },
    {
      "epoch": 1.9634703196347032,
      "grad_norm": 7.354028701782227,
      "learning_rate": 1.7275494672754947e-05,
      "loss": 0.5363,
      "step": 2150
    },
    {
      "epoch": 1.9726027397260273,
      "grad_norm": 0.5128328204154968,
      "learning_rate": 1.7123287671232875e-05,
      "loss": 0.3625,
      "step": 2160
    },
    {
      "epoch": 1.9817351598173516,
      "grad_norm": 0.5243252515792847,
      "learning_rate": 1.6971080669710807e-05,
      "loss": 0.3739,
      "step": 2170
    },
    {
      "epoch": 1.990867579908676,
      "grad_norm": 0.6587308049201965,
      "learning_rate": 1.6818873668188736e-05,
      "loss": 0.4656,
      "step": 2180
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.5550092458724976,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.2053,
      "step": 2190
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8985838282320694,
      "eval_f1_hate": 0.5274151436031331,
      "eval_f1_idk/skip": 0.0,
      "eval_f1_macro": 0.36805631754635293,
      "eval_f1_noHate": 0.9448101265822785,
      "eval_f1_relation": 0.0,
      "eval_loss": 0.38912519812583923,
      "eval_precision_global": 0.9045792916760659,
      "eval_precision_hate": 0.7062937062937062,
      "eval_precision_idk/skip": 1.0,
      "eval_precision_noHate": 0.9120234604105572,
      "eval_precision_relation": 1.0,
      "eval_recall_global": 0.350218837535014,
      "eval_recall_hate": 0.42083333333333334,
      "eval_recall_idk/skip": 0.0,
      "eval_recall_noHate": 0.9800420168067226,
      "eval_recall_relation": 0.0,
      "eval_runtime": 13.1181,
      "eval_samples_per_second": 166.869,
      "eval_steps_per_second": 20.887,
      "step": 2190
    }
  ],
  "logging_steps": 10,
  "max_steps": 3285,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 473364235048608.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
