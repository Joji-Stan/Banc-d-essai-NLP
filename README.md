## üß† Strat√©gie d'entra√Ænement des mod√®les

Nous avons entra√Æn√© plusieurs mod√®les de classification sur trois jeux de donn√©es : **HateSpeech**, **IMDB**, et **OHSUMED**, en comparant les performances avec et sans l'utilisation de **LoRA (Low-Rank Adaptation)**.

### üìä S√©paration des donn√©es
Chaque dataset a √©t√© divis√© comme suit :
- **80% pour l'entra√Ænement**
- **20% pour l'√©valuation**

Cette r√©partition assure un bon √©quilibre entre apprentissage et validation.

### ‚öôÔ∏è Param√®tres d'entra√Ænement communs
Les entra√Ænements ont √©t√© r√©alis√©s avec les configurations suivantes :
- **Optimiseur** : AdamW
- **Scheduler** : Linear ou Cosine selon les cas
- **Learning Rate** : adapt√© pour chaque mod√®le
- **Weight Decay** : utilis√© pour la r√©gularisation
- **Nombre d‚Äô√©poques** : d√©termin√© pour chaque jeu de donn√©es en fonction de la convergence
- **Gradient Accumulation Steps** : utilis√© pour simuler de plus grands batchs
- **Tokenizer** : `distilbert-base-uncased` (pour les mod√®les DistilBERT)

### üîí Fine-tuning sans LoRA
Pour les versions **sans LoRA**, une approche de fine-tuning partiel a √©t√© utilis√©e :
- **Toutes les couches du mod√®le ont √©t√© gel√©es**, √† l'exception **des deux derni√®res couches**.
- Cette m√©thode permet de r√©duire le temps d'entra√Ænement et la consommation de ressources tout en maintenant une capacit√© d‚Äôadaptation.

### üß© Entra√Ænement avec LoRA
Dans les versions **avec LoRA**, nous avons utilis√© un entra√Ænement avec adaptation √† faible-rang pour am√©liorer l'efficacit√© et la performance tout en utilisant moins de ressources et de temps de calcul.

### üóÇÔ∏è HateSpeech


| **M√©thode**     | **Mod√®le**              | **Accuracy** | **F1 (macro)** | **Precision** | **Recall** | **Loss** | **Epochs** | **Batch (train/eval)** | **LR**   | **Weight Decay** | **LoRA Config**            | **Tokenizer**              | **Temps/√©poque (s)** | **CO‚ÇÇ (kg)** | **Grad Acc Steps** | **Warmup Steps** | **Scheduler** | **Max Grad Norm** | **Seed** | **GPU** |
|------------------|--------------------------|--------------|----------------|---------------|-------------|-----------|-------------|-------------------------|----------|------------------|----------------------------|-----------------------------|----------------------|--------------|--------------------|------------------|----------------|-------------------|----------|----------|
| Avec LoRA        | distilbert-base-uncased  | 0.9054       | 0.5807         | 0.8878        | 0.5441      | 0.3171    | 3           | 8 / 8                   | 5e-05    | 0.01             | r=8, Œ±=16, dropout=0.1     | distilbert-base-uncased    | 225.9512             | 0.0010       | -                  | -                | -              | -                 | -        | T4       |
| Sans LoRA        | distilbert-base-uncased  | 0.9054       | 0.5807         | 0.8878        | 0.5441      | 0.3171    | 3           | 8 / 8                   | 5e-05    | 0.01             | non utilis√©                | distilbert-base-uncased    | 225.9512             | 0.0010       | -                  | -                | -              | -                 | -        | T4       |
| Avec LoRA        | modern-bert              |              |                |               |             |           |             |                         |          |                  |                            |                             |                      |              |                    |                  |                |                   |          |          |
| Sans LoRA        | modern-bert              |              |                |               |             |           |             |                         |          |                  |                            |                             |                      |              |                    |                  |                |                   |          |          |
| Avec LoRA        | deberta-v3-base           | 0.8913       | 0.3718         | 0.8808        | 0.3654      | 0.3820    | 3           | 8 / 8                   | 5e-05    | 0.01             | r=8, Œ±=16, dropout=0.1     | deberta-v3-base         | 468.1251             | 0.0071       | -                  | -                | -              | -                 | -        | T4       |
| Sans LoRA        | deberta-v3-base           | 0.9018       | 0.5673         | 0.8519        | 0.5406      | 0.3417    | 3           | 8 / 8                   | 5e-05    | 0.01             | non utilis√©                | deberta-v3-base         | 252.1042             | 0.0037       | -                  | -                | -              | -                 | -        | T4       |

### üóÇÔ∏è IMDB

| **M√©thode**     | **Mod√®le**              | **Accuracy** | **F1 (macro)** | **Precision** | **Recall** | **Loss** | **Epochs** | **Batch (train/eval)** | **LR**   | **Weight Decay** | **LoRA Config**            | **Tokenizer**              | **Temps/√©poque (s)** | **CO‚ÇÇ (kg)** | **Grad Acc Steps** | **Warmup Steps** | **Scheduler** | **Max Grad Norm** | **Seed** | **GPU** |
|------------------|--------------------------|--------------|----------------|---------------|-------------|-----------|-------------|-------------------------|----------|------------------|----------------------------|-----------------------------|----------------------|--------------|--------------------|------------------|----------------|-------------------|----------|----------|
| Avec LoRA        | distilbert-base-uncased  | 0.9026       | 0.9026         | 0.9026        | 0.9027      | 0.2434    | 5           | 32 / 32                 | 3e-05    | 0.05             | r=8, Œ±=16, dropout=0.1     | distilbert-base-uncased    | 1136.7510            | 0.0104       | 2                  | 1000             | COSINE         | 0.8               | 123      | T4       |
| Sans LoRA        | distilbert-base-uncased  | 0.9230       | 0.9230         | 0.9230        | 0.9230      | 0.1996    | 5           | 32 / 32                 | 3e-05    | 0.05             | non utilis√©                | distilbert-base-uncased    | 733.0113             | 0.0067       | 2                  | 1000             | COSINE         | 0.8               | 123      | T4       |
| Avec LoRA        | modern-bert              |              |                |               |             |           |             |                         |          |                  |                            |                             |                      |              |                    |                  |                |                   |          |          |
| Sans LoRA        | modern-bert              |              |                |               |             |           |             |                         |          |                  |                            |                             |                      |              |                    |                  |                |                   |          |          |
| Avec LoRA        | deberta-v3-base           | 0.9354       | 0.9354         | 0.9354        | 0.9354      | 0.2160    | 1           | 8 / 8                   | 5e-05    | 0.01             | r=8, Œ±=16, dropout=0.1     | deberta-v3-base         | 1220.1435            | 0.0138       | -                  | -                | -              | -                 | -        | T4       |
| Sans LoRA        | deberta-v3-base           | 0.9412       | 0.9412         | 0.9413        | 0.9411      | 0.1846    | 1           | 8 / 8                   | 5e-05    | 0.01             | non utilis√©                | deberta-v3-base         | 709.3585             | 0.0080       | -                  | -                | -              | -                 | -        | T4       |

### üóÇÔ∏è OHSUMED

| **M√©thode**     | **Mod√®le**              | **Accuracy** | **F1 (macro)** | **Precision** | **Recall** | **Loss** | **Epochs** | **Batch (train/eval)** | **LR**   | **Weight Decay** | **LoRA Config**            | **Tokenizer**              | **Temps/√©poque (s)** | **CO‚ÇÇ (kg)** | **Grad Acc Steps** | **Warmup Steps** | **Scheduler** | **Max Grad Norm** | **Seed** | **GPU** |
|------------------|--------------------------|--------------|----------------|---------------|-------------|-----------|-------------|-------------------------|----------|------------------|----------------------------|-----------------------------|----------------------|--------------|--------------------|------------------|----------------|-------------------|----------|----------|
| Avec LoRA        | distilbert-base-uncased  | 0.4727       | 0.4385         | 0.4402        | 0.4558      | 1.5091    | 3           | 8 / 8                   | 5e-05    | 0.01             | r=8, Œ±=16, dropout=0.1     | distilbert-base-uncased    | 1312.6677            | 0.0147       | -                  | -                | -              | -                 | -        | T4       |
| Sans LoRA        | distilbert-base-uncased  | 0.4702       | 0.4367         | 0.4348        | 0.4507      | 1.5054    | 3           | 8 / 8                   | 5e-05    | 0.01             | non utilis√©                | distilbert-base-uncased    | 1229.2075            | 0.0184       | -                  | -                | -              | -                 | -        | T4       |
| Avec LoRA        | modern-bert              |              |                |               |             |           |             |                         |          |                  |                            |                             |                      |              |                    |                  |                |                   |          |          |
| Sans LoRA        | modern-bert              |              |                |               |             |           |             |                         |          |                  |                            |                             |                      |              |                    |                  |                |                   |          |          |
| Avec LoRA        | deberta                  |              |                |               |             |           |             |                         |          |                  |                            |                             |                      |              |                    |                  |                |                   |          |          |
| Sans LoRA        | deberta                  |              |                |               |             |           |             |                         |          |                  |                            |                             |                      |              |                    |                  |                |                   |          |          |