{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 5700,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007017543859649123,
      "grad_norm": 1.2604175806045532,
      "learning_rate": 3.994385964912281e-05,
      "loss": 3.1428,
      "step": 10
    },
    {
      "epoch": 0.014035087719298246,
      "grad_norm": 1.3774422407150269,
      "learning_rate": 3.988771929824561e-05,
      "loss": 3.1305,
      "step": 20
    },
    {
      "epoch": 0.021052631578947368,
      "grad_norm": 1.4562627077102661,
      "learning_rate": 3.9831578947368425e-05,
      "loss": 3.1109,
      "step": 30
    },
    {
      "epoch": 0.028070175438596492,
      "grad_norm": 1.4487625360488892,
      "learning_rate": 3.977543859649123e-05,
      "loss": 3.0808,
      "step": 40
    },
    {
      "epoch": 0.03508771929824561,
      "grad_norm": 1.5011128187179565,
      "learning_rate": 3.9719298245614034e-05,
      "loss": 3.0743,
      "step": 50
    },
    {
      "epoch": 0.042105263157894736,
      "grad_norm": 1.277342438697815,
      "learning_rate": 3.9663157894736846e-05,
      "loss": 3.0305,
      "step": 60
    },
    {
      "epoch": 0.04912280701754386,
      "grad_norm": 1.5399539470672607,
      "learning_rate": 3.960701754385965e-05,
      "loss": 2.9921,
      "step": 70
    },
    {
      "epoch": 0.056140350877192984,
      "grad_norm": 1.383981466293335,
      "learning_rate": 3.955087719298246e-05,
      "loss": 2.9621,
      "step": 80
    },
    {
      "epoch": 0.06315789473684211,
      "grad_norm": 1.611066460609436,
      "learning_rate": 3.949473684210527e-05,
      "loss": 2.913,
      "step": 90
    },
    {
      "epoch": 0.07017543859649122,
      "grad_norm": 1.7793769836425781,
      "learning_rate": 3.943859649122807e-05,
      "loss": 2.8376,
      "step": 100
    },
    {
      "epoch": 0.07719298245614035,
      "grad_norm": 1.577679991722107,
      "learning_rate": 3.9382456140350883e-05,
      "loss": 2.8608,
      "step": 110
    },
    {
      "epoch": 0.08421052631578947,
      "grad_norm": 2.0095579624176025,
      "learning_rate": 3.932631578947369e-05,
      "loss": 2.8454,
      "step": 120
    },
    {
      "epoch": 0.0912280701754386,
      "grad_norm": 1.786095142364502,
      "learning_rate": 3.927017543859649e-05,
      "loss": 2.8373,
      "step": 130
    },
    {
      "epoch": 0.09824561403508772,
      "grad_norm": 1.9585458040237427,
      "learning_rate": 3.92140350877193e-05,
      "loss": 2.8047,
      "step": 140
    },
    {
      "epoch": 0.10526315789473684,
      "grad_norm": 1.7891299724578857,
      "learning_rate": 3.915789473684211e-05,
      "loss": 2.8204,
      "step": 150
    },
    {
      "epoch": 0.11228070175438597,
      "grad_norm": 2.3338568210601807,
      "learning_rate": 3.9101754385964914e-05,
      "loss": 2.7982,
      "step": 160
    },
    {
      "epoch": 0.11929824561403508,
      "grad_norm": 1.973358392715454,
      "learning_rate": 3.904561403508772e-05,
      "loss": 2.8458,
      "step": 170
    },
    {
      "epoch": 0.12631578947368421,
      "grad_norm": 2.152691125869751,
      "learning_rate": 3.898947368421053e-05,
      "loss": 2.8232,
      "step": 180
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 2.107297658920288,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 2.7975,
      "step": 190
    },
    {
      "epoch": 0.14035087719298245,
      "grad_norm": 2.069648265838623,
      "learning_rate": 3.887719298245615e-05,
      "loss": 2.8476,
      "step": 200
    },
    {
      "epoch": 0.14736842105263157,
      "grad_norm": 1.7535184621810913,
      "learning_rate": 3.882105263157895e-05,
      "loss": 2.8507,
      "step": 210
    },
    {
      "epoch": 0.1543859649122807,
      "grad_norm": 1.8440556526184082,
      "learning_rate": 3.876491228070176e-05,
      "loss": 2.845,
      "step": 220
    },
    {
      "epoch": 0.16140350877192983,
      "grad_norm": 2.739072322845459,
      "learning_rate": 3.870877192982457e-05,
      "loss": 2.7908,
      "step": 230
    },
    {
      "epoch": 0.16842105263157894,
      "grad_norm": 2.2442898750305176,
      "learning_rate": 3.865263157894737e-05,
      "loss": 2.8799,
      "step": 240
    },
    {
      "epoch": 0.17543859649122806,
      "grad_norm": 1.558708906173706,
      "learning_rate": 3.859649122807018e-05,
      "loss": 2.8199,
      "step": 250
    },
    {
      "epoch": 0.1824561403508772,
      "grad_norm": 2.080963611602783,
      "learning_rate": 3.854035087719298e-05,
      "loss": 2.8463,
      "step": 260
    },
    {
      "epoch": 0.18947368421052632,
      "grad_norm": 1.878458023071289,
      "learning_rate": 3.848421052631579e-05,
      "loss": 2.7909,
      "step": 270
    },
    {
      "epoch": 0.19649122807017544,
      "grad_norm": 2.703425884246826,
      "learning_rate": 3.84280701754386e-05,
      "loss": 2.8115,
      "step": 280
    },
    {
      "epoch": 0.20350877192982456,
      "grad_norm": 2.2161977291107178,
      "learning_rate": 3.8371929824561404e-05,
      "loss": 2.8731,
      "step": 290
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 1.74897038936615,
      "learning_rate": 3.8315789473684216e-05,
      "loss": 2.8434,
      "step": 300
    },
    {
      "epoch": 0.21754385964912282,
      "grad_norm": 2.2848715782165527,
      "learning_rate": 3.825964912280702e-05,
      "loss": 2.8003,
      "step": 310
    },
    {
      "epoch": 0.22456140350877193,
      "grad_norm": 1.5549567937850952,
      "learning_rate": 3.820350877192983e-05,
      "loss": 2.7947,
      "step": 320
    },
    {
      "epoch": 0.23157894736842105,
      "grad_norm": 2.6425788402557373,
      "learning_rate": 3.814736842105264e-05,
      "loss": 2.8205,
      "step": 330
    },
    {
      "epoch": 0.23859649122807017,
      "grad_norm": 1.802551507949829,
      "learning_rate": 3.809122807017544e-05,
      "loss": 2.7145,
      "step": 340
    },
    {
      "epoch": 0.24561403508771928,
      "grad_norm": 2.1852502822875977,
      "learning_rate": 3.8035087719298247e-05,
      "loss": 2.8111,
      "step": 350
    },
    {
      "epoch": 0.25263157894736843,
      "grad_norm": 2.368276357650757,
      "learning_rate": 3.797894736842106e-05,
      "loss": 2.8463,
      "step": 360
    },
    {
      "epoch": 0.2596491228070175,
      "grad_norm": 2.226445198059082,
      "learning_rate": 3.792280701754386e-05,
      "loss": 2.9111,
      "step": 370
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.7844003438949585,
      "learning_rate": 3.786666666666667e-05,
      "loss": 2.7675,
      "step": 380
    },
    {
      "epoch": 0.2736842105263158,
      "grad_norm": 2.330906629562378,
      "learning_rate": 3.781052631578947e-05,
      "loss": 2.8501,
      "step": 390
    },
    {
      "epoch": 0.2807017543859649,
      "grad_norm": 2.0081257820129395,
      "learning_rate": 3.7754385964912284e-05,
      "loss": 2.832,
      "step": 400
    },
    {
      "epoch": 0.28771929824561404,
      "grad_norm": 2.00434947013855,
      "learning_rate": 3.769824561403509e-05,
      "loss": 2.8165,
      "step": 410
    },
    {
      "epoch": 0.29473684210526313,
      "grad_norm": 1.8228042125701904,
      "learning_rate": 3.76421052631579e-05,
      "loss": 2.8218,
      "step": 420
    },
    {
      "epoch": 0.3017543859649123,
      "grad_norm": 2.1645519733428955,
      "learning_rate": 3.7585964912280705e-05,
      "loss": 2.7658,
      "step": 430
    },
    {
      "epoch": 0.3087719298245614,
      "grad_norm": 2.1925501823425293,
      "learning_rate": 3.752982456140352e-05,
      "loss": 2.7958,
      "step": 440
    },
    {
      "epoch": 0.3157894736842105,
      "grad_norm": 2.2661092281341553,
      "learning_rate": 3.747368421052632e-05,
      "loss": 2.7793,
      "step": 450
    },
    {
      "epoch": 0.32280701754385965,
      "grad_norm": 2.250624895095825,
      "learning_rate": 3.741754385964913e-05,
      "loss": 2.8316,
      "step": 460
    },
    {
      "epoch": 0.3298245614035088,
      "grad_norm": 2.080410957336426,
      "learning_rate": 3.736140350877193e-05,
      "loss": 2.7856,
      "step": 470
    },
    {
      "epoch": 0.3368421052631579,
      "grad_norm": 1.7985458374023438,
      "learning_rate": 3.7305263157894736e-05,
      "loss": 2.7606,
      "step": 480
    },
    {
      "epoch": 0.34385964912280703,
      "grad_norm": 2.2048001289367676,
      "learning_rate": 3.724912280701755e-05,
      "loss": 2.7462,
      "step": 490
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 2.1360793113708496,
      "learning_rate": 3.719298245614035e-05,
      "loss": 2.784,
      "step": 500
    },
    {
      "epoch": 0.35789473684210527,
      "grad_norm": 2.2884340286254883,
      "learning_rate": 3.713684210526316e-05,
      "loss": 2.732,
      "step": 510
    },
    {
      "epoch": 0.3649122807017544,
      "grad_norm": 2.053619384765625,
      "learning_rate": 3.708070175438597e-05,
      "loss": 2.7212,
      "step": 520
    },
    {
      "epoch": 0.3719298245614035,
      "grad_norm": 2.4182064533233643,
      "learning_rate": 3.7024561403508774e-05,
      "loss": 2.7451,
      "step": 530
    },
    {
      "epoch": 0.37894736842105264,
      "grad_norm": 3.0081398487091064,
      "learning_rate": 3.6968421052631586e-05,
      "loss": 2.6418,
      "step": 540
    },
    {
      "epoch": 0.38596491228070173,
      "grad_norm": 2.160547971725464,
      "learning_rate": 3.691228070175439e-05,
      "loss": 2.7656,
      "step": 550
    },
    {
      "epoch": 0.3929824561403509,
      "grad_norm": 1.9496092796325684,
      "learning_rate": 3.6856140350877195e-05,
      "loss": 2.7134,
      "step": 560
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.012284278869629,
      "learning_rate": 3.680000000000001e-05,
      "loss": 2.7416,
      "step": 570
    },
    {
      "epoch": 0.4070175438596491,
      "grad_norm": 2.2347826957702637,
      "learning_rate": 3.674385964912281e-05,
      "loss": 2.7713,
      "step": 580
    },
    {
      "epoch": 0.41403508771929826,
      "grad_norm": 2.124694585800171,
      "learning_rate": 3.6687719298245616e-05,
      "loss": 2.736,
      "step": 590
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 2.7977211475372314,
      "learning_rate": 3.663157894736842e-05,
      "loss": 2.7214,
      "step": 600
    },
    {
      "epoch": 0.4280701754385965,
      "grad_norm": 2.994447946548462,
      "learning_rate": 3.657543859649123e-05,
      "loss": 2.7129,
      "step": 610
    },
    {
      "epoch": 0.43508771929824563,
      "grad_norm": 1.8589369058609009,
      "learning_rate": 3.651929824561404e-05,
      "loss": 2.6292,
      "step": 620
    },
    {
      "epoch": 0.4421052631578947,
      "grad_norm": 2.6086878776550293,
      "learning_rate": 3.646315789473684e-05,
      "loss": 2.711,
      "step": 630
    },
    {
      "epoch": 0.44912280701754387,
      "grad_norm": 2.4385807514190674,
      "learning_rate": 3.640701754385965e-05,
      "loss": 2.7657,
      "step": 640
    },
    {
      "epoch": 0.45614035087719296,
      "grad_norm": 1.9509507417678833,
      "learning_rate": 3.635087719298246e-05,
      "loss": 2.6432,
      "step": 650
    },
    {
      "epoch": 0.4631578947368421,
      "grad_norm": 2.312866449356079,
      "learning_rate": 3.6294736842105264e-05,
      "loss": 2.5241,
      "step": 660
    },
    {
      "epoch": 0.47017543859649125,
      "grad_norm": 2.4003210067749023,
      "learning_rate": 3.6238596491228075e-05,
      "loss": 2.7032,
      "step": 670
    },
    {
      "epoch": 0.47719298245614034,
      "grad_norm": 2.3473784923553467,
      "learning_rate": 3.618245614035088e-05,
      "loss": 2.6075,
      "step": 680
    },
    {
      "epoch": 0.4842105263157895,
      "grad_norm": 2.4621615409851074,
      "learning_rate": 3.612631578947369e-05,
      "loss": 2.7459,
      "step": 690
    },
    {
      "epoch": 0.49122807017543857,
      "grad_norm": 2.4075381755828857,
      "learning_rate": 3.6070175438596497e-05,
      "loss": 2.5369,
      "step": 700
    },
    {
      "epoch": 0.4982456140350877,
      "grad_norm": 1.8976678848266602,
      "learning_rate": 3.60140350877193e-05,
      "loss": 2.653,
      "step": 710
    },
    {
      "epoch": 0.5052631578947369,
      "grad_norm": 2.1373486518859863,
      "learning_rate": 3.5957894736842106e-05,
      "loss": 2.6255,
      "step": 720
    },
    {
      "epoch": 0.512280701754386,
      "grad_norm": 2.804265260696411,
      "learning_rate": 3.590175438596491e-05,
      "loss": 2.6311,
      "step": 730
    },
    {
      "epoch": 0.519298245614035,
      "grad_norm": 3.347088575363159,
      "learning_rate": 3.584561403508772e-05,
      "loss": 2.5796,
      "step": 740
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 3.2459871768951416,
      "learning_rate": 3.578947368421053e-05,
      "loss": 2.612,
      "step": 750
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.4887938499450684,
      "learning_rate": 3.573333333333333e-05,
      "loss": 2.6504,
      "step": 760
    },
    {
      "epoch": 0.5403508771929825,
      "grad_norm": 3.0579938888549805,
      "learning_rate": 3.5677192982456144e-05,
      "loss": 2.6607,
      "step": 770
    },
    {
      "epoch": 0.5473684210526316,
      "grad_norm": 2.490828037261963,
      "learning_rate": 3.562105263157895e-05,
      "loss": 2.7033,
      "step": 780
    },
    {
      "epoch": 0.5543859649122806,
      "grad_norm": 2.008807420730591,
      "learning_rate": 3.556491228070176e-05,
      "loss": 2.7719,
      "step": 790
    },
    {
      "epoch": 0.5614035087719298,
      "grad_norm": 2.113839626312256,
      "learning_rate": 3.5508771929824565e-05,
      "loss": 2.6976,
      "step": 800
    },
    {
      "epoch": 0.5684210526315789,
      "grad_norm": 1.725063443183899,
      "learning_rate": 3.545263157894737e-05,
      "loss": 2.6096,
      "step": 810
    },
    {
      "epoch": 0.5754385964912281,
      "grad_norm": 2.627526044845581,
      "learning_rate": 3.539649122807018e-05,
      "loss": 2.644,
      "step": 820
    },
    {
      "epoch": 0.5824561403508772,
      "grad_norm": 2.834496259689331,
      "learning_rate": 3.5340350877192986e-05,
      "loss": 2.6244,
      "step": 830
    },
    {
      "epoch": 0.5894736842105263,
      "grad_norm": 2.663980007171631,
      "learning_rate": 3.528421052631579e-05,
      "loss": 2.6561,
      "step": 840
    },
    {
      "epoch": 0.5964912280701754,
      "grad_norm": 2.16851806640625,
      "learning_rate": 3.5228070175438596e-05,
      "loss": 2.7719,
      "step": 850
    },
    {
      "epoch": 0.6035087719298246,
      "grad_norm": 2.2657828330993652,
      "learning_rate": 3.517192982456141e-05,
      "loss": 2.6184,
      "step": 860
    },
    {
      "epoch": 0.6105263157894737,
      "grad_norm": 3.8279430866241455,
      "learning_rate": 3.511578947368421e-05,
      "loss": 2.5488,
      "step": 870
    },
    {
      "epoch": 0.6175438596491228,
      "grad_norm": 2.2188053131103516,
      "learning_rate": 3.505964912280702e-05,
      "loss": 2.7421,
      "step": 880
    },
    {
      "epoch": 0.624561403508772,
      "grad_norm": 3.396851062774658,
      "learning_rate": 3.500350877192983e-05,
      "loss": 2.6973,
      "step": 890
    },
    {
      "epoch": 0.631578947368421,
      "grad_norm": 2.3508105278015137,
      "learning_rate": 3.4947368421052634e-05,
      "loss": 2.6578,
      "step": 900
    },
    {
      "epoch": 0.6385964912280702,
      "grad_norm": 2.473639488220215,
      "learning_rate": 3.4891228070175445e-05,
      "loss": 2.7415,
      "step": 910
    },
    {
      "epoch": 0.6456140350877193,
      "grad_norm": 2.55133318901062,
      "learning_rate": 3.483508771929825e-05,
      "loss": 2.6536,
      "step": 920
    },
    {
      "epoch": 0.6526315789473685,
      "grad_norm": 2.5952935218811035,
      "learning_rate": 3.4778947368421055e-05,
      "loss": 2.6127,
      "step": 930
    },
    {
      "epoch": 0.6596491228070176,
      "grad_norm": 2.2811100482940674,
      "learning_rate": 3.472280701754386e-05,
      "loss": 2.5652,
      "step": 940
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 2.6754233837127686,
      "learning_rate": 3.466666666666667e-05,
      "loss": 2.6682,
      "step": 950
    },
    {
      "epoch": 0.6736842105263158,
      "grad_norm": 2.2592999935150146,
      "learning_rate": 3.4610526315789476e-05,
      "loss": 2.5958,
      "step": 960
    },
    {
      "epoch": 0.6807017543859649,
      "grad_norm": 2.110684871673584,
      "learning_rate": 3.455438596491228e-05,
      "loss": 2.6584,
      "step": 970
    },
    {
      "epoch": 0.6877192982456141,
      "grad_norm": 3.2575292587280273,
      "learning_rate": 3.4498245614035086e-05,
      "loss": 2.5861,
      "step": 980
    },
    {
      "epoch": 0.6947368421052632,
      "grad_norm": 2.5890519618988037,
      "learning_rate": 3.44421052631579e-05,
      "loss": 2.6398,
      "step": 990
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 2.7118802070617676,
      "learning_rate": 3.43859649122807e-05,
      "loss": 2.6392,
      "step": 1000
    },
    {
      "epoch": 0.7087719298245614,
      "grad_norm": 2.4427032470703125,
      "learning_rate": 3.4329824561403514e-05,
      "loss": 2.6997,
      "step": 1010
    },
    {
      "epoch": 0.7157894736842105,
      "grad_norm": 5.150656700134277,
      "learning_rate": 3.427368421052632e-05,
      "loss": 2.5541,
      "step": 1020
    },
    {
      "epoch": 0.7228070175438597,
      "grad_norm": 2.1586952209472656,
      "learning_rate": 3.421754385964913e-05,
      "loss": 2.5476,
      "step": 1030
    },
    {
      "epoch": 0.7298245614035088,
      "grad_norm": 2.615097999572754,
      "learning_rate": 3.4161403508771935e-05,
      "loss": 2.5683,
      "step": 1040
    },
    {
      "epoch": 0.7368421052631579,
      "grad_norm": 2.702409505844116,
      "learning_rate": 3.410526315789474e-05,
      "loss": 2.6038,
      "step": 1050
    },
    {
      "epoch": 0.743859649122807,
      "grad_norm": 4.925928592681885,
      "learning_rate": 3.4049122807017545e-05,
      "loss": 2.6016,
      "step": 1060
    },
    {
      "epoch": 0.7508771929824561,
      "grad_norm": 3.0463132858276367,
      "learning_rate": 3.3992982456140356e-05,
      "loss": 2.5698,
      "step": 1070
    },
    {
      "epoch": 0.7578947368421053,
      "grad_norm": 3.2607173919677734,
      "learning_rate": 3.393684210526316e-05,
      "loss": 2.5488,
      "step": 1080
    },
    {
      "epoch": 0.7649122807017544,
      "grad_norm": 4.5317792892456055,
      "learning_rate": 3.3880701754385966e-05,
      "loss": 2.5297,
      "step": 1090
    },
    {
      "epoch": 0.7719298245614035,
      "grad_norm": 3.968909502029419,
      "learning_rate": 3.382456140350877e-05,
      "loss": 2.6739,
      "step": 1100
    },
    {
      "epoch": 0.7789473684210526,
      "grad_norm": 2.962228536605835,
      "learning_rate": 3.376842105263158e-05,
      "loss": 2.5119,
      "step": 1110
    },
    {
      "epoch": 0.7859649122807018,
      "grad_norm": 2.5637850761413574,
      "learning_rate": 3.371228070175439e-05,
      "loss": 2.6663,
      "step": 1120
    },
    {
      "epoch": 0.7929824561403509,
      "grad_norm": 3.1981775760650635,
      "learning_rate": 3.36561403508772e-05,
      "loss": 2.5304,
      "step": 1130
    },
    {
      "epoch": 0.8,
      "grad_norm": 4.480305194854736,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 2.5741,
      "step": 1140
    },
    {
      "epoch": 0.8070175438596491,
      "grad_norm": 2.4116127490997314,
      "learning_rate": 3.3543859649122815e-05,
      "loss": 2.4903,
      "step": 1150
    },
    {
      "epoch": 0.8140350877192982,
      "grad_norm": 3.944413900375366,
      "learning_rate": 3.348771929824562e-05,
      "loss": 2.5365,
      "step": 1160
    },
    {
      "epoch": 0.8210526315789474,
      "grad_norm": 4.275918960571289,
      "learning_rate": 3.3431578947368425e-05,
      "loss": 2.5737,
      "step": 1170
    },
    {
      "epoch": 0.8280701754385965,
      "grad_norm": 3.6713762283325195,
      "learning_rate": 3.337543859649123e-05,
      "loss": 2.5388,
      "step": 1180
    },
    {
      "epoch": 0.8350877192982457,
      "grad_norm": 2.3156754970550537,
      "learning_rate": 3.3319298245614034e-05,
      "loss": 2.4443,
      "step": 1190
    },
    {
      "epoch": 0.8421052631578947,
      "grad_norm": 3.9870316982269287,
      "learning_rate": 3.3263157894736846e-05,
      "loss": 2.6256,
      "step": 1200
    },
    {
      "epoch": 0.8491228070175438,
      "grad_norm": 3.9808921813964844,
      "learning_rate": 3.320701754385965e-05,
      "loss": 2.6013,
      "step": 1210
    },
    {
      "epoch": 0.856140350877193,
      "grad_norm": 3.728199005126953,
      "learning_rate": 3.3150877192982456e-05,
      "loss": 2.5283,
      "step": 1220
    },
    {
      "epoch": 0.8631578947368421,
      "grad_norm": 4.356791019439697,
      "learning_rate": 3.309473684210527e-05,
      "loss": 2.5084,
      "step": 1230
    },
    {
      "epoch": 0.8701754385964913,
      "grad_norm": 3.26163387298584,
      "learning_rate": 3.303859649122807e-05,
      "loss": 2.5304,
      "step": 1240
    },
    {
      "epoch": 0.8771929824561403,
      "grad_norm": 4.752171993255615,
      "learning_rate": 3.298245614035088e-05,
      "loss": 2.68,
      "step": 1250
    },
    {
      "epoch": 0.8842105263157894,
      "grad_norm": 3.594406843185425,
      "learning_rate": 3.292631578947369e-05,
      "loss": 2.5157,
      "step": 1260
    },
    {
      "epoch": 0.8912280701754386,
      "grad_norm": 4.665381908416748,
      "learning_rate": 3.287017543859649e-05,
      "loss": 2.5215,
      "step": 1270
    },
    {
      "epoch": 0.8982456140350877,
      "grad_norm": 3.9278173446655273,
      "learning_rate": 3.2814035087719305e-05,
      "loss": 2.5225,
      "step": 1280
    },
    {
      "epoch": 0.9052631578947369,
      "grad_norm": 4.352803707122803,
      "learning_rate": 3.275789473684211e-05,
      "loss": 2.5333,
      "step": 1290
    },
    {
      "epoch": 0.9122807017543859,
      "grad_norm": 3.2656311988830566,
      "learning_rate": 3.2701754385964915e-05,
      "loss": 2.4274,
      "step": 1300
    },
    {
      "epoch": 0.9192982456140351,
      "grad_norm": 4.750765800476074,
      "learning_rate": 3.264561403508772e-05,
      "loss": 2.5407,
      "step": 1310
    },
    {
      "epoch": 0.9263157894736842,
      "grad_norm": 4.201137065887451,
      "learning_rate": 3.258947368421053e-05,
      "loss": 2.4999,
      "step": 1320
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 4.61777400970459,
      "learning_rate": 3.2533333333333336e-05,
      "loss": 2.4864,
      "step": 1330
    },
    {
      "epoch": 0.9403508771929825,
      "grad_norm": 5.378518581390381,
      "learning_rate": 3.247719298245614e-05,
      "loss": 2.5005,
      "step": 1340
    },
    {
      "epoch": 0.9473684210526315,
      "grad_norm": 3.8440842628479004,
      "learning_rate": 3.2421052631578945e-05,
      "loss": 2.475,
      "step": 1350
    },
    {
      "epoch": 0.9543859649122807,
      "grad_norm": 2.9044268131256104,
      "learning_rate": 3.236491228070176e-05,
      "loss": 2.4692,
      "step": 1360
    },
    {
      "epoch": 0.9614035087719298,
      "grad_norm": 5.908802032470703,
      "learning_rate": 3.230877192982456e-05,
      "loss": 2.4603,
      "step": 1370
    },
    {
      "epoch": 0.968421052631579,
      "grad_norm": 8.3141508102417,
      "learning_rate": 3.2252631578947373e-05,
      "loss": 2.3701,
      "step": 1380
    },
    {
      "epoch": 0.9754385964912281,
      "grad_norm": 5.039294242858887,
      "learning_rate": 3.219649122807018e-05,
      "loss": 2.5616,
      "step": 1390
    },
    {
      "epoch": 0.9824561403508771,
      "grad_norm": 3.7861204147338867,
      "learning_rate": 3.214035087719299e-05,
      "loss": 2.5694,
      "step": 1400
    },
    {
      "epoch": 0.9894736842105263,
      "grad_norm": 3.851377487182617,
      "learning_rate": 3.2084210526315795e-05,
      "loss": 2.4802,
      "step": 1410
    },
    {
      "epoch": 0.9964912280701754,
      "grad_norm": 4.255686283111572,
      "learning_rate": 3.20280701754386e-05,
      "loss": 2.3554,
      "step": 1420
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.26252522593665,
      "eval_f1_C01": 0.0,
      "eval_f1_C02": 0.0,
      "eval_f1_C03": 0.0,
      "eval_f1_C04": 0.3417624521072797,
      "eval_f1_C05": 0.0,
      "eval_f1_C06": 0.0,
      "eval_f1_C07": 0.0,
      "eval_f1_C08": 0.0,
      "eval_f1_C09": 0.0,
      "eval_f1_C10": 0.0025575447570332483,
      "eval_f1_C11": 0.0,
      "eval_f1_C12": 0.0,
      "eval_f1_C13": 0.0,
      "eval_f1_C14": 0.5859689476710753,
      "eval_f1_C15": 0.0,
      "eval_f1_C16": 0.0,
      "eval_f1_C17": 0.0,
      "eval_f1_C18": 0.1437908496732026,
      "eval_f1_C19": 0.0,
      "eval_f1_C20": 0.0,
      "eval_f1_C21": 0.0,
      "eval_f1_C22": 0.0,
      "eval_f1_C23": 0.28856592540710907,
      "eval_f1_macro": 0.05924546607024782,
      "eval_loss": 2.452338933944702,
      "eval_precision_C01": 1.0,
      "eval_precision_C02": 1.0,
      "eval_precision_C03": 1.0,
      "eval_precision_C04": 0.2120174938201179,
      "eval_precision_C05": 1.0,
      "eval_precision_C06": 0.0,
      "eval_precision_C07": 1.0,
      "eval_precision_C08": 1.0,
      "eval_precision_C09": 1.0,
      "eval_precision_C10": 0.08333333333333333,
      "eval_precision_C11": 1.0,
      "eval_precision_C12": 1.0,
      "eval_precision_C13": 1.0,
      "eval_precision_C14": 0.4512843224092117,
      "eval_precision_C15": 1.0,
      "eval_precision_C16": 1.0,
      "eval_precision_C17": 1.0,
      "eval_precision_C18": 0.44,
      "eval_precision_C19": 1.0,
      "eval_precision_C20": 0.0,
      "eval_precision_C21": 1.0,
      "eval_precision_C22": 1.0,
      "eval_precision_C23": 0.2174716283979942,
      "eval_precision_global": 0.7567002946939416,
      "eval_recall_C01": 0.0,
      "eval_recall_C02": 0.0,
      "eval_recall_C03": 0.0,
      "eval_recall_C04": 0.8807266982622433,
      "eval_recall_C05": 0.0,
      "eval_recall_C06": 0.0,
      "eval_recall_C07": 0.0,
      "eval_recall_C08": 0.0,
      "eval_recall_C09": 0.0,
      "eval_recall_C10": 0.0012987012987012987,
      "eval_recall_C11": 0.0,
      "eval_recall_C12": 0.0,
      "eval_recall_C13": 0.0,
      "eval_recall_C14": 0.8352459016393443,
      "eval_recall_C15": 0.0,
      "eval_recall_C16": 0.0,
      "eval_recall_C17": 0.0,
      "eval_recall_C18": 0.0859375,
      "eval_recall_C19": 0.0,
      "eval_recall_C20": 0.0,
      "eval_recall_C21": 0.0,
      "eval_recall_C22": 0.0,
      "eval_recall_C23": 0.4287200832466181,
      "eval_recall_global": 0.09704038628030032,
      "eval_runtime": 153.8668,
      "eval_samples_per_second": 74.071,
      "eval_steps_per_second": 2.32,
      "step": 1425
    },
    {
      "epoch": 1.0035087719298246,
      "grad_norm": 3.352334976196289,
      "learning_rate": 3.1971929824561404e-05,
      "loss": 2.413,
      "step": 1430
    },
    {
      "epoch": 1.0105263157894737,
      "grad_norm": 3.5179443359375,
      "learning_rate": 3.191578947368421e-05,
      "loss": 2.43,
      "step": 1440
    },
    {
      "epoch": 1.0175438596491229,
      "grad_norm": 5.062627792358398,
      "learning_rate": 3.185964912280702e-05,
      "loss": 2.3661,
      "step": 1450
    },
    {
      "epoch": 1.024561403508772,
      "grad_norm": 5.112011909484863,
      "learning_rate": 3.1803508771929826e-05,
      "loss": 2.437,
      "step": 1460
    },
    {
      "epoch": 1.0315789473684212,
      "grad_norm": 4.127882480621338,
      "learning_rate": 3.174736842105263e-05,
      "loss": 2.3331,
      "step": 1470
    },
    {
      "epoch": 1.03859649122807,
      "grad_norm": 3.2103652954101562,
      "learning_rate": 3.169122807017544e-05,
      "loss": 2.4082,
      "step": 1480
    },
    {
      "epoch": 1.0456140350877192,
      "grad_norm": 4.083942413330078,
      "learning_rate": 3.163508771929825e-05,
      "loss": 2.3657,
      "step": 1490
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 5.568539142608643,
      "learning_rate": 3.157894736842106e-05,
      "loss": 2.4369,
      "step": 1500
    },
    {
      "epoch": 1.0596491228070175,
      "grad_norm": 4.869932174682617,
      "learning_rate": 3.152280701754386e-05,
      "loss": 2.3785,
      "step": 1510
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 4.727568626403809,
      "learning_rate": 3.146666666666667e-05,
      "loss": 2.4251,
      "step": 1520
    },
    {
      "epoch": 1.0736842105263158,
      "grad_norm": 4.278483867645264,
      "learning_rate": 3.141052631578948e-05,
      "loss": 2.3825,
      "step": 1530
    },
    {
      "epoch": 1.080701754385965,
      "grad_norm": 4.590001583099365,
      "learning_rate": 3.1354385964912284e-05,
      "loss": 2.397,
      "step": 1540
    },
    {
      "epoch": 1.087719298245614,
      "grad_norm": 4.267868518829346,
      "learning_rate": 3.129824561403509e-05,
      "loss": 2.4603,
      "step": 1550
    },
    {
      "epoch": 1.0947368421052632,
      "grad_norm": 4.847787857055664,
      "learning_rate": 3.1242105263157894e-05,
      "loss": 2.4594,
      "step": 1560
    },
    {
      "epoch": 1.1017543859649124,
      "grad_norm": 7.898068904876709,
      "learning_rate": 3.1185964912280706e-05,
      "loss": 2.4067,
      "step": 1570
    },
    {
      "epoch": 1.1087719298245613,
      "grad_norm": 6.920315742492676,
      "learning_rate": 3.112982456140351e-05,
      "loss": 2.318,
      "step": 1580
    },
    {
      "epoch": 1.1157894736842104,
      "grad_norm": 5.103217601776123,
      "learning_rate": 3.1073684210526315e-05,
      "loss": 2.4409,
      "step": 1590
    },
    {
      "epoch": 1.1228070175438596,
      "grad_norm": 4.604344844818115,
      "learning_rate": 3.101754385964913e-05,
      "loss": 2.4666,
      "step": 1600
    },
    {
      "epoch": 1.1298245614035087,
      "grad_norm": 4.570755481719971,
      "learning_rate": 3.096140350877193e-05,
      "loss": 2.5074,
      "step": 1610
    },
    {
      "epoch": 1.1368421052631579,
      "grad_norm": 4.245136260986328,
      "learning_rate": 3.090526315789474e-05,
      "loss": 2.4041,
      "step": 1620
    },
    {
      "epoch": 1.143859649122807,
      "grad_norm": 6.885979652404785,
      "learning_rate": 3.084912280701755e-05,
      "loss": 2.273,
      "step": 1630
    },
    {
      "epoch": 1.1508771929824562,
      "grad_norm": 4.872701644897461,
      "learning_rate": 3.079298245614035e-05,
      "loss": 2.288,
      "step": 1640
    },
    {
      "epoch": 1.1578947368421053,
      "grad_norm": 5.850711822509766,
      "learning_rate": 3.073684210526316e-05,
      "loss": 2.3211,
      "step": 1650
    },
    {
      "epoch": 1.1649122807017545,
      "grad_norm": 6.322419166564941,
      "learning_rate": 3.068070175438597e-05,
      "loss": 2.4346,
      "step": 1660
    },
    {
      "epoch": 1.1719298245614036,
      "grad_norm": 4.245922565460205,
      "learning_rate": 3.0624561403508774e-05,
      "loss": 2.4625,
      "step": 1670
    },
    {
      "epoch": 1.1789473684210527,
      "grad_norm": 6.492407321929932,
      "learning_rate": 3.056842105263158e-05,
      "loss": 2.3055,
      "step": 1680
    },
    {
      "epoch": 1.1859649122807017,
      "grad_norm": 3.70711088180542,
      "learning_rate": 3.0512280701754387e-05,
      "loss": 2.3194,
      "step": 1690
    },
    {
      "epoch": 1.1929824561403508,
      "grad_norm": 4.266017436981201,
      "learning_rate": 3.0456140350877195e-05,
      "loss": 2.3328,
      "step": 1700
    },
    {
      "epoch": 1.2,
      "grad_norm": 5.330011367797852,
      "learning_rate": 3.0400000000000004e-05,
      "loss": 2.2194,
      "step": 1710
    },
    {
      "epoch": 1.207017543859649,
      "grad_norm": 4.36406135559082,
      "learning_rate": 3.034385964912281e-05,
      "loss": 2.3422,
      "step": 1720
    },
    {
      "epoch": 1.2140350877192982,
      "grad_norm": 4.72511100769043,
      "learning_rate": 3.0287719298245613e-05,
      "loss": 2.3306,
      "step": 1730
    },
    {
      "epoch": 1.2210526315789474,
      "grad_norm": 6.6082444190979,
      "learning_rate": 3.0231578947368425e-05,
      "loss": 2.3352,
      "step": 1740
    },
    {
      "epoch": 1.2280701754385965,
      "grad_norm": 5.410521984100342,
      "learning_rate": 3.017543859649123e-05,
      "loss": 2.333,
      "step": 1750
    },
    {
      "epoch": 1.2350877192982457,
      "grad_norm": 5.625265121459961,
      "learning_rate": 3.0119298245614038e-05,
      "loss": 2.2418,
      "step": 1760
    },
    {
      "epoch": 1.2421052631578948,
      "grad_norm": 6.6500349044799805,
      "learning_rate": 3.0063157894736843e-05,
      "loss": 2.3265,
      "step": 1770
    },
    {
      "epoch": 1.2491228070175437,
      "grad_norm": 5.18841552734375,
      "learning_rate": 3.0007017543859654e-05,
      "loss": 2.3631,
      "step": 1780
    },
    {
      "epoch": 1.256140350877193,
      "grad_norm": 6.000190258026123,
      "learning_rate": 2.995087719298246e-05,
      "loss": 2.4296,
      "step": 1790
    },
    {
      "epoch": 1.263157894736842,
      "grad_norm": 5.714511871337891,
      "learning_rate": 2.9894736842105264e-05,
      "loss": 2.3687,
      "step": 1800
    },
    {
      "epoch": 1.2701754385964912,
      "grad_norm": 5.697600841522217,
      "learning_rate": 2.9838596491228072e-05,
      "loss": 2.296,
      "step": 1810
    },
    {
      "epoch": 1.2771929824561403,
      "grad_norm": 4.978451251983643,
      "learning_rate": 2.978245614035088e-05,
      "loss": 2.2945,
      "step": 1820
    },
    {
      "epoch": 1.2842105263157895,
      "grad_norm": 7.050954341888428,
      "learning_rate": 2.972631578947369e-05,
      "loss": 2.3049,
      "step": 1830
    },
    {
      "epoch": 1.2912280701754386,
      "grad_norm": 6.645350933074951,
      "learning_rate": 2.9670175438596493e-05,
      "loss": 2.2493,
      "step": 1840
    },
    {
      "epoch": 1.2982456140350878,
      "grad_norm": 5.105212688446045,
      "learning_rate": 2.9614035087719298e-05,
      "loss": 2.2929,
      "step": 1850
    },
    {
      "epoch": 1.305263157894737,
      "grad_norm": 4.831392765045166,
      "learning_rate": 2.955789473684211e-05,
      "loss": 2.3432,
      "step": 1860
    },
    {
      "epoch": 1.312280701754386,
      "grad_norm": 6.868584632873535,
      "learning_rate": 2.9501754385964915e-05,
      "loss": 2.3975,
      "step": 1870
    },
    {
      "epoch": 1.3192982456140352,
      "grad_norm": 6.108323097229004,
      "learning_rate": 2.9445614035087723e-05,
      "loss": 2.2867,
      "step": 1880
    },
    {
      "epoch": 1.3263157894736843,
      "grad_norm": 6.238369464874268,
      "learning_rate": 2.9389473684210528e-05,
      "loss": 2.4191,
      "step": 1890
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 5.028003215789795,
      "learning_rate": 2.9333333333333333e-05,
      "loss": 2.3321,
      "step": 1900
    },
    {
      "epoch": 1.3403508771929824,
      "grad_norm": 4.82411003112793,
      "learning_rate": 2.9277192982456144e-05,
      "loss": 2.3233,
      "step": 1910
    },
    {
      "epoch": 1.3473684210526315,
      "grad_norm": 6.009828090667725,
      "learning_rate": 2.922105263157895e-05,
      "loss": 2.2195,
      "step": 1920
    },
    {
      "epoch": 1.3543859649122807,
      "grad_norm": 4.929795265197754,
      "learning_rate": 2.9164912280701757e-05,
      "loss": 2.2792,
      "step": 1930
    },
    {
      "epoch": 1.3614035087719298,
      "grad_norm": 4.8765549659729,
      "learning_rate": 2.9108771929824562e-05,
      "loss": 2.3024,
      "step": 1940
    },
    {
      "epoch": 1.368421052631579,
      "grad_norm": 5.817769527435303,
      "learning_rate": 2.9052631578947374e-05,
      "loss": 2.273,
      "step": 1950
    },
    {
      "epoch": 1.3754385964912281,
      "grad_norm": 5.3321380615234375,
      "learning_rate": 2.899649122807018e-05,
      "loss": 2.1755,
      "step": 1960
    },
    {
      "epoch": 1.3824561403508773,
      "grad_norm": 6.554256439208984,
      "learning_rate": 2.8940350877192983e-05,
      "loss": 2.4053,
      "step": 1970
    },
    {
      "epoch": 1.3894736842105262,
      "grad_norm": 7.97580099105835,
      "learning_rate": 2.888421052631579e-05,
      "loss": 2.3809,
      "step": 1980
    },
    {
      "epoch": 1.3964912280701753,
      "grad_norm": 7.029486179351807,
      "learning_rate": 2.88280701754386e-05,
      "loss": 2.3751,
      "step": 1990
    },
    {
      "epoch": 1.4035087719298245,
      "grad_norm": 5.708216667175293,
      "learning_rate": 2.8771929824561408e-05,
      "loss": 2.3141,
      "step": 2000
    },
    {
      "epoch": 1.4105263157894736,
      "grad_norm": 6.331243991851807,
      "learning_rate": 2.8715789473684213e-05,
      "loss": 2.293,
      "step": 2010
    },
    {
      "epoch": 1.4175438596491228,
      "grad_norm": 6.089460372924805,
      "learning_rate": 2.8659649122807018e-05,
      "loss": 2.3051,
      "step": 2020
    },
    {
      "epoch": 1.424561403508772,
      "grad_norm": 5.31583309173584,
      "learning_rate": 2.860350877192983e-05,
      "loss": 2.343,
      "step": 2030
    },
    {
      "epoch": 1.431578947368421,
      "grad_norm": 6.687865257263184,
      "learning_rate": 2.8547368421052634e-05,
      "loss": 2.3286,
      "step": 2040
    },
    {
      "epoch": 1.4385964912280702,
      "grad_norm": 6.770966529846191,
      "learning_rate": 2.8491228070175442e-05,
      "loss": 2.263,
      "step": 2050
    },
    {
      "epoch": 1.4456140350877194,
      "grad_norm": 5.422611713409424,
      "learning_rate": 2.8435087719298247e-05,
      "loss": 2.2283,
      "step": 2060
    },
    {
      "epoch": 1.4526315789473685,
      "grad_norm": 7.409144401550293,
      "learning_rate": 2.837894736842106e-05,
      "loss": 2.218,
      "step": 2070
    },
    {
      "epoch": 1.4596491228070176,
      "grad_norm": 7.448114395141602,
      "learning_rate": 2.8322807017543863e-05,
      "loss": 2.3449,
      "step": 2080
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 5.215080738067627,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 2.2612,
      "step": 2090
    },
    {
      "epoch": 1.4736842105263157,
      "grad_norm": 6.8806257247924805,
      "learning_rate": 2.8210526315789476e-05,
      "loss": 2.271,
      "step": 2100
    },
    {
      "epoch": 1.4807017543859649,
      "grad_norm": 5.618961334228516,
      "learning_rate": 2.815438596491228e-05,
      "loss": 2.2147,
      "step": 2110
    },
    {
      "epoch": 1.487719298245614,
      "grad_norm": 5.533901214599609,
      "learning_rate": 2.8098245614035093e-05,
      "loss": 2.4186,
      "step": 2120
    },
    {
      "epoch": 1.4947368421052631,
      "grad_norm": 6.76760196685791,
      "learning_rate": 2.8042105263157898e-05,
      "loss": 2.3097,
      "step": 2130
    },
    {
      "epoch": 1.5017543859649123,
      "grad_norm": 7.384055137634277,
      "learning_rate": 2.7985964912280702e-05,
      "loss": 2.1522,
      "step": 2140
    },
    {
      "epoch": 1.5087719298245614,
      "grad_norm": 6.516120433807373,
      "learning_rate": 2.792982456140351e-05,
      "loss": 2.2627,
      "step": 2150
    },
    {
      "epoch": 1.5157894736842106,
      "grad_norm": 7.412403583526611,
      "learning_rate": 2.787368421052632e-05,
      "loss": 2.201,
      "step": 2160
    },
    {
      "epoch": 1.5228070175438595,
      "grad_norm": 7.785566329956055,
      "learning_rate": 2.7817543859649124e-05,
      "loss": 2.2248,
      "step": 2170
    },
    {
      "epoch": 1.5298245614035086,
      "grad_norm": 6.905685901641846,
      "learning_rate": 2.7761403508771932e-05,
      "loss": 2.2137,
      "step": 2180
    },
    {
      "epoch": 1.5368421052631578,
      "grad_norm": 5.93710470199585,
      "learning_rate": 2.7705263157894737e-05,
      "loss": 2.3342,
      "step": 2190
    },
    {
      "epoch": 1.543859649122807,
      "grad_norm": 7.470151901245117,
      "learning_rate": 2.764912280701755e-05,
      "loss": 2.2466,
      "step": 2200
    },
    {
      "epoch": 1.550877192982456,
      "grad_norm": 5.307878494262695,
      "learning_rate": 2.7592982456140353e-05,
      "loss": 2.2459,
      "step": 2210
    },
    {
      "epoch": 1.5578947368421052,
      "grad_norm": 9.417170524597168,
      "learning_rate": 2.7536842105263158e-05,
      "loss": 2.2396,
      "step": 2220
    },
    {
      "epoch": 1.5649122807017544,
      "grad_norm": 5.283676624298096,
      "learning_rate": 2.7480701754385966e-05,
      "loss": 2.1899,
      "step": 2230
    },
    {
      "epoch": 1.5719298245614035,
      "grad_norm": 7.939864158630371,
      "learning_rate": 2.7424561403508774e-05,
      "loss": 2.1592,
      "step": 2240
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 8.377013206481934,
      "learning_rate": 2.7368421052631583e-05,
      "loss": 2.2353,
      "step": 2250
    },
    {
      "epoch": 1.5859649122807018,
      "grad_norm": 4.5263214111328125,
      "learning_rate": 2.7312280701754387e-05,
      "loss": 2.1759,
      "step": 2260
    },
    {
      "epoch": 1.592982456140351,
      "grad_norm": 8.07304859161377,
      "learning_rate": 2.7256140350877192e-05,
      "loss": 2.1598,
      "step": 2270
    },
    {
      "epoch": 1.6,
      "grad_norm": 5.810146808624268,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 2.1041,
      "step": 2280
    },
    {
      "epoch": 1.6070175438596492,
      "grad_norm": 7.78114652633667,
      "learning_rate": 2.714385964912281e-05,
      "loss": 2.274,
      "step": 2290
    },
    {
      "epoch": 1.6140350877192984,
      "grad_norm": 9.419349670410156,
      "learning_rate": 2.7087719298245617e-05,
      "loss": 2.2715,
      "step": 2300
    },
    {
      "epoch": 1.6210526315789475,
      "grad_norm": 7.486435413360596,
      "learning_rate": 2.7031578947368422e-05,
      "loss": 2.2216,
      "step": 2310
    },
    {
      "epoch": 1.6280701754385964,
      "grad_norm": 6.941974639892578,
      "learning_rate": 2.6975438596491233e-05,
      "loss": 2.127,
      "step": 2320
    },
    {
      "epoch": 1.6350877192982456,
      "grad_norm": 7.547976016998291,
      "learning_rate": 2.6919298245614038e-05,
      "loss": 2.1629,
      "step": 2330
    },
    {
      "epoch": 1.6421052631578947,
      "grad_norm": 6.792576789855957,
      "learning_rate": 2.6863157894736843e-05,
      "loss": 2.1126,
      "step": 2340
    },
    {
      "epoch": 1.6491228070175439,
      "grad_norm": 7.17717981338501,
      "learning_rate": 2.680701754385965e-05,
      "loss": 2.1915,
      "step": 2350
    },
    {
      "epoch": 1.656140350877193,
      "grad_norm": 9.284516334533691,
      "learning_rate": 2.6750877192982456e-05,
      "loss": 2.1265,
      "step": 2360
    },
    {
      "epoch": 1.663157894736842,
      "grad_norm": 6.731569290161133,
      "learning_rate": 2.6694736842105268e-05,
      "loss": 2.1695,
      "step": 2370
    },
    {
      "epoch": 1.670175438596491,
      "grad_norm": 7.223387718200684,
      "learning_rate": 2.6638596491228072e-05,
      "loss": 2.1087,
      "step": 2380
    },
    {
      "epoch": 1.6771929824561402,
      "grad_norm": 10.516717910766602,
      "learning_rate": 2.6582456140350877e-05,
      "loss": 2.2434,
      "step": 2390
    },
    {
      "epoch": 1.6842105263157894,
      "grad_norm": 8.88536262512207,
      "learning_rate": 2.6526315789473685e-05,
      "loss": 2.1783,
      "step": 2400
    },
    {
      "epoch": 1.6912280701754385,
      "grad_norm": 5.981401443481445,
      "learning_rate": 2.6470175438596494e-05,
      "loss": 2.303,
      "step": 2410
    },
    {
      "epoch": 1.6982456140350877,
      "grad_norm": 7.468846321105957,
      "learning_rate": 2.6414035087719302e-05,
      "loss": 2.3237,
      "step": 2420
    },
    {
      "epoch": 1.7052631578947368,
      "grad_norm": 7.450871467590332,
      "learning_rate": 2.6357894736842107e-05,
      "loss": 2.2302,
      "step": 2430
    },
    {
      "epoch": 1.712280701754386,
      "grad_norm": 7.649069786071777,
      "learning_rate": 2.630175438596491e-05,
      "loss": 2.1434,
      "step": 2440
    },
    {
      "epoch": 1.719298245614035,
      "grad_norm": 5.361613750457764,
      "learning_rate": 2.6245614035087723e-05,
      "loss": 2.2304,
      "step": 2450
    },
    {
      "epoch": 1.7263157894736842,
      "grad_norm": 9.959081649780273,
      "learning_rate": 2.6189473684210528e-05,
      "loss": 2.259,
      "step": 2460
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 6.624756813049316,
      "learning_rate": 2.6133333333333336e-05,
      "loss": 2.1732,
      "step": 2470
    },
    {
      "epoch": 1.7403508771929825,
      "grad_norm": 6.469422817230225,
      "learning_rate": 2.607719298245614e-05,
      "loss": 2.2444,
      "step": 2480
    },
    {
      "epoch": 1.7473684210526317,
      "grad_norm": 5.8155622482299805,
      "learning_rate": 2.6021052631578953e-05,
      "loss": 2.3413,
      "step": 2490
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 6.275063514709473,
      "learning_rate": 2.5964912280701757e-05,
      "loss": 2.2765,
      "step": 2500
    },
    {
      "epoch": 1.76140350877193,
      "grad_norm": 5.989220142364502,
      "learning_rate": 2.5908771929824562e-05,
      "loss": 2.0881,
      "step": 2510
    },
    {
      "epoch": 1.768421052631579,
      "grad_norm": 10.014102935791016,
      "learning_rate": 2.585263157894737e-05,
      "loss": 2.1615,
      "step": 2520
    },
    {
      "epoch": 1.775438596491228,
      "grad_norm": 8.324996948242188,
      "learning_rate": 2.579649122807018e-05,
      "loss": 2.1137,
      "step": 2530
    },
    {
      "epoch": 1.7824561403508772,
      "grad_norm": 6.744744777679443,
      "learning_rate": 2.5740350877192987e-05,
      "loss": 2.2451,
      "step": 2540
    },
    {
      "epoch": 1.7894736842105263,
      "grad_norm": 8.71381950378418,
      "learning_rate": 2.568421052631579e-05,
      "loss": 2.1092,
      "step": 2550
    },
    {
      "epoch": 1.7964912280701755,
      "grad_norm": 8.320711135864258,
      "learning_rate": 2.5628070175438596e-05,
      "loss": 2.1396,
      "step": 2560
    },
    {
      "epoch": 1.8035087719298246,
      "grad_norm": 8.399272918701172,
      "learning_rate": 2.5571929824561405e-05,
      "loss": 2.3282,
      "step": 2570
    },
    {
      "epoch": 1.8105263157894735,
      "grad_norm": 6.804418563842773,
      "learning_rate": 2.5515789473684213e-05,
      "loss": 2.2749,
      "step": 2580
    },
    {
      "epoch": 1.8175438596491227,
      "grad_norm": 7.321113109588623,
      "learning_rate": 2.545964912280702e-05,
      "loss": 2.1532,
      "step": 2590
    },
    {
      "epoch": 1.8245614035087718,
      "grad_norm": 7.992355823516846,
      "learning_rate": 2.5403508771929826e-05,
      "loss": 2.1099,
      "step": 2600
    },
    {
      "epoch": 1.831578947368421,
      "grad_norm": 9.716031074523926,
      "learning_rate": 2.534736842105263e-05,
      "loss": 2.1025,
      "step": 2610
    },
    {
      "epoch": 1.8385964912280701,
      "grad_norm": 8.386366844177246,
      "learning_rate": 2.5291228070175442e-05,
      "loss": 2.1148,
      "step": 2620
    },
    {
      "epoch": 1.8456140350877193,
      "grad_norm": 5.373336315155029,
      "learning_rate": 2.5235087719298247e-05,
      "loss": 2.2584,
      "step": 2630
    },
    {
      "epoch": 1.8526315789473684,
      "grad_norm": 5.363504409790039,
      "learning_rate": 2.5178947368421055e-05,
      "loss": 2.1805,
      "step": 2640
    },
    {
      "epoch": 1.8596491228070176,
      "grad_norm": 6.231789588928223,
      "learning_rate": 2.512280701754386e-05,
      "loss": 2.1948,
      "step": 2650
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 7.278851509094238,
      "learning_rate": 2.5066666666666672e-05,
      "loss": 2.2637,
      "step": 2660
    },
    {
      "epoch": 1.8736842105263158,
      "grad_norm": 6.644517421722412,
      "learning_rate": 2.5010526315789477e-05,
      "loss": 2.1575,
      "step": 2670
    },
    {
      "epoch": 1.880701754385965,
      "grad_norm": 7.254465579986572,
      "learning_rate": 2.495438596491228e-05,
      "loss": 2.1597,
      "step": 2680
    },
    {
      "epoch": 1.8877192982456141,
      "grad_norm": 6.8525071144104,
      "learning_rate": 2.489824561403509e-05,
      "loss": 2.3623,
      "step": 2690
    },
    {
      "epoch": 1.8947368421052633,
      "grad_norm": 7.920596599578857,
      "learning_rate": 2.4842105263157898e-05,
      "loss": 2.1031,
      "step": 2700
    },
    {
      "epoch": 1.9017543859649124,
      "grad_norm": 5.9179534912109375,
      "learning_rate": 2.4785964912280706e-05,
      "loss": 2.107,
      "step": 2710
    },
    {
      "epoch": 1.9087719298245616,
      "grad_norm": 6.949718952178955,
      "learning_rate": 2.472982456140351e-05,
      "loss": 2.0985,
      "step": 2720
    },
    {
      "epoch": 1.9157894736842105,
      "grad_norm": 11.526226997375488,
      "learning_rate": 2.4673684210526316e-05,
      "loss": 2.1367,
      "step": 2730
    },
    {
      "epoch": 1.9228070175438596,
      "grad_norm": 6.466208457946777,
      "learning_rate": 2.4617543859649127e-05,
      "loss": 2.2185,
      "step": 2740
    },
    {
      "epoch": 1.9298245614035088,
      "grad_norm": 6.723085403442383,
      "learning_rate": 2.4561403508771932e-05,
      "loss": 2.0791,
      "step": 2750
    },
    {
      "epoch": 1.936842105263158,
      "grad_norm": 7.635095119476318,
      "learning_rate": 2.450526315789474e-05,
      "loss": 2.1277,
      "step": 2760
    },
    {
      "epoch": 1.943859649122807,
      "grad_norm": 10.347837448120117,
      "learning_rate": 2.4449122807017545e-05,
      "loss": 2.1581,
      "step": 2770
    },
    {
      "epoch": 1.950877192982456,
      "grad_norm": 7.853734493255615,
      "learning_rate": 2.4392982456140353e-05,
      "loss": 2.0903,
      "step": 2780
    },
    {
      "epoch": 1.9578947368421051,
      "grad_norm": 7.0681023597717285,
      "learning_rate": 2.433684210526316e-05,
      "loss": 2.0885,
      "step": 2790
    },
    {
      "epoch": 1.9649122807017543,
      "grad_norm": 7.3175129890441895,
      "learning_rate": 2.4280701754385966e-05,
      "loss": 1.9894,
      "step": 2800
    },
    {
      "epoch": 1.9719298245614034,
      "grad_norm": 10.827235221862793,
      "learning_rate": 2.422456140350877e-05,
      "loss": 2.1543,
      "step": 2810
    },
    {
      "epoch": 1.9789473684210526,
      "grad_norm": 7.789448261260986,
      "learning_rate": 2.416842105263158e-05,
      "loss": 2.1105,
      "step": 2820
    },
    {
      "epoch": 1.9859649122807017,
      "grad_norm": 10.489773750305176,
      "learning_rate": 2.4112280701754388e-05,
      "loss": 1.994,
      "step": 2830
    },
    {
      "epoch": 1.9929824561403509,
      "grad_norm": 6.103429317474365,
      "learning_rate": 2.4056140350877196e-05,
      "loss": 2.1759,
      "step": 2840
    },
    {
      "epoch": 2.0,
      "grad_norm": 11.882257461547852,
      "learning_rate": 2.4e-05,
      "loss": 1.9987,
      "step": 2850
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.36676318329384927,
      "eval_f1_C01": 0.18114143920595532,
      "eval_f1_C02": 0.0,
      "eval_f1_C03": 0.0,
      "eval_f1_C04": 0.5784424379232506,
      "eval_f1_C05": 0.055401662049861494,
      "eval_f1_C06": 0.15782664941785252,
      "eval_f1_C07": 0.0,
      "eval_f1_C08": 0.3,
      "eval_f1_C09": 0.0,
      "eval_f1_C10": 0.4507042253521127,
      "eval_f1_C11": 0.029556650246305417,
      "eval_f1_C12": 0.39433962264150946,
      "eval_f1_C13": 0.03954802259887006,
      "eval_f1_C14": 0.6390134529147982,
      "eval_f1_C15": 0.0,
      "eval_f1_C16": 0.0,
      "eval_f1_C17": 0.19172932330827067,
      "eval_f1_C18": 0.4607594936708861,
      "eval_f1_C19": 0.011299435028248588,
      "eval_f1_C20": 0.3453815261044177,
      "eval_f1_C21": 0.4392041267501842,
      "eval_f1_C22": 0.0,
      "eval_f1_C23": 0.24013157894736842,
      "eval_f1_macro": 0.19628172374608224,
      "eval_loss": 2.0952224731445312,
      "eval_precision_C01": 0.24496644295302014,
      "eval_precision_C02": 1.0,
      "eval_precision_C03": 1.0,
      "eval_precision_C04": 0.4499561018437226,
      "eval_precision_C05": 0.4,
      "eval_precision_C06": 0.3485714285714286,
      "eval_precision_C07": 1.0,
      "eval_precision_C08": 0.40728476821192056,
      "eval_precision_C09": 1.0,
      "eval_precision_C10": 0.41113490364025695,
      "eval_precision_C11": 1.0,
      "eval_precision_C12": 0.37589928057553956,
      "eval_precision_C13": 0.2413793103448276,
      "eval_precision_C14": 0.5872252747252747,
      "eval_precision_C15": 1.0,
      "eval_precision_C16": 1.0,
      "eval_precision_C17": 0.24401913875598086,
      "eval_precision_C18": 0.4482758620689655,
      "eval_precision_C19": 0.25,
      "eval_precision_C20": 0.2391841779975278,
      "eval_precision_C21": 0.38701298701298703,
      "eval_precision_C22": 1.0,
      "eval_precision_C23": 0.21893744644387317,
      "eval_precision_global": 0.5762542227454489,
      "eval_recall_C01": 0.1437007874015748,
      "eval_recall_C02": 0.0,
      "eval_recall_C03": 0.0,
      "eval_recall_C04": 0.8096366508688784,
      "eval_recall_C05": 0.02976190476190476,
      "eval_recall_C06": 0.1020066889632107,
      "eval_recall_C07": 0.0,
      "eval_recall_C08": 0.23745173745173745,
      "eval_recall_C09": 0.0,
      "eval_recall_C10": 0.4987012987012987,
      "eval_recall_C11": 0.015,
      "eval_recall_C12": 0.4146825396825397,
      "eval_recall_C13": 0.021538461538461538,
      "eval_recall_C14": 0.7008196721311475,
      "eval_recall_C15": 0.0,
      "eval_recall_C16": 0.0,
      "eval_recall_C17": 0.15789473684210525,
      "eval_recall_C18": 0.4739583333333333,
      "eval_recall_C19": 0.005780346820809248,
      "eval_recall_C20": 0.6211878009630819,
      "eval_recall_C21": 0.5076660988074957,
      "eval_recall_C22": 0.0,
      "eval_recall_C23": 0.26586888657648283,
      "eval_recall_global": 0.21763721499322006,
      "eval_runtime": 154.0138,
      "eval_samples_per_second": 74.0,
      "eval_steps_per_second": 2.318,
      "step": 2850
    },
    {
      "epoch": 2.007017543859649,
      "grad_norm": 8.545990943908691,
      "learning_rate": 2.3943859649122805e-05,
      "loss": 2.101,
      "step": 2860
    },
    {
      "epoch": 2.0140350877192983,
      "grad_norm": 7.497531414031982,
      "learning_rate": 2.3887719298245617e-05,
      "loss": 2.0688,
      "step": 2870
    },
    {
      "epoch": 2.0210526315789474,
      "grad_norm": 10.107085227966309,
      "learning_rate": 2.3831578947368422e-05,
      "loss": 1.9869,
      "step": 2880
    },
    {
      "epoch": 2.0280701754385966,
      "grad_norm": 9.492416381835938,
      "learning_rate": 2.377543859649123e-05,
      "loss": 2.0964,
      "step": 2890
    },
    {
      "epoch": 2.0350877192982457,
      "grad_norm": 7.154890060424805,
      "learning_rate": 2.3719298245614035e-05,
      "loss": 2.0107,
      "step": 2900
    },
    {
      "epoch": 2.042105263157895,
      "grad_norm": 7.82717227935791,
      "learning_rate": 2.3663157894736846e-05,
      "loss": 2.1025,
      "step": 2910
    },
    {
      "epoch": 2.049122807017544,
      "grad_norm": 6.893733978271484,
      "learning_rate": 2.360701754385965e-05,
      "loss": 2.1577,
      "step": 2920
    },
    {
      "epoch": 2.056140350877193,
      "grad_norm": 8.539582252502441,
      "learning_rate": 2.3550877192982456e-05,
      "loss": 2.0478,
      "step": 2930
    },
    {
      "epoch": 2.0631578947368423,
      "grad_norm": 7.2356109619140625,
      "learning_rate": 2.3494736842105264e-05,
      "loss": 1.9722,
      "step": 2940
    },
    {
      "epoch": 2.0701754385964914,
      "grad_norm": 7.735520362854004,
      "learning_rate": 2.3438596491228073e-05,
      "loss": 2.0906,
      "step": 2950
    },
    {
      "epoch": 2.07719298245614,
      "grad_norm": 8.961602210998535,
      "learning_rate": 2.338245614035088e-05,
      "loss": 2.0822,
      "step": 2960
    },
    {
      "epoch": 2.0842105263157893,
      "grad_norm": 10.693706512451172,
      "learning_rate": 2.3326315789473686e-05,
      "loss": 2.0579,
      "step": 2970
    },
    {
      "epoch": 2.0912280701754384,
      "grad_norm": 6.3085527420043945,
      "learning_rate": 2.327017543859649e-05,
      "loss": 2.102,
      "step": 2980
    },
    {
      "epoch": 2.0982456140350876,
      "grad_norm": 7.0131683349609375,
      "learning_rate": 2.3214035087719302e-05,
      "loss": 2.116,
      "step": 2990
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 6.976935863494873,
      "learning_rate": 2.3157894736842107e-05,
      "loss": 2.1499,
      "step": 3000
    },
    {
      "epoch": 2.112280701754386,
      "grad_norm": 6.745612621307373,
      "learning_rate": 2.3101754385964915e-05,
      "loss": 2.0677,
      "step": 3010
    },
    {
      "epoch": 2.119298245614035,
      "grad_norm": 10.991101264953613,
      "learning_rate": 2.304561403508772e-05,
      "loss": 2.1431,
      "step": 3020
    },
    {
      "epoch": 2.126315789473684,
      "grad_norm": 9.920827865600586,
      "learning_rate": 2.298947368421053e-05,
      "loss": 2.002,
      "step": 3030
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 10.044236183166504,
      "learning_rate": 2.2933333333333336e-05,
      "loss": 2.1239,
      "step": 3040
    },
    {
      "epoch": 2.1403508771929824,
      "grad_norm": 8.602239608764648,
      "learning_rate": 2.287719298245614e-05,
      "loss": 2.1359,
      "step": 3050
    },
    {
      "epoch": 2.1473684210526316,
      "grad_norm": 6.345386981964111,
      "learning_rate": 2.282105263157895e-05,
      "loss": 2.0666,
      "step": 3060
    },
    {
      "epoch": 2.1543859649122807,
      "grad_norm": 8.21120834350586,
      "learning_rate": 2.2764912280701754e-05,
      "loss": 2.0918,
      "step": 3070
    },
    {
      "epoch": 2.16140350877193,
      "grad_norm": 15.525452613830566,
      "learning_rate": 2.2708771929824566e-05,
      "loss": 2.0015,
      "step": 3080
    },
    {
      "epoch": 2.168421052631579,
      "grad_norm": 9.083897590637207,
      "learning_rate": 2.265263157894737e-05,
      "loss": 2.0308,
      "step": 3090
    },
    {
      "epoch": 2.175438596491228,
      "grad_norm": 9.457547187805176,
      "learning_rate": 2.2596491228070175e-05,
      "loss": 2.1052,
      "step": 3100
    },
    {
      "epoch": 2.1824561403508773,
      "grad_norm": 9.654973983764648,
      "learning_rate": 2.2540350877192984e-05,
      "loss": 2.177,
      "step": 3110
    },
    {
      "epoch": 2.1894736842105265,
      "grad_norm": 8.098217010498047,
      "learning_rate": 2.2484210526315792e-05,
      "loss": 2.0273,
      "step": 3120
    },
    {
      "epoch": 2.1964912280701756,
      "grad_norm": 8.032642364501953,
      "learning_rate": 2.24280701754386e-05,
      "loss": 2.0346,
      "step": 3130
    },
    {
      "epoch": 2.2035087719298247,
      "grad_norm": 7.172603607177734,
      "learning_rate": 2.2371929824561405e-05,
      "loss": 1.9972,
      "step": 3140
    },
    {
      "epoch": 2.2105263157894735,
      "grad_norm": 9.289922714233398,
      "learning_rate": 2.231578947368421e-05,
      "loss": 2.1358,
      "step": 3150
    },
    {
      "epoch": 2.2175438596491226,
      "grad_norm": 10.893730163574219,
      "learning_rate": 2.225964912280702e-05,
      "loss": 2.2187,
      "step": 3160
    },
    {
      "epoch": 2.2245614035087717,
      "grad_norm": 7.0014190673828125,
      "learning_rate": 2.2203508771929826e-05,
      "loss": 2.1428,
      "step": 3170
    },
    {
      "epoch": 2.231578947368421,
      "grad_norm": 7.4132585525512695,
      "learning_rate": 2.2147368421052634e-05,
      "loss": 2.1355,
      "step": 3180
    },
    {
      "epoch": 2.23859649122807,
      "grad_norm": 7.355268478393555,
      "learning_rate": 2.209122807017544e-05,
      "loss": 1.9506,
      "step": 3190
    },
    {
      "epoch": 2.245614035087719,
      "grad_norm": 9.291889190673828,
      "learning_rate": 2.203508771929825e-05,
      "loss": 2.1112,
      "step": 3200
    },
    {
      "epoch": 2.2526315789473683,
      "grad_norm": 6.180786609649658,
      "learning_rate": 2.1978947368421055e-05,
      "loss": 2.0415,
      "step": 3210
    },
    {
      "epoch": 2.2596491228070175,
      "grad_norm": 7.772360324859619,
      "learning_rate": 2.192280701754386e-05,
      "loss": 2.0848,
      "step": 3220
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 8.023002624511719,
      "learning_rate": 2.186666666666667e-05,
      "loss": 2.1733,
      "step": 3230
    },
    {
      "epoch": 2.2736842105263158,
      "grad_norm": 8.284566879272461,
      "learning_rate": 2.1810526315789477e-05,
      "loss": 1.9093,
      "step": 3240
    },
    {
      "epoch": 2.280701754385965,
      "grad_norm": 7.630727767944336,
      "learning_rate": 2.1754385964912285e-05,
      "loss": 2.1268,
      "step": 3250
    },
    {
      "epoch": 2.287719298245614,
      "grad_norm": 8.776656150817871,
      "learning_rate": 2.169824561403509e-05,
      "loss": 2.0937,
      "step": 3260
    },
    {
      "epoch": 2.294736842105263,
      "grad_norm": 6.968307018280029,
      "learning_rate": 2.1642105263157895e-05,
      "loss": 1.9224,
      "step": 3270
    },
    {
      "epoch": 2.3017543859649123,
      "grad_norm": 6.85884428024292,
      "learning_rate": 2.1585964912280703e-05,
      "loss": 1.9567,
      "step": 3280
    },
    {
      "epoch": 2.3087719298245615,
      "grad_norm": 10.546195983886719,
      "learning_rate": 2.152982456140351e-05,
      "loss": 1.997,
      "step": 3290
    },
    {
      "epoch": 2.3157894736842106,
      "grad_norm": 10.441716194152832,
      "learning_rate": 2.147368421052632e-05,
      "loss": 2.0616,
      "step": 3300
    },
    {
      "epoch": 2.3228070175438598,
      "grad_norm": 8.349130630493164,
      "learning_rate": 2.1417543859649124e-05,
      "loss": 2.0118,
      "step": 3310
    },
    {
      "epoch": 2.329824561403509,
      "grad_norm": 10.658695220947266,
      "learning_rate": 2.136140350877193e-05,
      "loss": 1.9196,
      "step": 3320
    },
    {
      "epoch": 2.336842105263158,
      "grad_norm": 8.296221733093262,
      "learning_rate": 2.130526315789474e-05,
      "loss": 1.8999,
      "step": 3330
    },
    {
      "epoch": 2.343859649122807,
      "grad_norm": 8.696432113647461,
      "learning_rate": 2.1249122807017545e-05,
      "loss": 2.1261,
      "step": 3340
    },
    {
      "epoch": 2.3508771929824563,
      "grad_norm": 10.596046447753906,
      "learning_rate": 2.1192982456140353e-05,
      "loss": 1.9717,
      "step": 3350
    },
    {
      "epoch": 2.3578947368421055,
      "grad_norm": 7.162372589111328,
      "learning_rate": 2.1136842105263158e-05,
      "loss": 1.9031,
      "step": 3360
    },
    {
      "epoch": 2.3649122807017546,
      "grad_norm": 8.457854270935059,
      "learning_rate": 2.108070175438597e-05,
      "loss": 1.9969,
      "step": 3370
    },
    {
      "epoch": 2.3719298245614033,
      "grad_norm": 11.251320838928223,
      "learning_rate": 2.1024561403508775e-05,
      "loss": 1.8646,
      "step": 3380
    },
    {
      "epoch": 2.3789473684210525,
      "grad_norm": 8.521843910217285,
      "learning_rate": 2.096842105263158e-05,
      "loss": 2.1237,
      "step": 3390
    },
    {
      "epoch": 2.3859649122807016,
      "grad_norm": 10.008540153503418,
      "learning_rate": 2.0912280701754388e-05,
      "loss": 2.0068,
      "step": 3400
    },
    {
      "epoch": 2.3929824561403508,
      "grad_norm": 8.301568031311035,
      "learning_rate": 2.0856140350877196e-05,
      "loss": 2.0947,
      "step": 3410
    },
    {
      "epoch": 2.4,
      "grad_norm": 6.8441925048828125,
      "learning_rate": 2.08e-05,
      "loss": 1.9671,
      "step": 3420
    },
    {
      "epoch": 2.407017543859649,
      "grad_norm": 8.167037010192871,
      "learning_rate": 2.074385964912281e-05,
      "loss": 1.9712,
      "step": 3430
    },
    {
      "epoch": 2.414035087719298,
      "grad_norm": 8.827249526977539,
      "learning_rate": 2.0687719298245614e-05,
      "loss": 1.9573,
      "step": 3440
    },
    {
      "epoch": 2.4210526315789473,
      "grad_norm": 10.489113807678223,
      "learning_rate": 2.0631578947368425e-05,
      "loss": 2.0298,
      "step": 3450
    },
    {
      "epoch": 2.4280701754385965,
      "grad_norm": 8.791000366210938,
      "learning_rate": 2.057543859649123e-05,
      "loss": 2.1956,
      "step": 3460
    },
    {
      "epoch": 2.4350877192982456,
      "grad_norm": 7.304502964019775,
      "learning_rate": 2.0519298245614035e-05,
      "loss": 1.9839,
      "step": 3470
    },
    {
      "epoch": 2.442105263157895,
      "grad_norm": 12.081367492675781,
      "learning_rate": 2.0463157894736843e-05,
      "loss": 1.99,
      "step": 3480
    },
    {
      "epoch": 2.449122807017544,
      "grad_norm": 8.352821350097656,
      "learning_rate": 2.040701754385965e-05,
      "loss": 2.0666,
      "step": 3490
    },
    {
      "epoch": 2.456140350877193,
      "grad_norm": 8.60183334350586,
      "learning_rate": 2.035087719298246e-05,
      "loss": 1.9482,
      "step": 3500
    },
    {
      "epoch": 2.463157894736842,
      "grad_norm": 10.659974098205566,
      "learning_rate": 2.0294736842105264e-05,
      "loss": 2.0436,
      "step": 3510
    },
    {
      "epoch": 2.4701754385964914,
      "grad_norm": 14.77488899230957,
      "learning_rate": 2.023859649122807e-05,
      "loss": 1.9719,
      "step": 3520
    },
    {
      "epoch": 2.4771929824561405,
      "grad_norm": 8.843817710876465,
      "learning_rate": 2.0182456140350877e-05,
      "loss": 1.9725,
      "step": 3530
    },
    {
      "epoch": 2.4842105263157896,
      "grad_norm": 7.6101555824279785,
      "learning_rate": 2.0126315789473686e-05,
      "loss": 2.0315,
      "step": 3540
    },
    {
      "epoch": 2.4912280701754383,
      "grad_norm": 9.226347923278809,
      "learning_rate": 2.0070175438596494e-05,
      "loss": 1.8509,
      "step": 3550
    },
    {
      "epoch": 2.4982456140350875,
      "grad_norm": 8.00627613067627,
      "learning_rate": 2.00140350877193e-05,
      "loss": 2.041,
      "step": 3560
    },
    {
      "epoch": 2.5052631578947366,
      "grad_norm": 7.257517337799072,
      "learning_rate": 1.9957894736842107e-05,
      "loss": 1.9929,
      "step": 3570
    },
    {
      "epoch": 2.512280701754386,
      "grad_norm": 7.545470237731934,
      "learning_rate": 1.9901754385964912e-05,
      "loss": 2.0945,
      "step": 3580
    },
    {
      "epoch": 2.519298245614035,
      "grad_norm": 7.345672607421875,
      "learning_rate": 1.984561403508772e-05,
      "loss": 2.0079,
      "step": 3590
    },
    {
      "epoch": 2.526315789473684,
      "grad_norm": 9.176690101623535,
      "learning_rate": 1.9789473684210528e-05,
      "loss": 1.9265,
      "step": 3600
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 8.384129524230957,
      "learning_rate": 1.9733333333333336e-05,
      "loss": 2.0538,
      "step": 3610
    },
    {
      "epoch": 2.5403508771929824,
      "grad_norm": 9.24203109741211,
      "learning_rate": 1.967719298245614e-05,
      "loss": 1.9741,
      "step": 3620
    },
    {
      "epoch": 2.5473684210526315,
      "grad_norm": 8.0255126953125,
      "learning_rate": 1.962105263157895e-05,
      "loss": 1.8311,
      "step": 3630
    },
    {
      "epoch": 2.5543859649122806,
      "grad_norm": 9.408360481262207,
      "learning_rate": 1.9564912280701754e-05,
      "loss": 1.9511,
      "step": 3640
    },
    {
      "epoch": 2.56140350877193,
      "grad_norm": 8.40324592590332,
      "learning_rate": 1.9508771929824562e-05,
      "loss": 1.9881,
      "step": 3650
    },
    {
      "epoch": 2.568421052631579,
      "grad_norm": 8.179137229919434,
      "learning_rate": 1.945263157894737e-05,
      "loss": 1.9334,
      "step": 3660
    },
    {
      "epoch": 2.575438596491228,
      "grad_norm": 8.511422157287598,
      "learning_rate": 1.939649122807018e-05,
      "loss": 1.9909,
      "step": 3670
    },
    {
      "epoch": 2.5824561403508772,
      "grad_norm": 8.08769416809082,
      "learning_rate": 1.9340350877192984e-05,
      "loss": 2.0176,
      "step": 3680
    },
    {
      "epoch": 2.5894736842105264,
      "grad_norm": 8.352048873901367,
      "learning_rate": 1.9284210526315792e-05,
      "loss": 1.8631,
      "step": 3690
    },
    {
      "epoch": 2.5964912280701755,
      "grad_norm": 10.13821792602539,
      "learning_rate": 1.9228070175438597e-05,
      "loss": 1.9405,
      "step": 3700
    },
    {
      "epoch": 2.6035087719298247,
      "grad_norm": 11.692384719848633,
      "learning_rate": 1.9171929824561405e-05,
      "loss": 1.9951,
      "step": 3710
    },
    {
      "epoch": 2.610526315789474,
      "grad_norm": 7.927577495574951,
      "learning_rate": 1.9115789473684213e-05,
      "loss": 1.8876,
      "step": 3720
    },
    {
      "epoch": 2.617543859649123,
      "grad_norm": 11.96503734588623,
      "learning_rate": 1.905964912280702e-05,
      "loss": 2.0414,
      "step": 3730
    },
    {
      "epoch": 2.624561403508772,
      "grad_norm": 7.8500213623046875,
      "learning_rate": 1.9003508771929826e-05,
      "loss": 2.1818,
      "step": 3740
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 9.51324462890625,
      "learning_rate": 1.894736842105263e-05,
      "loss": 1.9864,
      "step": 3750
    },
    {
      "epoch": 2.6385964912280704,
      "grad_norm": 10.208233833312988,
      "learning_rate": 1.889122807017544e-05,
      "loss": 1.9596,
      "step": 3760
    },
    {
      "epoch": 2.6456140350877195,
      "grad_norm": 8.144102096557617,
      "learning_rate": 1.8835087719298247e-05,
      "loss": 1.9779,
      "step": 3770
    },
    {
      "epoch": 2.6526315789473687,
      "grad_norm": 9.496050834655762,
      "learning_rate": 1.8778947368421056e-05,
      "loss": 2.0078,
      "step": 3780
    },
    {
      "epoch": 2.659649122807018,
      "grad_norm": 9.403242111206055,
      "learning_rate": 1.872280701754386e-05,
      "loss": 1.9785,
      "step": 3790
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 7.977657318115234,
      "learning_rate": 1.866666666666667e-05,
      "loss": 2.1354,
      "step": 3800
    },
    {
      "epoch": 2.6736842105263157,
      "grad_norm": 9.641973495483398,
      "learning_rate": 1.8610526315789473e-05,
      "loss": 1.9905,
      "step": 3810
    },
    {
      "epoch": 2.680701754385965,
      "grad_norm": 8.072540283203125,
      "learning_rate": 1.855438596491228e-05,
      "loss": 2.0323,
      "step": 3820
    },
    {
      "epoch": 2.687719298245614,
      "grad_norm": 11.361764907836914,
      "learning_rate": 1.849824561403509e-05,
      "loss": 2.0461,
      "step": 3830
    },
    {
      "epoch": 2.694736842105263,
      "grad_norm": 8.083582878112793,
      "learning_rate": 1.8442105263157898e-05,
      "loss": 1.9703,
      "step": 3840
    },
    {
      "epoch": 2.7017543859649122,
      "grad_norm": 9.794818878173828,
      "learning_rate": 1.8385964912280703e-05,
      "loss": 2.1386,
      "step": 3850
    },
    {
      "epoch": 2.7087719298245614,
      "grad_norm": 9.846572875976562,
      "learning_rate": 1.832982456140351e-05,
      "loss": 2.0312,
      "step": 3860
    },
    {
      "epoch": 2.7157894736842105,
      "grad_norm": 7.843800067901611,
      "learning_rate": 1.8273684210526316e-05,
      "loss": 2.0459,
      "step": 3870
    },
    {
      "epoch": 2.7228070175438597,
      "grad_norm": 8.485706329345703,
      "learning_rate": 1.8217543859649124e-05,
      "loss": 1.9752,
      "step": 3880
    },
    {
      "epoch": 2.729824561403509,
      "grad_norm": 12.822267532348633,
      "learning_rate": 1.8161403508771932e-05,
      "loss": 1.9941,
      "step": 3890
    },
    {
      "epoch": 2.736842105263158,
      "grad_norm": 8.181888580322266,
      "learning_rate": 1.810526315789474e-05,
      "loss": 1.8976,
      "step": 3900
    },
    {
      "epoch": 2.743859649122807,
      "grad_norm": 8.212350845336914,
      "learning_rate": 1.8049122807017545e-05,
      "loss": 2.0789,
      "step": 3910
    },
    {
      "epoch": 2.7508771929824563,
      "grad_norm": 10.157790184020996,
      "learning_rate": 1.7992982456140354e-05,
      "loss": 1.9324,
      "step": 3920
    },
    {
      "epoch": 2.7578947368421054,
      "grad_norm": 7.5741729736328125,
      "learning_rate": 1.793684210526316e-05,
      "loss": 1.9344,
      "step": 3930
    },
    {
      "epoch": 2.7649122807017545,
      "grad_norm": 13.05418586730957,
      "learning_rate": 1.7880701754385967e-05,
      "loss": 1.9255,
      "step": 3940
    },
    {
      "epoch": 2.7719298245614032,
      "grad_norm": 8.422226905822754,
      "learning_rate": 1.7824561403508775e-05,
      "loss": 2.0375,
      "step": 3950
    },
    {
      "epoch": 2.7789473684210524,
      "grad_norm": 11.68148136138916,
      "learning_rate": 1.7768421052631583e-05,
      "loss": 2.1169,
      "step": 3960
    },
    {
      "epoch": 2.7859649122807015,
      "grad_norm": 11.120832443237305,
      "learning_rate": 1.7712280701754388e-05,
      "loss": 1.9189,
      "step": 3970
    },
    {
      "epoch": 2.7929824561403507,
      "grad_norm": 7.895505428314209,
      "learning_rate": 1.7656140350877193e-05,
      "loss": 2.0476,
      "step": 3980
    },
    {
      "epoch": 2.8,
      "grad_norm": 7.843966007232666,
      "learning_rate": 1.76e-05,
      "loss": 1.985,
      "step": 3990
    },
    {
      "epoch": 2.807017543859649,
      "grad_norm": 7.885977268218994,
      "learning_rate": 1.754385964912281e-05,
      "loss": 2.037,
      "step": 4000
    },
    {
      "epoch": 2.814035087719298,
      "grad_norm": 7.49483585357666,
      "learning_rate": 1.7487719298245617e-05,
      "loss": 1.9313,
      "step": 4010
    },
    {
      "epoch": 2.8210526315789473,
      "grad_norm": 7.663474082946777,
      "learning_rate": 1.7431578947368422e-05,
      "loss": 1.9961,
      "step": 4020
    },
    {
      "epoch": 2.8280701754385964,
      "grad_norm": 9.274680137634277,
      "learning_rate": 1.737543859649123e-05,
      "loss": 2.0729,
      "step": 4030
    },
    {
      "epoch": 2.8350877192982455,
      "grad_norm": 11.069994926452637,
      "learning_rate": 1.7319298245614035e-05,
      "loss": 2.002,
      "step": 4040
    },
    {
      "epoch": 2.8421052631578947,
      "grad_norm": 8.281255722045898,
      "learning_rate": 1.7263157894736843e-05,
      "loss": 2.1011,
      "step": 4050
    },
    {
      "epoch": 2.849122807017544,
      "grad_norm": 13.435393333435059,
      "learning_rate": 1.7207017543859648e-05,
      "loss": 2.012,
      "step": 4060
    },
    {
      "epoch": 2.856140350877193,
      "grad_norm": 12.189566612243652,
      "learning_rate": 1.7150877192982456e-05,
      "loss": 1.9925,
      "step": 4070
    },
    {
      "epoch": 2.863157894736842,
      "grad_norm": 7.140296936035156,
      "learning_rate": 1.7094736842105265e-05,
      "loss": 2.021,
      "step": 4080
    },
    {
      "epoch": 2.8701754385964913,
      "grad_norm": 11.070499420166016,
      "learning_rate": 1.7038596491228073e-05,
      "loss": 1.9937,
      "step": 4090
    },
    {
      "epoch": 2.8771929824561404,
      "grad_norm": 13.220436096191406,
      "learning_rate": 1.6982456140350878e-05,
      "loss": 2.0041,
      "step": 4100
    },
    {
      "epoch": 2.8842105263157896,
      "grad_norm": 9.712313652038574,
      "learning_rate": 1.6926315789473686e-05,
      "loss": 2.0058,
      "step": 4110
    },
    {
      "epoch": 2.8912280701754387,
      "grad_norm": 12.886127471923828,
      "learning_rate": 1.687017543859649e-05,
      "loss": 1.9767,
      "step": 4120
    },
    {
      "epoch": 2.898245614035088,
      "grad_norm": 9.557117462158203,
      "learning_rate": 1.68140350877193e-05,
      "loss": 1.9132,
      "step": 4130
    },
    {
      "epoch": 2.905263157894737,
      "grad_norm": 9.688295364379883,
      "learning_rate": 1.6757894736842107e-05,
      "loss": 1.9415,
      "step": 4140
    },
    {
      "epoch": 2.912280701754386,
      "grad_norm": 9.742439270019531,
      "learning_rate": 1.6701754385964915e-05,
      "loss": 1.9638,
      "step": 4150
    },
    {
      "epoch": 2.9192982456140353,
      "grad_norm": 8.879212379455566,
      "learning_rate": 1.664561403508772e-05,
      "loss": 2.053,
      "step": 4160
    },
    {
      "epoch": 2.9263157894736844,
      "grad_norm": 10.639249801635742,
      "learning_rate": 1.658947368421053e-05,
      "loss": 2.0153,
      "step": 4170
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 10.851990699768066,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 1.9753,
      "step": 4180
    },
    {
      "epoch": 2.9403508771929827,
      "grad_norm": 7.957789421081543,
      "learning_rate": 1.647719298245614e-05,
      "loss": 1.9645,
      "step": 4190
    },
    {
      "epoch": 2.9473684210526314,
      "grad_norm": 8.198902130126953,
      "learning_rate": 1.642105263157895e-05,
      "loss": 1.9598,
      "step": 4200
    },
    {
      "epoch": 2.9543859649122806,
      "grad_norm": 9.536026954650879,
      "learning_rate": 1.6364912280701754e-05,
      "loss": 1.9975,
      "step": 4210
    },
    {
      "epoch": 2.9614035087719297,
      "grad_norm": 11.901841163635254,
      "learning_rate": 1.6308771929824563e-05,
      "loss": 1.9632,
      "step": 4220
    },
    {
      "epoch": 2.968421052631579,
      "grad_norm": 6.978350639343262,
      "learning_rate": 1.6252631578947367e-05,
      "loss": 1.918,
      "step": 4230
    },
    {
      "epoch": 2.975438596491228,
      "grad_norm": 12.328312873840332,
      "learning_rate": 1.6196491228070176e-05,
      "loss": 2.0213,
      "step": 4240
    },
    {
      "epoch": 2.982456140350877,
      "grad_norm": 12.514579772949219,
      "learning_rate": 1.6140350877192984e-05,
      "loss": 2.1178,
      "step": 4250
    },
    {
      "epoch": 2.9894736842105263,
      "grad_norm": 7.922096252441406,
      "learning_rate": 1.6084210526315792e-05,
      "loss": 2.0793,
      "step": 4260
    },
    {
      "epoch": 2.9964912280701754,
      "grad_norm": 8.187885284423828,
      "learning_rate": 1.6028070175438597e-05,
      "loss": 1.9598,
      "step": 4270
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.4108976046327981,
      "eval_f1_C01": 0.3601108033240997,
      "eval_f1_C02": 0.0,
      "eval_f1_C03": 0.0,
      "eval_f1_C04": 0.5865921787709497,
      "eval_f1_C05": 0.26804123711340205,
      "eval_f1_C06": 0.4429065743944637,
      "eval_f1_C07": 0.0,
      "eval_f1_C08": 0.3779816513761468,
      "eval_f1_C09": 0.0,
      "eval_f1_C10": 0.46792913866829566,
      "eval_f1_C11": 0.45776566757493187,
      "eval_f1_C12": 0.470873786407767,
      "eval_f1_C13": 0.33568406205923834,
      "eval_f1_C14": 0.6330988522769345,
      "eval_f1_C15": 0.0,
      "eval_f1_C16": 0.0,
      "eval_f1_C17": 0.38386648122392214,
      "eval_f1_C18": 0.46936416184971097,
      "eval_f1_C19": 0.011428571428571429,
      "eval_f1_C20": 0.41946697566628044,
      "eval_f1_C21": 0.4951644100580271,
      "eval_f1_C22": 0.0,
      "eval_f1_C23": 0.1612797910545217,
      "eval_f1_macro": 0.27571975405422877,
      "eval_loss": 1.9467121362686157,
      "eval_precision_C01": 0.3391304347826087,
      "eval_precision_C02": 1.0,
      "eval_precision_C03": 1.0,
      "eval_precision_C04": 0.4831288343558282,
      "eval_precision_C05": 0.3170731707317073,
      "eval_precision_C06": 0.3778040141676505,
      "eval_precision_C07": 1.0,
      "eval_precision_C08": 0.36013986013986016,
      "eval_precision_C09": 1.0,
      "eval_precision_C10": 0.4417531718569781,
      "eval_precision_C11": 0.5029940119760479,
      "eval_precision_C12": 0.3975409836065574,
      "eval_precision_C13": 0.3098958333333333,
      "eval_precision_C14": 0.5773126266036462,
      "eval_precision_C15": 1.0,
      "eval_precision_C16": 1.0,
      "eval_precision_C17": 0.3484848484848485,
      "eval_precision_C18": 0.42203742203742206,
      "eval_precision_C19": 0.5,
      "eval_precision_C20": 0.32819582955575705,
      "eval_precision_C21": 0.5727069351230425,
      "eval_precision_C22": 1.0,
      "eval_precision_C23": 0.21647677475898336,
      "eval_precision_global": 0.5867249891962727,
      "eval_recall_C01": 0.3838582677165354,
      "eval_recall_C02": 0.0,
      "eval_recall_C03": 0.0,
      "eval_recall_C04": 0.7464454976303317,
      "eval_recall_C05": 0.23214285714285715,
      "eval_recall_C06": 0.5351170568561873,
      "eval_recall_C07": 0.0,
      "eval_recall_C08": 0.39768339768339767,
      "eval_recall_C09": 0.0,
      "eval_recall_C10": 0.4974025974025974,
      "eval_recall_C11": 0.42,
      "eval_recall_C12": 0.5773809523809523,
      "eval_recall_C13": 0.36615384615384616,
      "eval_recall_C14": 0.7008196721311475,
      "eval_recall_C15": 0.0,
      "eval_recall_C16": 0.0,
      "eval_recall_C17": 0.42724458204334365,
      "eval_recall_C18": 0.5286458333333334,
      "eval_recall_C19": 0.005780346820809248,
      "eval_recall_C20": 0.5810593900481541,
      "eval_recall_C21": 0.43611584327086883,
      "eval_recall_C22": 0.0,
      "eval_recall_C23": 0.12851196670135276,
      "eval_recall_global": 0.3027983524919875,
      "eval_runtime": 153.5599,
      "eval_samples_per_second": 74.219,
      "eval_steps_per_second": 2.325,
      "step": 4275
    },
    {
      "epoch": 3.0035087719298246,
      "grad_norm": 9.240141868591309,
      "learning_rate": 1.5971929824561405e-05,
      "loss": 1.9976,
      "step": 4280
    },
    {
      "epoch": 3.0105263157894737,
      "grad_norm": 8.8314208984375,
      "learning_rate": 1.591578947368421e-05,
      "loss": 1.8462,
      "step": 4290
    },
    {
      "epoch": 3.017543859649123,
      "grad_norm": 10.388683319091797,
      "learning_rate": 1.5859649122807018e-05,
      "loss": 1.9307,
      "step": 4300
    },
    {
      "epoch": 3.024561403508772,
      "grad_norm": 8.504325866699219,
      "learning_rate": 1.5803508771929826e-05,
      "loss": 1.9758,
      "step": 4310
    },
    {
      "epoch": 3.031578947368421,
      "grad_norm": 11.418204307556152,
      "learning_rate": 1.5747368421052635e-05,
      "loss": 1.9217,
      "step": 4320
    },
    {
      "epoch": 3.0385964912280703,
      "grad_norm": 7.990997791290283,
      "learning_rate": 1.569122807017544e-05,
      "loss": 2.0564,
      "step": 4330
    },
    {
      "epoch": 3.0456140350877194,
      "grad_norm": 10.048404693603516,
      "learning_rate": 1.5635087719298248e-05,
      "loss": 1.9182,
      "step": 4340
    },
    {
      "epoch": 3.0526315789473686,
      "grad_norm": 9.680136680603027,
      "learning_rate": 1.5578947368421052e-05,
      "loss": 1.7831,
      "step": 4350
    },
    {
      "epoch": 3.0596491228070177,
      "grad_norm": 8.032546997070312,
      "learning_rate": 1.552280701754386e-05,
      "loss": 1.9685,
      "step": 4360
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 12.204691886901855,
      "learning_rate": 1.546666666666667e-05,
      "loss": 1.9399,
      "step": 4370
    },
    {
      "epoch": 3.0736842105263156,
      "grad_norm": 10.6093111038208,
      "learning_rate": 1.5410526315789477e-05,
      "loss": 1.9913,
      "step": 4380
    },
    {
      "epoch": 3.0807017543859647,
      "grad_norm": 7.629257678985596,
      "learning_rate": 1.5354385964912282e-05,
      "loss": 1.9165,
      "step": 4390
    },
    {
      "epoch": 3.087719298245614,
      "grad_norm": 8.966997146606445,
      "learning_rate": 1.529824561403509e-05,
      "loss": 2.0068,
      "step": 4400
    },
    {
      "epoch": 3.094736842105263,
      "grad_norm": 9.06633186340332,
      "learning_rate": 1.5242105263157897e-05,
      "loss": 1.8441,
      "step": 4410
    },
    {
      "epoch": 3.101754385964912,
      "grad_norm": 13.313360214233398,
      "learning_rate": 1.5185964912280705e-05,
      "loss": 1.9284,
      "step": 4420
    },
    {
      "epoch": 3.1087719298245613,
      "grad_norm": 11.867471694946289,
      "learning_rate": 1.512982456140351e-05,
      "loss": 1.9942,
      "step": 4430
    },
    {
      "epoch": 3.1157894736842104,
      "grad_norm": 9.028772354125977,
      "learning_rate": 1.5073684210526316e-05,
      "loss": 1.9599,
      "step": 4440
    },
    {
      "epoch": 3.1228070175438596,
      "grad_norm": 10.318090438842773,
      "learning_rate": 1.5017543859649124e-05,
      "loss": 1.9965,
      "step": 4450
    },
    {
      "epoch": 3.1298245614035087,
      "grad_norm": 10.545577049255371,
      "learning_rate": 1.496140350877193e-05,
      "loss": 1.9531,
      "step": 4460
    },
    {
      "epoch": 3.136842105263158,
      "grad_norm": 8.911795616149902,
      "learning_rate": 1.4905263157894739e-05,
      "loss": 2.0085,
      "step": 4470
    },
    {
      "epoch": 3.143859649122807,
      "grad_norm": 10.931745529174805,
      "learning_rate": 1.4849122807017544e-05,
      "loss": 1.8604,
      "step": 4480
    },
    {
      "epoch": 3.150877192982456,
      "grad_norm": 10.652191162109375,
      "learning_rate": 1.4792982456140352e-05,
      "loss": 1.9551,
      "step": 4490
    },
    {
      "epoch": 3.1578947368421053,
      "grad_norm": 9.80560302734375,
      "learning_rate": 1.4736842105263159e-05,
      "loss": 1.8561,
      "step": 4500
    },
    {
      "epoch": 3.1649122807017545,
      "grad_norm": 13.810879707336426,
      "learning_rate": 1.4680701754385967e-05,
      "loss": 1.9056,
      "step": 4510
    },
    {
      "epoch": 3.1719298245614036,
      "grad_norm": 7.759746074676514,
      "learning_rate": 1.4624561403508772e-05,
      "loss": 1.8527,
      "step": 4520
    },
    {
      "epoch": 3.1789473684210527,
      "grad_norm": 8.022253036499023,
      "learning_rate": 1.456842105263158e-05,
      "loss": 1.9618,
      "step": 4530
    },
    {
      "epoch": 3.185964912280702,
      "grad_norm": 12.390351295471191,
      "learning_rate": 1.4512280701754386e-05,
      "loss": 1.9079,
      "step": 4540
    },
    {
      "epoch": 3.192982456140351,
      "grad_norm": 9.854644775390625,
      "learning_rate": 1.4456140350877195e-05,
      "loss": 1.8443,
      "step": 4550
    },
    {
      "epoch": 3.2,
      "grad_norm": 11.769840240478516,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 1.9902,
      "step": 4560
    },
    {
      "epoch": 3.2070175438596493,
      "grad_norm": 14.286839485168457,
      "learning_rate": 1.434385964912281e-05,
      "loss": 2.0073,
      "step": 4570
    },
    {
      "epoch": 3.2140350877192985,
      "grad_norm": 14.655596733093262,
      "learning_rate": 1.4287719298245614e-05,
      "loss": 2.001,
      "step": 4580
    },
    {
      "epoch": 3.221052631578947,
      "grad_norm": 9.180536270141602,
      "learning_rate": 1.4231578947368422e-05,
      "loss": 1.8806,
      "step": 4590
    },
    {
      "epoch": 3.2280701754385963,
      "grad_norm": 9.934532165527344,
      "learning_rate": 1.4175438596491229e-05,
      "loss": 2.0268,
      "step": 4600
    },
    {
      "epoch": 3.2350877192982455,
      "grad_norm": 8.201224327087402,
      "learning_rate": 1.4119298245614037e-05,
      "loss": 1.9363,
      "step": 4610
    },
    {
      "epoch": 3.2421052631578946,
      "grad_norm": 10.338747024536133,
      "learning_rate": 1.4063157894736844e-05,
      "loss": 2.0053,
      "step": 4620
    },
    {
      "epoch": 3.2491228070175437,
      "grad_norm": 10.032586097717285,
      "learning_rate": 1.4007017543859652e-05,
      "loss": 1.8548,
      "step": 4630
    },
    {
      "epoch": 3.256140350877193,
      "grad_norm": 11.355742454528809,
      "learning_rate": 1.3950877192982457e-05,
      "loss": 1.9162,
      "step": 4640
    },
    {
      "epoch": 3.263157894736842,
      "grad_norm": 9.780509948730469,
      "learning_rate": 1.3894736842105265e-05,
      "loss": 1.8316,
      "step": 4650
    },
    {
      "epoch": 3.270175438596491,
      "grad_norm": 8.99147891998291,
      "learning_rate": 1.3838596491228071e-05,
      "loss": 1.8883,
      "step": 4660
    },
    {
      "epoch": 3.2771929824561403,
      "grad_norm": 11.485651016235352,
      "learning_rate": 1.378245614035088e-05,
      "loss": 2.0009,
      "step": 4670
    },
    {
      "epoch": 3.2842105263157895,
      "grad_norm": 9.629156112670898,
      "learning_rate": 1.3726315789473686e-05,
      "loss": 1.865,
      "step": 4680
    },
    {
      "epoch": 3.2912280701754386,
      "grad_norm": 8.62463092803955,
      "learning_rate": 1.367017543859649e-05,
      "loss": 1.9354,
      "step": 4690
    },
    {
      "epoch": 3.2982456140350878,
      "grad_norm": 11.384925842285156,
      "learning_rate": 1.3614035087719299e-05,
      "loss": 1.9753,
      "step": 4700
    },
    {
      "epoch": 3.305263157894737,
      "grad_norm": 11.448533058166504,
      "learning_rate": 1.3557894736842106e-05,
      "loss": 1.9304,
      "step": 4710
    },
    {
      "epoch": 3.312280701754386,
      "grad_norm": 9.669979095458984,
      "learning_rate": 1.3501754385964914e-05,
      "loss": 2.027,
      "step": 4720
    },
    {
      "epoch": 3.319298245614035,
      "grad_norm": 8.349492073059082,
      "learning_rate": 1.344561403508772e-05,
      "loss": 1.9383,
      "step": 4730
    },
    {
      "epoch": 3.3263157894736843,
      "grad_norm": 9.664323806762695,
      "learning_rate": 1.3389473684210528e-05,
      "loss": 1.9167,
      "step": 4740
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 12.569245338439941,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 1.9044,
      "step": 4750
    },
    {
      "epoch": 3.3403508771929826,
      "grad_norm": 8.372212409973145,
      "learning_rate": 1.3277192982456142e-05,
      "loss": 1.8495,
      "step": 4760
    },
    {
      "epoch": 3.3473684210526318,
      "grad_norm": 11.279500007629395,
      "learning_rate": 1.3221052631578948e-05,
      "loss": 2.0035,
      "step": 4770
    },
    {
      "epoch": 3.3543859649122805,
      "grad_norm": 11.619889259338379,
      "learning_rate": 1.3164912280701756e-05,
      "loss": 1.761,
      "step": 4780
    },
    {
      "epoch": 3.3614035087719296,
      "grad_norm": 9.369706153869629,
      "learning_rate": 1.3108771929824563e-05,
      "loss": 2.0992,
      "step": 4790
    },
    {
      "epoch": 3.3684210526315788,
      "grad_norm": 13.83840274810791,
      "learning_rate": 1.305263157894737e-05,
      "loss": 1.8723,
      "step": 4800
    },
    {
      "epoch": 3.375438596491228,
      "grad_norm": 9.62003231048584,
      "learning_rate": 1.2996491228070176e-05,
      "loss": 1.9541,
      "step": 4810
    },
    {
      "epoch": 3.382456140350877,
      "grad_norm": 11.141939163208008,
      "learning_rate": 1.2940350877192984e-05,
      "loss": 1.9586,
      "step": 4820
    },
    {
      "epoch": 3.389473684210526,
      "grad_norm": 9.325346946716309,
      "learning_rate": 1.288421052631579e-05,
      "loss": 2.1119,
      "step": 4830
    },
    {
      "epoch": 3.3964912280701753,
      "grad_norm": 11.692471504211426,
      "learning_rate": 1.2828070175438599e-05,
      "loss": 1.7794,
      "step": 4840
    },
    {
      "epoch": 3.4035087719298245,
      "grad_norm": 9.450589179992676,
      "learning_rate": 1.2771929824561404e-05,
      "loss": 1.9238,
      "step": 4850
    },
    {
      "epoch": 3.4105263157894736,
      "grad_norm": 11.390249252319336,
      "learning_rate": 1.2715789473684212e-05,
      "loss": 1.9044,
      "step": 4860
    },
    {
      "epoch": 3.4175438596491228,
      "grad_norm": 10.940054893493652,
      "learning_rate": 1.2659649122807018e-05,
      "loss": 1.9564,
      "step": 4870
    },
    {
      "epoch": 3.424561403508772,
      "grad_norm": 10.873330116271973,
      "learning_rate": 1.2603508771929826e-05,
      "loss": 1.9274,
      "step": 4880
    },
    {
      "epoch": 3.431578947368421,
      "grad_norm": 10.007798194885254,
      "learning_rate": 1.2547368421052633e-05,
      "loss": 2.0031,
      "step": 4890
    },
    {
      "epoch": 3.43859649122807,
      "grad_norm": 11.748281478881836,
      "learning_rate": 1.2491228070175441e-05,
      "loss": 1.8015,
      "step": 4900
    },
    {
      "epoch": 3.4456140350877194,
      "grad_norm": 12.582006454467773,
      "learning_rate": 1.2435087719298246e-05,
      "loss": 1.9248,
      "step": 4910
    },
    {
      "epoch": 3.4526315789473685,
      "grad_norm": 10.589475631713867,
      "learning_rate": 1.2378947368421053e-05,
      "loss": 1.8688,
      "step": 4920
    },
    {
      "epoch": 3.4596491228070176,
      "grad_norm": 13.7921724319458,
      "learning_rate": 1.232280701754386e-05,
      "loss": 1.9084,
      "step": 4930
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 8.050477027893066,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 1.9038,
      "step": 4940
    },
    {
      "epoch": 3.473684210526316,
      "grad_norm": 7.868845462799072,
      "learning_rate": 1.2210526315789475e-05,
      "loss": 1.836,
      "step": 4950
    },
    {
      "epoch": 3.480701754385965,
      "grad_norm": 11.49260425567627,
      "learning_rate": 1.215438596491228e-05,
      "loss": 1.8323,
      "step": 4960
    },
    {
      "epoch": 3.487719298245614,
      "grad_norm": 11.769508361816406,
      "learning_rate": 1.2098245614035088e-05,
      "loss": 1.9234,
      "step": 4970
    },
    {
      "epoch": 3.4947368421052634,
      "grad_norm": 11.222522735595703,
      "learning_rate": 1.2042105263157895e-05,
      "loss": 1.8592,
      "step": 4980
    },
    {
      "epoch": 3.5017543859649125,
      "grad_norm": 9.007338523864746,
      "learning_rate": 1.1985964912280703e-05,
      "loss": 1.9797,
      "step": 4990
    },
    {
      "epoch": 3.5087719298245617,
      "grad_norm": 10.07315731048584,
      "learning_rate": 1.192982456140351e-05,
      "loss": 1.9421,
      "step": 5000
    },
    {
      "epoch": 3.515789473684211,
      "grad_norm": 14.456786155700684,
      "learning_rate": 1.1873684210526318e-05,
      "loss": 1.8033,
      "step": 5010
    },
    {
      "epoch": 3.5228070175438595,
      "grad_norm": 10.468914031982422,
      "learning_rate": 1.1817543859649123e-05,
      "loss": 1.8632,
      "step": 5020
    },
    {
      "epoch": 3.5298245614035086,
      "grad_norm": 14.536645889282227,
      "learning_rate": 1.1761403508771931e-05,
      "loss": 1.8498,
      "step": 5030
    },
    {
      "epoch": 3.536842105263158,
      "grad_norm": 13.903800010681152,
      "learning_rate": 1.1705263157894737e-05,
      "loss": 1.9702,
      "step": 5040
    },
    {
      "epoch": 3.543859649122807,
      "grad_norm": 9.256387710571289,
      "learning_rate": 1.1649122807017546e-05,
      "loss": 1.927,
      "step": 5050
    },
    {
      "epoch": 3.550877192982456,
      "grad_norm": 10.271100044250488,
      "learning_rate": 1.1592982456140352e-05,
      "loss": 1.8603,
      "step": 5060
    },
    {
      "epoch": 3.557894736842105,
      "grad_norm": 8.543031692504883,
      "learning_rate": 1.153684210526316e-05,
      "loss": 1.9242,
      "step": 5070
    },
    {
      "epoch": 3.5649122807017544,
      "grad_norm": 12.925894737243652,
      "learning_rate": 1.1480701754385965e-05,
      "loss": 2.0324,
      "step": 5080
    },
    {
      "epoch": 3.5719298245614035,
      "grad_norm": 9.813592910766602,
      "learning_rate": 1.1424561403508773e-05,
      "loss": 1.8757,
      "step": 5090
    },
    {
      "epoch": 3.5789473684210527,
      "grad_norm": 13.191670417785645,
      "learning_rate": 1.136842105263158e-05,
      "loss": 1.9172,
      "step": 5100
    },
    {
      "epoch": 3.585964912280702,
      "grad_norm": 7.488553047180176,
      "learning_rate": 1.1312280701754388e-05,
      "loss": 1.813,
      "step": 5110
    },
    {
      "epoch": 3.592982456140351,
      "grad_norm": 9.835049629211426,
      "learning_rate": 1.1256140350877193e-05,
      "loss": 1.8633,
      "step": 5120
    },
    {
      "epoch": 3.6,
      "grad_norm": 11.052849769592285,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 1.8917,
      "step": 5130
    },
    {
      "epoch": 3.6070175438596492,
      "grad_norm": 8.069244384765625,
      "learning_rate": 1.1143859649122808e-05,
      "loss": 1.8655,
      "step": 5140
    },
    {
      "epoch": 3.6140350877192984,
      "grad_norm": 11.661484718322754,
      "learning_rate": 1.1087719298245614e-05,
      "loss": 1.943,
      "step": 5150
    },
    {
      "epoch": 3.6210526315789475,
      "grad_norm": 8.763596534729004,
      "learning_rate": 1.1031578947368422e-05,
      "loss": 1.899,
      "step": 5160
    },
    {
      "epoch": 3.6280701754385962,
      "grad_norm": 10.158817291259766,
      "learning_rate": 1.0975438596491227e-05,
      "loss": 1.8782,
      "step": 5170
    },
    {
      "epoch": 3.6350877192982454,
      "grad_norm": 10.7891845703125,
      "learning_rate": 1.0919298245614035e-05,
      "loss": 1.8828,
      "step": 5180
    },
    {
      "epoch": 3.6421052631578945,
      "grad_norm": 11.574134826660156,
      "learning_rate": 1.0863157894736842e-05,
      "loss": 1.9419,
      "step": 5190
    },
    {
      "epoch": 3.6491228070175437,
      "grad_norm": 12.590579986572266,
      "learning_rate": 1.080701754385965e-05,
      "loss": 1.8305,
      "step": 5200
    },
    {
      "epoch": 3.656140350877193,
      "grad_norm": 12.447957038879395,
      "learning_rate": 1.0750877192982457e-05,
      "loss": 1.964,
      "step": 5210
    },
    {
      "epoch": 3.663157894736842,
      "grad_norm": 10.617534637451172,
      "learning_rate": 1.0694736842105265e-05,
      "loss": 1.9113,
      "step": 5220
    },
    {
      "epoch": 3.670175438596491,
      "grad_norm": 10.151193618774414,
      "learning_rate": 1.063859649122807e-05,
      "loss": 1.8916,
      "step": 5230
    },
    {
      "epoch": 3.6771929824561402,
      "grad_norm": 14.251895904541016,
      "learning_rate": 1.0582456140350878e-05,
      "loss": 1.7669,
      "step": 5240
    },
    {
      "epoch": 3.6842105263157894,
      "grad_norm": 9.766584396362305,
      "learning_rate": 1.0526315789473684e-05,
      "loss": 1.817,
      "step": 5250
    },
    {
      "epoch": 3.6912280701754385,
      "grad_norm": 11.374027252197266,
      "learning_rate": 1.0470175438596493e-05,
      "loss": 1.9491,
      "step": 5260
    },
    {
      "epoch": 3.6982456140350877,
      "grad_norm": 11.81872844696045,
      "learning_rate": 1.04140350877193e-05,
      "loss": 1.9081,
      "step": 5270
    },
    {
      "epoch": 3.705263157894737,
      "grad_norm": 10.177250862121582,
      "learning_rate": 1.0357894736842107e-05,
      "loss": 1.7893,
      "step": 5280
    },
    {
      "epoch": 3.712280701754386,
      "grad_norm": 12.459339141845703,
      "learning_rate": 1.0301754385964912e-05,
      "loss": 1.9798,
      "step": 5290
    },
    {
      "epoch": 3.719298245614035,
      "grad_norm": 10.584752082824707,
      "learning_rate": 1.024561403508772e-05,
      "loss": 1.9291,
      "step": 5300
    },
    {
      "epoch": 3.7263157894736842,
      "grad_norm": 12.15002155303955,
      "learning_rate": 1.0189473684210527e-05,
      "loss": 1.9794,
      "step": 5310
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 9.204102516174316,
      "learning_rate": 1.0133333333333335e-05,
      "loss": 1.7644,
      "step": 5320
    },
    {
      "epoch": 3.7403508771929825,
      "grad_norm": 14.110267639160156,
      "learning_rate": 1.0077192982456142e-05,
      "loss": 1.8984,
      "step": 5330
    },
    {
      "epoch": 3.7473684210526317,
      "grad_norm": 16.902006149291992,
      "learning_rate": 1.002105263157895e-05,
      "loss": 1.9947,
      "step": 5340
    },
    {
      "epoch": 3.754385964912281,
      "grad_norm": 10.067474365234375,
      "learning_rate": 9.964912280701755e-06,
      "loss": 1.8357,
      "step": 5350
    },
    {
      "epoch": 3.76140350877193,
      "grad_norm": 8.179083824157715,
      "learning_rate": 9.908771929824563e-06,
      "loss": 1.8883,
      "step": 5360
    },
    {
      "epoch": 3.768421052631579,
      "grad_norm": 13.455382347106934,
      "learning_rate": 9.85263157894737e-06,
      "loss": 1.8889,
      "step": 5370
    },
    {
      "epoch": 3.7754385964912283,
      "grad_norm": 12.378520011901855,
      "learning_rate": 9.796491228070176e-06,
      "loss": 1.8002,
      "step": 5380
    },
    {
      "epoch": 3.7824561403508774,
      "grad_norm": 8.164970397949219,
      "learning_rate": 9.740350877192984e-06,
      "loss": 1.8821,
      "step": 5390
    },
    {
      "epoch": 3.7894736842105265,
      "grad_norm": 9.439321517944336,
      "learning_rate": 9.68421052631579e-06,
      "loss": 1.9837,
      "step": 5400
    },
    {
      "epoch": 3.7964912280701757,
      "grad_norm": 10.705720901489258,
      "learning_rate": 9.628070175438597e-06,
      "loss": 2.1019,
      "step": 5410
    },
    {
      "epoch": 3.803508771929825,
      "grad_norm": 9.209419250488281,
      "learning_rate": 9.571929824561404e-06,
      "loss": 1.9158,
      "step": 5420
    },
    {
      "epoch": 3.8105263157894735,
      "grad_norm": 9.350144386291504,
      "learning_rate": 9.515789473684212e-06,
      "loss": 1.8375,
      "step": 5430
    },
    {
      "epoch": 3.8175438596491227,
      "grad_norm": 10.328899383544922,
      "learning_rate": 9.459649122807018e-06,
      "loss": 1.7667,
      "step": 5440
    },
    {
      "epoch": 3.824561403508772,
      "grad_norm": 12.078008651733398,
      "learning_rate": 9.403508771929825e-06,
      "loss": 2.0195,
      "step": 5450
    },
    {
      "epoch": 3.831578947368421,
      "grad_norm": 9.44749641418457,
      "learning_rate": 9.347368421052633e-06,
      "loss": 1.8573,
      "step": 5460
    },
    {
      "epoch": 3.83859649122807,
      "grad_norm": 15.326362609863281,
      "learning_rate": 9.29122807017544e-06,
      "loss": 1.9415,
      "step": 5470
    },
    {
      "epoch": 3.8456140350877193,
      "grad_norm": 9.163156509399414,
      "learning_rate": 9.235087719298246e-06,
      "loss": 1.8377,
      "step": 5480
    },
    {
      "epoch": 3.8526315789473684,
      "grad_norm": 9.12943172454834,
      "learning_rate": 9.178947368421053e-06,
      "loss": 1.8366,
      "step": 5490
    },
    {
      "epoch": 3.8596491228070176,
      "grad_norm": 10.666245460510254,
      "learning_rate": 9.12280701754386e-06,
      "loss": 1.8457,
      "step": 5500
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 13.318765640258789,
      "learning_rate": 9.066666666666667e-06,
      "loss": 1.7961,
      "step": 5510
    },
    {
      "epoch": 3.873684210526316,
      "grad_norm": 10.204618453979492,
      "learning_rate": 9.010526315789474e-06,
      "loss": 1.9045,
      "step": 5520
    },
    {
      "epoch": 3.880701754385965,
      "grad_norm": 14.045491218566895,
      "learning_rate": 8.95438596491228e-06,
      "loss": 1.9012,
      "step": 5530
    },
    {
      "epoch": 3.887719298245614,
      "grad_norm": 14.135141372680664,
      "learning_rate": 8.898245614035089e-06,
      "loss": 1.8711,
      "step": 5540
    },
    {
      "epoch": 3.8947368421052633,
      "grad_norm": 10.459697723388672,
      "learning_rate": 8.842105263157895e-06,
      "loss": 1.8703,
      "step": 5550
    },
    {
      "epoch": 3.9017543859649124,
      "grad_norm": 13.042546272277832,
      "learning_rate": 8.785964912280702e-06,
      "loss": 1.9648,
      "step": 5560
    },
    {
      "epoch": 3.9087719298245616,
      "grad_norm": 9.386075019836426,
      "learning_rate": 8.72982456140351e-06,
      "loss": 1.9392,
      "step": 5570
    },
    {
      "epoch": 3.9157894736842103,
      "grad_norm": 10.600252151489258,
      "learning_rate": 8.673684210526316e-06,
      "loss": 1.8955,
      "step": 5580
    },
    {
      "epoch": 3.9228070175438594,
      "grad_norm": 10.810517311096191,
      "learning_rate": 8.617543859649123e-06,
      "loss": 1.7984,
      "step": 5590
    },
    {
      "epoch": 3.9298245614035086,
      "grad_norm": 9.92311954498291,
      "learning_rate": 8.561403508771931e-06,
      "loss": 1.8819,
      "step": 5600
    },
    {
      "epoch": 3.9368421052631577,
      "grad_norm": 11.781765937805176,
      "learning_rate": 8.505263157894738e-06,
      "loss": 1.9705,
      "step": 5610
    },
    {
      "epoch": 3.943859649122807,
      "grad_norm": 11.492618560791016,
      "learning_rate": 8.449122807017544e-06,
      "loss": 1.9217,
      "step": 5620
    },
    {
      "epoch": 3.950877192982456,
      "grad_norm": 10.490256309509277,
      "learning_rate": 8.392982456140352e-06,
      "loss": 1.9415,
      "step": 5630
    },
    {
      "epoch": 3.957894736842105,
      "grad_norm": 9.839261054992676,
      "learning_rate": 8.336842105263159e-06,
      "loss": 1.7813,
      "step": 5640
    },
    {
      "epoch": 3.9649122807017543,
      "grad_norm": 10.692439079284668,
      "learning_rate": 8.280701754385965e-06,
      "loss": 1.8579,
      "step": 5650
    },
    {
      "epoch": 3.9719298245614034,
      "grad_norm": 13.375019073486328,
      "learning_rate": 8.224561403508774e-06,
      "loss": 1.9153,
      "step": 5660
    },
    {
      "epoch": 3.9789473684210526,
      "grad_norm": 11.519383430480957,
      "learning_rate": 8.16842105263158e-06,
      "loss": 2.0241,
      "step": 5670
    },
    {
      "epoch": 3.9859649122807017,
      "grad_norm": 11.512309074401855,
      "learning_rate": 8.112280701754387e-06,
      "loss": 1.8933,
      "step": 5680
    },
    {
      "epoch": 3.992982456140351,
      "grad_norm": 12.843256950378418,
      "learning_rate": 8.056140350877195e-06,
      "loss": 1.9374,
      "step": 5690
    },
    {
      "epoch": 4.0,
      "grad_norm": 14.983956336975098,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.8141,
      "step": 5700
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.4256383258752303,
      "eval_f1_C01": 0.37520938023450584,
      "eval_f1_C02": 0.0,
      "eval_f1_C03": 0.0,
      "eval_f1_C04": 0.588462777956816,
      "eval_f1_C05": 0.3377777777777778,
      "eval_f1_C06": 0.4518464880521361,
      "eval_f1_C07": 0.0,
      "eval_f1_C08": 0.40379637618636754,
      "eval_f1_C09": 0.0,
      "eval_f1_C10": 0.46570605187319886,
      "eval_f1_C11": 0.46436781609195404,
      "eval_f1_C12": 0.5152263374485597,
      "eval_f1_C13": 0.4,
      "eval_f1_C14": 0.6393972012917115,
      "eval_f1_C15": 0.07913669064748201,
      "eval_f1_C16": 0.018018018018018018,
      "eval_f1_C17": 0.41359773371104813,
      "eval_f1_C18": 0.459546925566343,
      "eval_f1_C19": 0.011363636363636364,
      "eval_f1_C20": 0.45614035087719296,
      "eval_f1_C21": 0.5074106364428945,
      "eval_f1_C22": 0.0,
      "eval_f1_C23": 0.13213885778275475,
      "eval_f1_macro": 0.292136654622713,
      "eval_loss": 1.8836743831634521,
      "eval_precision_C01": 0.32653061224489793,
      "eval_precision_C02": 0.0,
      "eval_precision_C03": 1.0,
      "eval_precision_C04": 0.49700598802395207,
      "eval_precision_C05": 0.336283185840708,
      "eval_precision_C06": 0.39846743295019155,
      "eval_precision_C07": 1.0,
      "eval_precision_C08": 0.36505460218408736,
      "eval_precision_C09": 0.0,
      "eval_precision_C10": 0.41865284974093264,
      "eval_precision_C11": 0.4297872340425532,
      "eval_precision_C12": 0.44022503516174405,
      "eval_precision_C13": 0.3477272727272727,
      "eval_precision_C14": 0.568602425015954,
      "eval_precision_C15": 0.4782608695652174,
      "eval_precision_C16": 0.4,
      "eval_precision_C17": 0.381201044386423,
      "eval_precision_C18": 0.39226519337016574,
      "eval_precision_C19": 0.3333333333333333,
      "eval_precision_C20": 0.38318777292576417,
      "eval_precision_C21": 0.5196428571428572,
      "eval_precision_C22": 1.0,
      "eval_precision_C23": 0.23381770145310435,
      "eval_precision_global": 0.4456541482656156,
      "eval_recall_C01": 0.4409448818897638,
      "eval_recall_C02": 0.0,
      "eval_recall_C03": 0.0,
      "eval_recall_C04": 0.7211690363349131,
      "eval_recall_C05": 0.3392857142857143,
      "eval_recall_C06": 0.5217391304347826,
      "eval_recall_C07": 0.0,
      "eval_recall_C08": 0.4517374517374517,
      "eval_recall_C09": 0.0,
      "eval_recall_C10": 0.5246753246753246,
      "eval_recall_C11": 0.505,
      "eval_recall_C12": 0.621031746031746,
      "eval_recall_C13": 0.4707692307692308,
      "eval_recall_C14": 0.730327868852459,
      "eval_recall_C15": 0.043137254901960784,
      "eval_recall_C16": 0.009216589861751152,
      "eval_recall_C17": 0.4520123839009288,
      "eval_recall_C18": 0.5546875,
      "eval_recall_C19": 0.005780346820809248,
      "eval_recall_C20": 0.5634028892455859,
      "eval_recall_C21": 0.4957410562180579,
      "eval_recall_C22": 0.0,
      "eval_recall_C23": 0.09209157127991675,
      "eval_recall_global": 0.327945651184365,
      "eval_runtime": 154.0017,
      "eval_samples_per_second": 74.006,
      "eval_steps_per_second": 2.318,
      "step": 5700
    }
  ],
  "logging_steps": 10,
  "max_steps": 7125,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.874788047394406e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
